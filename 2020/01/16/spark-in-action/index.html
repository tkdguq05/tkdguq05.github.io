<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유 - Unreasonable Effectiveness</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Unreasonable Effectiveness"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Unreasonable Effectiveness"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Spark를 사용해서 데이터를 읽고 분석하자"><meta property="og:type" content="article"><meta property="og:title" content="Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유"><meta property="og:url" content="http://tkdguq05.github.io/2020/01/16/spark-in-action/"><meta property="og:site_name" content="Unreasonable Effectiveness"><meta property="og:description" content="Spark를 사용해서 데이터를 읽고 분석하자"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://tkdguq05.github.io/images/spark-logo-trademark.png"><meta property="article:published_time" content="2020-01-16T08:49:47.000Z"><meta property="article:modified_time" content="2020-02-24T10:20:12.000Z"><meta property="article:author" content="SangHyub Lee, Jose"><meta property="article:tag" content="spark"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://tkdguq05.github.io/images/spark-logo-trademark.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://tkdguq05.github.io/2020/01/16/spark-in-action/"},"headline":"Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유","image":["http://tkdguq05.github.io/images/spark-logo-trademark.png"],"datePublished":"2020-01-16T08:49:47.000Z","dateModified":"2020-02-24T10:20:12.000Z","author":{"@type":"Person","name":"SangHyub Lee, Jose"},"publisher":{"@type":"Organization","name":"Unreasonable Effectiveness","logo":{"@type":"ImageObject"}},"description":"Spark를 사용해서 데이터를 읽고 분석하자"}</script><link rel="canonical" href="http://tkdguq05.github.io/2020/01/16/spark-in-action/"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">Unreasonable Effectiveness</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2020-01-16T08:49:47.000Z" title="2020. 1. 16. 오후 5:49:47">2020-01-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2020-02-24T10:20:12.000Z" title="2020. 2. 24. 오후 7:20:12">2020-02-24</time></span><span class="level-item"><a class="link-muted" href="/categories/spark/">spark</a><span> / </span><a class="link-muted" href="/categories/spark/engineering/">engineering</a></span></div></div><h1 class="title is-3 is-size-4-mobile">Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유</h1><div class="content"><p>Spark를 사용해서 데이터를 읽고 분석하자</p>
<span id="more"></span>
<h1 id="데이터-분석하기-전-데이터부터-읽자"><a href="#데이터-분석하기-전-데이터부터-읽자" class="headerlink" title="데이터 분석하기 전, 데이터부터 읽자"></a>데이터 분석하기 전, 데이터부터 읽자</h1><h3 id="Spark-Session-conf-설정"><a href="#Spark-Session-conf-설정" class="headerlink" title="Spark Session, conf 설정"></a>Spark Session, conf 설정</h3><p>기존 python의 pandas를 이용해서 데이터를 읽으려면 pd.DataFrame(‘…….’)를 통해 파일을 읽으면 간단히 해결 되었다. 하지만 spark에서 데이터를 읽기 위해서는 조금 더 손을 거쳐야 한다. 물론 Zeppelin을 이용한다면 바로 파일을 읽어들일 수 있겠지만, pycharm을 이용해서 pyspark application을 만드는 작업을 할 것이기 때문에 직접 spark세팅을 해주어야 한다. </p>
<p>pycharm에서는 Spark Session을 설정해줘야 spark를 사용할 수 있다. 이 Spark Session에 대한 설정값으로 Spark Conf를 설정해주어야 한다.<br>먼저 필요한 라이브러리를 불러들이고 Spark conf와 session을 설정한다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.<span class="built_in">set</span>(<span class="string">&#x27;spark.jars.packages&#x27;</span>, <span class="string">&#x27;org.mongodb.spark:mongo-spark-connector_2.11:2.3.1&#x27;</span>)</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;spark test&quot;</span>).config(conf=conf).getOrCreate()</span><br></pre></td></tr></table></figure>
<p>conf.set에는 mongoDB와의 연결을 위한 spark connector를 넣어줬다. 이렇게 하면 mongoDB에 있는 데이터를 바로 읽을 수 있을까? 아직 할 작업이 조금 남았다. data를 불러오기 전에 스키마 지정을 해줘야 하기 때문이다.</p>
<h3 id="스키마-지정"><a href="#스키마-지정" class="headerlink" title="스키마 지정"></a>스키마 지정</h3><p>스키마란 간단하게 말해서 데이터 구조와 제약 조건에 대한 명세(Specification) 기술한 것을 의미한다.<br>여기서 설정할 스키마는 이 데이터의 칼럼이 어떤타입으로 들어갈 것인지(string, integer, double …)를 주로 뜻하게 될 것이다.<br>mongoDB에서 사용자들이 거래한 내용 중 카트에 어떤 상품을 담았는지 알기 위해서 다음과 같이 코드를 작성했다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cartSchema =  StructType([</span><br><span class="line">    StructField(<span class="string">&quot;cartGoodsName&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;cartGoodsCode&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;cartGoodsAmount&quot;</span>, IntegerType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;cartGoodsCount&quot;</span>, IntegerType(),<span class="literal">True</span>),</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">userSchema = StructType([</span><br><span class="line">    StructField(<span class="string">&quot;cookieId&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;currentTime&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;sessionSeq&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;userSeq&quot;</span>, StringType(),<span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;cart&quot;</span>, ArrayType(cartSchema),<span class="literal">True</span>)</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure>
<p>이렇게 카트 데이터에 대한 스키마를 작성해서 유저스키마의 cart 부분에 넣어준 뒤 합쳐진 userSchema를 이용해 데이터를 읽었다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.schema(userSchema).<span class="built_in">format</span>(<span class="string">&quot;com.mongodb.spark.sql.DefaultSource&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;spark.mongodb.input.uri&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mongodb://******/*****.userDataInfo.******&quot;</span>) \</span><br><span class="line">    .option(<span class="string">&quot;spark.mongodb.output.uri&quot;</span>,</span><br><span class="line">            <span class="string">&quot;mongodb://******/*****.userDataInfo.******&quot;</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure>
<p>읽은 결과는 따로 dataframe을 지정할 필요없이 바로 dataframe으로 떨어진다. 이제 바로 데이터에 대해서 작업을 수행할 수 있게 되었다.<br>카트에 담은 상품이 무엇인지 알고 싶어서 actionType이 viewCart인 부분을 가져왔다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">view_cart_df = df.<span class="built_in">filter</span>(df.actionType ==<span class="string">&#x27;viewCart&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p>가져오고 나서 전처리 작업을 하려고 했는데, 데이터프레임에 대한 이해가 적었었던 때라 어떻게 작업해야 할지 몰랐다. 그래서 먼저 RDD로 작업을 했고 뼈저리게 후회했다. 절대 발생하면 안되는 일이 일어났기 때문이다.</p>
<h1 id="RDD를-사용한-결과"><a href="#RDD를-사용한-결과" class="headerlink" title="RDD를 사용한 결과"></a>RDD를 사용한 결과</h1><p>RDD를 사용해서 전처리를 해보고 Cart에 담긴 Top N개의 상품을 가져와보기로 했다. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_info</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> i:</span><br><span class="line">            test = Row(code=k[<span class="number">1</span>],cart_count=k[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> test</span><br></pre></td></tr></table></figure>
<p>RDD를 이용해 전처리를 할 때 쓸 함수를 지정해 놓고 작업을 하기로 했다. 함수는 다음과 같이 작성했고 상품의 코드와 그 상품이 얼마나 담겼는지를 Row로 생성했다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = df.<span class="built_in">filter</span>(df.cart.isNotNull()).withColumn(<span class="string">&quot;currentTime&quot;</span>, to_timestamp(<span class="string">&quot;currentTime&quot;</span>, <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>))</span><br><span class="line">view_cart_count = df.select(<span class="string">&#x27;cart&#x27;</span>).rdd.<span class="built_in">map</span>(get_info).toDF()</span><br><span class="line"></span><br><span class="line">view_cart_count.groupBy(<span class="string">&#x27;code&#x27;</span>).count().show()</span><br></pre></td></tr></table></figure>
<p>이렇게 만든 함수를 df의 cart에서 rdd의 map을 이용해서 결과를 가져왔다. 그리고 상품코드 별로 그룹화 하고 sum을 해서 결과를 출력했다.</p>
<p><img src="/images/rdd_res.png" alt="RDD에 map한 결과"><br>그런데 뭔가 이상했다. sum을 했으면 결과값이 적어도 100은 넘어야 했는데, 100넘는 값이 너무 적었다. 그래서 특정 상품코드에 대해서 python으로 데이터 분석을 실시해서 결과를 매칭시켜 비교해보기로 했다.</p>
<p><img src="/images/python_res.png" alt="python을 통한 결과"><br>결과가 너무 차이가 났다. 이렇게 나온 결과로 아이템을 추천하게 되면 제대로 된 상품이 추천되지 않을  것이다. RDD에 함수를 map하는 것에 뭔가 문제가 있는 것이 분명했다. 방법을 찾다가 Dataframe으로 작업을 해보기로 했다.</p>
<h1 id="Dataframe을-사용한-결과"><a href="#Dataframe을-사용한-결과" class="headerlink" title="Dataframe을 사용한 결과"></a>Dataframe을 사용한 결과</h1><p>데이터 프레임으로 작업해야 결과값이 바뀌지 않는 다는 정보를 알게 되어 기존에 있던 df에 filter를 걸어 새 DF를 만들고 이걸 가지고 전처리 해보기로 했다.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cartDF = df.<span class="built_in">filter</span>(df.cart.isNotNull()).withColumn(<span class="string">&quot;currentTime&quot;</span>,</span><br><span class="line">                  to_timestamp(<span class="string">&quot;currentTime&quot;</span>, <span class="string">&quot;yyyy-MM-dd HH:mm:ss&quot;</span>)).select(<span class="string">&quot;cart&quot;</span>) \</span><br><span class="line">    .withColumn(<span class="string">&quot;cart&quot;</span>, explode(<span class="string">&quot;cart&quot;</span>))</span><br><span class="line"></span><br><span class="line">cart_all = cartDF.withColumn(<span class="string">&quot;goodsCode&quot;</span>, cartDF[<span class="string">&quot;cart&quot;</span>].getItem(<span class="string">&quot;cartGoodsCode&quot;</span>))\</span><br><span class="line">    .withColumn(<span class="string">&quot;goodsCount&quot;</span>, cartDF[<span class="string">&quot;cart&quot;</span>].getItem(<span class="string">&quot;cartGoodsCount&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">results_df = cart_all.groupby(<span class="string">&#x27;goodsCode&#x27;</span>).<span class="built_in">sum</span>().orderBy(<span class="string">&#x27;sum(goodsCount)&#x27;</span>, ascending=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>작업은 다음과 같이 실시했고 상품 갯수를 정렬하기 위해서 orderBy를 사용했다.<br>결과는 어떻게 나왔을까?</p>
<p><img src="/images/df_res.png" alt="Dataframe을 사용한 결과"></p>
<p>python을 사용한 결과와 똑같은 값이 등장했다. 성공했다!!!</p>
<h1 id="왜-값이-다를까"><a href="#왜-값이-다를까" class="headerlink" title="왜 값이 다를까?"></a>왜 값이 다를까?</h1><p>그렇다면 왜 RDD를 사용해서 함수를 적용할 때랑 Dataframe을 갖고 작업한 결과가 다른 것일까?<br>일단 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">view_cart_count = df.select(<span class="string">&#x27;cart&#x27;</span>).rdd.<span class="built_in">map</span>(get_info)</span><br></pre></td></tr></table></figure>
<p>이 코드에서 rdd.map한 부분까지 가져와서 확인해보니 결과값이 많지 않았다. rdd에서 df로 바꿀때 데이터가 변하는 일은 없다는 것이다.<br>그렇다면 이 코드 전에 rdd.map(get_info)하는 부분에서 변형이 일어난 거라고 추측할 수 있다. 하지만 spark 이론에서 map을 적용할 때는<br>map 자체가 narrow transformation에 해당되기 때문에, shuffle이 일어나지 않는다고 나와있다. 결국 shuffle에 의한 데이터 변형의 가능성도 없다고 할 수 있는 것이다. 함수 자체에 이상이 있는 것일까? 그렇다고 보기엔 어렵다. 이 코드를 갖고 구매-할인율에 대한 것을 집계했을 때는 정확한 값이 나왔기 때문이다.</p>
<p>조금 더 공부해보고 왜 값이 다른지에 대해서는 추후에 계속 수정을 해 나가야겠다.</p>
<p>결국은 spark에서는 RDD를 사용할지 Dataframe을 사용할지, 그리고 Dataset을 사용할지 먼저 생각하고 작업하는 것이 중요하다.<br>이것에 관련해서는 Databricks에서 나온 <a target="_blank" rel="noopener" href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">문서</a>가 있는데, 이것은 추후에 번역해서 업로드할 예정이다.</p>
</div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/spark/">spark</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/02/24/geultto4/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">글또 4기를 시작하며 다짐하기</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/01/06/spark-zeppelin/"><span class="level-item">Zeppelin으로 Spark를 다뤄보자 01</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">Unreasonable Effectiveness</a><p class="is-size-7"><span>&copy; 2023 SangHyub Lee, Jose</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>