<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Unreasonable Effectiveness</title>
    <link>http://tkdguq05.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sun, 21 Jan 2024 11:35:00 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>LLM과 LLMOps란</title>
      <link>http://tkdguq05.github.io/2024/01/21/LLM/</link>
      <guid>http://tkdguq05.github.io/2024/01/21/LLM/</guid>
      <pubDate>Sun, 21 Jan 2024 07:24:59 GMT</pubDate>
      <description>
      
        &lt;p&gt;LLM과 LLMOps에 대해 알아보고 관련 내용에 대해 알아보기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>LLM과 LLMOps에 대해 알아보고 관련 내용에 대해 알아보기</p><span id="more"></span><h2 id="LLM이란"><a href="#LLM이란" class="headerlink" title="LLM이란?"></a>LLM이란?</h2><p>LLM은 Large Language Model의 약자로 많은 양의 텍스트 데이터를 사용하여 학습된 자연어 처리 모델을 의미합니다. LLM은 수백만 또는 수십억 개의 문장과 단어로 이루어진 대량의 텍스트 데이터를 학습하여 문맥을 이해하고, 문법 규칙을 학습하며, 다양한 언어 작업에 대한 예측을 수행하게 됩니다. 대표적인 모델로는 OpenAI의 GPT와 Google의 Gemini가 있습니다. 활용 분야로는 자연어 이해, 기계 번역, 텍스트 생성, 질문 응답, 감정 분석 등 다양한 자연어 처리 작업에서 활용됩니다. 이러한 LLM은 풍부한 문맥 이해와 언어 생성 능력을 바탕으로 다양한 언어 작업을 수행할 수 있습니다. </p><h2 id="LLM-개발-성숙도"><a href="#LLM-개발-성숙도" class="headerlink" title="LLM 개발 성숙도"></a>LLM 개발 성숙도</h2><p>당장 LLM은 개발해보자! 라고 할 수 있겠지만, 이름 그대로 Large한 모델이기 때문에 바로 시작하기는 어렵습니다. 큰 모델을 학습하기에는 관련 인력도 필요하며, 고성능의 머신과 고도화된 인프라가 필요합니다. 그렇다고 LLM 애플리케이션을 만들 수 없는 것은 아닙니다. LLM을 직접 만들고 학습시킬 수 도 있지만, 공개된 LLM 모델을 사용하여 애플리케이션을 만들고 서비스에 활용할 수 있습니다. LLM 개발 성숙도가 낮다면 이 처럼 LLM을 활용하는 것을 추천하며 성숙도가 높아질수록 LLM의 더 깊은 영역에서 프로젝트를 진행할 수 있습니다. LLM의 개발 성숙도는 다음과 같습니다.</p><ol><li>In Context Learning/Promt Engineering</li><li>Retrieval Augmented Generation(RAG)</li><li>Model Fine Tuning</li><li>Foundataion Model Training</li></ol><p>1번에서처럼 맨 처음 프로젝트를 시작한다면 GPT나 기타 공개된 모델의 API를 활용할 수 있습니다. Promt를 잘 작성하여 서비스에 이용만 해도 큰 비즈니스 임팩트를 줄 수 있습니다. 다음 단계는 RAG를 구축하는 것입니다. RAG은 2020년 한 논문 ‘지식 집약적 NLP 작업을 위한 검색 증강 생성(Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks-다운)’에서 해당 용어가 처음 만들어졌습니다. RAG는 외부 소스에서 가져온 정보로 생성 AI 모델의 정확성과 신뢰성을 향상시키는 기술입니다. </p><p><img src="https://i.namu.wiki/i/yRaZmvn2i09N1GA5ej7JZLZKL3w2bXUzrvBLFMUjPioUrAQ1aXXuXeuxO_sbN8uRzDGoJSNCW-Mlaxs3QSw6JA.webp" alt="LLM의 할루시네이션에 사용자가 떠나는 모습이다"></p><p>1번의 접근에서 애플리케이션을 사용하다보면 가끔 모델이 Hallucination이라고 하는, 그럴듯한 헛소리를 하는 것을 볼 수 있습니다. 할루시네이션은 언어 생성 AI는 물론 스테이블 디퓨전, 달리2 등 이미지 생성 AI 등에서도 발생하는 오류 현상으로, 주로 잘못된 답변이나 기이한 이미지를 생성하는 것을 말합니다. 이 현상은 모델이 질문 내용에 가장 가까운 데이터(단어, 이미지 등)의 부스러기를 선택해 조합하기 때문에 발생하는 것으로 애당초 잘못된 질문을 받거나 학습한 데이터에 오류가 있을 경우 할루시네이션 현상이 주로 발생하게 됩니다. RAG은 이러한 현상을 줄일 수 있는 방법입니다.</p><p>RAG는 외부에서 데이터를 가져와 임베딩하고, 이 결과를 Vector DB에 저장하고, 사용자가 질문을 하면 관련도가 가장 높은 임베딩을 찾아내서 모델에 이를 제공하는 방식입니다. 쉽게 말하면 입력을 받아 이와 관련있는 소스에서 답을 찾게하는 방법입니다. 미리 답변에 대한 데이터를 저장해놓고 애플리케이션이 데이터가 있는 곳에 접근해서 답변을 만든다면 더 퀄리티 좋은 답변이 나오게 될 것입니다. 일반 LLM 모델의 답변보다는 명확한 근거가 있기 때문에 답변이 더 신뢰할 수 있다고 말할 수 있겠습니다.</p><p>그 다음은 파인튜닝입니다. 파인튜닝은 특정 작업이나 도메인에 높은 적합성을 확보하기 위해, 이미 훈련된 대규모 언어 모델에 특정 데이터셋을 사용하여 추가적인 학습을 수행하는 작업을 말합니다. 사전 학습한 모델을 초기 가중치로 사용하고, 특정 작업에 대한 추가 학습 데이터로 모델을 재학습하는 것입니다.</p><p>보통 2번째 단계에도 파인튜닝을 하는 접근이 있지만 ‍<a href="https://platform.openai.com/docs/guides/fine-tuning/fine-tuning-examples">OpenAI가 제공하는 파인튜닝 가이드 문서</a>에서 나온 것처럼 파인튜닝은 시간과 노력 측면에서 신중하게 접근해야 해야합니다. 답변이 잘 나오는 것 같지 않아보이지만 적절한 프롬프트를 통해 원하는 답이 나오는 경우도 있기 때문에 프롬프트 엔지니어링(Prompt Engineering), 프롬프트 체이닝(Prompt Chaining), 함수 호출(Function Calling)과 같은 다양한 방법을 먼저 시도해 본 후에 파인 튜닝을 할 것을 권장하고 있습니다. </p><p>그 다음은 직접 학습시키는 것입니다. 바로 윗 단계라 금방 될 것 같아 보이지만 이 단계는 매우 높은 단계입니다. 직접 Foundation Model을 학습하는 것이기 때문입니다. </p><blockquote><p>Foundation Model이란 대규모 데이터 세트를 기반으로 훈련된 모델을 말하며, 데이터 사이언티스트가 기계 학습(ML)에 접근하는 방식을 변화시킨 대규모 딥 러닝 신경망입니다. 데이터 사이언티스트는 처음부터 인공 지능(AI)을 개발하지 않고 파운데이션 모델을 출발점으로 삼아 새로운 애플리케이션을 더 빠르고 비용 효율적으로 지원하는 ML 모델을 개발합니다. <em>파운데이션 모델</em>이라는 용어는 연구자들이 광범위한 일반화된 데이터와 레이블이 지정되지 않은 데이터에 대해 훈련되고 언어 이해, 텍스트 및 이미지 생성, 자연어 대화와 같은 다양한 일반 작업을 수행할 수 있는 ML 모델을 설명하기 위해 만들어졌습니다.</p><p>from <a href="https://aws.amazon.com/ko/what-is/foundation-models/">https://aws.amazon.com/ko/what-is/foundation-models/</a></p></blockquote><p>이런 Foundataion Model을 학습하기 위해서는 거대한 데이터 셋이 필요하고 이를 학습하기 위한 인프라 및 자원도 필수적입니다. 따라서 1,2,3의 단계를 충분히 거친 후에 대규모 투자를 통해서 이루어져야 하는 접근법인 것입니다.</p><h2 id="LLM-Application-Flow"><a href="#LLM-Application-Flow" class="headerlink" title="LLM Application Flow"></a>LLM Application Flow</h2><p><img src="https://pbs.twimg.com/media/F_trGKQaUAAAM3f?format=jpg&name=4096x4096" alt="LLM Flow"></p><p>이제 LLM 애플리케이션의 흐름에 대해서 살펴보겠습니다. 애플리케이션의 흐름은 생각보다 간단합니다.</p><ol><li><p>클라이언트에게 입력이 들어온다.</p></li><li><p>미리 설정한 프롬프트에 질문을 넣어 LLM에게 요청한다</p><ol><li>질문에 필요한 데이터를 포함시킨다. (외부API, DB, VectorDB…)</li></ol></li><li><p>요청 결과를 바탕으로 새로운 프롬프트, 함수를 실행한다. (Chaining)</p></li><li><p>최종 결과가 나오면 답변으로 반환한다.</p><p>세부적인 내용은 들어갈수록 더 깊은 내용이 있겠지만, 큰 흐름은 이러합니다. 입력이 들어가고 미리 프롬프트를 만들어놓고 필요하다면 RAG등과 같은 시스템을 이용해 자세한 답변을 할 수 있게 만들고 바로 답변으로 반환하거나 결과를 바탕으로 다듬을 수 있는 프롬프트를 다시 한 번 실행해 결과를 돌려줍니다. </p></li></ol><h2 id="LLMOps"><a href="#LLMOps" class="headerlink" title="LLMOps"></a>LLMOps</h2><p><img src="https://miro.medium.com/v2/resize:fit:1400/1*C8J65SDbgaGgrBcCoXZSCg.png" alt="LLMOps Architecture"></p><p>이와 같이 LLM 애플리케이션은 생각보다 간단하게 만들 수 있고 쉽게 서비스할 수 있습니다. 하지만 동시에 대규모 모델에 대한 직접 학습에 대한 요구가 발생하면서 LLMOps도 탄생하게 되었습니다. 일반적으로 MLOps의 운영 요구사항은 일반적으로 LLM에도 적용되지만, LLM을 잘 훈련하고 배포하려면 LLM에 특화된 접근 방식이 필요하기 때문입니다. 따라서 MLOps 시스템이 고도화되어 있고 ML모델에 대한 배포경험과 운영 경험이 충분하다면 LLMOps에 대한 전환도 매우 빠르게 이루어질 수 있다고 생각합니다. </p><p>하지만 MLOps와 LLMOps는 중요한 차이점이 존재합니다. MLOps는 모델 개발 부터 서빙 및 모니터링의 자동화, 시스템화를 의미하는 반면 LLMOps는 이미 만들어진 모델에 대한 Fine Tuning 등 각자의 시스템에 맞게 조정하고 업데이트하는데 초점이 맞춰집니다. </p><p>LLMOps의 주요한 특징은 다음과 같습니다.</p><h3 id="Fine-Tuning"><a href="#Fine-Tuning" class="headerlink" title="Fine Tuning"></a><strong>Fine Tuning</strong></h3><p>대부분의 기계 학습 모델은 처음부터 생성되고 학습되는 반면, LLM은 기초 모델에서 시작하여 성능 향상을 위해 엔지니어가 새로운 데이터로 Fine Tuning합니다. 그 이유는 대규모 언어 모델이라는 이름에서 느낄 수 있습니다. 이러한 모델은 말 그대로 수십억 개의 매개변수를 가진 신경망인데 이로 인해 모델을 훈련하는 데 비용이 매우 많이 들기 때문에 Transfer Learning이 널리 사용됩니다. 이를 통해 더 적은 데이터와 리소스를 사용하여 특정 애플리케이션의 정확성을 높일 수 있습니다. PEFT(Parameter-Efficient Fine-Tuning)는 오픈 소스 LLM을 위한 파인 튜닝 방법의 예시이고, OpenAI는 인프라 내에서 이를 수행하기 위한 API를 제공하고 있습니다.</p><h3 id="Feedback-Loop"><a href="#Feedback-Loop" class="headerlink" title="Feedback Loop"></a>Feedback Loop</h3><p>LLM은 사람의 피드백을 통해 최근 몇 년 동안 크게 개선되었습니다. LLM의 애플리케이션 성능을 평가하는 데 최종 사용자의 피드백이 매우 중요하기 때문에 이를 통해 엔지니어는 LLMOps 파이프라인 내에서 필요한 변경을 수행할 수 있습니다. </p><h3 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h3><p>일반 ML 모델에는 계산하기 쉬운 명확하게 정의된 성능 지표가 있습니다. 그러나 LLM은 BLEU 및 ROGUE와 복잡한 평가 지표를 사용합니다. 그럼에도 불구하고 LLM에 적합한 측정항목을 선택하는 것은 매우 어려운 일이며 이를 통해 해결하려는 작업 유형에 따라 크게 달라질 수 있습니다.</p><h3 id="Chains"><a href="#Chains" class="headerlink" title="Chains"></a>Chains</h3><p>대부분의 LLM 응용 프로그램은 처음부터 새로운 LLM을 구축하는 대신 기존의 LLM과 함께 많은 외부 시스템을 연결하는 데 중점을 둡니다. 이것이 바로 LangChain과 같은 도구가 인기를 얻은 이유입니다. LangChain은 도구, 구성 요소 및 인터페이스 제품군을 사용하여 LLM 기반 응용 프로그램을 구축하는 프로세스를 빠르게 만들 수 있습니다. </p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://modulabs.co.kr/blog/llmops-intro/">https://modulabs.co.kr/blog/llmops-intro/</a></li><li><a href="https://blog-ko.superb-ai.com/why-llmops-is-gaining-traction-from-devops-to-llmops/">https://blog-ko.superb-ai.com/why-llmops-is-gaining-traction-from-devops-to-llmops/</a></li><li><a href="https://medium.com/@bakingai/llmops-the-future-of-mlops-for-generative-ai-aed95decf21e">https://medium.com/@bakingai/llmops-the-future-of-mlops-for-generative-ai-aed95decf21e</a></li><li><a href="https://www.promptingguide.ai/kr/techniques/rag">https://www.promptingguide.ai/kr/techniques/rag</a></li><li><a href="https://www.instacart.com/company/how-its-made/supercharging-ml-ai-foundations-at-instacart/"><img src="https://www.instacart.com/company/wp-content/themes/instacartcorporate/assets/images/favicon/favicon-16x16.png" alt="img">Supercharging ML/AI Foundations at Instacart</a> </li><li><a href="https://www.youtube.com/watch?v=TJ2mYNpUTAY&list=LL&index=2&t=6639s"><img src="https://www.youtube.com/s/desktop/0b03494e/img/favicon_32x32.png" alt="img">그랩의 LLM Application 개발 경험/레슨런 공유회</a> </li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2024/01/21/LLM/#disqus_thread</comments>
    </item>
    
    <item>
      <title>kubernetes 버전업 1.24 에서 1.26까지</title>
      <link>http://tkdguq05.github.io/2023/12/05/kubernetes-version-up/</link>
      <guid>http://tkdguq05.github.io/2023/12/05/kubernetes-version-up/</guid>
      <pubDate>Mon, 04 Dec 2023 23:54:21 GMT</pubDate>
      <description>
      
        &lt;p&gt;쿠버네티스 버전을 1.24 에서 1.26으로 업그레이드한 과정을 공유합니다. (EKS)&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>쿠버네티스 버전을 1.24 에서 1.26으로 업그레이드한 과정을 공유합니다. (EKS)</p><span id="more"></span><h2 id="버전-업을-해야하는-상황"><a href="#버전-업을-해야하는-상황" class="headerlink" title="버전 업을 해야하는 상황"></a>버전 업을 해야하는 상황</h2><p>EKS를 사용하고 있다 보면 AWS에 어느 순간 메일이 전달됩니다. </p><blockquote><p>안녕하세요,</p><p>Amazon EKS의 Kubernetes 버전 1.24에 대한 표준 지원이 2024년 1월 31일에 종료됩니다. 2024년 2월 1일부터는 이 버전에서 실행되는 모든 Amazon EKS 클러스터에 확장 지원(extended support)이 적용됩니다. Kubernetes 버전 1.24는 2025년 1월 31일까지 확장 지원 상태로 유지되며, 그 이후에는 이 버전이 Amazon EKS에서 더 이상 지원되지 않습니다.2025년 1월 31일 이후에는 더 이상 새로운 Kubernetes 버전 1.24 클러스터를 생성할 수 없으며, 이 버전을 실행하는 모든 EKS 클러스터는 점진적인 배포 프로세스를 통해 Kubernetes 버전 1.25로 업데이트됩니다.</p><p>현재 고객님께서는 Kubernetes 버전 1.24로 실행 중인 EKS 클러스터가 1개 이상 있고 해당 리소스가 AWS 상태 대시보드의 ‘영향을 받는 리소스’ 탭에 나열되어 있기 때문에 이 메시지가 표시되었습니다.</p><p>확장 지원은 현재 평가판을 제공하고 있으며 모든 고객이 무료로 이용할 수 있습니다. 2024년 초에 확장 지원이 일반적으로 제공되면 확장 지원 버전을 실행하는 모든 클러스터에 대해 시간당 클러스터당 추가 요금이 부과됩니다. 비용 정보는 해당 기능이 정식 출시되면 발표될 예정입니다. </p><p>확장 지원을 사용하지 않으려면 표준 지원 종료일인 2024년 1월 31일 이전에 1.24 클러스터를 Kubernetes 버전 1.25 이상으로 업데이트하는 것이 좋습니다. Kubernetes 버전에 대한 지원 확대에 대해 자세히 알아보려면 출시 발표 [1] 를 참조하세요. 클러스터를 업데이트하는 방법에 대한 지침은 Amazon EKS 서비스 설명서 [2] 를 참조하십시오. </p><p>Kubernetes 버전 지원에 대한 자세한 내용은 Amazon EKS 서비스 설명서 [3] 를 참조하시기 바랍니다. </p><p>질문이나 우려 사항이 있는 경우 AWS 지원 [4] 에 문의주시기 바랍니다.</p></blockquote><p>이 메일을 요약하자면 현재 사용하고 있는 1.24버전에 대한 표준 지원이 종료되니, 1.25 또는 그 이상의 버전으로 업데이트 하라는 것입니다. 만약 업데이트하지 않고 있는다면 메일의 내용처럼 25년 1월 31일 이후에 자동으로 1.25로 업데이트가 될 것입니다. 만약 k8s 안에 있는 애플리케이션들이 이에 대한 대비가 되어 있지 않는다면 장애로 이어질 수 있을 것 입니다. 따라서 메일이 전달된 이후 k8s 버전 업에 대한 계획을 세우기 시작했습니다.</p><h2 id="EKS-버전-업-계획"><a href="#EKS-버전-업-계획" class="headerlink" title="EKS 버전 업 계획"></a>EKS 버전 업 계획</h2><p>EKS 버전 업은 1.21에서 1.24로 이미 경험을 한 적이 있었습니다. 당시에 업데이트 하면서 굉장히 오랜시간이 걸렸기 때문에 애플리케이션 다운타임이 상당해서 골치 아팠던 경험이었습니다. 다행히 운영쪽에 사용되는 애플리케이션은 없었고 대부분 개발계 airflow나 mlflow 정도였기 때문에 운영 상의 피해는 거의 없었습니다. 여기서 경험한 바로는 EKS 클러스터의 플러그인 버전을 해당 클러스터 버전에 맞게 맞춰줄 것, 특히 ebs-csi controller log를 확인하면서 ebs 볼륨이 detach, attach 잘 되고 있는지 확인하는 것이 중요했습니다. 또한 AWS 콘솔 상에서 클러스터 버전 업은 금방 처리 되지만 이후에 노드 그룹을 업그레이드를 콘솔에서 하게 되면 상당히 시간이 지체되곤 했습니다. Rancher를 통해 노드 그룹 업데이트를 하는 것도 가능했습니만 이 역시 시간이 상당히 지체되거나 행이 걸리는 경우가 발생했습니다. 그래서 버전 업한 노드그룹을 새로 만들고 기존 노드에 있던 앱들을 옮기는 방식으로 진행했습니다.</p><p>*참고로 이 방식은 권장되는 방식이 아닙니다. 이 클러스터는 MLOps용 클러스터였고 특히나 개발계에서 사용되는 클러스터였기 때문에 잠시 순단이 일어나도 용인 가능한 상황이었습니다. 운영에서 버전 업을 할 때는 순단이 최소한으로 일어날 수 있도록 더 많은 조치가 필요합니다.</p><p>버전 업을 할 때 주로 참고했던 문서는 AWS에서 제공한 이 <a href="https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/kubernetes-versions-standard.html#kubernetes-1.26">문서</a>였습니다. 여기서 기본적인 정보를 파악하고 큰 틀의 계획을 잡았습니다.</p><ol><li>플러그인 업데이트 (1.25 기준)<ul><li><strong>CoreDNS</strong> — v1.9.3-eksbuild.2<ul><li>현재 coredns:v1.8.4-eksbuild.1 사용 중</li></ul></li><li><strong>Kube-proxy</strong> — 1.25.6-eksbuild.1<ul><li>현재 v1.21.2-eksbuild.2 사용 중</li></ul></li><li><strong>VPC CNI</strong> — 1.12.2-eksbuild.1<ul><li>현재 v1.10.1-eksbuild.1 사용 중</li></ul></li><li><strong>aws-ebs-csi-driver</strong>- v1.16.0-eksbuild.1<ul><li>현재 v1.14.0-eksbuild.1 사용 중</li></ul></li></ul></li><li>클러스터 버전 업 (AWS 콘솔)</li><li>버전 업된 노드그룹 생성</li><li>기존 노드 Drain</li></ol><h2 id="버전-업-진행"><a href="#버전-업-진행" class="headerlink" title="버전 업 진행"></a>버전 업 진행</h2><p>버전 업을 진행하면서 가장 힘들었던 부분은 플러그인 업데이트에 관한 것이었습니다. 플러그인 업데이트만 딱 하면 끝인 줄 알았더니, 업데이트를 위한 Role이나 IAM조정 등이 필요했습니다. 그 중에서 VPC CNI를 설치하면서 나왔었던 <code>Retrying waiting for IPAM-D</code> 가 대표적이었습니다. VPN-CNI 에 대한 설치는 기존에 AWS 콘솔에 있는 애드 온을 이용했었는데, 이를 이용해 업데이트를 하니 발생했던 에러였습니다. VPN-CNI는 k8s클러스터 내에 노드마다 설치가 되며 이는 <code>aws-node-xxxx</code> 와 같이 나타납니다. 이 파드들이 제대로 running되지 못하고 있었고 이에 따라 노드가 정상적으로 실행되지 못하고 있었습니다. 이에 관해서 이 <a href="https://velog.io/@egoavara/AWS-EKS-VPC-CNI-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EB%B2%95-Retrying-waiting-for-IPAM-D">블로그 글</a>을 참조했습니다만 저는 AWS의 <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html">EKS 문서</a>에서 Add-ons 부분을 보고 IAM Role을 override하는 방식으로 진행했습니다. Add-ons 부분에서는 VPC-CNI, Core-DNS, Kube-Proxy에 대해서 설정이 가능하고, ebs-csi의 경우에는 Storage 부분에서 해당 내용을 확인할 수 있습니다. 그 외에 참고할 만한 내용들이 있으니 문서를 보시면 버전 업 외에도 클러스터 운영에 관한 도움을 받을 수 있을 것으로 보입니다.</p><p>설치 중에 ebs-csi에서도 에러가 발생했습니다. 다음과 같은 에러였습니다.</p><blockquote><p>Conflicts found when trying to apply. Will not continue due to resolve conflicts mode. Conflicts: DaemonSet.apps ebs-csi-node - .metadata.labels.app.kubernetes.io/managed-by DaemonSet.apps ebs-csi-node - .metadata.labels.app.kubernetes.io/version DaemonSet.apps ebs-csi-node - .spec.selector DaemonSet.apps ebs-csi-node - .spec.template.metadata.labels.app.kubernetes.io/managed-by DaemonSet.apps ebs-csi-node - .spec.template.metadata.labels.app.kubernetes.io/version DaemonSet.apps ebs-csi-node - .spec.template.spec.containers[name=”ebs-plugin”].env DaemonSet.apps ebs-csi-node - .spec.template.spec.containers[name=”ebs-plugin”].env[name=”CSI_NODE_NAME”] </p><p>…</p></blockquote><p>충돌이 발생했다는 에러메세지였습니다. 아마 AWS 콘솔에서 모든 add-on을 다 설치했다면 이런 에러는 볼 수 없을지도 모릅니다. 왜냐하면 이 에러는 클러스터에 따로 helm으로 ebs-csi를 설치하고, 콘솔을 통해 add-on을 업데이트 하려고 하면 발생하는 에러이기 때문입니다. 클러스터에 설치된 ebs-csi를 제거했고 AWS 콘솔의 add-on으로 설치하니 해당 에러는 발생하지 않았고 정상적으로 설치가 완료 되었습니다.</p><p>플러그인 설치 이후에는 모든 작업이 순조로웠습니다. 클러스터 버전 업도 잘되었고 1.25 노드그룹으로의 이전, 그리고 1.26클러스터 업데이트, 1.26 노드그룹 이전 까지 순차적으로 작업이 완료되었습니다.</p><p>버전업 이후에는 Airflow나 Kubeflow, mlflow 등 클러스터 안에 설치된 애플리케이션의 기능과 실행에 대해서 확인을 했고 정상적으로 실행이 되었습니다.</p><h2 id="주의-점"><a href="#주의-점" class="headerlink" title="주의 점"></a>주의 점</h2><p>앞에서 잠깐 나왔지만, 운영 클러스터에는 이 방법을 적용하기는 어려울 것 같았습니다. 노드그룹을 옮기면서 순단이 발생할 수 있고 애플리케이션에 graceful shutdown이 적용되어 있지 않다면 순단이 매우 길어질 수 있기 때문입니다. 특히 실제 애플리케이션에 노출되는 API의 경우에 순단이 발생한다면 연관 백엔드 API에 장애전파가 일어날 수 있어 서킷 브레이커가 적용되어있는지, 만약 API가 동작하지 않는다면 백업 로직은 어떻게 되어있는지를 면밀히 파악하고, 사전에 stg쪽에서 비슷한 상황으로 시뮬레이션하여 백업로직이 정확히 어떻게 동작하는지를 알아놓는 것이 좋습니다. </p><p>또한 클러스터 내에서 배치작업 등이 실행된다면 순단이 얼마나 발생하는지 예측하여 해당 시간대에 어떤 배치가 동작해야하는지 알고 있어야 합니다. 따라서 배치 담당자들과 사전에 협의를 해놓는 것이 필요하고 실패시에 어떤 방식으로 복구가 가능한지 계획해놓아야 합니다. 그리고 현재 클러스터에는 Karpenter가 적용이 되어 있습니다. Karpenter에 관한 글은 <a href="https://helloworld.kurly.com/blog/first-mlops/">Kurly의 기술블로그</a> 내용을 참조하면 좋을 것 같습니다. k8s를 업데이트 하면서 deprecated된 API 들이 발생하기 때문에 이를 참고해서 Karpenter의 업그레이드도 버전을 신경써줘야 합니다. 예를 들어 Karpenter <code>v0.28.0</code> 버전은 Kubernetes version 1.26 이상 버전과 호환되지 않습니다. 따라서 1.27이나 1.28을 사용한다면 Karpenter버전을 더 올려주는 것이 좋겠습니다.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><p><a href="https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/kubernetes-versions-standard.html#kubernetes-1.26">https://docs.aws.amazon.com/ko_kr/eks/latest/userguide/kubernetes-versions-standard.html#kubernetes-1.26</a></p></li><li><p><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html">https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html</a></p></li><li><p><a href="https://velog.io/@egoavara/AWS-EKS-VPC-CNI-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EB%B2%95-Retrying-waiting-for-IPAM-D">https://velog.io/@egoavara/AWS-EKS-VPC-CNI-%EC%97%90%EB%9F%AC-%ED%95%B4%EA%B2%B0%EB%B2%95-Retrying-waiting-for-IPAM-D</a></p></li><li><p><a href="https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25">https://kubernetes.io/docs/reference/using-api/deprecation-guide/#v1-25</a></p></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2023/12/05/kubernetes-version-up/#disqus_thread</comments>
    </item>
    
    <item>
      <title>argocd로 App of apps 패턴 적용해서 Kubeflow 배포하기</title>
      <link>http://tkdguq05.github.io/2023/05/29/argocd/</link>
      <guid>http://tkdguq05.github.io/2023/05/29/argocd/</guid>
      <pubDate>Mon, 29 May 2023 08:15:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;argoCD란? 언제 사용해야할까? App of Apps패턴을 적용해보자&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>argoCD란? 언제 사용해야할까? App of Apps패턴을 적용해보자</p><span id="more"></span><h2 id="argoCD-왜-사용해야할까"><a href="#argoCD-왜-사용해야할까" class="headerlink" title="argoCD 왜 사용해야할까?"></a>argoCD 왜 사용해야할까?</h2><p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TjWns1hCwL-aYsag9AoCnA.png" style="zoom:67%;" /></p><p><br></br></p><p>argoCD는 공식 홈페이지에서 선언적으로 GitOps를 Continuous Delivery 할 수 있게 해주는 툴이라고 소개가 되어 있습니다. 설명만으로는 사실 이게 어떤 툴이고 왜 사용해야 하는지 잘 와닿지 않을 것 같습니다. argoCD를 왜 사용하려고 했는지 그 배경을 먼저 말씀드리도록 하겠습니다. 현재 회사에서 Kubernetes기반의 Kubeflow를 이용한 MLOps 및 ML Platform을 만들고 있습니다. 이 플랫폼을 만드는 데에 있어서 kubeflow의 각 컴포넌트들을 관리해야 했는데 가장 처음 사용한 것은 <strong>Kustomize</strong>였습니다. <a href="https://github.com/kubeflow/manifests">Kubeflow 공식 manifest 레포</a>에서도 manifest 관리를 Kustomize를 이용하고 있었고 이것을 그대로 활용해 보았습니다. Kustomize를 활용한 manifest관리는 나쁘지 않았습니다. 한번에 작성된 manifest를 클러스터에 적용할 수 있어서 굉장히 편리했었습니다. 하지만 kubeflow를 혼자 관리하지 않았고, 이에 따라 시간이 지날 수록 코드 관리가 필요해지게 되었으며, 언제 누가 어떤 변경을 했는지 기록을 할 필요가 생기기 시작했습니다. Continuous Integration의 필요성이 느껴지고 있었고 이에 따라 누구나 사용하기 쉬운 github과 같은 도구로 관리하면 좋겠다고 생각을 했습니다. Devtron, RancherCD 등 다양한 도구를 찾아보고 시도를 해봤습니다만, 맘에 쏙 드는 기능은 없었고 그러던 찰나에 argoCD라는 툴을 알게 되었습니다. argoCD는 딱 제가 원하던 기능을 갖고 있던 툴이었습니다. Github으로 코드관리가 가능하고, Kubernetes클러스터로 배포가 가능했으며, UI로 배포상황을 확인할 수 있고, 편하게 배포된 컴포넌트를 수정, 삭제가 가능했습니다.</p><p><img src="https://argo-cd.readthedocs.io/en/stable/assets/argocd-ui.gif" alt="argoCD 사용예시"></p><p><br></br></p><h2 id="argoCD"><a href="#argoCD" class="headerlink" title="argoCD"></a>argoCD</h2><p>argoCD는 위에서 설명한 대로 GitOps를 가능하게 하는, CI/CD에 활용할 수 있는 툴입니다. Kubernetes위에 설치하고 원하는 레포를 등록하면 깃헙에 반영되는 내용을 쿠버네티스로 배포할 수 있습니다. 커밋이 되었을때마다 자동으로 이를 확인하는 AutoSync 기능이 있고, 커밋 후에 수동으로 이를 확인하여 Sync할 수도 있습니다. 운영환경이라면 수동으로 Sync하는게 심적으로 더 안정적일 수 있겠습니다. </p><h3 id="argoCD-구조"><a href="#argoCD-구조" class="headerlink" title="argoCD 구조"></a>argoCD 구조</h3><p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xqXOtFs4sms0_-G-UmJvgA.png" alt="argoCD architecture"></p><p>argoCD의 구조는 위 그림과 같습니다. 한 눈에 그림이 들어오지 않기 때문에, 하나하나 이해해보도록 하겠습니다. argoCD를 이루는 큰 구성요소들은 다음과 같습니다.</p><ul><li><strong>argocd-repo-server</strong>: 검색을 담당합니다. 센싱하는 소스 github repository의 파일들을 가져와서 kubernetes manifest를 생성하고 이를 로컬에 저장하는 역할을 합니다. 해당 파드에 argocd-vault-plugin을 셋팅해 두면 해당 플러그인을 기반으로 vault에서 credential 정보를 retrieve 한뒤 파일에 주입시켜 manifest를 완성할 수 있습니다.</li><li><strong>argocd-application-controller</strong>: 조정, reconcile을 담당합니다. argocd-repo-server가 생성한 manifest와 현재 쿠버네티스에 배포된 manifest의 데이터와 비교합니다. 만일 다른 점이 있다면 이를 기반으로 새로운 object들을 배포합니다.</li><li><strong>argocd-server</strong>: argocd web UI를 담당합니다. stateless web으로서 reconcile의 결과를 단순 출력하는 역할만 담당합니다.</li></ul><p>그리고 그 외의 캐싱의 역할을 하기 위해 레디스가 설치되고 인증 등을 위해 dex등이 추가로 설치됩니다. 실제로 설치가 된 이후의 파드들은 다음과 같이 생성됩니다.</p><p><img src="/images/argocd/argocd_pods.png" alt="argocd를 설치했을 때 등장하는 Pods" style="zoom:50%;" /></p><p><br></br></p><h3 id="argoCD-설치하기"><a href="#argoCD-설치하기" class="headerlink" title="argoCD 설치하기"></a>argoCD 설치하기</h3><p>argoCD를 설치하는 방법은 두 가지로 나뉩니다. Helm으로 설치할 수도 있고, install.yaml을 이용할 수도 있습니다. 두 방법 모두 설치가 잘 되지만 개인적으로는 helm으로 설치하는게 깔끔하다고 생각됩니다.</p><p><br></br></p><h4 id="Helm으로-설치하기"><a href="#Helm으로-설치하기" class="headerlink" title="Helm으로 설치하기"></a>Helm으로 설치하기</h4><p>ArgoCD helm레포를 등록하고 설치를 진행해줍니다. argocd만의 namespace를 만들고 이 namespace에 argocd 설치하는 것을 권장합니다.</p><ul><li><code>helm repo add argo https://argoproj.github.io/argo-helm</code></li><li><code>kubectl create namespace argocd</code></li><li><code>helm -n argocd install argocd argo/argo-cd</code></li></ul><p><br></br></p><p>설치를 진행하면서 주의할 점이 있습니다. 설치가 한 번에 된다면 다행이지지만, 잘 되지않아 여러번 설치를 진행하게 되었을 때 마주하기 쉬운 에러 메세지입니다.</p><p>이미 CRD로 <a href="http://applications.argoproj.io/">applications.argoproj.io</a> 가 설치되어있다면, 다음의 에러메세지가 발생할 수 있습니다.</p><blockquote><p>Error: INSTALLATION FAILED: rendered manifests contain a resource that already exists. Unable to continue with install: CustomResourceDefinition “applications.argoproj.io” in namespace “” exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key “app.kubernetes.io/managed-by”: must be set to “Helm”; annotation validation error: missing key “meta.helm.sh/release-name”: must be set to “argo-cd”; annotation validation error: missing key “meta.helm.sh/release-namespace”: must be set to “argocd”</p></blockquote><p><br></br></p><p>이러한 경우에는 해당하는 CRD를 완전히 삭제 후에 설치를 진행해줍니다.</p><ul><li><code>kubectl delete crd applicationsets.argoproj.io applications.argoproj.io appprojects.argoproj.io</code></li><li>만약 제거를 했지만 <code>Removing</code> 으로 남아있는 경우 다음의 명령어를 통해 삭제한다.</li><li><code>kubectl patch crd/&#123;CRD-NAME&#125; -p &#39;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;:[]&#125;&#125;&#39; --type=merge</code></li></ul><p><br></br></p><h4 id="install-yaml로-설치하기"><a href="#install-yaml로-설치하기" class="headerlink" title="install.yaml로 설치하기"></a>install.yaml로 설치하기</h4><ul><li><code>kubectl create namespace argocd</code></li><li><code>kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</code></li></ul><p><br></br></p><p>설치내용 삭제하기 위해서는 다음의 명령어를 사용합니다.</p><ul><li><code>kubectl delete -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</code></li></ul><p><br></br></p><p>설치가 된 이후에는 argocd-server를 통해 web ui를 사용할 수 있습니다. kubernetes에 설치를 했다면 argocd가 사용하는 Service가  구성되어 있습니다. 이 Service에 port-forward로 접근하면 service에 연결된 argocd-server로 접속할 수 있습니다.</p><p><code>kubectl port-forward svc/argocd-server -n argocd 8080:80</code></p><p>이 명령어는 로컬 시스템의 포트 8080과 Kubernetes 클러스터 내의 포트 80을 연결하는 명령어 입니다. 포트 포워딩에 대해 첨언을 하자면, 포트 포워딩은 로컬 시스템과 Kubernetes 클러스터 사이에서 특정 포트의 트래픽을 전달하는 메커니즘입니다. 위의 명령어에서 <code>8080:80</code>은 로컬 시스템의 포트와 Kubernetes 클러스터 내의 포트를 연결하기 위해 사용됩니다. 따라서, 로컬 시스템의 8080 포트로 들어오는 트래픽은 Kubernetes 클러스터 내의 80 포트로 전달됩니다.</p><p>이 명령어를 실행하면 로컬 시스템의 8080 포트를 통해 Argo CD 서비스에 접근할 수 있게 됩니다. 각자의 환경에 맞게 조정하면 Web UI가 다음과 같이 등장합니다.</p><p><img src="https://redhat-scholars.github.io/argocd-tutorial/argocd-tutorial/_images/argocd-login.png" alt="argocd Web UI, 반갑다 꼴뚜기" style="zoom:80%;" /></p><p>하지만 바로 이 UI에 로그인을 할 수는 없습니다. 유저와 초기 비밀번호를 알아야 하기 때문입니다. </p><p><br></br></p><h3 id="초기-비밀번호-알아내기-비밀번호-설정"><a href="#초기-비밀번호-알아내기-비밀번호-설정" class="headerlink" title="초기 비밀번호 알아내기, 비밀번호 설정"></a>초기 비밀번호 알아내기, 비밀번호 설정</h3><p>argocd에는 기본으로 생성되는 아이디와 비밀번호가 존재합니다. 기본 아이디는 admin이며 초기 비밀번호는 다음의 명령어를 통해 알아낼 수 있습니다.</p><h4 id="초기-비밀번호"><a href="#초기-비밀번호" class="headerlink" title="초기 비밀번호"></a>초기 비밀번호</h4><ul><li><code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d</code></li></ul><p>이 명령어를 통해 비밀번호가 등장한다면 이 비밀번호로 로그인이 가능합니다. 하지만 길고 기억하기 힘든 비밀번호이기 때문에, 비밀번호 재설정을 하는 것이 필수적입니다.</p><h4 id="비밀번호-설정"><a href="#비밀번호-설정" class="headerlink" title="비밀번호 설정"></a>비밀번호 설정</h4><ul><li>argocd-server 파드의 쉘로 접속합니다.</li><li><code>argocd login $ARGOCD_SERVER_HOST:80 --grpc-web</code> 또는 <code>argocd login $ARGOCD_SERVER_HOST:8080 --grpc-web</code> 으로 로그인, 초기 비밀번호를 사용해줍니다.</li><li>로그인 후 비밀번호를 업데이트 한다.<ul><li><code>argocd account update-password</code></li></ul></li></ul><p><br></br></p><h2 id="Repo-등록하기"><a href="#Repo-등록하기" class="headerlink" title="Repo 등록하기"></a>Repo 등록하기</h2><p>Public Repo를 등록하는 것은 간단합니다. 사실 가이드도 필요없이 UI에서 보고 알아서 등록을 할 수 있을 정도입니다. 하지만 중요한 것은 Private Repository입니다. 보통 회사에서 또는 공개하고 싶지 않은 이유로 <strong>Private Repository</strong>를 많이 사용하고 있기 때문에 이를 위한 연결방법을 아는 것이 필요합니다. </p><p>GitOps 파이프라인을 만들기 위해서는 레포를 등록해야하며, 회사의 레포는 비공개 레포지토리입니다. 따라서 일반 방법으로는 레포지토리를 아예 찾을 수 없고 이로인해 연결도 불가능합니다. 여러 연결 방법들이 있지만, 간단하게 연결하는 방법은 <strong>PAT</strong>를 이용한 방법입니다. PAT의 발급을 위해서는 github로그인을 한 상태에서 메뉴 -&gt; Settings -&gt; Developer settings -&gt; Personal access token 의 경로에서 Generate new token 버튼을 눌러 토큰을 생성합니다.</p><p>이렇게 생성된 토큰을 레포를 연결할 때 username 하단에 있는 password에 넣어주면 끝입니다!</p><p><img src="/images/argocd/argocd-pw.png" alt="username, password 설정" style="zoom:50%;" /></p><hr><p><br></br></p><h2 id="App-of-Apps"><a href="#App-of-Apps" class="headerlink" title="App of Apps"></a>App of Apps</h2><p>argoCD도 설치했고 Repo도 등록했고 이제 문제없이 kubeflow manifest를 배포하면 끝입니다. 하지만 또 다른 문제가 있었습니다. 이는 사실 무시해도 되는 문제일 수도 있습니다. kubeflow의 manifest를 바로 argocd를 이용해 배포를 하게 된다면, kubeflow manifest에 굉장히 많은 컴포넌트들이 있기 때문에 <strong>argocd의 스크롤이 엄청나게 길어지게</strong> 됩니다. 배포 상황을 편하게 보려고 argocd를 사용하는 것인데, 오히려 보기가 어려워져버린 것입니다. 그래서 어떻게 해야 할까 고민하다가 <a href="https://towardsdatascience.com/deploying-kubeflow-1-3-rc-with-argo-cd-ca98606b98eb">다음의 아티클</a>을 읽어보게 되었습니다. 이 아티클은 글의 저자가 만든 argoflow를 통해 kubeflow를 설치하는 글인데 argoflow를 사용해볼까 하다가 눈에 들어온 것은 다음의 그림이었습니다.</p><p><img src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMK-5RqsaZaEl8xWA_c-Ww.png" alt="kubeflow를 컴포넌트 별로 분리"></p><p>kubeflow의 주요 컴포넌트들을 관리하기 쉽게 나눠 놓은 것이었습니다. 이미 manifest는 갖고 있고 이를 분리하기만 하면 되기 때문에 생각보다 간단하게 진행할 수 있겠다 싶어서 바로 나눠보았습니다. 대략 16-17개의 요소로 나눠지게 되었고 각각의 요소들을 argocd에 등록하기 시작했습니다. 이 작업을 하다보니 현타가 오기 시작했습니다. 관리하기 편할려고 이렇게 나눈건데 노가다로 하나하나 등록해야되나 싶은 것이었습니다. 일일이 나누는 것도 일이었고 하나하나 등록하는 것도 힘들었습니다. manifest를 한 번에 배포하기 위해서 argoCD를 사용하는 것인데, 이렇게 배포하는 방식은 사용목적에 어긋나는 것으로 생각되었습니다. 그러던 와중에 <strong>App of Apps</strong>라는것을 발견하게 됩니다.</p><p>App of Apps는 무려 <a href="https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/">공식 페이지에 소개</a>되어있습니다. Cluster Bootstrapping 파트에 나와있는데  <strong>cluster bootstrapping</strong>이란 구축한 쿠버네티스 클러스터에 필요한 쿠버네티스 리소스를 빠르게 배포하는 것을 말합니다. 다시말하면, 클러스터에 쉽게 manifest를 배포할 수 있는 방식인 것입니다. 공식 홈페이지에서 소개하는 방식은 Helm을 사용하는 방식입니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── guestbook.yaml</span><br><span class="line">│   ├── helm-dependency.yaml</span><br><span class="line">│   ├── helm-guestbook.yaml</span><br><span class="line">│   └── kustomize-guestbook.yaml</span><br><span class="line">└── values.yaml</span><br></pre></td></tr></table></figure><p>위와 같이 차트를 구성하고 해당하는 구성요소를 yaml로 넣어 큰 어플리케이션 안에 여러 어플리케이션을 넣어 관리할 수 있게 됩니다. values.yaml은 다음과 같이 구성할 수 있습니다. </p><ul><li>values.yaml</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">additionalApplications:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubeflow-app-of-apps</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">argocd</span></span><br><span class="line">    <span class="attr">project:</span> <span class="string">default</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="attr">repoURL:</span> &#123;<span class="string">repo</span> <span class="string">url</span>&#125;</span><br><span class="line">      <span class="attr">targetRevision:</span> &#123;<span class="string">repo</span> <span class="string">branch</span>&#125;</span><br><span class="line">      <span class="attr">path:</span> &#123;<span class="string">repo</span> <span class="string">path</span>&#125;</span><br><span class="line">    <span class="attr">destination:</span></span><br><span class="line">      <span class="attr">server:</span> <span class="string">https://kubernetes.default.svc</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">argocd</span></span><br><span class="line">    <span class="attr">syncPolicy:</span></span><br><span class="line">      <span class="attr">syncOptions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">CreateNamespace=true</span></span><br><span class="line">      <span class="comment">#automated:</span></span><br><span class="line">      <span class="comment">#  selfHeal: true</span></span><br><span class="line">      <span class="comment">#  prune: true</span></span><br></pre></td></tr></table></figure><p>values.yaml name에는 큰 묶음의 이름을 명시해줍니다. 저의 경우에는 kubeflow 내지 kubeflow-app-of-apps로 넣었습니다. 이렇게 놓고 해당 values.yaml이 바라볼 path를 넣어주면 되는데 이 path가 실제로 배포될 앱들이 모여있는 곳이면 됩니다. 해당 path에 있는 한 앱의 구성 방식을 보여드리겠습니다.</p><ul><li><p>application.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">argoproj.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Application</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">jupyter</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">argocd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">project:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">source:</span></span><br><span class="line">    <span class="attr">repoURL:</span> &#123;<span class="string">repo</span> <span class="string">url</span>&#125;</span><br><span class="line">    <span class="attr">targetRevision:</span> &#123;<span class="string">repo</span> <span class="string">branch</span>&#125;</span><br><span class="line">    <span class="attr">path:</span> &#123;<span class="string">app</span> <span class="string">path</span>&#125;</span><br><span class="line">  <span class="attr">destination:</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">https://kubernetes.default.svc</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">syncPolicy:</span></span><br><span class="line">    <span class="attr">syncOptions:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">CreateNamespace=true</span></span><br><span class="line">    <span class="attr">automated:</span></span><br><span class="line">      <span class="attr">selfHeal:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">prune:</span> <span class="literal">true</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><p>이렇게 application에 대한 yaml을 작성할 수 있습니다. 저의 경우에는 여기서 바라보는 path가 하나 더 있습니다. 이렇게 구성한 이유는 kubeflow의 jupyter에 대한 <a href="https://github.com/kubeflow/manifests/tree/master/apps/jupyter">manifest</a>를 살펴보면 아실 수 있습니다. kubeflow는 위에서 소개해드렸듯 kustomize를 사용하고 있었고 앱을 실제로 배포하는 것은 kustomization.yaml로 감싸져 있었습니다. 저는 앱을 이전에 한 번 분기한 적이 있었기 때문에 이것을 그대로 활용한 것입니다. 따라서 정리하자면 jupyter앱을 배포한다고 했을 때, argocd가 바라보게 되는 작업 경로는 <code>values.yaml -&gt; application.yaml -&gt; 해당 위치에 있는 kustomization.yaml -&gt; kustomization이 바라보는 실제 deployment 등의 yaml들</code> 이 됩니다. 결론적으로 app of apps를 위한 큰 껍데기를 <strong>values.yaml과 application.yaml를 활용해 기존의 manifest에 감싸놓은  꼴</strong>인 것입니다.</p><p><br></br></p><p>이렇게 App of Apps 패턴까지 적용하고 나면 kubeflow에 대한 관리를 쉽게 할 수 있게 되고, dev과 같은 prod 환경을 구성해야 한다고 할지라도 manifest들을 활용해 prod 쿠버네티스에 배포할 수 있게 됩니다. argoCD를 활용해서 각 구성 요소의 어떤 부분에서 Sync가 잘 안되고 있는지, 어디가 지금 최신상태가 아닌지, 커밋 내역을 어디까지 반영했는지도 쉽게 파악할 수 있게 되죠. github을 통해서 누가 어떤 커밋을 보냈는지, 언제 보냈는지 확인할 수 있고 롤백을 할 수 있는 점은 덤입니다. 이렇게 argoCD를 활용해서 kubeflow를 쉽게 관리하고 배포할 수 있는 방법에 대해서 소개해 봤습니다. kubeflow를 운영, 배포하셔도 좋고, 이 방법을 활용해서 다른 manifest들에 적용하셔도 좋을 것 같습니다.</p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://argo-cd.readthedocs.io/en/stable/">https://argo-cd.readthedocs.io/en/stable/</a></li><li><a href="https://medium.com/@outlier.developer/getting-started-with-argocd-for-gitops-kubernetes-deployments-fafc2ad2af0">https://medium.com/@outlier.developer/getting-started-with-argocd-for-gitops-kubernetes-deployments-fafc2ad2af0</a></li><li><a href="https://towardsdatascience.com/deploying-kubeflow-1-3-rc-with-argo-cd-ca98606b98eb">https://towardsdatascience.com/deploying-kubeflow-1-3-rc-with-argo-cd-ca98606b98eb</a></li><li><a href="https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/">https://argo-cd.readthedocs.io/en/stable/operator-manual/cluster-bootstrapping/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2023/05/29/argocd/#disqus_thread</comments>
    </item>
    
    <item>
      <title>지난 5개월에 대한 회고</title>
      <link>http://tkdguq05.github.io/2023/05/10/gabozagu/</link>
      <guid>http://tkdguq05.github.io/2023/05/10/gabozagu/</guid>
      <pubDate>Wed, 10 May 2023 12:39:59 GMT</pubDate>
      <description>
      
        &lt;p&gt;회고하기, 주저리 주저리&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>회고하기, 주저리 주저리</p><span id="more"></span><h2 id="어느새-5월"><a href="#어느새-5월" class="headerlink" title="어느새 5월"></a>어느새 5월</h2><p>시간은 참 성실하게 간다. 엊그제가 1월이었고 글또를 시작한 날이었는데 벌써 5월이 되었다. 상반기가 벌써 다 지나가고 있고 이미 1/4분기는 지나가버렸다. 시간이 빠르게 간다는건 예전부터 느끼고 있었지만, 최근들어 더 빠르게 지나가고 있다. 지나가보니 한건 별로 없는데 시간만 간 것 같다. 그래도 내가 무엇을 했는지, 어떻게 살아왔는지를 뒤돌아보고자 한다.</p><p>지난 시간의 큰 축은 강의였던 것 같다. 22년 연말에 크리스마스 선물인지 강의 제안이 왔다. 이전에도 간단한 강의나 멘토링을 진행했었는데 직접 강의를 맡게되는 것은 이번이 처음이었다. 특히 내가 데이터 직군에 발을 들이면서 강의를 들었었던 곳에서 제안이 와서 더 뜻 깊었던 것 같다. 당시에 강의를 들으면서 강의를 해주시는 강사님들에 대해서 무한한 동경을 했었던 나였다. 그래서 들었던 생각은 ‘내가? 여기서 강의를 한다고?’ 였다. 하지만 바로 거절하기는 싫어서 일단 얘기나 들어보자 하고 미팅요청을 보냈다.</p><p>미팅을 하고 나니 더 확실해졌다. 지난 번에 했던 간단한 강의가 아니라, 진짜 내가 들었었던 그러한 강의였다. ‘’내가 할 수 있을까?, 내가 해도 되는걸까? 수강생들을 만족시키는 강의를 만들 수 있을까?’ 하는 불안이 몰려왔다. 하지만 이번이 아니면 뭔가 기회가 다시 오지 않을 것 같았고, 제안을 수락했다.</p><p><br></br></p><h2 id="강의를-만들어보다"><a href="#강의를-만들어보다" class="headerlink" title="강의를 만들어보다"></a>강의를 만들어보다</h2><p>강의에 대한 주제는 잡혀있었지만, 세부 커리큘럼을 정하는 것은 내 몫이였다. 중간 중간에 매니저님과 소통하면서 조정을 하긴 했지만, 내가 수강생을 예상하고 주제에 맞게 어떤 걸 가르칠지 정해야 했었다. 강의를 찍는 것도 아니고 그냥 커리큘럼만 정하는 거였는데 너무 어려웠다. 이 내용을 토대로 수강생 분들이 듣게 될 걸 생각하니 신중해졌고 부담감이 몰려왔다. 내가 이 주제에 대해서 누굴 가르칠만큼 아는게 있는건지, 퀄러티 있는 강의를 낼 수 있을지, 괜히 욕만 먹는건 아닌지… </p><p>부담스럽고 걱정이 많이 됐지만, 내가 수강생이라고 가정하고 데이터 엔지니어로서 일을 시작했을때 궁금했던 점을 생각해보기 시작했다. 데이터 엔지니어 세상에는 정말 많은 툴들이 있었다. 항상 보이고 들리는 Kafka, Kubernetes, 그나마 친숙했던 Airflow, Elasticsearch… 이런 툴들을 보면서 느끼는 건, ‘언제 저 도구를 쓰는거고 왜 쓰는거지?’였다. 정말 잘하시는 엔지니어 분들을 보면 어떤 목적이 있을 때 툴들을 툭툭 설치해서 붙이고 연결해서 어떤 프로덕트를 만들어내는데, 주니어 시절의 나로서는 왜 저 도구를 쓰는지, 왜 저 도구를 써야만 하는지 이해가 되지 않았다. 물론 기본기가 부족해서 그런 것도 있긴 했었다. CS나 DB 등등의 기본기가 부족한 상태에서는 뭐가 어떻게 굴러가는지 알 턱이 없기 때문이다. 하지만 기술문서를 보더라도 친절하게 떠먹여 주는 문서는 없었다. “이 툴은 이래서 좋아, Super Fast!, 안정적이다” 사용목적에 대해서 친절하게 가이드해주는 문서는 참 찾기 힘들었었던 기억이 났다. 어떤 툴이 궁금해서 찾고 리서치해보긴 했는데 그래도 뭔가 잘 모르겠는… 항상 찜찜한 느낌이 들었었다. 그래서 사용목적과 기능을 잘 알려줄 수 있고, 이론적으로 어떤 점에서 장점이 있다는 것을 보여주면서, 실습으로 이걸 체험할 수 있게 하는 강의를 만들면 좋겠다고 결론을 내렸다. 이렇게 결론을 짓고 나니 커리큘럼을 구상하는 것은 한결 편해졌다.</p><p><br></br></p><h2 id="시간"><a href="#시간" class="headerlink" title="시간"></a>시간</h2><p>커리큘럼을 구상하고 나니 마이크를 전달받게 되었다. 작은 마이크도 아니고 손보다 큰 마이크를 받게되니 실감이 나기 시작했다. 커리큘럼대로 강의 자료를 만들었는데, 생각한대로 자료만드는 게 굉장히 어려웠다. 강의를 만들다 보니 내가 잘 모르는 부분이 꽤 많았다. 잘 모르는 걸 몰랐던 경우라도 강의를 촬영하면서 해당 개념을 설명하는데 강의 녹화중에 말문이 막혀버렸다. 이를 통해 다시 느낀 것은 혼자 공부만 하는 것보다 글로 정리하는 것이 낫고, 글로 정리하는 것보다는 남에게 설명을 할 수 있어야 이해를 완전히 하는 것이라는 점이었다. 그래서 강의 자료를 만들다가 혼자 공부하는 시간에 투자를 많이 하게 되었다. </p><p>그러다보니 시간이 모자르게 되었다. 업무가 끝난 다음에는 공부를 해야했고, 강의자료를 만들어야 했다. 운동도 제대로 하지 못했고 약속도 잡지 못하게 되었다. 업무에도 일부 영향이 가는 것이 느껴졌다. 항상 공부하고 강의 자료를 만들다보니 스트레스를 제대로 해소하지 못했고 누적된 스트레스로 업무에 집중하지 못했던 것 같았다. 스트레스를 풀기위해 운동을 하자니 몸이 피곤해지면서 강의 자료도 못 만들 것 같았고 그 시간에 얼른 자료 만들고 강의 녹화해야겠다는 생각이었다. 이렇게 지나오다 보니 쉴수도 없고 스트레스도 풀지 못해 정신적으로 많이 지치게 된 것 같았다.</p><p><br></br></p><h2 id="어느새-5월-1"><a href="#어느새-5월-1" class="headerlink" title="어느새 5월"></a>어느새 5월</h2><p>어떻게든 버티자 버티자 하다보니 강의 최종마감인 날이 다가왔다. 4월 마지막 주 까지 작업을 계속 했었었고 교안과 강의에 대해서 최종 업로드를 하게 되었다. 아주 후련하면서도 더 나은 강의를 만들 수 있지 않았나 싶었다. 좀 더 좋고 좀 더 완벽한 강의를 만들고 싶었는데… 강의를 하면서도 계속 느낀 생각이었고 강의가 끝난 후에도 생각이 조금씩 들었다. 중간 중간 강의말고 챙겨야 할 일들도 있었고, 부족한 부분을 채워야 했어서 시간도 여유롭지 않았고 강의는 정말 보통일이 아니구나… 업무에 여유가 있을정도로 실력이 있어야 강의도 무리 없이 진행할 수 있는 것이란걸 느끼게 되었다. 그러면서 글또에 글은 거의 올리지도 못하고 허허허.. 글또를 4기부터 해왔는데 패스 못하고 글을 못쓴 적이 이번이 처음이다. 그 만큼 다른 공부를 하고 글을 작성할 것도 없었기도 했고 강의 자료로 만들어 둔건 많은데 이걸 블로그에 올리자니 찜찜하고 그래서 글을 올리지 못하게 되었다. 강의를 시작하면서 글도 꾸준히 잘 써봐야지! 했던 나였는데, 막판에 강의 진도가 잘 안나가게 되니까 모든 투자를 여기에만 하게 되어버렸다. 차마 계약을 어길 순 없으니… 내가 가꿔왔던 삶을 포기해버린다! 이 마인드였던 것일까? 그렇게 강의를 마치고 지저분해진 책상을 제대로 치우지도 못하고 해외여행을 갔다.</p><p><br></br></p><h2 id="돌아보며"><a href="#돌아보며" class="headerlink" title="돌아보며"></a>돌아보며</h2><p>강의를 통해 많은 것을 얻었지만, 또 많은 걸 잃었구나 생각하게 되었다. 삶의 균형이 깨진 채로 살았던 것 같았다. 이전에 나는 일과 삶의 균형을 맞추면서 스트레스가 있을때는 운동과 사람만나는 것으로 풀곤했고, 업무를 하거나 개인 공부를 하면서 얻은 지식들을 글로 적고 공유하는 것을 좋아했었다. 회사에서도 기술문서를 적게 쓰게 되고 운동이나 기타 다른 것들로 스트레스 해소가 되지 않으니, 단시간에 재미를 느낄 수 있는 SNS나 유튜브 등등에 시간을 쓸데 없이 쓰게 되었다. 무엇보다도 삶의 균형이 깨진채로 지내다 보니 정신적인 체력이 거의 바닥난 상태로 있게 되어서 어떤 걸 시작하기가 어려웠었고 해야할 것도 못하게 되면서 더 많은 스트레스가 몰려오게 되었었다. 이렇게 글을 적을 수 있는 이유는 강의도 끝났기도 했거니와 이전의 삶을 되돌리고 있기 때문이다. 불어난 체중을 줄이기 위해 운동을 다시 시작했고 이전에 재미를 느꼈던 것들에 재미를 느끼기 위해 노력을 들이고 있다. 예전으로 돌아오고 있는 것 같아서 조금 기쁘기도 하고, 드디어 오랫동안 쓰지 못했던 글을 쓰게 되어서 힘을 얻게 된 것 같다. 이제 주변도 챙기면서 어떻게 살아봐야 할지 심플한 계획을 세워보고 싶다. 너무 장황하고 무거운 계획은 잠시 내려놓고 가까운 것부터 손에 잡아야겠다. 책상이 더러워보인다. 글을 업로드하고 책상부터 치워야겠다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2023/05/10/gabozagu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>kubernetes에서 locust를 이용해 부하테스트해보기</title>
      <link>http://tkdguq05.github.io/2023/02/25/locust-on-kubernetes/</link>
      <guid>http://tkdguq05.github.io/2023/02/25/locust-on-kubernetes/</guid>
      <pubDate>Sat, 25 Feb 2023 12:20:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;Locust를 이용한 부하테스트를 K8S를 사용해서 진행해보았습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Locust를 이용한 부하테스트를 K8S를 사용해서 진행해보았습니다.</p><span id="more"></span><h2 id="Locust"><a href="#Locust" class="headerlink" title="Locust"></a>Locust</h2><p>Locust는 파이썬으로 부하테스트를 할 수 있는 오픈소스입니다. 웹이나 앱에서 시스템이 처리할 수 있는 동시 사용자 수를 파악하기 위해서 사용하고, 서버가 감당할 수 있는 TPS/RPS(Transaction Per Second/Request Per Second)를 대략적으로 파악할 수 있습니다.</p><p>Locust 말고도 Jmeter나 nGrinder와 같은 부하테스트 도구가 있습니다만, 좀 더 쉽고 빠르게 테스트를 할 수 있고 python으로 간편하게 작업할 수 있다는 점에서 좋다고 생각되네요.</p><blockquote><p>Locust란 말의 뜻은 썸네일에서 보이듯이 메뚜기떼를 말합니다!<br>그래서 Locust에서 부하를 주는 것을 swarming이라고 하는 것입니다.</p></blockquote><p>locust를 보통 서버에 올려서 사용하지만 Kubernetes를 사용하는 환경이라면 이것을 k8s에 설치해서 부하테스트를 진행할 수 있습니다. 서버에 올려서 설치한다면 워커 수나 유저 수를 늘렸을 때 리소스의 한계가 있기 마련인데, k8s를 사용한다면 원하는 만큼 유저와 부하를 늘려서 테스트를 진행할 수 있습니다.(사실 서버에 올려져있는 다른 툴을 사용하다가 원하는 만큼 부하를 주지 못해서 k8s를 사용하게 되었답니다.)</p><p><br></br></p><h2 id="Kubernetes-in-Locust"><a href="#Kubernetes-in-Locust" class="headerlink" title="Kubernetes in Locust"></a>Kubernetes in Locust</h2><p>Locust를 쿠버네티스에 설치하기 위해서 Helm차트를 찾아보았습니다. 하지만 공식 차트는 따로 없는 것 같았습니다. 그래서 <code>Delivery Hero</code> 에서 제공하는 Helm 차트를 사용하기로 했습니다. 살펴보니 나름 사용할 만 했고 최근 커밋도 4개월 전에 있었습니다. 또 간단하게 스크립트를 적용해서 테스트를 할 수 있었기 때문에 이 차트로 설치해서 진행해보기로 했습니다.</p><h3 id="Locust-Helm-Chart"><a href="#Locust-Helm-Chart" class="headerlink" title="Locust Helm Chart"></a>Locust Helm Chart</h3><p>Helm Chart는 <a href="https://github.com/deliveryhero/helm-charts/tree/master/stable/locust">여기</a>에 제공되고 있습니다. 이 링크로 가면 어떻게 차트를 설치하고 사용하는지에 대해서 나와있습니다. 먼저 helm 차트를 등록하고 설치해주시면 됩니다.</p><p><code>helm repo add deliveryhero https://charts.deliveryhero.io/</code> </p><p><code>helm install deliveryhero/locust</code></p><p><code>helm install my-release deliveryhero/locust</code></p><p>커스텀하게 values.yaml를 작성해서 설치하고 싶다면 다음 명령어를 사용합니다. 한 번 설치한 다음 커스텀할 값이 있다면 이렇게 설치하는 것도 좋아보이네요!</p><p><code>helm install my-release deliveryhero/locust -f values.yaml</code></p><p>Locust용 namespace를 만들어 놓고 설치하는 것을 추천드립니다. 해당 namespace에 리소스를 많이 줘야하는 상황이 있을 수 있기 때문에 k8s에 설치된 다른 앱들에 영향을 주지 않기위해서라도 namespace를 별도로 사용해주세요.</p><p><br></br></p><h3 id="Chart-구성"><a href="#Chart-구성" class="headerlink" title="Chart 구성"></a>Chart 구성</h3><p>이 차트에는 기본적으로 ConfigMap을 사용해서 부하테스트를 진행합니다. 샘플로 제공되어 있는 파일은 main.py와 lib에 있는 example_functions.py 입니다. 전자는 실제 실행하는 locust 스크립트이고, 다른 하나는 여기에 사용하는 라이브러리가 되겠습니다.</p><h4 id="main-py"><a href="#main-py" class="headerlink" title="main.py"></a>main.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> locust <span class="keyword">import</span> HttpUser, task, between</span><br><span class="line"><span class="keyword">from</span> lib.example_functions <span class="keyword">import</span> choose_random_page</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">default_headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WebsiteUser</span>(<span class="title class_ inherited__">HttpUser</span>):</span><br><span class="line">    wait_time = between(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @task(<span class="params"><span class="number">1</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_index</span>(<span class="params">self</span>):</span><br><span class="line">        self.client.get(<span class="string">&quot;/&quot;</span>, headers=default_headers)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @task(<span class="params"><span class="number">3</span></span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_random_page</span>(<span class="params">self</span>):</span><br><span class="line">        self.client.get(choose_random_page(), headers=default_headers)</span><br></pre></td></tr></table></figure><p>main.py를 먼저 살펴보면 정말 간단하게 짜여있는 것을 볼 수 있습니다. 먼저 lib에서 example_functions를 갖고와서 미리 만들어둔 함수를 사용할 수 있다는 것이 보입니다. 그 이후에는 헤더를 지정하고, 그 밑 부분부터는 locust 스크립트 작성하는 것과 똑같습니다. locust의 스크립트 작성은 개념만 이해하고 있다면 전혀 어려운게 없어서 개념만 익히고 원하는 스크립트를 작성하시면 되겠습니다. 간단하게 아래에 작성했으니, 참고하시면 됩니다.</p><p><br></br></p><h4 id="Locust-file-작성하기"><a href="#Locust-file-작성하기" class="headerlink" title="Locust file 작성하기"></a>Locust file 작성하기</h4><ul><li><p><strong>HttpUser</strong></p><ul><li>부하를 가할 유저를 뜻한다.</li><li>이 유저는 tasks 애트리뷰트에 선언된 작업 또는 @task 데코레이터가 붙여진 작업을 수행한다.</li></ul></li><li><p><strong>TaskSet</strong></p><ul><li>유저가 수행할 작업들을 하나의 클래스로 만들어, tasks 애트리뷰트에 선언된 작업들 또는 @task 데코레이터가 붙여진 작업들 중 랜덤으로 수행한다.</li></ul></li><li><p><strong>task</strong></p><ul><li><p>HttpUser는 @task 데코레이터가 붙은 메서드를 찾아 수행하게 된다.</p></li><li><p>weight 파라미터를 넣어주면 각 테스크마다 가중치를 부여할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@task(<span class="params">weight=<span class="number">1</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">first_task</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@task(<span class="params">weight=<span class="number">2</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">second_task</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><ul><li>second_task가 실행될 확률은 first_task의 두 배 이다.</li></ul></li></ul></li><li><p><strong>between</strong></p><ul><li>HttpUser나 TaskSet의 wait_time 애트리뷰트를 사용할 때 해당 함수를 사용할 수 있다.</li><li>wait_time = between(1, 4) 선언해주면 1초 ~ 4초 사이 간격으로 랜덤하게 작업이 수행된다는 뜻이다.</li><li>당연하게 constant라는 이름의 함수도 제공해서 일정한 간격으로 작업을 수행할 수 있도록 해준다.</li><li>wait_time = constant(5) 는 5초마다 작업을 수행한다.</li></ul></li></ul><p><br></br></p><h4 id="lib-example-functions-py"><a href="#lib-example-functions-py" class="headerlink" title="lib/example_functions.py"></a>lib/example_functions.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">choose_random_page</span>():</span><br><span class="line">    pages = [</span><br><span class="line">        <span class="string">&#x27;/policies/privacy/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;/contact/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;/about/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;/search/howsearchworks/crawling-indexing/&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;/search/howsearchworks/algorithms/&#x27;</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> random.choice(pages)</span><br></pre></td></tr></table></figure><p>example_functions.py에는 원하는 함수를 작성해놓으면 됩니다. 여기서는 테스트할 페이지들을 나열해놓았네요.</p><p><br></br></p><p>이렇게 main.py와 example_functions.py를 작성했다면 이것을 해당 namespace에 configmap으로 등록해줍니다. kubectl명령어를 사용합니다. configmap과 locust를 함께 설치하고 싶다면 다음 명령어를 사용합니다.</p><p><code>helm install locust -n &#123;namespace&#125;  deliveryhero/locust \  --set loadtest.name=loadtest \  --set loadtest.locust_locustfile_configmap=my-loadtest-locustfile \  --set loadtest.locust_lib_configmap=my-loadtest-lib</code> </p><p>이렇게 설치되면 locust의 master와 worker가 올라오게 되는데, 정상적으로 설치되었다면 바로 UI에 접속해서 부하테스트를 진행할 수 있습니다. </p><p>그런데 궁금한 점은 이게 어떻게 pod로 전달되어서 부하테스트가 진행되는지입니다. configmap으로 등록되었는데 이게 어떻게 Pod에 들어갈 수 있게 되는 걸까요? 바로 configmap을 volume mount시키기 때문입니다.</p><p>pod의 yaml를 확인하면 어떻게 이 코드들이 실행되는지 볼 수 있습니다. 아래는 제가 생성한 locust worker 파드의 yaml 일부 입니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/mnt/locust</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">locustfile</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/mnt/locust/lib</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lib</span></span><br></pre></td></tr></table></figure><p>보시면 volumeMounts가 되어있는 것이 나와있고 파드의 /mnt/locust에 configmap으로 잡은 locustfile과 lib가 있는 것을 볼 수 있습니다. </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">    <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">my-loadtest-lib</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lib</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">    <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">my-loadtest-locustfile</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">locustfile</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">configMap:</span></span><br><span class="line">    <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">locust-config</span></span><br></pre></td></tr></table></figure><p>volume은 configMap으로 아까 전에 지정한 my-loadtest-lib과 my-loadtest-locustfile 입니다. 이렇게 되면 방금 configMap으로 지정한 코드들이 volume으로 생성되고 파드가 만들어지면서 /mnt/locust 경로에 마운트 됩니다. 이렇게 마운트 되었다면, 해당 경로에서 코드들이 실행되는 것입니다.</p><p><br></br></p><h3 id="UI-접속방법"><a href="#UI-접속방법" class="headerlink" title="UI 접속방법"></a>UI 접속방법</h3><p>띄워진 Pod에 접근하는 여러 방법이 있겠지만, port forwarding하는 것을 추천합니다.  load balancer를 붙여서 띄우는 건 그닥 추천하지 않는 방법인데 그 이유는 로드 테스트 용 툴인데 아무에게나 접속되는 점이 좋아보이지 않기 때문입니다. 부하테스트 주소를 누군가 알아낸다면 원하지 않는 곳으로 Ddos 공격을 할 수 있는 환경이 조성되어 버립니다. 그래서! port forward를 사용해서 접속해보겠습니다. </p><p>locust의 포트는 기본적으로 8089를 사용합니다. 원하지 않는다면 values.yaml를 수정해서 재배포하시면 됩니다.</p><p><code>kubectl --namespace &#123;namespace&#125; port-forward service/locust 8089:8089</code></p><p><img src="https://docs.locust.io/en/stable/_images/webui-splash-screenshot.png" alt="Locust UI"></p><p><strong>Start swarming</strong>을 하면 메뚜기 떼들이 몰려들기 시작합니다! 🦗🦗🦗🦗</p><p>아래는 실험 결과에 대한 그래프들입니다.</p><p><img src="https://docs.locust.io/en/stable/_images/total_requests_per_second.png" alt="Locust 실험 결과 - TPS"></p><p><img src="https://docs.locust.io/en/stable/_images/response_times.png" alt="Locust 실험 결과 - Response Time"></p><p><img src="https://docs.locust.io/en/stable/_images/number_of_users.png" alt="Locust 실험 결과 - Users"></p><p><br></br></p><h3 id="삽질하면서-얻어낸-꿀-팁"><a href="#삽질하면서-얻어낸-꿀-팁" class="headerlink" title="삽질하면서 얻어낸 꿀 팁"></a>삽질하면서 얻어낸 꿀 팁</h3><h4 id="라이브러리-설치하기"><a href="#라이브러리-설치하기" class="headerlink" title="라이브러리 설치하기"></a>라이브러리 설치하기</h4><p>바로 사용하면 좋겠지만, locust 스크립트에 다른 라이브러리를 사용하게 된다면 어떨까요? 아마 locust 파드들이 제대로 올라오지 못하게 될 것입니다. crashloopbackoff가 계속 보이게 되겠습니다… 만약 설치가 필요한 라이브러리가 있다면 설치된 namespace의 configmap을 잘 살펴보시길 바랍니다. </p><p>namespace의 configmap에는 locust-config의 이름으로 등록된 것이 있는데 여기에는 docker-entrypoint.sh이라는 쉘 파일이 있습니다. 이 파일은 다음과 같이 짜여져 있는데요,</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/sh</span></span><br><span class="line"></span><br><span class="line">set -eu</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">exec /opt/venv/bin/locust $@</span><br></pre></td></tr></table></figure><p>엄청나게 간단하게 짜여져 있는 코드입니다. 단순히 locust를 실행하는 쉘 파일인 것입니다. 만약 pandas라는 라이브러리를 설치하고 싶다면 다음과 같이 넣어주면 됩니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/sh</span></span><br><span class="line"></span><br><span class="line">set -eu</span><br><span class="line"></span><br><span class="line">pip3 install pandas</span><br><span class="line"></span><br><span class="line">exec /opt/venv/bin/locust $@</span><br></pre></td></tr></table></figure><p>locust가 python으로 동작하므로 별도로 pip를 설치해 줄 필요가 없어서 이렇게만 넣어주면, locust가 올라올때 라이브러리가 설치된 채로 실행됩니다.</p><p><br></br></p><h4 id="Locust-조정"><a href="#Locust-조정" class="headerlink" title="Locust 조정"></a>Locust 조정</h4><p>Locust로 엄청 많은 부하를 주고 싶은 마음은 굴뚝같습니다만 실제로 유저를 1000, 2000씩 주게되면 워커들이 죽기 시작합니다. 이로 인해 워커가 다시 올라올때까지 Request Time이 증가하게 되어 실험 결과를 오염시켜버리게 되는데요, 이를 미리 막기 위해 워커들을 늘려서 지정한 유저를 나눠서 실행하도록 해야합니다. 워커들은 deployment의 replicas를 찾아서 원하는 만큼 늘려주면 됩니다. master 또한 수 많은 워커들의 결과를 취합하다보면 파드가 터져버려서 UI가 동작하지 않기도 하고 결과 값 자체를 잃어버리기도 합니다. master도 숫자를 늘려서 해결하면 되지 않을까 싶지만, master는 하나로 운영하는게 좋습니다. 하나로 안전하게 운영하기 위해서는 리소스를 충분히 많이 늘려주면 됩니다. 역시 master의 deployment를 찾아서 resource를 원하는 만큼 조정하시면 되겠습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">1000Mi</span></span><br></pre></td></tr></table></figure><p>아마 resource가 yaml에 작성되어 있지 않을텐데, 걱정하지 마시고 resource 부분을 넣어서 실행해주세요. 워커도 또한 할당된 유저를 제대로 소화하지 못한다면 위와 같이 리소스를 늘려주면 해결됩니다.</p><p><br></br></p><h3 id="K8S를-사용할-때-주의할-점"><a href="#K8S를-사용할-때-주의할-점" class="headerlink" title="K8S를 사용할 때 주의할 점"></a>K8S를 사용할 때 주의할 점</h3><p>위에서 설정한대로 잘 되면 좋겠지만 마스터와 워커에 리소스를 늘려서 사용하다보면, k8s에 있는 노드의 리소스를 다 써버리게 됩니다. 이로 인해 노드에 node pressure가 일어나면서 다른 앱들에 장애가 발생할 수 있는 요소가 생겨버리게 되는 것입니다. 이를 막기 위해 노드를 충분히 늘려주고 테스트를 진행해야 합니다. 충분한 노드가 없다면 리소스를 Locust 워커들이 쭉쭉 소비하게 되면서 노드에 장애가 발생하고, 다른 앱들에 영향이 가게 됩니다. 큰 노드를 하나 만들어 놓고 태그를 붙인다음, 해당 태그가 있는 곳에만 locust 파드들이 뜨도록 affinity 또는 node selector를 설정하는 것도 방법이 되겠습니다.</p><p>그리고 실험이 종료되면 꼭 노드들을 정리해주세요!</p><p><br></br></p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://github.com/deliveryhero/helm-charts/tree/master/stable/locust">https://github.com/deliveryhero/helm-charts/tree/master/stable/locust</a></li><li><a href="https://wookkl.tistory.com/67">https://wookkl.tistory.com/67</a></li><li><a href="https://suen0904.tistory.com/m/24">https://suen0904.tistory.com/m/24</a></li><li><a href="https://docs.locust.io/en/stable/index.html">https://docs.locust.io/en/stable/index.html</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2023/02/25/locust-on-kubernetes/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또 8기와 2022년 회고 및 23년 다짐</title>
      <link>http://tkdguq05.github.io/2023/02/12/geultto-8/</link>
      <guid>http://tkdguq05.github.io/2023/02/12/geultto-8/</guid>
      <pubDate>Sat, 11 Feb 2023 23:50:24 GMT</pubDate>
      <description>
      
        &lt;p&gt;글또 8기를 시작하며&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>글또 8기를 시작하며</p><span id="more"></span><h2 id="글또-8기"><a href="#글또-8기" class="headerlink" title="글또 8기"></a>글또 8기</h2><p>어느새 글또가 8기가 되었다. 커리어를 시작하면서 지금까지 계속 글또를 하고 있는데 벌써 4기수째이다. 글쓰는 것에 대한 압박감을 이겨내면서 배운 것을 정리해보자는 다짐으로 시작했는데, 부수적으로 좋은 글을 쓰는 법, 좋은 글을 구분하는 능력 등이 생긴 것 같아 기분이 좋다. 글은 생각보다 회사에서 도움이 된다. 글을 잘 쓰는 사람이 있는 반면, 글을 썼는데 도대체 무슨 내용을 적은건지 모르겠는 사람도 있다. 일을 같이 하고 싶은 쪽은 당연히 전자이다. 왜냐하면 그들이 작성한 문서를 보면 훨씬 커뮤니케이션 하기도 쉽고 일을 하면서 배우는 것도 많기 때문이다. 일을 하면 할 수록 이런 것을 더 많이 느끼고 있는데, 꾸준히 활동한 글또에 감사함을 느끼고 있다.</p><h2 id="2022-회고"><a href="#2022-회고" class="headerlink" title="2022 회고"></a>2022 회고</h2><p>2022년에 대한 회고를 1월 1일에 잠시 했었던 적이 있었다. 간단히 무엇을 해왔는지 정리를 했었는데,</p><h3 id="2022년-간단한-요약"><a href="#2022년-간단한-요약" class="headerlink" title="2022년 간단한 요약"></a>2022년 간단한 요약</h3><ul><li>새 회사에 왔다.</li><li>데이터 엔지니어로 새 시작을 했다.</li><li>MLOps업무를 시작했다.</li><li>테니스를 배웠다.</li></ul><p>2022년에 가장 큰 일은 새회사에 온 것이었다. 2021년에 이직을 하긴했지만 12월이었고 본격적으로 일을 시작한 것은 1월이라고 볼 수 있다. 이전 회사에서도 데이터 엔지니어 직무로 일을 하긴 했지만, 본격적으로 데이터 엔지니어로 회사에 면접도 보고 입사를 한 것은 처음이라 설레기도 하고 긴장되기도 했다. 너무 모르는게 많아서 걱정했지만, 차근차근 회사 데이터의 흐름을 알게되고 하나씩 살펴보면서 적응을 해나갔다. 이렇게 하던 와중에 MLOps 업무도 시작하게 되었다. K8s를 다루게 되면서 같이 하게 된 업무지만 회사에 꼭 필요한 시스템이었고, 고도화된 ML을 서비스하기 위해서 누군가는 해야할 업무였다. 다행히 하고 싶었던 업무였기도 해서 뚝딱뚝딱 만들어나가고 있다. 그리고 배우고 싶었던 테니스를 드디어 배웠다. 5-6개월 정도로 포핸드, 백핸드, 발리 를 배웠다. 보이는 것보다 훨씬 어렵고 정교한 운동이라고 생각되었다. 날씨가 풀리면 다시 해보고 싶다.</p><h3 id="2022년의-목표는-무엇이었을까"><a href="#2022년의-목표는-무엇이었을까" class="headerlink" title="2022년의 목표는 무엇이었을까?"></a>2022년의 목표는 무엇이었을까?</h3><ul><li>새 회사에 적응 잘하기</li><li>본격 데이터 엔지니어링 업무를 익히기</li><li>대용량 데이터 처리에 대한 경험</li><li>Kubernetes를 다뤄보자</li><li>MLOps 업무를 시작해보자</li><li>마라톤, 10km 뛰어보기</li></ul><p>2022년의 목표를 정리해보니 위와 같았다. 너무 러프한 목표였을까 싶었지만, 목표때문에 스트레스를 받기는 싫었던 것 같다. 일단 첫번째로 새 회사에 적응은 잘 한 것 같다. 볼꼴 안볼꼴 다 봤기 때문에 어떤 회사인지 이해가 어느정도는 된 것 같다. 사람들도 너무 좋은 곳이다. 두번째로는 본격 데이터 엔지니어링 업무를 익히는 것이었는데, 나름 데이터 엔지니어로 회사의 데이터 파이프라인이 어떻게 돌아가는지, 각 툴들은 어떤 일을 하고있는지 모두 파악이 되었다. 파이프라인 고도화를 한다면 참여해서 이해도를 높이고 싶다. 세번째로는 대용량 데이터 처리에 대한 경험인데, 이전 회사보다 훨씬 많은 데이터를 다루게 되어서 고민할 부분이 많아졌다. 실 서비스로 나가기 위해 대용량 처리를 어떻게 해야하는지를 요즘 배우고 있다. 네번째는 쿠버네티스인데 하루에 한번은 꼭 다루게 될 일이 생기는 것 같다. 정말 좋은 툴이고 이 환경에 다른 앱들을 어떻게 관리할지, 쿠버네티스의 특징이 무엇인지 잘 알아야 관련 업무에 잘 활용할 수 있는 것 같았다. 배우면 배울수록 참 좋다고 느끼는 툴 중 하나이다. 다섯번째는 MLOps업무를 시작한 것인데, 그 시작으로 다뤘던 것이 GPU자원을 잘 쓸 수 있도록 한 것이다. 영광스럽게도 회사 <a href="https://helloworld.kurly.com/blog/first-mlops/">테크블로그</a>에도 올릴 수 있게 되었다. 마지막은 마라톤이다. 예전부터 오래 뛰는 것을 잘 못했다. 이상하게 축구는 잘하는데 오래뛰는 것만큼은 잘 안되더라. 그래서 작년의 목표를 정해놓고 일단 대회부터 신청했다. 연습할때도 10km는 뛰어본적이 없어 걱정했는데, 생각보다 뛸만했다. 2022년에 2개의 대회에 나갔던 것으로 기억하는데, 메달볼때마다 뿌듯하다.</p><h3 id="감사한-일"><a href="#감사한-일" class="headerlink" title="감사한 일"></a>감사한 일</h3><ul><li>올해는 크게 아픈 일이 없었던 것 같다.<ul><li>몸이 건강해서 정말 다행이다.</li></ul></li><li>좋은 인연들을 많이 만난 것</li><li>멘토링 제안이나 강의제안을 받은 것</li><li>K8s아무것도 몰랐는데… MLOps 플랫폼을 만들고 있다니…</li><li>다양한 DE툴들을 다뤄볼 수 있어서 좋았다.</li></ul><p>2022년에 감사했던 일을 정리했는데, 올해는 크게 아픈 일이 없어서 감사했다. 코로나 때문에 난리였지만, 확진 판정을 받은 일은 없었다. 스치듯 지나간 것 같지만… 잘 지나간듯? 몸이 건강해서 너무 다행이지만, 운동을 못하다보니 점점 허리 등등이 아파오는 것을 느낀다. 다시 운동을 해야겠다. 그리고 좋은 인연들을 많이 만났다. 7기를 운영진으로 참여하면서 다른 분들과 친해졌는데, 모두 너무 좋은 분들이라 배울 점들이 참 많았다. 8기에도 참여하시는 분들이 있어서 자주 얘기하면 좋을듯?하다. 또 멘토링 제안이나 강의제안을 받았다. 이전에 잠깐 짧은 분량의 강의를 찍은적이 있었는데, 이 인연으로 멘토링까지 하게 되었다. 대상은 학생이 아니라 기업 임원들이었는데 나름 준비를 하고, 임원분들의 고민을 들으면서 실무적으로 어떤 부분을 신경써야될지를 더 생각해보게 된 것 같다. 정말 좋은 기회였다. 또 2022년이 끝나기 전에 강의 제안을 받았고 거절하기엔 너무 좋은 제안이라 일단 해보기로 했다. 강의자료 준비… 쉽지 않다.</p><p>또 작년에 k8s를 정말 많이 다뤘는데, 조금조금씩 발전하면서 MLOps 플랫폼을 만들고 있다. 좋은 툴들을 다 올려서 사용해보다가 안맞는 것들이나, 한 툴때문에 좋은 기능을 잃어버렸던 적도 있어서 절충해서 만들었다. 현재까지는 사용하시는 분들이 만족해하고, 분석해야 할때 로컬 notebook이 아니라 플랫폼의 notebook에 먼저 접속하셔서 사용하시는데, 사용감이 좋다가 하셔서 아주 뿌듯하다. 이를 통해서 다양한 DE툴들을 사용해보게 되었는데 현대 Data Engineering에서 다루는 것들을 대부분 사용해봐서 왜 이렇게 파이프라인을 구축 해야만 했는지에 대해서 이해를 할 수 있게 된 것 같다.</p><h3 id="부족하다고-느낀-점"><a href="#부족하다고-느낀-점" class="headerlink" title="부족하다고 느낀 점"></a>부족하다고 느낀 점</h3><ul><li>DB에 대한 이해</li><li>SQL을 많이 써보지 못했다.</li><li>CS 및 네트워크</li><li>ML 모델을 실서비스로 올려서 운영하는데까지 알아야 하는게 생각보다 많았다.</li><li>Airflow 운영만 하고 자잘한 관리는 못했던 것 같다. 내가 좀 더 잘 쓰고 기능들을 세밀하게 알아둬야할 것 같다.</li><li>월별 회고를 계속해오다가 미적지근해졌다. 한달회고를 하려니 뭔가 잘 기억이 나지 않는다<ul><li>일주일별 기록을 할게 필요해질 것 같다.</li><li>관련해서 생각날 때 만들어놔야겠다.</li></ul></li></ul><p>부족하다고 느낀점은 DE업무를 하면 할 수록 DB를 더 잘알아야 된다는 점이다. DB를 잘 알아야 실제 처리가 어떻게 되는지를 알 수 있었다. 팀 내에 DBA출신이신 분이 있어서 많이 배우지만, 스스로 더 많이 학습해야겠다고 느끼고 있다. 또 SQL을 다양하게 많이 사용하지 못했다. 단순 데이터 연동용으로는 많이 사용해봤지만, 좀 더 복잡한 쿼리를 다루고 이를 시스템에 녹여보려면 어떻게 해야되는지를 많이 고민해보지 못했다. 올해에는 더 많은 쿼리들을 해봐야겠다. 또 DE를 하면서 느끼게 되는 점 중 하나인데 기본기가 정말 중요한 것 같다. CS나 네트워크를 제대로 모르면 더 깊고 복잡한 시스템을 이해하는데 제한이 생겼다. 틈틈이 시간내서 이 부분을 보완해야할 것 같다. ML모델을 실 서비스로 올리는데 정말 많은 테스트들이 필요하다는 것을 깨닫았다. 이전 회사에서는 API만 올려놓으면 끝이었는데, 서비스의 안정성을 위해 망을 나누고 개발망에서 테스트하고 이것을 어떻게 운영에 올리고 개발팀과 협의를 해야하는지 등등의 프로세스가 있었다. 올해에 꼭 서비스를 올리고 이 내용들을 정리해봐야지. </p><p>Airflow 운영 업무만 거의 했던 것 같다. DAG를 더 많이 다루고 새 기능들을 테스트해봤어야 했는데 싶다. 앞으로 다룰 일이 많아질 것 같아서 기대가 된다. 마지막은 월별 회고에 대한 것이다. 작년에 월 별 회고를 해왔는데 제대로 유지가 되지 않았다. 생각보다 한 달에 있었던 것들이 기억이 잘 나지 않았다. 일주일 별 기록을 하면 좀 나아질 것 같긴한데, 음 너무 바빠지려나…</p><h3 id="2023년에-하고싶은-것"><a href="#2023년에-하고싶은-것" class="headerlink" title="2023년에 하고싶은 것"></a>2023년에 하고싶은 것</h3><ul><li>강의 찍기<ul><li>ElasticSearch 제대로 이해해보기</li><li>DIA 참고하기</li></ul></li><li>CS 스터디</li><li>ML 실서비스 운영하기<ul><li>Feature Engineering</li></ul></li><li>MLOps 레벨 올려보기</li><li>쿠버네티스 심화, Kubernetes에서 GPU제대로 사용해보기</li><li>블로그 공사 또는 이사가기</li><li>건강<ul><li>10km를 무리 없이 달릴 수 있게 만들자</li><li>체지방률 19프로!</li></ul></li></ul><p>2023년 백로그들을 정리해봤더니 하고 싶은게 참 많았다. Elasticsearch를 사용하는데 제대로 이해를 하지 못한 상태에서 쓰는 것 같았다. 좀 더 공부를 해서 정확하게 사용하고 싶었다. 위에서 작성을 하긴 했지만 CS 스터디를 하고 싶다. 다양한 직군들이 모여서 하면 더 좋을 것 같고 학습할 개념에 대한 사례들을 풍부하게 얻을 수 있을 것 같다. 일단 4월까진 패스…</p><p>ML실서비스 배포를 하고 운영을 하고 싶다. 이에 대해서 Feature들을 다루고 엔지니어링까지 해서 본격적으로 관리를 해보고 싶다. 회사의 ML레벨 자체를 올리고 싶다. 실서비스 운영 준비를 하다보니 부족한 부분들이 눈에 띄기 시작했다. 이런 점들을 개선해서 더 높은 레벨의 ML 시스템을 가진 회사로 바꾸고 싶다. 다음은 쿠버네티스의 심화인데 GPU를 제대로 사용해보고 싶다. 복잡한 모델이 많아져야 이 부분을 다룰 수 있을 것 같은데, 그 기반 시스템을 먼저 다뤄보면 좋을 것 같다. 그리고 현재 사용하는 블로그가 너무 오래되어서 이사를 가고 싶다. hexo가 참 쉽고 좋은데 버전 충돌이 나는 일이 많고 에러 핸들링 하려면 중국말과 싸워야한다… 그게 좀 힘들어서 기회가 된다면 공사를 좀 해봐야겠다. 마지막은 건강이다. 근육을 키우는 것도 중요한데 체력은 심폐지구력에서 나오는 것 같다. 잘 뛰지 못해서 호흡이 되지 않을때 건강이 나빠지는 것을 급격하게 체감했다. 올해에도 10km 마라톤에 도전할 것이고 무리 없이 달려서 좋은 기록을 낼 수 있도록 해볼 것이다. 또 이번에는 체지방률을 20프로 미만으로 관리해보려고 한다. 무거운 몸이여 안녕!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2023/02/12/geultto-8/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Kubeflow에 Istio가 왜 필요할까?</title>
      <link>http://tkdguq05.github.io/2022/10/02/kubeflow-istio/</link>
      <guid>http://tkdguq05.github.io/2022/10/02/kubeflow-istio/</guid>
      <pubDate>Sun, 02 Oct 2022 02:17:00 GMT</pubDate>
      <description>
      
        &lt;p&gt;Istio가 Kubeflow에서 어떤 역할을 하는지 알아보자&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Istio가 Kubeflow에서 어떤 역할을 하는지 알아보자</p><span id="more"></span><h2 id="Istio"><a href="#Istio" class="headerlink" title="Istio?"></a>Istio?</h2><blockquote><p>Istio addresses the challenges developers and operators face with a distributed or microservices architecture. Whether you’re building from scratch or migrating existing applications to cloud native, Istio can help.</p><p>Istio는 개발자와 운영자가 분산 또는 마이크로서비스 아키텍처에서 직면하는 문제를 해결합니다. 처음부터 구축하든 기존 애플리케이션을 클라우드 네이티브로 마이그레이션하든 Istio가 도움이 될 수 있습니다.</p></blockquote><p>위에 글은 Istio(이스티오라고 읽는다고 합니다) 공식 홈페이지에서 제공하는 Istio에 대한 설명입니다. 이어서 설명을 하자면, Istio는 기존 분산 애플리케이션에 투명하게 계층화되는 오픈 소스 서비스 매쉬이며, Istio는 서비스를 보호, 연결 및 모니터링하는 균일하고 효율적인 방법을 제공하고, 서비스 코드 변경 거의 없이 로드 밸런싱, 서비스 간 인증 및 모니터링을 위한 path를 컨트롤하는 역할을 맡고 있습니다. 읽어봤지만 아직까지는 어떤 역할을 하는지 제대로 이해하기 어렵습니다. 왜냐하면 Istio는 서비스 매쉬의 개념인데, 이 개념을 잘 모르고 있기 때문입니다. 뭔가를 연결하고 중개하고 있는 것 같은데, 무엇을 연결하는지 또 이 연결에서 어떤 일을 하는지 잘 이해가 가지 않습니다. 서비스 매쉬에 대해서 살펴 본 다음 Istio로 다시 돌아와 보도록 하겠습니다.</p><p><br></br></p><h2 id="Service-Mesh"><a href="#Service-Mesh" class="headerlink" title="Service Mesh"></a>Service Mesh</h2><p>서비스 매쉬에 대해서 얘기 하기 전에 또 가정해야할 것이 있습니다. 서비스 매쉬는 모놀리식 서비스보다 Micro Service Architecture 구조에 더 잘 어울리는 아키텍쳐입니다. 마이크로 서비스는 기능을 서비스라는 단위로 잘게 나누다 보니 전체 시스템이 커질 수록 서비스가 많아지고, 그로 인해서 서비스 간의 연결이 복잡해지고 여러가지 문제가 발생하게 됩니다. 이러한 문제들을 해결하기 위해 서비스 매쉬가 등장하게 된 것이죠. 대표적으로 <code>장애전파 현상</code> 이 있고, 또 서비스가 커지면서 연결이 너무 복잡하다보니 기본적으로 전체 구조를 알기 어렵게 되어 장애가 나면 어디서 났는지 추적하기가 힘들어지게 됩니다.</p><p>소프트웨어적으로 해결하는 방법이 있겠지만, 이것을 인프라적으로 해결하기 위해 등장한 것이 서비스 매쉬라는 아키텍쳐 입니다. 서비스 A와 B가 있다고 해봅시다. </p><p><img src="https://user-images.githubusercontent.com/15958325/70859721-3af76c00-1f5b-11ea-9d3f-fc868218bc3c.png" style="zoom:67%;" /></p><p>이 구조에서 서비스는 서로를 직접 호출해야 합니다. 이렇게 되면 너무 단순한 구조이기 때문에 트래픽을 조정한다던가 서비스로 들어오고 나가는 트래픽을 통제하기 어렵습니다. 만약 서비스 A에서 장애가 났다면 어떨까요? A에서 응답이 없는데도 조치를 취할 수 없기 때문에 장애는 B로까지 확산될 수 밖에 없습니다. 서킷 브레이커 같은 구조를 통해 트래픽을 끊어버리면 장애가 전파되지 않을 것입니다. 그래서 단순 서비스만 놓지 않고, 여기에 프록시를 붙여보겠습니다. </p><p><img src="https://user-images.githubusercontent.com/15958325/70859730-742fdc00-1f5b-11ea-9582-66492eef9d8a.png" alt="" style="zoom:67%;" /></p><p>이렇게 서비스에 사이드카로 프록시를 붙이게 되면 프록시를 통해 구조적으로 서킷 브레이커를 사용할 수 있게 되며, 더 나아가 더 다양한 기능을 붙일 수 있게 됩니다. </p><p><img src="https://t1.daumcdn.net/cfile/tistory/99D0373D5BF3A99703" alt="" style="zoom:67%;" /></p><p>이렇게 헤더를 통해서 Clinet를 구분해서 프록시를 타게 해서 더 최적화된 서비스를 구현할 수도 있습니다. 하지만 하나 더 생각해야 할 것은 우리는 MSA구조라는 가정 하에 시작을 했으니, 이런 서비스가 딱 하나 있는 것이 아니라 엄청나게 많이 존재한다는 것입니다. 위와 같은 서비스 구조가 100개 이상이 된다면 어떻게 될까요? 방금 살펴 본 프록시를 사용해서 여러 기능을 구성할 수는 있었지만 서비스 수에 따라 프록시 수도 Linear하게 증가하기 때문에, 이 프록시에 대한 설정이 까다로워 진다는 것입니다.</p><p>프록시에 대한 설정을 <strong>중앙 집중화</strong>해서 관리할 수 있다면 어떨까요? 설정 정보를 중앙 집중화된 컨트롤러가 통제한다면 프록시의 기능도 살리고 관리하기도 편해질 것 같습니다. 데이터 플레인과 컨트롤 플레인이 바로 <strong>중앙집중화된 컨트롤러</strong>에 해당합니다. 프록시를 관리하는 각 프록시들로 이루어져서 트래픽을 설정값에 따라 트래픽을 컨트롤 하는 부분을 <code>데이터 플레인(Data Plane)</code>이라고 하고, 데이타 플레인의 프록시 설정값들을 저장하고, 프록시들에 설정값을 전달하는 컨트롤러 역할을 하는 부분을 <code>컨트롤 플레인(Control Plane)</code> 이라고 합니다.</p><p>이것을 구현한 것이 <strong>서비스 매쉬</strong>(Service Mesh)인 것이고, 우리는 구현체 중 하나인 <code>Istio</code>를 살펴보고 있는 것입니다.</p><p><img src="https://user-images.githubusercontent.com/15958325/70860414-c3c6d580-1f64-11ea-85d9-fdf9b384a058.png" style="zoom:80%;" /></p><p>Istio가 적용된다면 그림은 다음과 같아질 것입니다.</p><p><img src="https://istio.io/latest/img/service-mesh.svg" alt="After utilizing Istio"></p><p><br></br></p><h2 id="Envoy"><a href="#Envoy" class="headerlink" title="Envoy"></a>Envoy</h2><p>사실 이 Istio는 <strong>Envoy Proxy</strong>를 활용해서 만들어진 것입니다. <code>Data Plane</code>의 메인 프록시로 Envoy proxy를 사용하기 때문입니다. Envoy는 Lyft사에서 만들어졌고 C++로 개발된 고성능 프록시 사이드카로 dynamic service discovery, load balancing, TLS termination, circuit breaker..등등의 기능을 포함합니다. Envoy가 만들어질 때 목표는 다음과 같았습니다. </p><blockquote><p><em>“The network should be transparent to applications. When network and application problems do occur it should be easy to determine the source of the problem.”</em></p><p>네트워크는 애플리케이션에<em> <strong>투명</strong>해야하며, 장애가 발생했을시 <em>*어디에서 문제가 발생했는지 쉽게 파악</em></em>할 수 있어야 한다.</p></blockquote><p>주요 기능은 다음과 같습니다.</p><ul><li>HTTP, TCP, gRPC 프로토콜을 지원</li><li>TLS client certification 지원</li><li>HTTP L7 라우팅 지원을 통한 URL 기반 라우팅, 버퍼링, 서버간 부하 분산량 조절 등</li><li>HTTP2 지원Auto retry, circuit breaker, 부하량 제한등 다양한 로드밸런싱 기능 제공</li><li>다양한 통계 추적 기능 제공 및 Zipkin 통합을 통한 MSA 서비스간의 분산 트렌젝션 성능 측정 제공함으로써 서비스에 대한 다양한 가시성 (visibility)을 제공</li><li>Dynamic configuration 지원을 통해서 중앙 레파지토리에 설정 정보를 동적으로 읽어와서 서버 재시작없이 라우팅 설정 변경이 가능함</li><li>MongoDB 및 AWS Dynamo 에 대한 L7 라우팅 기능 제공</li></ul><p>이렇게 다양한 기능을 가진 Envoy는 배포 위치에 따라 지원하는 기능이 달라집니다. 요약하자면 다음 그림과 같습니다.</p><p><img src="https://t1.daumcdn.net/cfile/tistory/991F05505BF8066419" alt=""></p><p>서비스 매쉬 관점에서 보자면 데이터 플레인은 이 Envoy를 서비스 옆에 사이드카로 배포해서 이를 통해 서비스로 들어오고 나가는 트래픽을  통제하는 것입니다.</p><p><br></br></p><h2 id="Istio-As-a-Service-Mesh"><a href="#Istio-As-a-Service-Mesh" class="headerlink" title="Istio As a Service Mesh"></a>Istio As a Service Mesh</h2><p><img src="https://www.solo.io/wp-content/uploads/2022/09/Istio_Architecture_Components_1.svg" alt="Istio 1.5버전 이후로 Istiod로 컨트롤 플레인이 통합되었다" style="zoom:80%;" /></p><p>Istio로 구성된 Service Mesh를 살펴보면 다음과 같습니다. Proxy로 Envoy를 사용하고 있고 그 밑에 Istio로 구성된 다양한 컴포넌트들이 있습니다. 자세히 보니 Control Plane이라고 되어있네요. 앞 단에서 Envoy Proxy를 통해서 트래픽이 전달되면 뒤에서 Envoy로 구성된 <strong>데이터 플레인을 컨트롤</strong>해 줄 것이 필요해집니다. 이 역할을 해주는 것이 바로 Control Plane인데, Istio는 이 Control Plane에 해당하는 오픈 소스입니다. 그럼 이 Control Plane, Istio는 어떤 역할을 하는지, 어떤 기능이 있는지 알아보겠습니다.</p><h3 id="Pilot"><a href="#Pilot" class="headerlink" title="Pilot"></a>Pilot</h3><p>파일럿은 Envoy에 대한 설정을 관리해주는 역할을 합니다. 주요 기능으로는 서비스 디스커버리, 서비스들의 엔드포인트 주소를 얻을 수 있습니다. 트래픽의 경로를 컨트롤하고 싶다면 파일럿을 통해서 서비스에서 서비스로 호출하는 경로를 조정할 수 있습니다. 이외에도 재시도(retry), 장애 전파를 막기 위한 써킷 브레이커 (Circuit breaker), Timeout 등의 기능이 있습니다.</p><h3 id="Mixer"><a href="#Mixer" class="headerlink" title="Mixer"></a>Mixer</h3><p>믹서는 액세스 제어와 정책을 관리하며 각종 모니터링에 필요한 지표들을 수집합니다. 서비스의 총 처리량을 정책으로 지정하면 그 처리량 이상으로 요청을 못받게 하거나 특정 헤더값이 일치해야 요청을 받을 수 있게 할 수 있습니다.</p><h3 id="Citadel"><a href="#Citadel" class="headerlink" title="Citadel"></a>Citadel</h3><p>시타델은 “성(Castle)” 이라는 말로써, 단어 뜻에서 유추할 수 있듯, 보안과 관련이 되어 있습니다. 사용자 인증(Authentication)과 인가(Authorization)와 관련된 역할을 수행합니다. Istio의 모든 트래픽은 TLS를 통해서 암호화가 가능한데 이 암호화에 필요한 인증서를 관리합니다.</p><h3 id="Gallery"><a href="#Gallery" class="headerlink" title="Gallery"></a>Gallery</h3><p>Istio 설정을 Validation, Ingestion, Processing, Distribution 하는 역할을 합니다. 즉, Istio의 구성 및 설정을 검증하고 배포 관리를 진행합니다.</p><p><img src="https://t1.daumcdn.net/cfile/tistory/99D9B3375BF8090D27" alt="1.5이전의 버전" style="zoom:25%;" /></p><blockquote><p>Istio 1.4버전까지는 위에 4가지 요소로 나뉘어져 있었으나 그 이후에는 <strong>Istiod</strong> 라는 하나의 모듈로 통합되었습니다. </p><p>istiod는 Mixer 가 없어지고, Pilot 이 Mixer 의 기능까지도 함께 수행합니다.</p></blockquote><h4 id="Istiod"><a href="#Istiod" class="headerlink" title="Istiod"></a>Istiod</h4><p>Istio 1.5 이상 버전에서 제공되며 서비스 디스커버리, 설정관리, 인증관리 등을 수행합니다.</p><ul><li>트래픽 동작을 제어하는 라우팅 규칙을 Envoy 전용 설정으로 변환하고 마이크로 서비스에 사이드카 방식으로 Envoy를 배포합니다.</li><li>Envoy 설정 변경을 통해 서비스 매쉬 트래픽을 제어합니다.</li><li>내장된 Identity나 Credential Management(증명 관리)을 통해서 강력한 서비스 간 인증 및 사용자 인증 기능을 지원합니다.</li><li>인증 기관의 역할을 수행하며, 데이터 플레인에서 안전한 mTLS 통신을 허용하는 인증서를 생성합니다.</li></ul><p><br></br></p><h3 id="Istio의-주요-기능"><a href="#Istio의-주요-기능" class="headerlink" title="Istio의 주요 기능"></a>Istio의 주요 기능</h3><p>Istio의 주요 기능으로는 트래픽 통제와 서비스간 안정성 제공 (Resilience), 보안 그리고 모니터링입니다. </p><p>Istio는 트래픽 분할이 가능한데, 이를 통해 들어오는 트래픽의 양을 조절할 수 있습니다. 카날리 테스트 등 배포 테스트를 이를 통해 수행할 수 있게됩니다. 또한 위에서 잠깐 살펴봤듯이 컨텐츠 기반의 트래픽 분할이 가능해서, 예를 들어 헤더의 User-Agent값에 따라서 Android 유저면 안드로이드 쪽으로 라우팅하고, IOS유저면 Ios쪽으로 라우팅할 수 있습니다.</p><p>Istio의 <strong>파일럿</strong>을 통해서 헬스체크 및 서비스 디스커버리의 기능을 수행할 수 있습니다. 파일럿은 여러 대상 인스턴스에 대해서 로드밸런싱하고 이 서비스들에 대해 Health 상태를 주기적으로 점검하고 문제가 있다면 서비스에서 제거해버립니다. </p><p><img src="https://t1.daumcdn.net/cfile/tistory/999D603E5BF8090D04" alt="" style="zoom:67%;" /></p><p>Istio의 중요한 특징 중 하나는 기본적으로 envoy를 통해서 통신하는 모든 트래픽을 자동으로 TLS를 이용해서 암호화한다는 것입니다. 서비스 간 통신이 기본적으로 TLS로 암호화 되기 때문에 이후에 설명하겠지만, Kubeflow에 적용하기 위해서는 꼭 Https 설정을 해주어야 합니다.</p><p><img src="https://istio.io/v1.3/docs/concepts/security/architecture.svg" alt="Istio Security Architecture"></p><p>위에서 인증서를 관리한다고 설명한 Citadel에서 인증서를 가져와서 이를 이용해 TLS통신을 하게 됩니다.</p><p>모니터링은 Mixer에서 담당하게 됩니다. 다음 그림에서와 같이 서비스에 Proxy가 있고 이것을 믹스로 보내게 됩니다.</p><p><img src="https://lh3.googleusercontent.com/YVD5DUh3MfsKQ2F9fCZvfI8jwU266n6z0WBjGLeoiF7N6VE1Uoed4kOG5vSS_LbvonsVkYC6H2bRpDf6QWxt7OiIRsxfpIdM4AIWw9AzxQD1uqMtpc5YAFZhw2qa5e8W1H3wAVnO" alt="" style="zoom:67%;" /></p><p>서비스 A가 서비스 B를 호출할 때 호출 트래픽은 각각의 Proxy를 타게 됩니다. 호출을 하게되면 응답 시간과 서비스의 처리량과 같은 정보가 Mixer로 전달되고 전달된 각종 지표는 Mixer에 연결된 Logging Backend에 저장됩니다. 이렇게 저장된 로그들은 GCP의 Stackdriver, AWS, Prometheus, DataDog 등으로 플러그인을 통해 쉽게 전달이 가능합니다.</p><p><br></br></p><h2 id="Kubeflow에서-Istio"><a href="#Kubeflow에서-Istio" class="headerlink" title="Kubeflow에서 Istio"></a>Kubeflow에서 Istio</h2><p>Kubeflow를 설치해보고 조금이라도 사용해보면 알겠지만, 정말 다양한 ML을 위한 도구들이 모여져있습니다. 이런 도구와 프레임워크, 서비스의 모음은 각각 독립적으로 개발이 되어 있고 서로 조금씩 다른 부분들을 돕고 있습니다. 좀 더 완전하고 ML개발 환경을 만들고 싶다면 여러 서비스와 구성 요소들을 결합하는 것은 필수적입니다. Kubeflow는 이러한 구성요소들의 모음을 결합할 수 있는 기본 인프라를 제공하고 있고, <strong>Istio를 통해 마이크로서비스를 보호하고 연결하며, 모니터링합니다.</strong></p><ul><li>강력한 ID 기반 인증 및 권한 부여를 사용하여 Kubeflow 배포에서 서비스 간 통신을 보호합니다.</li><li>액세스 제어 및 할당량을 지원하기 위한 정책 레이어</li><li>클러스터 ingress 및 egress를 포함하여 배포 내 트래픽에 대한 자동 메트릭, 로그 및 추적이 가능합니다.</li></ul><p><img src="https://v1-6-branch.kubeflow.org/docs/images/Istio-in-KF.svg" alt="Istio in Kubeflow" style="zoom:80%;" /></p><p>Kubeflow에서 Istio가 사용되는 모습은 위의 그림과 같습니다. 사용자가 각 컴포넌트에 접속하고 사용하기 위해 인증하고 역할을 제공하는데에 Istio가 사용되는 것이라고 보면 그림이 이해가 되실 것입니다. 이 방식은 다음과 같습니다.</p><ol><li>User Request는 IAM on Cloud Services Provider 또는 Active Directory/LDAP 온프레미스와 같은 SSO 서비스 공급자와 통신하는 식별 프록시에 의해 가로채어집니다.</li><li>사용자가 인증되면 User ID가 포함된 JWT 헤더 토큰을 포함하도록 요청이 Istio 게이트웨이에 의해 수정됩니다. 서비스 메쉬 전체의 모든 요청은 이 토큰을 전달합니다.</li><li>Istio RBAC 정책은 서비스 및 요청된 네임스페이스에 대한 액세스를 검증하기 위해 수신 요청에 적용됩니다. 사용자가 둘 중 하나에 액세스할 수 없으면 오류 응답이 다시 전송됩니다.</li><li>요청이 확인되면 해당 컨트롤러(이 경우 Notebook 컨트롤러)로 전달됩니다.</li><li>노트북 컨트롤러는 Kubernetes RBAC로 권한 부여를 검증하고 사용자가 요청한 네임스페이스에 Notebook Pod를 생성합니다.</li></ol><p>네임스페이스에서 Training 작업 또는 기타 리소스를 생성하기 위해 노트북을 사용하는 사용자의 추가 작업은 유사한 프로세스를 거칩니다. 프로필 컨트롤러는 프로필 생성을 관리하고 적절한 Istio 정책을 생성 및 적용합니다. </p><blockquote><p><strong>현재 Istio 없이 Kubeflow를 배포할 수 없습니다</strong>. Kubeflow는 게이트웨이에서 생성된 노트북에 액세스하기 위한 새 경로를 표현하기 위해 Istio용 사용자 지정 리소스 정의(CRD)가 필요합니다.</p></blockquote><p><br></br></p><h3 id="Https"><a href="#Https" class="headerlink" title="Https"></a>Https</h3><p>Istio를 사용하기 때문에 기본적으로 Kubeflow를 <strong>https</strong>설정이 되어있지 않다면, 기능을 제대로 수행할 수 없습니다. 인증서 없이 http로 접근한다면, 각 컴포넌트를 이용하는데 에러가 뜨거나 제약이 발생합니다. 그래서 Kubernetes를 사용한다면 Ingress 설정시에 Https 설정을 위한 <code>Annotation</code>을 꼭 설정해주어야 하고 Kubeflow 도메인에 대한 인증서를 미리 발급해놓아야 합니다. 인증서에 대한 내용은 온프레미스나 클라우드 Provider에 따라 다르니 생략하고, <code>AWS</code> <code>EKS</code> 기준 Annotation에 대한 설정 정보를 남겨놓겠습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Annotation Reference:</span></span><br><span class="line"><span class="comment"># https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.4/guide/ingress/annotations/</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ingress-kubeflow-central-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">istio-system</span>  <span class="comment"># this ingress should be in the same namespace as &#x27;istio-ingressgateway&#x27; service</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/load-balancer-name:</span> <span class="string">kubeflow-central-dashboard-lab</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internal</span>  <span class="comment"># internal load balancer</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span>  <span class="comment"># connect to pod directly</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/subnets:</span>  &#123;&#125; <span class="comment"># subnets of EKS</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/listen-ports:</span> <span class="string">&#x27;[&#123;&quot;HTTP&quot;: 80, &quot;HTTPS&quot;: 443&#125;]&#x27;</span>  <span class="comment"># listener for alb</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/ssl-redirect:</span> <span class="string">&#x27;443&#x27;</span> <span class="comment"># enables SSLRedirect and specifies the SSL port that redirects to</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">/healthz/ready</span>  <span class="comment"># health check path of istio-ingressgateway</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/healthcheck-port:</span> <span class="string">&#x27;15021&#x27;</span>  <span class="comment"># health check port of istio-ingressgateway</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="comment"># Attach a certificate for &#x27;kubeflow.dev.data.kurlycorp.kr&#x27; or &#x27;*.dev.data.kurlycorp.kr&#x27; to the ALB</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">kubeflow.hyubyworld.co.kr</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">istio-ingressgateway</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://cloud.google.com/learn/what-is-istio?hl=ko">https://cloud.google.com/learn/what-is-istio?hl=ko</a></li><li><a href="https://istio.io/">https://istio.io/</a></li><li><p><a href="https://medium.com/dtevangelist/service-mesh-%EB%9E%80-8dfafb56fc07">https://medium.com/dtevangelist/service-mesh-%EB%9E%80-8dfafb56fc07</a></p></li><li><p><a href="https://bcho.tistory.com/">https://bcho.tistory.com/</a></p></li><li><a href="https://gruuuuu.github.io/cloud/service-mesh-istio/">https://gruuuuu.github.io/cloud/service-mesh-istio/</a></li><li><a href="https://v1-6-branch.kubeflow.org/docs/external-add-ons/istio/istio-in-kubeflow/">https://v1-6-branch.kubeflow.org/docs/external-add-ons/istio/istio-in-kubeflow/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/10/02/kubeflow-istio/#disqus_thread</comments>
    </item>
    
    <item>
      <title>jovyan root 문제를 해결해보자</title>
      <link>http://tkdguq05.github.io/2022/09/18/jovyan-sudoers/</link>
      <guid>http://tkdguq05.github.io/2022/09/18/jovyan-sudoers/</guid>
      <pubDate>Sun, 18 Sep 2022 12:07:48 GMT</pubDate>
      <description>
      
        &lt;p&gt;JupyterHub나 Kubeflow를 다룰때 마주칠 수 밖에 없는&lt;/p&gt;
&lt;p&gt;jovyan root권한 문제를 해결해보는 글입니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>JupyterHub나 Kubeflow를 다룰때 마주칠 수 밖에 없는</p><p>jovyan root권한 문제를 해결해보는 글입니다.</p><span id="more"></span><h1 id="Jovyan"><a href="#Jovyan" class="headerlink" title="Jovyan"></a>Jovyan</h1><p><img src="/Users/mk-mac-330/Library/Application Support/typora-user-images/스크린샷 2022-09-18 오후 9.10.22.png" alt="jovyan에 격분하는 사람들"></p><p>구글에 jovyan만 쳐봐도 격정이 느껴지는 글들이 참 많습니다. 왜냐하면 jupyter를 다룰 때 jovyan이라는 user가 생성되고 이 유저에 root권한이 필요한 경우가 많은데, jovyan 유저에 대한 패스워드를 공개하고 있지 않기 때문입니다. 그래서 root권한을 얻기위해서 여러가지 삽질을 정말 많이 시도를 했었고, 관련 글도 수도없이 봐왔습니다.</p><p>*Jovyan이 뭔지 잠깐 알아보자면</p><ul><li><a href="https://ko.wiktionary.org/wiki/목성">목성</a>의.</li><li>Jovian planets 목성형 행성</li><li><em>noun</em> – an inhabitant of Jupyter</li></ul><p>을 뜻하는데, jupyter를 사용하는 유저들을 지칭하기 위해 만든 말이라고 합니다.</p><h2 id="sudoer"><a href="#sudoer" class="headerlink" title="sudoer"></a>sudoer</h2><p>그 전에 알아야할 개념이 있습니다. 바로 sudoer입니다. 먼저 sudo란 특정 사용자의 권한을 얻어서 수행할 수 있게 하는 명령어 입니다. 그리고 일반 사용자에게 sudo 권한을 부여하기 위해서는 /etc/sudoers 파일에 계정을 설정해 주어야 합니다. 그렇다면 jovyan이 sudo권한을 갖기 위해서는? 바로 /etc/sudoers에 jovyan을 등록시켜주면 됩니다.</p><p>저는 Dockerfile에서 수정이 필요했습니다. Kubeflow든 JupyterHub든 kubernetes 상에서 운영을 하고 있었거든요.</p><p>그래서 <code>RUN printf &#39;\n# jovyan user can assume sudo privilege\njovyan  ALL=(ALL:ALL) NOPASSWD:ALL&#39; &gt;&gt; etc/sudoers</code> 를 Dockerfile에 넣어줌으로써, /etc/sudoers에 jovyan을 올릴 수 있게 되었습니다. 저대로 올라간다면</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># User privilege specification</span><br><span class="line">root    ALL=(ALL:ALL) ALL</span><br><span class="line">jovyan  ALL=(ALL) NOPASSWD:ALL </span><br></pre></td></tr></table></figure><p>이렇게 되겠네요.</p><p>여러 우회 방법들이 있습니다만, 제 기준에서는 이게 가장 깔끔하고 쉬웠던 것 같습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/09/18/jovyan-sudoers/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또콘 with Kurly</title>
      <link>http://tkdguq05.github.io/2022/09/01/geultto-con/</link>
      <guid>http://tkdguq05.github.io/2022/09/01/geultto-con/</guid>
      <pubDate>Thu, 01 Sep 2022 02:29:05 GMT</pubDate>
      <description>
      
        &lt;p&gt;제 1회 글또콘을 컬리에서 진행했다!&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>제 1회 글또콘을 컬리에서 진행했다!</p><span id="more"></span><h2 id="제-1회-글또콘"><a href="#제-1회-글또콘" class="headerlink" title="제 1회 글또콘"></a>제 1회 글또콘</h2><p>지난 8월 25일에는 제 1회 글또콘이 열렸습니다. 글또에 여러번 참여하면서 오프라인 행사를 했으면 좋겠다… 싶었는데, 마침 운영진으로 참여하게되었고 또 일하고 있는 회사에서 진행하게 되어 뜻 깊었던 행사였습니다. </p><p>정말 많은 분들이 오셔서 발표를 들어주시고 네트워킹을 진행해주셨습니다. 코로나만 아니었다면 진작에 이런 모임을 여러번 했을 것 같은데… 모여서 두런두런 얘기를 나누시는 모습을 보면서 남다른 감회를 느꼈습니다.</p><p>발표를 해주신 분들도 너무 잘 해주셔서 좋았습니다. 글또 뿐만 아니라 다른 발표에 가셔서 충분히 발표를 하실 수 있으실 만큼 내용도 훌륭하고 발표 스킬도 좋았던 것 같습니다. </p><p><br></br></p><h2 id="준비"><a href="#준비" class="headerlink" title="준비"></a>준비</h2><p>글또콘 기획은 운영진 회의때 하게 되었습니다. 글또 7기 운영진이 조직되고 초기에 어떤 걸 해볼까 하면서 회의를 진행했었죠. 글또의 본질인 글쓰기와 피드백도 좋지만, 워낙 좋은 분들이 많아 온라인 보다는 오프라인으로 교류를 많이 했으면 했습니다. 그래서 커피챗을 활성화 하는게 좋겠다 싶었고 또 글을 쓰다보니, 이 글을 좀만 다듬으면 발표를 할 수 있지 않을까 생각했습니다. 그래서 작은 발표를 하면 어떨까 하면서 의견을 냈고(글또 컨퍼런스), 이것이 발전되어 8월 25일까지 오게 되었습니다(글또콘). </p><p>가장 걱정되었던 것은 발표자들을 모집할 수 있는지와 장소였습니다. 일단 대략 몇 분 정도가 오실지 예측이 전혀 안되었는데, 글또에 인원자체가 많아서 큰 곳을 빌려서 진행하면 좋겠다는 생각이 들었습니다. 그러다가 회사에 와서 라운지에서 커피를 마시고 있었는데, 여기서 하면 괜찮을 것 같았습니다. 월마다 타운홀 미팅도 여기서 진행을 했었고, 가끔 개발관련 행사도 했던 것으로 알아서 대관을 해야겠다고 마음먹었고, 대관을 어떻게 해야하는지를 찾아봤습니다.</p><p><img src="/images/geultto_con/lounge.jpg" alt="평소의 라운지 및 카페" style="zoom:50%;" /></p><h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p><br></br></p><h2 id="대관-및-장비-대여"><a href="#대관-및-장비-대여" class="headerlink" title="대관 및 장비 대여"></a>대관 및 장비 대여</h2><p>장소 대관을 위해서 컬리 라운지 문의와 다른 회사들 문의를 동시에 진행했습니다. 성윤님께서 다른 회사들에 컨택을 해주셨고 저는 회사에 대관 문의를 날리고 기존 스레드에 관련 내용이 없나 확인을 해봤습니다. 결국 라운지 대관 담당자를 찾았고 문의를 드려보니 대관 관련 결재가 필요하다는 답을 얻었습니다. 이때부터 사실 쫄리기는 했지만, 결재라인에 있는 분들과 그래도 친분이 있는  편이었고 글또에 대해 궁금해 하시길래 글또 소개 자료와 더불어 어떤 커뮤니티인지, 어떤 성격의 행사인지를 설명드리게 되었습니다. 다행히도 커뮤니티에 대해서 긍정적으로 생각하셨습니다. 물론 기존에 글또 활동에 대해서도 말씀을 드려놨었고 글또에서 참고할 만한 글들을 공유드려왔었기 때문에 가능한 것이었습니다(모두 여러분 덕입니다!!).</p><p>미리 설득을 한 후에 결재를 올리게되었고, 순탄하게 결재승인이 나게 되었습니다. 결재라인에 있진 않았지만 다른 시니어 리더님들께도 설명드렸고, 좋은 행사 같다며 진행해보라고 답을 들었습니다(나중에 기술블로그도 한 번 써달라는 말씀과 함께ㅎㅎ ). 장비 대여와 1층 출입 통제 문제가 있었는데, 25일이 마침 타운홀이 있어서 끝나고 말씀드려보기로 했습니다.</p><p>필요한 장비는 대략 빔 프로젝트, 마이크, 빔 프로젝트 컨트롤러, 리모컨, HDMI 케이블 등등 이었습니다. 나머지 필요한 것들은 성윤님이나 다른 분들이 와서 챙겨주시기로 했고 글또콘 당일이 되었습니다.</p><p><br></br></p><h2 id="글또콘-진행"><a href="#글또콘-진행" class="headerlink" title="글또콘 진행"></a>글또콘 진행</h2><p>타운홀 미팅이 끝나고 16시쯤 인사총무팀에서 DM이 왔습니다. 라운지로 올라가서 마이크나 빔 프로젝트 사용법을 듣고, 행사 시 주의할 점 등을 가이드 받았습니다. 놓칠 수 있는 점들을 친절하게 잘 알려주셔서 별 문제 없이 행사를 마무리 지을 수 있었던 것 같습니다. 이왕 오신 김에 글또 굿즈와 더불어 컬리 스티커를 배부하고 싶었는데 행사용 스티커는 따로 없다고 하셔서 조금 아쉬운 부분이 있었습니다ㅠㅠㅠ. 그래도 글또 분들이 라운지 입구에서 사진을 많이 찍어주셔서 위안을 삼았습니다.</p><p>필요한 장비를 받고 미팅을 진행하다 보니 5시 반쯤이 넘어서 성윤님이 오셨고 다른 운영진 분들도 속속 도착하기 시작했습니다. 글또 굿즈를 어떻게 배부할지, 명찰은 어떻게 배부할지, 피자는 어떻게 나눠드려야 할지 등등을 긴급하게 정했습니다. 전날부터 정하긴 했지만 현장에 와보니 좀 더 고려해야할 점들이 있어서 정리하느라 시간을 보냈습니다. </p><p>내부 슬랙을 통해 행사공지를 하긴 했지만 회사 분들이 남아계셔서 행사가 있다는 것을 알리기 위해 빔 프로젝트를 연결하고 글또콘 안내 ppt페이지를 하나 만들어서 얼른 띄워놨습니다. 행사에 대해서 궁금해하시는 분들도 있었고 직접 오셔서 어떤 커뮤니티인지 물어보시는 분들도 있었습니다. 회사 분들이 슬슬 떠나감과 동시에 글또 분들이 오시기 시작했습니다. 이때 피자가 제 시간에 오지 않아서 살짝 불안한 감이 있었는데 그래도 행사 시작 전에 와서 오시는 분들께 하나씩 나눠드릴 수 있게 되었습니다. 소스들도 같이 나눠드렸어야 했는데 정신이 없어서 나중에 챙겨드리게 되었네요. </p><p>피자까지 도착하고 하나씩 나눠드리면서 이제 발표 준비와 행사 진행에 대해서 논의를 하게 되었는데, 어쩌다보니 진행MC를 맡게 되었습니다. 행사 진행을 몇 번 해보긴 했지만 생각보다 많은 분들이 참석해주셔서 조금씩 떨리기 시작했습니다. 오프닝때 컬리에 대한 소개나 개발팀, 데이터팀에 대해서도 말씀드리고 싶었는데 느끼신 분도 있으시겠지만, 떨려서 빠르게 인트로를 마치고(어버버버) 바로 발표로 들어가게 되었습니다.</p><p><img src="/images/geultto_con/owner1.jpg" alt="첫 발표를 해주시는 성윤님" style="zoom:80%;" /></p><p>발표가 순조롭게 진행될까? 듣는 분들이 잘 들어주실까? 걱정을 많이 했는데 생각보다 더 발표도 잘 진행되었고 청중 분들의 리액션도 너무 좋았어서 진행하는 내내 신이나고 재밌었습니다. 진행도 했어야 했어서 발표를 온전히 집중해서 듣지는 못했지만, 발표해주신 분들 모두 너무 너무 잘해주셨고 멋있게 발표를 마무리 지어주셨습니다. 듣고 계시는 다른 글또 분들도 너무 잘 들어주시고 집중해서 보고 계셔서 뿌듯 뿌듯 했습니다.</p><p><img src="/images/geultto_con/airflow.jpg" alt="airflow 발표를 해주고 계신 학건님" style="zoom:80%;" /></p><p><img src="/images/geultto_con/haenara.jpg" alt="마지막 발표를 해주시는 해나라님" style="zoom:80%;" /></p><p>(현구님 사진은 못찍었네요..ㅠㅠㅠ 죄송)</p><p><br></br></p><h2 id="후기"><a href="#후기" class="headerlink" title="후기"></a>후기</h2><p>글또콘을 할 수 있을까? 하면서 시작을 했는데 막상 끝나고 나니 해냈다..!! 라는 생각과 함께 굉장히 뿌듯한 기분이었습니다. 장소 대관이 무난하게 되서 참 다행이라고 생각되었습니다. 대관 가이드나 안내를 너무나 잘해주신 인사총무팀께 감사하다는 말씀을 드리고 싶습니다. </p><p>이렇게 큰 오프라인 행사 준비는 처음 해봐서 어떻게 해야할지 감이 잘 안왔지만, 참여하고자 하는 분들이 있으면 준비가 그래도 쉬워지는 것을 느꼈습니다. 글또에 대해서 애정을 많이 갖고 계시는 분들이 많아서 행사를 잘 마칠 수 있었던 것 같네요. 행사를 오후 7시부터 하고 4개의 발표를 진행하느라 시간이 꽤 지난 와중에 네트워킹 시간에도 잘 참여해주셔서 너무 감사했습니다. 대부분의 참석자 분들이 가실 줄 알았는데, 많은 분들이 남아주셔서 얘기를 해주시는 것을 보고 오프라인 행사에 대한 니즈가 있었구나를 깨닫게 되었습니다. 이전 기수에서도 오프라인 행사를 기대해주신 분들이 있었는데, 코로나로 오프라인 모임을 못하게 되는 기간이 길어지면서 이번 기수에는 참여를 못하게 된 분들이 있으셔서 참 아쉬웠습니다. 코로나 상황이 좀 더 빨리 나아졌으면 어땠을까 싶었습니다. </p><p>네트워킹 시간에 다른 채널분들과는 얘기를 거의 못했지만, 그래도 머신러닝-데이터엔지니어 채널 분들과 얘기도 하고 안면을 틀 수 있어서 참 좋았습니다. 채널 통합 모임을 한 번 가져야지..! 기획을 해보려다 일정이 바빠지면서 손을 놓게되었는데 글또콘을 통해 채널 분들과 결국 만나게 되었네요 너무 반가웠습니다. 네트워킹 시간이 더 길었다면 여기저기 인사라도 드릴텐데 집에는 가야되니까…(출퇴근 1시간 반ㅠㅠ) 10시 10분쯤 마무리하게 되었습니다.</p><p>행사를 기획하고 준비하는 과정은 처음이었는데 참 의미있는 시간이었습니다. 같이 준비해주신 운영진 분들 정말 감사하고 행사 진행하면서 정말 바빴지만 꽤 재밌었습니다ㅎㅎ! 이제 정말 얼마 안남았는데 남은 글또 회차도 화이팅해서 좋은 교류 이어나갔으면 합니다. 글또 여러분 참여해주셔서 감사했습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/09/01/geultto-con/#disqus_thread</comments>
    </item>
    
    <item>
      <title>karpenter에 대한 설명, eks에 적용하기</title>
      <link>http://tkdguq05.github.io/2022/08/20/karpenter/</link>
      <guid>http://tkdguq05.github.io/2022/08/20/karpenter/</guid>
      <pubDate>Sat, 20 Aug 2022 08:09:19 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubernetes 노드 관리를 편리하게. Karpenter&lt;/p&gt;
&lt;p&gt;(Thumbnail image is generated from Dall-e(&lt;a href=&quot;https://labs.openai.com/&quot;&gt;https://labs.openai.com/&lt;/a&gt;))&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Kubernetes 노드 관리를 편리하게. Karpenter</p><p>(Thumbnail image is generated from Dall-e(<a href="https://labs.openai.com/">https://labs.openai.com/</a>))</p><span id="more"></span><h1 id="Karpenter"><a href="#Karpenter" class="headerlink" title="Karpenter"></a>Karpenter</h1><h2 id="🪚-Intro"><a href="#🪚-Intro" class="headerlink" title="🪚 Intro."></a>🪚 <strong>Intro.</strong></h2><hr><p><strong>AWS</strong>에서 만들고 운영중인 프로젝트로 빠르게 버전이 올라가고 있습니다. 글을 작성하는 와중에도 0.14에서 0.15버전이 업데이트 되었습니다. </p><p>AWS re:Invent 2021 행사에서 Karpenter v0.5이 드디어 정식으로 오픈되었고 GA(Generally Available)로 릴리스 되었습니다. 정식으로 오픈되었다 보니, 이것을 가져다가 운영환경에서도 사용을 하는 것도 가능합니다.</p><p>Karpenter는 간단히 말하면, Kubernetes의 worker node 자동 확장 기능을 담당하는 오픈소스 프로젝트라고 할 수 있는데, 기존의 Node Auto Scaling과 비교해서 어떤 장점이 있는지를 알아보도록 하겠습니다.</p><p><br></br></p><h1 id="Kubernetes의-장점-Node-Auto-Scaling"><a href="#Kubernetes의-장점-Node-Auto-Scaling" class="headerlink" title="Kubernetes의 장점, Node Auto Scaling"></a><strong>Kubernetes의 장점, Node Auto Scaling</strong></h1><p>먼저 쿠버네티스에 대해서 간략하게 알아보도록 하겠습니다. 쿠버네티스는 컨테이너 관리 플랫폼으로 빠른 배포와 뛰어난 확장성을 가졌습니다. 쿠버네티스를 활용해서 배포 관리를 하면 정말정말 편한 부분이 많지만, 운영 부담은 줄여야 하는게 중요합니다. 리소스가 모자라서 파드가 안 뜬다던가 하면 곤란해지거든요. 그래서 AWS를 사용하는 회사에서는 Managed Kubernetes 서비스인 EKS를 통해서 노드 관리의 부담을 줄이려고 하고 있습니다.</p><p>결국 EKS 통해서 <a href="https://kubernetes.io/ko/docs/concepts/overview/components/">Control Plane</a> 운영 부담을 줄여야 하는 게 중요한데, 특히 컨테이너 사용량에 따라 지속적으로 Computing Node를 빠르게 추가하고 줄일 수 있는 Elasticity가 필요하게 됩니다. 이 개념에 딱 맞는게 AWS에 이미 존재하는데 바로 EC2 인스턴스입니다. 추가적인 노드가 필요할 때 Elastic한 성질을 지닌 AWS 컴퓨팅의 기본 단위인 EC2(Amazon Elastic Compute Cloud)를 불러와서 이것을 노드로 붙이는 것입니다. 그리고 여기에 Auto Scaling을 적극적으로 활용하면서 Worker node의 운영 부담을 줄일 수 있게 되는 것입니다.</p><p>EKS에서는 Auto Scaling이 있는데 왜 그럼 Karpenter를 굳이 사용하려고 하는 건가요? 이제 그 이유를 차근차근 알아보겠습니다.</p><p><br></br></p><h2 id="Karpenter에-대한-짧막한-소개"><a href="#Karpenter에-대한-짧막한-소개" class="headerlink" title="Karpenter에 대한 짧막한 소개."></a><strong>Karpenter에 대한 짧막한 소개.</strong></h2><p>Karpenter는 신규 배포될 pod를 지속적으로 체크하고 Worker Node가 부족하면 자동으로 Worker node를 추가배포하고 확장하는 역할을 담당합니다. 추가적인 노드를 확장하는 것 뿐만 아니라, 불필요한 Worker node도 정리하기도 합니다. 따라서, 노드의 비용 효율화와 운영 부담을 최소화 하기 위한 자동화 도구로 Karpenter를 사용할 수 있게 되는 것입니다. 자세한 설명은 아래에서 계속 하도록 하고 이제 기존의 Auto Scaling 방식은 어떤 것이 있는지, 어떻게 이루어지는 지에 대해서 알아보겠습니다.</p><p><br></br></p><h2 id="기존의-Node-Auto-Scaling"><a href="#기존의-Node-Auto-Scaling" class="headerlink" title="기존의 Node Auto Scaling"></a><strong>기존의 Node Auto Scaling</strong></h2><p>기존의 노드 오토스케일링 방법은 <strong>CA</strong>(Cluster Auto Scaler)가 대표적입니다.</p><p>CA는 Cloud Provider(AWS, GCP)마다 각자 다른 방법으로 지원합니다. AWS의 경우에는 EC2 Auto Scaling Group을 사용하여 CA를 구현하는데, EKS에서 제공되는 Node Group기능은 다음과 같습니다.</p><p><strong>Node Group</strong></p><ul><li>Worker Node를 그룹핑하여 관리하는 기능 제공</li><li>Auto Scaling Group과 Launch Template으로 구현됨<ul><li>Launch Template에 어떤 스펙의 Instance, AMI를 사용할 것인지 작성되어 있음.</li></ul></li><li>Worker Node들은 ASG를 통해 확장하는 구조</li><li>ASG를 Cluster Autoscaler가 컨트롤</li></ul><p><img src="/images/karpenter/ng.png" alt="Node Group"></p><p><br></br></p><h3 id="Node-Group의-자동-확장-시나리오"><a href="#Node-Group의-자동-확장-시나리오" class="headerlink" title="Node Group의 자동 확장 시나리오"></a><strong>Node Group의 자동 확장 시나리오</strong></h3><p>그렇다면 이 Node Groupb이 어떻게 Node를 자동으로 확장하는 것인지 알아보겠습니다. 가상의 상황을 가정해보겠습니다. pod가 가득 찼고 더 이상 pod를 배포할 수 없는 상태입니다. 이런 상태에서 신규 pod 생성 요청이 왔습니다. 대략 Airflow에서 KubernetesPodOperator를 통해서 새 파드를 통해 작업을 하려는 것으로 상상해보겠습니다(이 Operator는 Airflow를 통해 작업을 하려고 할때, 작업을 워커에서 수행하지 않고 새 파드를 띄워서 해당 파드에서 작업하도록 합니다).  Kube-Scheduler는 신규 Pod를 배치할 적절한 Node를 선정하려고 합니다. </p><aside>💡 **kube-scheduler**는[노드](https://kubernetes.io/ko/docs/concepts/architecture/nodes/)가 배정되지 않은 새로 생성된 [파드](https://kubernetes.io/ko/docs/concepts/workloads/pods/) 를 감지하고, 실행할 노드를 선택하는 컨트롤 플레인의 컴포넌트입니다. 스케줄링 결정을 위해서 고려되는 요소는 리소스에 대한 개별 및 총체적 요구 사항, 하드웨어/소프트웨어/정책적 제약, 어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세, 데이터 지역성, 워크로드-간 간섭, 데드라인을 포함합니다.</aside><p>이제 다양한 Label 정보, 가용량 등을 통해서 pod가 배치될 대상 노드를 필터링 하고 최종으로 자리잡을 노드를 선정하려고 합니다. 이 노드 선정 전까지는 pod는 Pending상태로 나오게 됩니다(Unschedulable pod). 이 상태에서 머물다가 적당한 노드가 없다면 pod의 프로비저닝은 실패하게 됩니다. pod의 상태를 확인하고 노드 선정에 실패하게 된다면 Node Group의 ASG값 중 Desired Capacity 값을 수정하면서 워커 노드의 개수를 증가시도록 설정합니다. </p><p><img src="/images/karpenter/ng_1.png" alt=""></p><p>ASG는 수정된 Desired Capacity값을 읽고 EC2 워커 노드를 추가로 배포하게 됩니다. 이것은 Kubernetes와는 무관하고, AWS의 ASG에 의해 실행된 작업입니다. EC2 AMI, Instance Type은 AWS에 정의된 Launch Template에 있는 내용을 따르게 됩니다. 배포가 완료되어 노드가 Ready 상태가 되면 kube-scheduler는 배포되고 있지 못하던 pod를 새로운 워커 노드로 할당시킵니다. kube-apiserver에 해당 정보를 전달하고 kube-apiserver는 워커 노드에서 실행중인 kubelet에게 pod 배포 명령을 보내게 되고 파드가 해당노드에 프로비저닝 됩니다.</p><p><img src="/images/karpenter/ng_2.png" alt=""></p><p><br></br></p><h2 id="Karpenter의-동작방식"><a href="#Karpenter의-동작방식" class="headerlink" title="Karpenter의 동작방식"></a><strong>Karpenter의 동작방식</strong></h2><p>Karpenter는 ASG와는 조금 다른 구조로 동작하게 됩니다. Karpenter는 AWS에서 개발은 했지만 Cloud Provider와 무관하게 동작 가능한 구조로 설계되었습니다. Karpenter는 지속적으로 신규 pod의 상태를 확인하고 필요하다면 워커노드의 배포와 삭제도 직접 수행하고 kube-scheduler를 대신해서 pod를 특정 워커 노드쪽으로 바인딩 되도록 하는 요청도 수행합니다.</p><p><strong>Karpenter의 자동 확장 시나리오</strong></p><p>위에서 살펴본 상황과 같이 가용공간이 없는 상태에서 신규 pod생성 요청이 왔습니다. kube-scheduler가 신규 pod 배치할 적정 노드를 선정하는 것과 필터링 하는 과정은 같습니다. Unschedulabel 한 pod를 프로비저닝 해주려고 하는 상황인 것입니다. </p><p><img src="/images/karpenter/karpenter_1.png" alt=""></p><p>워커 노드를 생성할 때 어떤 인스턴스 타입을 선택해서 배포할 것인지를 정하는 과정에서 Node Group을 사용하는 Auto Scaling과 차이점이 있습니다. Karpenter는 자체 Custom Resource인 Provisioner를 등록하고 이것에 의해서 새 노드가 어떤 스펙일지를 결정합니다. </p><p><img src="/images/karpenter/karpenter_2.png" alt=""></p><p>Karpenter는 결국 이 Provisioner를 통해 모든 워커 노드의 lifecycle을 결정할 수 있게 됩니다. Karpenter 역시 노드가 배포된 이후에 Ready상태가 되면 직접 pod를 새로운 워커 노드에 배포될 수 있도록 바인딩 요청을 하게 됩니다.</p><p><br></br></p><hr><h3 id="둘의-차이점"><a href="#둘의-차이점" class="headerlink" title="둘의 차이점?"></a>둘의 차이점?</h3><p>둘의 차이점을 본다면, CA는 ASG와 같이 노드확장 같은 작업을 할때 Cloud Provider가 제공하는 기능들과 연계할 수 밖에 없습니다. 그렇기 때문에 훨씬 더 많은 단계를 거쳐야 하게 되고, 이 과정 때문에 노드의 배포과정이 느리고 번거롭게 됩니다.</p><p>Karpenter는 Auto Scaling 할때 일어나는 많은 부분을 Karpenter가 직접처리 합니다. 기존 ASG대비 훨씬 심플한 구조로 빠르게 처리가 가능해지는 것입니다.</p><p><br></br></p><h2 id="🪚-Karpenter의-주요-개념"><a href="#🪚-Karpenter의-주요-개념" class="headerlink" title="🪚 Karpenter의 주요 개념"></a>🪚 <strong>Karpenter의 주요 개념</strong></h2><p>이제 이유를 알았고 둘의 차이를 대충 파악했으니, Karpenter의 주요 개념을 살펴보도록 하겠습니다. Karpenter는 크게 4가지 특성을 갖고 설명할 수 있습니다.</p><ul><li><strong>Watching :</strong> unschedulable한 pod를 계속 보고 있습니다.(파드 자체를 계속 체크)</li><li><strong>Evaluating :</strong> 스케쥴링 하는데 제약이 없는지를 확인합니다.</li><li><strong>Provisioning :</strong> 요구사항에 맞는 노드에 파드를 배포합니다.</li><li><strong>Removing :</strong> 더 이상 노드가 필요없다면 삭제합니다.</li></ul><p>그렇다면 무엇에 의해 노드가 생성되고 어떤 종류의 인스턴스일 것인지, 언제 노드를 삭제할 것인지 등등을 결정할 무엇인가가 필요합니다. 그 설정이 바로 Provisioner입니다.</p><p><br></br></p><h3 id="Provisioner"><a href="#Provisioner" class="headerlink" title="Provisioner"></a><strong>Provisioner</strong></h3><p>Provisioner는 Custom Resource에 대해 작성한 것으로서, 노드 프로비저닝에 대한 제약사항이나 노드가 필요없다고 판단할 설정들, 예를 들어 timeout과 같은 것들을 설정합니다. 여러 설정들이 많지만 주요 설정들을 살펴보겠습니다. </p><ul><li>taints : 프로비저닝 된 노드들에 테인트를 정합니다. 파드가 taint에 대한 toleration이 맞지 않다면 taint에 설정된 대로 이벤트가 발생합니다.<ul><li>NoSchedule, PreferNoSchedule, or NoExecute</li></ul></li><li>labels : 파드에 매칭될 수 있는, 임의의 key-value를 노드에 붙입니다.</li><li>requirements : Acceptable한 것에 In, Unacceptable한 것에 Out을 설정합니다.<ul><li><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/">Well-Known Labels, Annotations and Taints</a> 를 참조해 설정합니다.</li><li><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type">instance types</a>, <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone">zones</a>, <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-arch">computer architecture</a>, <a href="https://karpenter.sh/v0.15.0/provisioner/#capacity-type">capacity type</a></li></ul></li><li>limits : 클러스터에서 사용할 전체 CPU와 메모리의 제한을 설정합니다.<ul><li>이 설정을 통해 노드 프로비저닝을 효과적으로 중지할 수 있습니다.</li></ul></li></ul><hr><p><br></br></p><h2 id="Karpenter-Demo-시나리오"><a href="#Karpenter-Demo-시나리오" class="headerlink" title="Karpenter Demo 시나리오"></a><strong>Karpenter Demo 시나리오</strong></h2><p>이제 Karpenter가 얼마나 효과적이고 빠른지를 살펴보기 위해 Karpenter를 직접 세팅해 보겠습니다. 설치 과정은 <a href="https://karpenter.sh/v0.15.0/getting-started/getting-started-with-eksctl/">공식 문서</a>를 참조하시는 게 가장 깔끔합니다. 꼭 최신 버전인지를 한 번 더 확인하시기 바랍니다. 설치가 완료되었다면 샘플 Provisioner를 넣어보겠습니다. </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">karpenter.sh/v1alpha5</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Provisioner</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">requirements:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">karpenter.sh/capacity-type</span>         <span class="comment"># optional, set to on-demand by default, spot if both are listed</span></span><br><span class="line">      <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">      <span class="attr">values:</span> [<span class="string">&quot;spot&quot;</span>]</span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">cpu:</span> <span class="number">1000</span>                               <span class="comment"># optional, recommended to limit total provisioned CPUs</span></span><br><span class="line">      <span class="attr">memory:</span> <span class="string">1000Gi</span></span><br><span class="line">  <span class="attr">ttlSecondsAfterEmpty:</span> <span class="number">30</span>                    <span class="comment"># optional, but never scales down if not set</span></span><br><span class="line">  <span class="attr">ttlSecondsUntilExpired:</span> <span class="number">2592000</span>             <span class="comment"># optional, but never expires if not set</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">karpenter.k8s.aws/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">AWSNodeTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">subnetSelector:</span>                             <span class="comment"># required</span></span><br><span class="line">    <span class="attr">karpenter.sh/discovery:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br><span class="line">  <span class="attr">securityGroupSelector:</span>                      <span class="comment"># required, when not using launchTemplate</span></span><br><span class="line">    <span class="attr">karpenter.sh/discovery:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>이제 테스트를 할 새로운 namespace를 만들어주겠습니다(<code>kubectl create namespace test</code>). 그리고 해당 네임스페이스에 파드를 생성할 deployment를 만들고 파드를 10개로 늘려보겠습니다. (<code>kubectl scale deployment inflate --replicas 10 -n test</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">inflate</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">inflate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">inflate</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">0</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">inflate</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">public.ecr.aws/eks-distro/kubernetes/pause:3.2</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>이 설정에서 cpu를 굉장히 작게 요청했기 때문에 pod를 갑자기 늘렸을 때, 파드는 제대로 생성되지 못하고 Pending이 되어 있을 것입니다. </p><p><code>kubectl get po -n test</code>를 통해 확인해보면 다음과 같습니다.</p><p><img src="/images/karpenter/get_pod.png" alt="파드가 제대로 생성되지 못하고 Pending되어 있다."></p><p>Provisioner에서는 어떤일이 일어나고 있을까요?(<code>kubectl logs -f deployment/karpenter -c controller -n karpenter</code>) Provisioner에서는 어떤 리소스가 부족한지를 파악하고 어떤 클러스터의 어떤 서브넷에 어떤 스펙의 노드를 추가할 지를 정하고 있습니다. 노드를 올리게 되고 Ready상태와 함께 pod들은 프로비저닝이 완료됩니다.</p><p><img src="/images/karpenter/provisioner_log.png" alt=""></p><p><br></br></p><h3 id="🤯-Trouble-Shooting"><a href="#🤯-Trouble-Shooting" class="headerlink" title="🤯 Trouble Shooting"></a>🤯 Trouble Shooting</h3><p>karpenter v0.14.0를 사용하면서 provisioner의 requirement에 <code>topology.kubernetes.io/zone</code> 를 설정하고 파드를 프로비저닝 할 때 다음과 같은 에러를 만날 수 있습니다.</p><p><code>2022-08-15T21:16:31.881Z        ERROR   controller.provisioning Could not schedule pod, incompatible with provisioner &quot;default&quot;, incompatible requirements, key topology.kubernetes.io/region does not have known values; incompatible with provisioner &quot;labeltest&quot;, incompatible requirements, key topology.kubernetes.io/region does not have known values &#123;&quot;commit&quot;: &quot;5edcce3&quot;, &quot;pod&quot;: &quot;monitoring/prometheus-helm-kube-prometheus-stack-prometheus-1&quot;&#125;</code> (From Kuberentes Slack, karpenter channel. visokoo’s thread) </p><p>그래서 열심히 구글 검색도 해보고 하다가 슬랙방에 들어가서 비슷한 사례가 없는지 봤는데, 8월 16일에 비슷한 문제를 겪는 사람이 있었습니다. 결국 이유는 버그였습니다. 프로비저닝이 안되어서 2-3일 정도를 허비했는데 조금 허무 했습니다. 하지만 빠르게 버그 픽스가 되었고 v0.15.0이 올라오면서 이 문제는 해결 되었습니다. 꼭 최신 버전을 사용하시기 바랍니다!</p><hr><p><br></br></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h2><ul><li><a href="https://karpenter.sh/">https://karpenter.sh/</a></li><li><a href="https://karpenter.sh/v0.15.0/">https://karpenter.sh/v0.15.0/</a></li><li><a href="https://youtu.be/EsmG7nCD_hI">https://youtu.be/EsmG7nCD_hI</a></li><li><a href="https://kubernetes.io/ko/docs/concepts/overview/components/">https://kubernetes.io/ko/docs/concepts/overview/components/</a></li><li><a href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/">https://kubernetes.io/ko/docs/concepts/architecture/nodes/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/08/20/karpenter/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Nifi with Jolt</title>
      <link>http://tkdguq05.github.io/2022/08/06/jolt/</link>
      <guid>http://tkdguq05.github.io/2022/08/06/jolt/</guid>
      <pubDate>Sat, 06 Aug 2022 04:06:47 GMT</pubDate>
      <description>
      
        &lt;p&gt;Nifi를 잘 활용하는 방법! Jolt&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Nifi를 잘 활용하는 방법! Jolt</p><span id="more"></span><h1 id="Jolt"><a href="#Jolt" class="headerlink" title="Jolt"></a>Jolt</h1><ul><li><p><a href="https://github.com/bazaarvoice/jolt">Jolt</a>?</p><p>Jolt(<strong><strong>JsOn Language for Transform</strong></strong>) 는 JSON을 JSON으로 변환하는 데 사용할 수 있는 Java 라이브러리입니다. Jolt 변환 사양 자체도 JSON 파일이기 때문에, Apache NiFi 및 Apache Camel과 같은 제품에서 사용할 수 있습니다.</p><p>Jolt는 전체 JSON을 JSON으로 변환하기 위해 함께 연결할 수 있는 변환 셋들을 제공합니다. Jolt의 특징은 특정 값을 조작하는 것이 아니라 JSON 데이터의 구조를 변환하는 데 중점을 두고 있다는 것입니다.</p><p>Jolt의 공식 github에서는 다음과 같이 설명하고 있습니다.</p><blockquote><p>JSON to JSON transformation library written in Java where the “specification” for the transform is itself a JSON document.</p></blockquote></li></ul><p><br></br></p><p>Jolt 는 다음과 같이 사용할 수 있습니다.</p><ol><li>Transforming JSON data from ElasticSearch, MongoDb, Cassandra, etc before sending it off to the world</li><li>Extracting data from a large JSON documents for your own consumption</li></ol><p><br></br></p><p>JSON 데이터를 변환하는 데 특화되었다는 것을 설명을 통해 알 수 있을 것입니다. Jolt가 갖고 있는 변환 셋들은 5가지가 있습니다.</p><ul><li>shift       : copy data from the input tree and put it the output tree</li><li>default     : apply default values to the tree</li><li>remove      : remove data from the tree</li><li>sort        : sort the Map key values alphabetically ( for debugging and human readability )</li><li>cardinality : “fix” the cardinality of input data.  Eg, the “urls” element is usually a List, but if there is only one, then it is a String</li></ul><p>이 글에서는 shift를 사용해서 nested array로 이루어진 Json을 Flatten하는 것을 예시로 살펴보겠습니다.</p><p><br></br></p><h2 id="Jolt의-간단한-문법"><a href="#Jolt의-간단한-문법" class="headerlink" title="Jolt의 간단한 문법"></a>Jolt의 간단한 문법</h2><p>그 전에 참고할 내용을 먼저 알려드리겠습니다. <strong>LHS (Left Hand Side)</strong>와 <strong>RHS (Right Hand Side)</strong>입니다. LHS는 콜론, 즉, : 를 기준으로 왼쪽을 뜻하고, RHS는 :를 기준으로 오른쪽을 뜻합니다.</p><p>예를 들자면 이렇습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;shift&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;spec&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">LHS -&gt; <span class="attr">&quot;customer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">  LHS -&gt;  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;client.fullName&quot;</span><span class="punctuation">,</span>  &lt;- RHS</span><br><span class="line">  LHS -&gt;  <span class="attr">&quot;birhtDate&quot;</span><span class="punctuation">:</span> <span class="string">&quot;client.dateOfBirth&quot;</span><span class="punctuation">,</span>  &lt;- RHS</span><br><span class="line">  LHS -&gt;  <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span> <span class="string">&quot;client.address.street&quot;</span><span class="punctuation">,</span>  &lt;- RHS</span><br><span class="line">  LHS -&gt;  <span class="attr">&quot;country&quot;</span><span class="punctuation">:</span> <span class="string">&quot;client.address.country&quot;</span>  &lt;- RHS</span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><p>이제 기본 구조에 대해서 알아보겠습니다. Jolt는 다음과 같은 기본적인 구조를 갖고 있습니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;operation&quot;: &quot;&quot;,</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ul><li><strong>“operations”</strong>: 적용할 변환 유형을 정의합니다.</li><li><strong>“spec”</strong>: 어떤 변환을 넣을지 적는 필드입니다.</li><li><strong>“[]”</strong>: Jolt의 기본 구조도 JSON이므로 목록 내에서 여러 작업을 연결할 수 있습니다.</li></ul><p>위에서 변환 셋들에 대해서 간단히 다루긴 했지만, 더 자세한 설명을 보고 싶다면 <a href="https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt">여기</a>에서 확인할 수 있습니다.</p><p><br></br></p><h2 id="Jolt를-활용한-Flatten-작업"><a href="#Jolt를-활용한-Flatten-작업" class="headerlink" title="Jolt를 활용한 Flatten 작업"></a>Jolt를 활용한 Flatten 작업</h2><p>Json 데이터가 nested로 이루어졌을때  Flatten이 필요한 경우가 있습니다. 이럴 때는 AWS라면 EMR, GCP라면 Dataproc에 Spark를 사용해서 처리를 하곤 합니다. 물론 대규모로 이루어진 데이터에 대해서 작업하려면 어쩔 수 없이 사용하겠지만, 미리 Flatten해서 저장을 하면 어떨까요? 그렇다면 비싼 EMR을 사용할 일도 없어질 듯 합니다.</p><p>먼저 이 작업을 위한 방식은 두 가지입니다. <code>JoltTransformJSON</code>, <code>JoltTransformRecord</code> 입니다. </p><p>둘의 차이는 간단합니다. <code>JoltTransformRecord</code>는 레코드 판독기와 레코드 작성기를 사용하는 반면 <code>JoltTransformJSON</code>은 작업할 JSON 흐름 파일 콘텐츠를 찾을 것으로 예상합니다. 두 경우 모두 출력은 플로우 파일 내용으로 끝나고 변환 구성은 동일합니다.</p><p>이제 작업할 대상 데이터를 보겠습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="number">1000009195</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;code&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;************&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;array_deal_products&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[ &#123; \&quot;deal_product_no\&quot; : ***, \&quot;code\&quot; : \&quot;***\&quot;, \&quot;name\&quot; : \&quot;***\&quot;, \&quot;seq\&quot; : 1, \&quot;master_product_code\&quot; : \&quot;***\&quot;, ]&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;short_description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;*******&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;array_site_attributes&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[ \&quot;MARKET\&quot; ]&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;array_product_tags&quot;</span><span class="punctuation">:</span> <span class="string">&quot;[ 2 ]&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2022-**-**T01:12:05.790692Z&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;record-type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;data&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;load&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;partition-key-type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;attribute-name&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;schema-name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;commerce_product&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;table-name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;contents_products&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>여기서 작업할 내용을 미리 정의하면 </p><ol><li>data부분과 metadata에서 필요한 것만 가져올 것입니다.</li><li>필요한 것만 가져와서 합칠 것입니다.</li><li>그리고 <strong>array_deal_products</strong>에 싸여있는 데이터를 풀어줄 것입니다.</li></ol><p><br></br></p><h3 id="data-metadata-특정-키-값만-합치기"><a href="#data-metadata-특정-키-값만-합치기" class="headerlink" title="data, metadata 특정 키 값만 합치기"></a>data, metadata 특정 키 값만 합치기</h3><p>미리 만들어 둔 Nifi Task에서 properties로 들어갑니다. Jolt Transformation DSL을 Chain으로 설정하고 Jolt Specification을 작성해줍니다.</p><ul><li><p>Jolt Specification</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;shift&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;spec&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;data&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;array_deal_products&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;metadata&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li></ul><p>이제 spec 부분에 어떤 작업을 원하는지를 넣어주면 됩니다.</p><ul><li>위의 specification은 “data” 부분과 “metadata” 부분에 대해서 작업을 할 것이라는 뜻입니다.<ul><li>data에서는 id와 array_deal_products를 가져올 것이고, metedata에서는 operation과 timestamp를 가져올 것입니다.</li></ul></li></ul><p>이렇게 사용했을때 나오는 결과는 다음과 같습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;_id&quot;</span> <span class="punctuation">:</span> <span class="number">1000009195</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;array_deal_products&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;[ &#123; \&quot;deal_product_no\&quot; : ***, \&quot;code\&quot; : \&quot;***\&quot;, \&quot;name\&quot; : \&quot;***\&quot;, \&quot;seq\&quot; : 1, \&quot;master_product_code\&quot; : \&quot;***\&quot;, ]&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;operation&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;load&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span> <span class="punctuation">:</span> <span class="string">&quot;2022-**-**T01:12:05.790692Z&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><br></br></p><h3 id="List로-묶인-값-풀어주기"><a href="#List로-묶인-값-풀어주기" class="headerlink" title="List로 묶인 값 풀어주기"></a>List로 묶인 값 풀어주기</h3><ul><li><strong>SplitJson</strong>을 사용합니다.<ul><li><code>$.[*]</code></li><li><a href="https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624">https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624</a></li></ul></li></ul><p>SplitJson 프로세서를 만들어주고 위에서 작성한 <code>$.[*]</code> 을 써주면, 원하는 값을 가져와서 리스트로 묶인 값을 풀어줄 수 있습니다.</p><p>파이프라인으로 확인한다면 이렇게 되겠네요</p><p><img src="images/Jolt/jolt1.png" alt=""></p><p><img src="images/Jolt/jolt2.png" alt="EvaluateJsonPath"></p><p>중간에 <strong>EvaluateJsonPath</strong>는 Json을 파싱하는 프로세서입니다. 이 설정에 Destination을 <code>flowfile-content</code>로 설정해두시면 파싱 결과를 content에 저장하고 하나의 JSON path expression만 가질 수 있습니다. 반면 <code>flowfile-attribute</code> 라면 일단 attribute로 저장하겠죠? 그리고 각 JSON path가 명명된 속성 값으로 추출됩니다. 미리 지정한 <em>**</em>attribute에 저장해둔 array_deal_products에 대해서 SplitJson을 해주는 것이라고 보면 되겠습니다.</p><p><br></br></p><h3 id="을-로-변환하기"><a href="#을-로-변환하기" class="headerlink" title=".을 _로 변환하기"></a>.을 _로 변환하기</h3><p>DB에서 데이터 연동을 하다보면 <code>~~~.~~~.~~</code>로 된 컬럼들이 있습니다. 해당 DB에서야 문제가 없겠는데, 다른 DB로 넣을 때면 문제가 발생하곤 합니다. 보통 _로 처리해서 넣으면 별 일 없기에 이렇게 변환해서 저장을 하거나 연동을 합니다. 하지만 이 간단한 작업을 하는데 Spark로 처리하기에는 너무 아까운 것 같습니다. 별다른 데이터 변환 없이 컬럼 명만 수정하면 되기 때문입니다. 그렇다고 손으로 일일이 하기엔 너무 힘이 들 것 같습니다. 이런 경우에 Jolt를 활용하면 쉽게 .이 들어간 컬럼(또는 키)을 _로 변환할 수 있습니다. </p><ul><li>데이터 예시</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="number">1234</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;purchase_policy.min&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;product_notice.type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;product_notice.template_id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;****&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;product_notice.is_free_template&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;normal_order_type_policy&quot;</span><span class="punctuation">:</span> <span class="string">&quot;DEFAULT&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;is_search_enabled&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;is_expose_product_list&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">8</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;thumbnail.original.service_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;thumbnail.share.service_type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;update&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;timestamp&quot;</span><span class="punctuation">:</span> <span class="string">&quot;***************&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p>이런 데이터가 있다고 하겠습니다. 이 Json데이터의 키를 잘 보면 중간 중간에 .이 들어간 것을 볼 수 있습니다. <code>thumbnail.original.service_type</code> 처럼 처음, 중간에 .이 들어간 키가 있는 반면에, <code>purchase_policy.min</code> 처럼 중간에 .이 있는 것처럼 보이는 키도 있습니다. 이런 경우에 .을 _로 변경하기 위한 Jolt Spec은 다음과 같습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">[</span></span><br><span class="line"></span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;operation&quot;</span><span class="punctuation">:</span> <span class="string">&quot;shift&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;spec&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;*.*&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;(0,1)_&amp;(0,2)&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;*.*.*&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;(0,1)_&amp;(0,2)_&amp;(0,3)&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;*&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&amp;&quot;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure><p>이번에도 shift operation을 사용할 것입니다. 자세히 봐야할 부분은 <code>*</code> 가 있는 부분입니다. 인풋으로 들어오는 데이터의 키를 쭉 보다보니처음에 .이 붙는 경우와 처음과 그 다음 부분에 .이 붙는 경우가 있습니다. 그래서 spec에 <code>&quot;*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)&quot;</code>,    <code>&quot;*.*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)_&amp;(0,3)&quot;</code> 이렇게 넣어줬습니다. 괄호에 있는 숫자는 depth의 level을 뜻합니다. level 0이라면 첫 번째 {}안에 있는 키들을 의미하고 1이라면 키의 첫번째 값을 의미합니다. 예를 들어 <code>purchase_policy.min</code> 자체는 level 0이겠고, purchase_policy는 level 1이 되겠습니다. 그렇다면 min은 level2가 되겠습니다.</p><p>따라서 <code>thumbnail.original.service_type</code> 를 <code>thumbnail_original_service_type</code> 로 바꾸겠다 하면 <code>&quot;*.*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)_&amp;(0,3)&quot;</code> 이렇게 Spec을 작성하면 되는 것입니다. </p><p>그리고 마지막으로 나머지 값들을 다 사용할 것이므로 <code>&quot;*&quot;: &quot;&amp;&quot;</code> 를 통해 가져오면 끝입니다.</p><hr><p><br></br></p><p>이렇게 해서 Jolt에 대해서 간단하게 알아봤습니다. 생각보다 간단하게 데이터를 Transform할 수 있는데, 이를 통해서 대대적인 상품개편 시에 큰 도움을 받았었습니다. 이것을 일일이 내려서 EMR을 돌리고 하는 작업들을 했었으면 너무 파이프라인이 복잡해지고, 모니터링 할 것도 많아졌을 것 같네요. Nifi를 사용하시는 분들이나 Spark를 사용하시는 분들이라면 Jolt를 한 번쯤 살펴보면 좋을 것 같습니다.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/bazaarvoice/jolt#stock-transforms">https://github.com/bazaarvoice/jolt#stock-transforms</a></li><li><a href="https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt">https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt</a></li><li>[<a href="https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624](">https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624](</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/08/06/jolt/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2022년 상반기를 되돌아보기</title>
      <link>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/</link>
      <guid>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/</guid>
      <pubDate>Sat, 09 Jul 2022 08:51:20 GMT</pubDate>
      <description>
      
        &lt;p&gt;2022년 상반기에 대한 회고&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>2022년 상반기에 대한 회고</p><span id="more"></span><h2 id="2022년-요즘"><a href="#2022년-요즘" class="headerlink" title="2022년, 요즘"></a>2022년, 요즘</h2><p>7월 2번째 주가 흘러가고 있다. 매우 무덥고 습하지만 기분이 나쁘지만은 않다. 나름 하는 일을 계속 하고 있고 계획했던 일을 해나가고 있다. 상반기 인사평가도 지났고, 리더님을 통해서 결과도 전달을 받았다. 나름 잘 해나가고 있는 것 같았다. 회사에 입사하면서 맡고 싶었던 일이었고 이 일의 한 축이 되면서도 나름의 인정을 받은 것 같아 기뻤다. 하지만 내가 어떤 일을 해왔고 어떤 고민 때문에 지금의 일을 하고 있는지에 대해서는 회고 기록이 없어서 알기 어려 웠다. 사실 2022년을 시작하면서 월 회고를 하고 있었다. 하지만 글또 운영진 활동과 본격적인 회사 업무, 루틴화된 일들과 더불어 나의 게으름 덕분에 회고를 최근까지 하지 못했었다. 귀찮아서 미뤄두었는데 지나고 나니 한 달에 대한 기억이 없어지고 내가 무엇을 했는지 알 수가 없어서 너무 너무 아쉬웠다. 다시금 회고하는 습관을 들여서 남은 5개월은 잘 적어둬야겠다.(이렇게 보니 2022년이 얼마남지 않았다…)</p><p>먼저 가장 최근인 6월에 대한 회고를 시작해본다.</p><hr><p><br></br></p><h3 id="2022년-6월"><a href="#2022년-6월" class="headerlink" title="2022년 6월"></a>2022년 6월</h3><p>이미 지나가서 잘 기억이 나지 않는 달에 대한 회고를 위해서 가장 먼저 확인한 것은 캘린더였다. 캘린더를 열어서 어떤 미팅이 잡혔는지를 확인했고, Google Task에 적어둔, 완료된 To Do List들을 체크했다. 그리고 <a href="https://tkdguq05.github.io/2022/05/15/geultto7/#more">이전에 하던 회고의 형식</a>을 사용해서 작성해봤다. </p><p>나는 보통 평일, 주말, 직장, 개인 파트로 나누고 이에 대해서 회고를 진행한다. 사실 평일, 주말에 대해서는 잘 기억이 나지 않는다.. . 아마 글 주제 잡고 스터디 준비하고 운동하지 않았을까…? 이전 달에 뭘 계획하고 계획한 것에 대해서 얼마나 잘 진행되고 있는지를 파악해야 했는데, 아쉽다. </p><p>아쉬운대로 직장, 개인 파트로 넘어가서(개인도 잘 생각나지 않아 직장에만 집중하자면…) 회사에서는 6월에 굉장히 다양한 일을 진행하고 있었다. 기존에 하던 DE업무와 Kubernetes 활용도 계속하고 있고, ML업무에도 투입이 되었다. 여기서 자세하게 작성하지는 못하겠지만 적용하고 싶었던 것들도 적용해보고 하고 싶었던 ML쪽에도 일을 할 수 있게 됐다. 물론 회고를 해보니 보완해야 할점들이 수두룩하게 나왔지만… 덕분에 뭘 더 해야될지 더 명확해진 것 같았다. 외부 연동 프로젝트를 하면서 회사의 연동시스템과 프로세스를 더 익혀야겠다고 생각했고, 카프카를 더 공부하면 연동업무에 더 좋을 것 같다고 느꼈다. 쿠버네티스 쪽에서는 글또에서 활동하고 계시는 <a href="https://swalloow.github.io/eks-karpenter-groupless-autoscaling/">준영님의 Karpenter 글</a> 을 회사 팀 채널에 공유했고, 이걸 적용하고 싶어하셨던 분이 이미 계셔서 얘기를 해보다가 Dev쪽에 먼저 적용을 해보게 되었다. 다만 Provisioner까지 적용를 해보고 테스트까지 해보고 싶었는데 다른 일 때문에 여기까지는 해보지 못했다. </p><p>ML프로젝트를 하면서 자연스레 MLOps에 대한 관심이 커졌다. 사실 MLOps에 대해 고민할 단계가 아니라는 것을 알고는 있지만, 이것을 플랫폼화 하고, 사용하기 쉽게 만드려면 지금 논의해보는게 나쁘지 않을 것 같았다. 여러 자료들을 보고는 있는데 이제 시작되고 있는 분야라 Best Practice에 대한 자료가 거의 없다. 다른 회사의 사례를 참고해 보면서 우리 회사만의 MLOps를 구상해봐야겠다. 그래서 현재 사용하고 있는 구조를 활용해서 아키텍쳐를 그려보고 있는데, DS나 ML쪽 팀원들이 어떤 MLOps환경을 원하는지 - 예를 들어 “빠른 연산이 필요해요”, “배포를 쉽게하고 싶어요”, “실험 환경이 필요해요”… 등등을 잘 들어보고 결정해야겠다.</p><p><br></br></p><p>개인적으로는 테니스를 쭉 해오고 있는데 백핸드를 원핸드로 배우면서 스텝이 꼬여버려서 이걸 극복해보고 싶어졌다. 그래서 랠리가 참 안되는데, 같이하시는 분이 투핸드 백핸드를 하시는데 안정적으로 포핸드 - 백핸드 랠리를 하셔서 투핸드로 바꿀까 고민을 했었다. 하지만! 페더러가 백핸드를 하는 것을 보고 다시 열심히 해보기로!</p><p>그리고 글또 런닝 채널에서 양평 마라톤을 올려주신 분이 있어서 10km 마라톤을 등록했다. 덕분에 축구로 뛴 걸 제외하고 인생에서 최초로 10km를 뛰어보게 되었다. 7월 9일에 결국 완주했는데, 진짜 힘들지만 보람찼다. 왜 하는지 알 것 같았고 약간 슬럼프가 올려던 때였는데 잘 극복할 수 있게 되었다. 생각하면 할 수록 뿌듯했고, 다음엔 한 시간 안쪽으로 들어오고 싶은 욕심도 조금 났다…!</p><p><br></br></p><h3 id="2022년-5월-4월"><a href="#2022년-5월-4월" class="headerlink" title="2022년 5월 4월"></a>2022년 5월 4월</h3><p>사실 4월 5월은 기간이 너무 지나서 세세한 것 까지는 기억이 나지 않는다. 캘린더를 봐도 뭘 했는지도 잘 생각이 안나서 메모들을 통해서 떠오른 과거의 기억들을 조합해서 회고를 해봤다.</p><p>회사에서는 Kubernetes를 다루기 위해 Rancher에 대해서 살펴보고 공유를 받았다. 대충 어떻게 쓰는 건지는 알고 있었는데 어떻게 클러스터와 연결이 되는건지 이때에 알게 되었었다. 회사에서 Airflow관련 세션도 진행을 했었고, 하고싶었던 스터디도 이때에 진행을 하게 되었다. <strong>데이터 중심 어플리케이션</strong> 이라는 책으로 번역의 퀄이… 그리 좋지는 않아서 혼자 읽기는 매우 힘들고 중도 포기했었을 것 같은 책이었는데, 스터디를 통해서 7월에 결국 완주를 하게 되었다. 최근에 책걸이로 맛있는 점심을 먹었는데 기분이 참 좋았다. 다른 책들이나 주제들로 스터디를 만들어보고도 싶고, 다른 분들도 스터디를 만드시는 것을 지원해드리고도 싶다.</p><p>또 회사에 최근에 입사하시는 분이 있는데, 이 분을 회사에 추천을 했다. 전 직장에서 같이 일했던 분인데 회사에서 고통을 받고 계시길래, 그리고 같이 일하면 시너지가 잘 나는 분이라 모셔오고 싶었다. 7월에 입사하시는데, 회사에 MLE로 계시는 분이 퇴사를 하시게 되면서 이 분이 안왔으면 큰일날 뻔 했다… 하는 생각도 요즘 하고 있다. 여러 고민이 참 많으셔서 내가 있는 회사에서 어떤 것을 할 수 있는지를 설명해드리고 선택은 본인이 하셔야 된다고 말씀드렸는데, 참 감사히도 여기를 선택해주셨다. 온보딩 잘해드리고 가이드 잘 해드려야지…! 다른 분들과도 시너지를 잘 내셨으면 좋겠고, 그 시너지 효과가 기대가 된다.</p><p><br></br></p><p>개인적으로는 부업?이라고 해야할까 모 회사의 멘토링에 참여하게 되었다. 그 분들의 고민이 무엇인지 알게 되었고, 대략적으로 어떤 시스템을 가지고 있는지 유추할 수 있었다. 굉장히 재밌었던 경험이었고, 보수도 넉넉히 받아서 여러 번 참여하고 싶은 활동이었다.</p><p>캘린터를 보니 5월에는 테니스를 시작했다. 2022년에 하기로 한 일 중에 가장 잘한 일인 것 같다. 페더러와 나달을 좋아해서 테니스를 꼭 배워보고 싶었는데, 뜻이 맞는 분이 있어서 시작하게 되었다. 덕분에 요즘에는 원핸드 백핸드도 하고 나름 폼이 조금 나는 것 같다. 코트 한 번 나가서 시원하게 쭉쭉 쳐보고 싶다… 얼른 실력을 키워봐야지.</p><p><br></br></p><hr><h3 id="Outro"><a href="#Outro" class="headerlink" title="Outro"></a>Outro</h3><p>지나간 달에 대한 세밀한 회고가 잘 되지는 않았지만, 그래도 의미가 있었던 것 같다. 오랜만에 회고를 해보니 재미도 있었고 어떻게 살아가야 할지에 대해서 방향성이 좀 더 명확해지는 느낌이다. 일주일 단위로도 하면 어떨까도 잠깐 생각해봤는데, 너무 스스로를 옭아매는 것 같아 한 달 단위로만 진행하는 게 좋을 것 같다. 한 달간의 마일스톤 내지는 To-Do-List를 통해 시간을 잘 활용해봐야겠다. 지나가버린 달들에 대해 회고를 제대로 하지 못해 아쉽긴하지만, 남은 달들을 더 잘 살아보고 스스로 회고를 잘 해봐야겠다. 7월도 잘 즐겨보자.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/#disqus_thread</comments>
    </item>
    
    <item>
      <title>분산시스템의 문제점, zookeeper &amp; etcd</title>
      <link>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/</link>
      <guid>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/</guid>
      <pubDate>Sun, 26 Jun 2022 01:29:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;코디네이션 시스템, 왜 이런 도구들을 사용해야 하는 걸일까요?&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>코디네이션 시스템, 왜 이런 도구들을 사용해야 하는 걸일까요?</p><span id="more"></span><h1 id="분산시스템의-문제점-zookeeper-amp-etcd"><a href="#분산시스템의-문제점-zookeeper-amp-etcd" class="headerlink" title="분산시스템의 문제점, zookeeper &amp; etcd"></a>분산시스템의 문제점, zookeeper &amp; etcd</h1><p>데이터 엔지니어링 관련 애플리케이션을 다루다보면 심심치 않게 등장하는 것이 Zookeper입니다. Kafka를 다룰때도 등장하고, 하둡-스파크를 다룰 때도 역시 등장합니다. 쿠버네티스를 다룰때도 비슷하게 등장하는 것이 etcd입니다. 가끔 개발 세션을 볼 때나 블로그 글을 살펴볼때에도 etcd가 나오는데 정확히 어떤 역할을 하는지 이해가 가지 않았습니다. 둘의 공통점이라고 하면 분산 환경에서 사용되는 코디네이션 시스템인데, 왜 이런 도구들을 사용해야 하는 걸일까요?</p><p><br></br></p><h2 id="분산-시스템에서-코디네이션의-필요성"><a href="#분산-시스템에서-코디네이션의-필요성" class="headerlink" title="분산 시스템에서 코디네이션의 필요성"></a>분산 시스템에서 코디네이션의 필요성</h2><p>요즘의 대부분의 데이터 엔지니어링 시스템은 분산 시스템을 사용하고 있습니다. 아파치 카프카, 하둡, 스파크 등등 많은 애플리케이션들이 고가용성과 내결함성, 짧은 지연 등의 이유로 이를 활용하고 있습니다. 하지만 이러한 분산 시스템을 잘 유지하기 위해서는 몇가지 조건들이 필요합니다. 분산 시스템이 가진 문제점들이 있기 때문입니다. 여러 노드들로 나누어진 스토리지가 있다고 해봅시다. 이 노드들이 데이터를 write하고 클라이언트는 이 데이터를 읽을 것입니다. 노드들 중 한 대가 리더 노드로 write를 담당할 것이고 이 노드를 통해 모든 클라이언트의 write명령을 받을 것입니다. 명령은 리더가 아닌 다른 팔로워 노드들이 차차 처리하겠죠. 시스템이 완벽해 보이나요? 리더 노드가 죽으면 어떻게 될까요? 데이터를 한 노드에 write하는 동안 클라이언트가 다른 노드에서 데이터를 읽으려고 하면 어떻게 될까요?</p><p>분산시스템은 멀리서 보면 안정적이고 성능도 굉장히 뛰어나보이지만, 가까이에서 보면 처리해야될 문제들이 이처럼 상당합니다. 이러한 문제들을 잘 다루기 위해 사용하는 것이 코디네이션 시스템이고 대표적인 것이 바로 zookeeper와 etcd인 것입니다.</p><p>분산 시스템의 대표적인 문제 몇 가지를 살펴보고 Zookeeper와 etcd가 왜 필요한지, 어떻게 이런 문제들을 관리하는지 살펴보겠습니다.</p><p><img src="https://80000hours.org/wp-content/uploads/2016/02/Screen-Shot-2016-02-09-at-12.26.20-AM.png" alt="힙을 합칠땐 신뢰가 필요해..!" style="zoom:80%;" /></p><p><br></br></p><hr><p><br></br></p><h2 id="분산-시스템의-문제점"><a href="#분산-시스템의-문제점" class="headerlink" title="분산 시스템의 문제점"></a>분산 시스템의 문제점</h2><p>우리가 잘 사용하고 있는 분산시스템은 사실 고장날 것을 가정하고 설계하고 있습니다. 여러 노드를 통해 시스템을 구축하고 있기 때문에, 고장날 가능성이 단일 노드보다 훨씬 큰 것이죠. 그래서 장애가 났을 때는 잘못된 결과를 반환하는 것보다 아예 동작하지 않기를 바랍니다. 장애에는 여러 종류가 있는데 부분 장애는 시스템의 어떤 부분은 잘 동작하지만 다른 부분은 예측할 수 없는 방식으로 실패 하는 것을 말합니다. 이러한 비결정성과 부분 장애 가능성이 분산 시스템을 어렵게 만드는 것입니다.</p><p><br></br></p><h3 id="1-신뢰성-없는-네트워크"><a href="#1-신뢰성-없는-네트워크" class="headerlink" title="1. 신뢰성 없는 네트워크"></a>1. 신뢰성 없는 네트워크</h3><p>분산 시스템은 어떻게 연결되어있을까요? 분산 시스템은 비공유 시스템, 네트워크로 다수의 장비를 연결합니다. 네트워크는 장비들이 통신하는 유일한 수단이기에 굉장히 주의깊게 봐야합니다. 장비들간의 네트워크는 <code>비동기 패킷 네트워크</code>를 주로 사용합니다. 왜냐면 순간 몰리는 데이터 전송에 특화되어야 하기 때문입니다. 패킷 전송 지연시간 최대치가 고정되어 있고 유실되는 데이터가 없는 극단적인 신뢰성을 지닌 네트워크를 구성하면 안될까요? 이렇게 하기 위해서는 고정된 대역폭을 설정해야 합니다. 이렇게 되면 정확히 몇 초에 얼마의 데이터를 보낼 수 있는 것이 보장됩니다. 이런 방식을 <code>회선</code> 이라고 합니다. 회선은 데이터 전송량이 예상 가능한 경우에 적절하겠습니다. 우리가 사용하는 일반적인 상황에서는 데이터 전송량이 예상이 가능할까요? 아마 불가능할 것입니다. 또한 우리가 원하는 시스템의 목적은 가능한 빨리 완료되는 것입니다. 그러니까 순간적으로 몰리는 트래픽에 최적화된 방식이 필요한 것입니다. 그래서 데이터센터 네트워크와 인터넷은 <code>패킷 교환</code> 방식을 사용하는 것입니다. 결국 이 방식을 사용하다보니 어쩔 수 없이 네트워크에 신뢰성이 떨어질 수 밖에 없습니다. 타임아웃등의 방식을 사용할 수 있지만, 완벽하지는 않습니다.</p><p><br></br></p><h3 id="2-신뢰성-없는-시계"><a href="#2-신뢰성-없는-시계" class="headerlink" title="2. 신뢰성 없는 시계"></a>2. 신뢰성 없는 시계</h3><p>분산 시스템에서는 통신이 즉각적이지 않아, 시간은 다루기 까다로운 개념 중 하나입니다. 시간은 절대적인 것이라고 생각할 수 있지만, 유감스럽게도 시계가 정확한 시간을 알려주게 하는 방법은 기대만큼 신뢰성이 있거나 정확하지 않습니다. 각 장비는 사실 자신만의 시계를 갖고 있지만 완벽하게 정확하지 않습니다. 이를 위한 동기화 매커니즘 중 하나는 서버 그룹에서 보고한 시간에 따라 컴퓨터 시계를 조정할 수 있게 합니다.</p><p>이렇게 동기화된 시계에 의종할 수 있습니다. 보통 대부분의 시간에 잘 동작하지만, 견고하게 소프트웨어를 설계하고자 한다면, 잘못된 시계에 대비할 필요가 있습니다. 하루는 정확히 86,400초가 아닐 수도 있고, 일 기준 시계가 시간이 거꾸로 갈 수도 있으며, 노드의 시간이 다른 노드의 시간과 차이가 많이 날 수도 있습니다. 문제는 시계가 잘못된다는 것을 눈치채기 쉽지 않다는 것입니다. 천천히 조금씩 실제 시간으로부터 차이가 나기 시작하지만, 대부분 잘 동작하는 것처럼 보입니다. 이렇게 되면 극적인 고장보다는 조용하고 미묘한 데이터 손실이 발생할 가능성이 높을 것입니다.</p><p><br></br></p><h3 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h3><p>이외에도 네트워크에 비대칭적인 결함이 있어 노드가 메세지는 받지만 밖으로 보내지 못한다면, 노드에 타임아웃을 통해 죽었다고 잘못 선언해버리는 일이 발생할 수도 있고, 죽은 상태인 줄 알았던 노드가 갑자기 되돌아와서 리더처럼 역할을 수행하기도 합니다.</p><p>결국 위와 같은 문제들로 인해서 분산시스템의 균형이 무너져버립니다. 리더와 팔로워의 구조를 통해서 명령을 리더로 부터 내려받고 실행된 내용을 리더에게 보고해서 완료처리를 해야하지만, 갑자기 리더가 사라진다던가 리더가 둘 이상이 되어 버린다던가, 결국 쓰기가 충돌되고 파일이 오염되기 시작합니다. 더 자세하게는 리더와 락의 문제를 더 설명해야하지만, 너무 길어질 것 같아서 생략하겠습니다. 결론은 네트워크나 시계등의 문제로 신뢰성이 깨지면서 분산시스템이 제대로 동작하지 않는다는 것입니다.</p><p>하지만 엔지니어로서의 우리의 임무는 모든 게 잘못되더라도 제 역할을 해내도록 만드는 것입니다. 그래서 부분 장애 가능성을 받아들이고 소프트웨어에 내결함성 메커니즘을 넣어서 신뢰성 있는 시스템을 만들려고 하고 있습니다. 이런 상황에서 모든 노드가 어떤 것에 동의하도록 만드는 것이 중요한데, 이런 <code>합의</code> 에 신뢰성있게 도달하는 것은 또 다른 까다로운 문제입니다. 다만, Zookeeper라던가 etcd 등의 코디네이션 도구들을 도움을 통해 좀 더 쉽게 합의에 도달할 수 있습니다.</p><p><br></br></p><hr><p><br></br></p><h2 id="Part1-Zookeeper"><a href="#Part1-Zookeeper" class="headerlink" title="Part1. Zookeeper"></a>Part1. Zookeeper</h2><p>Zookeeper는 분산 애플리케이션을 위한 코디네이션 시스템입니다. 분산 애플리케이션이 안정적인 서비스를 할 수 있도록 분산되어 있는 각 애플리케이션의 정보를 중앙에 집중하고 구성 관리, 그룹 관리 네이밍, 동기화 등의 서비스를 제공합니다.</p><p>분산시스템에 여러 문제들이 있기 때문에, 시스템에서 정보를 어떻게 공유할 것이고 상태를 어떻게 체크할 것이며, 분산 서버들간의 잠금을 처리하는 방법에 대한 해결방법이 필요해졌고 여기서 주키퍼는 해결책을 일부 제시해주고 있습니다.</p><p>분산 시스템의 코디네이션에서도 마찬가지로, 코디네이션 시스템에 장애가 발생하면 전체 시스템에 장애가 발생해 버리게 됩니다. 따라서 코디네이션 시스템 역시 이중화 등 고가용성을 위한 시스템을 갖고 있습니다.</p><p>엄청나게 대단한 일을 하는 것처럼 보이긴 하는데, 기능은 단순한 편입니다. <code>Znode</code> 라는 Key-value로 이루어진 데이터 저장 객체를 제공하고, 여기에 데이터를 넣고 빼는 기능만을 제공하는 것입니다. 아키텍쳐를 살펴보면서 Znode까지 살펴보겠습니다.</p><p><br></br></p><h3 id="Zookeeper-아키텍쳐"><a href="#Zookeeper-아키텍쳐" class="headerlink" title="Zookeeper 아키텍쳐"></a>Zookeeper 아키텍쳐</h3><p>고가용성을 위해 zookeeper는 클러스터링화 해서 최대한 정상 동작을 보장하려고 합니다. 이 클러스터를 <code>앙상블</code>이라고 부릅니다. 앙상블로 묶인 서버 중 한대가 쓰기 명령을 담당하는 리더 역할을 맡고, 나머지는 팔로어가 되는 구조입니다. 클라이언트가 쓰기 명령을 내리면 앙상블 중 리더 역할을 맡는 zookeeper서버로 바로 전달되고, 리더는 팔로어들에게 쓰기를 수행할 수 있는지 확인합니다. 만약 팔로어 중 과반 수의 팔로우로부터 쓸 수 있다는 응답이 오면 리더는 팔로어에게 Write하도록 지시합니다.</p><p><img src="/images/zookeeper_etcd/Untitled.png" alt="Zookeeper Architecture"></p><p><br></br></p><p>이런 이유로 앙상블을 이루기 위해서는 최소 3대의 서버가 필요합니다. 만약 5대의 서버가 있다면 3대의 서버가 살아있어야겠습니다. </p><p>리더는 업데이트 명령을 받으면 트랜잭션의 순서를 반영하는 번호로 업데이트를 스탬프 처리합니다(번호 붙이기). 그리고 리더는 이 번호와 함께 업데이트 요청을 브로드캐스트하고 다음서버의 다수가 메세지에 응답할 때까지 기다립니다. 다음 서버는 업데이트 요청을 받으면 스탬핑된 번호를 확인하고 이 숫자가 자신의 로그에 기록된 트랜잭션보다 크면 리더에게 동의한다고 응답합니다. 리더는 앙상블의 서버로부터 응답을 받고 앙상블에 있는 팔로워들이 승인을 한 경우에만 리더는 요청된 트랜잭션을 자체 로그에 입력하고 앙상블을 통해 복제합니다. 이제 쿼리의 업데이트 실행이 발생하고 응답이 클라이언트 쪽으로 다시 전송됩니다. 이를 통해 <code>합의</code>가 유지되는 것입니다.</p><p><br></br></p><h3 id="Zookeeper의-데이터-모델"><a href="#Zookeeper의-데이터-모델" class="headerlink" title="Zookeeper의 데이터 모델"></a>Zookeeper의 데이터 모델</h3><p>Zookeeper는 Znode라는 노드의 계층구조를 유지합니다. 네임스페이스의 각 znode에는 연관된 데이터가 있습니다. </p><p><img src="/images/zookeeper_etcd/Untitled1.png" alt="Znode"></p><p><br></br></p><p>Znode의 이름은 슬래시로 구분되어있는 경로의 집합이고, 모든 znode는 경로로 식별됩니다. znode에는 두 유형이 있습니다.</p><ol><li>Persistent znode: 이 znode는 znode를 생성한 세션이 살아있지 않는 경우에도 디스크에 유지됩니다.</li><li>Ephermeral znode: znode를 생성한 세션이 종료될 때 까지 존재합니다. 세션이 끝나면 삭제됩니다. </li></ol><p><br></br></p><h3 id="Quorum-정족수"><a href="#Quorum-정족수" class="headerlink" title="Quorum(정족수)"></a>Quorum(정족수)</h3><p>리더가 새 트랜잭션을 수행하기 위해서는 자신을 포함해 과반수 이상의 서버의 합의를 얻어야 한다. 과반수의 합의를 위해 필요한 서버들이 바로 Quorum이다. 앙상블을 구성하는 서버 수가 5개라면 quorum은 3개로 구성된다. 앙상블로 구성되어 있는 주키퍼는 과반수 방식에 따라 살아 있는 노드 수가 과반 수 이상 유지되기만 하면 지속적인 서비스가 가능하다.</p><p>결론적으로 Zookeeper 서비스는 이런 방식들을 통해 클라이언트의 업데이트가 전송된 순서대로 적용되고 연결된 서버에 관계없이 클라이언트가 동일한 데이터를 읽을 수  있도록 보장합니다.</p><p><br></br></p><hr><p><br></br></p><h2 id="Part2-etcd"><a href="#Part2-etcd" class="headerlink" title="Part2. etcd"></a>Part2. etcd</h2><p>머신의 분산된 시스템 또는 클러스터의 설정 공유, 서비스 검색, 스케쥴러 조정을 위한 오픈소스 분산형 키-값 저장소입니다. 사실상 쿠버네티스의 기본 데이터 저장소로 사용되는데(컨트롤 플레인 컴포넌트로 채택됨), 클라우드 네이티브 애플리케이션은 etcd 사용을 통해 일관성 있는 가동시간을 유지하고 개별 서버에 장애가 발생하더라도 작동 상태를 유지할 수 있게 되었습니다. 애플리케이션은 이 etcd에서 데이터를 읽고 쓸 수 있고, 이를 통해 설정 데이터를 배포해 노드 설정에 대한 이중화 및 복구 능력을 제공할 수 있습니다. 이 etcd는 쿠버네티스에서 마스터노드에 구성되어 있습니다.</p><p>만약 etcd에 문제가 발생해 데이터가 유실된다면, 컨테이너 뿐 아니라 클러스터가 사용하는 모든 리소스가 길을 읽게 되어버립니다. 따라서 etcd에는 높은 신뢰성이 꼭 필요합니다.</p><p><br></br></p><h3 id="RSM-Replicated-State-Machine"><a href="#RSM-Replicated-State-Machine" class="headerlink" title="RSM(Replicated State Machine)"></a>RSM(Replicated State Machine)</h3><p>분산 환경에서 서버가 몇 개 죽더라도 잘 동작하는 시스템을 만들 수 있는 방법 중 하나입니다. 똑같은 데이터를 여러서버에 복제하는 것인데, 이 역할을 수행하는 머신을 RSM이라고 부릅니다. 데이터를 여러 서버에 복제하면 모든게 해결될까요? 오히려 이 상황때문에 문제가 발생하기도 합니다. 데이터 복제과정에서는 합의가 꼭 필요한데, 합의란 RSM이 4가지 속성을 만족한다는 의미입니다. etcd는 이 조건을 만족하기 위해 Raft를 사용합니다.</p><ul><li>Safety : 항상 올바른, 의도하는 결과를 리턴해야 합니다.</li><li>Available : 서버가 몇 대 다운되더라도 항상 응답해야 합니다.</li><li>Independent from Timing : 네트워크 지연이 발생해도 로그의 일관성이 깨져서는 안됩니다.</li><li>Reactivity : 모든 서버에 복제되지 않았더라도 조건을 만족하면 빠르게 요청에 응답해야 합니다.</li></ul><p><br></br></p><h3 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h3><p>Raft는 분산 시스템에서 특히 etcd에서 합의를 도출하는 프로토콜입니다. Follower, Candidate, Leader로 State가 나눠져있습니다.</p><p><strong>Leader Election</strong></p><p>Follower가 리더로부터 정보를 받지 못하고 있으면 후보자가 됩니다. 그러니까 시작상태에도 적용이 됩니다. 이 후보자들은 다른 노드들에 투표를합니다. 그러면 다른 노드들은 투표에 대한 응답을 보내게 되고, 과반수에 따라 한 노드가 리더로 승격됩니다.</p><p><strong><strong>Log Replication.</strong></strong></p><p>이렇게 리더가 선정되면 시스템의 모든 변화들은 리더에게 전달되고 각 변화는 노드의 로그 항목에 추가됩니다. 하지만 커밋이 되지 않은 상태이기 때문에 노드의 value를 업데이트 하지는 않습니다. 항목을 커밋하기 위해 다른 팔로워 노드에 복제를 합니다. 이제 리더가 다른 과반수의 노드들에 항목이 쓰여질 때까지 기다립니다.</p><p>이제 항목이 커밋되었고 리더노드들과 다른 노드들의 상태가 업데이트 되었습니다. 리더는 팔로워들에게 항목이 커밋되었다고 공지하는데, 이 때 클러스터는 합의가 되었다고 말할 수 있습니다.</p><p>간단하게는 이렇게 설명할 수 있는데 알고리즘의 자세한 동작을 알아보고 싶다면, 아래 링크 애니메이션을 통해 확인할 수 있습니다. etcd가 이루는 합의에 대해서 제대로 이해할 수 있을 것입니다.</p><p><a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a></p><p><br></br></p><h3 id="참고-Consul"><a href="#참고-Consul" class="headerlink" title="참고, Consul"></a>참고, Consul</h3><p>서비스 디스커버리 측면에서 Consul도 여기에 낄 수 있습니다. Consul는 서비스 디스커버리(Service discovery)와 설정을 관리하는 툴입니다. Consule는 <strong>분산&amp;클라우드</strong> 환경에 적응하기 위한 고가용성, 유연한 스케일링, 분산시스템의 특징을 가집니다. 리더 선출을 위해 서버는 역시 Raft 알고리즘을 사용합니다.</p><p>Consul의 핵심 기능은 아래와 같습니다.</p><ul><li>서비스 디스커버리 : DNS 나 HTTP 인터페이스를 통해서 서비스를 찾을 수 있게 합니다. 외부의 SaaS 서비스업체도 등록할 수 있습니다.</li><li>Health Checking : 클러스터의 건강 상태를 모니터링하며 문제가 생길 경우 신속하게 전파합니다. 헬스체크는 서비스 디스커버리와 함께 작동하며, 문제가 생긴 호스트로 서비스 요청이 흐르는 걸 막습니다. 이를 이용해서 서비스레벨에서의 서킷 브레이커(circuit breaker)를 구현 할 수 있습니다.</li><li>KV(Key/Value) 저장소(Store) : Consul은 계층적으로 구성 할 수 있는 KV 저장소를 제공합니다. 애플리케이션은 설정, 플래그, 리더 선출 등 다양한 목적을 위해서 이 저장소를 사용 할 수 있습니다. 이 저장소는 HTTP API를 이용해서 간단하게 사용 할 수 있습니다.</li><li>멀티 데이터센터 대응 : 데이터센터 규모에서 사용 할 수 있으며, 복잡한 구성없이 여러 리전(region)을 지원 할 수 있습니다.</li><li>Service Segmentation : Consul Connect는 TLS를 이용 서비스와 서비스사이에 안전한 통신이 가능하게 합니다.</li></ul><p><br></br></p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Designing Data-Intensive Applications(데이터 중심 어플리케이션 설계)</p><p><a href="https://www.bizety.com/2019/01/17/service-discovery-consul-vs-zookeeper-vs-etcd/">https://www.bizety.com/2019/01/17/service-discovery-consul-vs-zookeeper-vs-etcd/</a></p><p><a href="https://cornswrold.tistory.com/523">https://cornswrold.tistory.com/523</a></p><p><a href="https://bcho.tistory.com/1016">https://bcho.tistory.com/1016</a></p><p><a href="https://d2.naver.com/helloworld/583580">https://d2.naver.com/helloworld/583580</a></p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Kubernetes CI CD devtron, GitOps</title>
      <link>http://tkdguq05.github.io/2022/05/29/devtron/</link>
      <guid>http://tkdguq05.github.io/2022/05/29/devtron/</guid>
      <pubDate>Sun, 29 May 2022 00:52:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubernetes를 편리하게, Devtron&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Kubernetes를 편리하게, Devtron</p><span id="more"></span><h2 id="Devtron"><a href="#Devtron" class="headerlink" title="Devtron"></a>Devtron</h2><p><img src="/images/devtron/isthisyou.png" alt=""></p><p><a href="https://devtron.ai/">Devtron</a>은 Kubernetes를 위한 오픈소스 딜리버리 워크플로우 툴입니다. 쿠버네티스에서 CI/CD 뿐 아니라 GitOps와도 연동이 되어 굉장히 편리하게 Kubernetes를 관리할 수 있습니다. 이런 비슷한 툴로 Rancher를 많이 사용하곤 합니다. 저희 팀에서도 Rancher를 이용해 쿠버네티스를 살펴보고 있습니다. Rancher의 좋은 점은 쿠버네티스 관리를 눈으로 보면서 할 수 있다는 것인데, 일일이 <code>kubectl</code> 을 사용하지 않더라도, 마우스 클릭으로 Pod나 Service 등등의 상태를 확인할 수 있고 조정할 수 있어서 참 편합니다.</p><p><br></br></p><p><img src="/images/devtron/1.png" alt="Devtron 메인 화면"></p><p>Devtron은 이러한 Rancher의 장점에 GitOps까지 추가된 툴이라고 보면 되겠습니다. ArgoCD를 사용해보신 분이 있다면, 마치 Rancher + ArgoCD의 느낌입니다. 기본적인 쿠버네티스의 리소스 확인이나 관리는 Rancher의 기능을 거의 다 갖고 있고, 여기에 Git을 연결하면, ArgoCD처럼 GitOps도 가능합니다. 즉, 배포 관리를 한 번에 처리할 수 있다는 것입니다.</p><p><br></br></p><h3 id="Devtron-1-차트관리"><a href="#Devtron-1-차트관리" class="headerlink" title="Devtron 1. 차트관리"></a>Devtron 1. 차트관리</h3><p>Rancher에서도 마찬가지지만 레포지토리에 차트 주소만 넣어주면, 해당 주소에서 helm chart를 가져올 수 있습니다.  차트를 등록하는 방법은 Global Configurations에서 Chart Repository로 들어가면 됩니다. </p><p><img src="/images/devtron/2.png" alt="차트 등록"></p><p>이름과 차트의 URL, 인증방식을 넣어주고 Save를 해주면 끝입니다! Rancher와 거의 비슷한 방식입니다. 다만, 랜쳐는 Chart페이지에서 등록과 차트확인을 같이 할 수 있지만, Devtron은 차트 등록은 Configuration에서 하고, 차트 관리는 Chart Store에서 하네요.</p><p><br></br></p><p><img src="/images/devtron/3.png" alt="왼쪽 바를 살펴보면 차트스토어가 있다" style="zoom:50%;" /></p><p>차트 스토어에 들어가면 아까 등록한 차트들을 볼 수 있습니다. 정말 간단하게 원하는 차트를 누르고 Deploy하면 됩니다. Deploy를 누르게 되면 앱의 이름과 프로젝트, 환경 등을 선택할 수 있습니다. 차트 버전과 values.yaml도 선택가능하며, 하단에는 values.yaml이 나와서 수정을 할 수 있습니다.</p><p><br></br></p><p><img src="/images/devtron/4.png" alt="Airflow 예시"></p><p>이제 배포를 하면 끝!</p><p><br></br></p><h3 id="Devtron-2-App-배포"><a href="#Devtron-2-App-배포" class="headerlink" title="Devtron 2. App 배포"></a>Devtron 2. App 배포</h3><p>Devtron에서 App을 배포하는 방법 역시 간단합니다. 아까 봤었던 왼쪽 바에서 application을 눌러 들어가서 custom app을 만들건지 helm chart기반으로 만들건지 선택하면 됩니다. custom app으로 들어가면 다음과 같은 화면이 등장합니다. <img src="/images/devtron/5.png" alt="App생성 화면"></p><p><br></br></p><p>기본적으로 UI가 너무 깔끔합니다. Rancher는 정말 편하긴 하지만, old한 느낌을 지울 수 없는데 참 깔끔하고 산뜻하네요. 아무튼 화면으로 와서 Git Material을 등록해줍니다. </p><p>여기서 뭔가 의아해 하실 분도 있을겁니다. ‘Git을 꼭 등록해야하는건가?’</p><p> <img src="/images/devtron/6.png" alt="Image세팅"></p><p><br></br></p><p>네 git을 등록해야 사용할 수 있습니다. 이 부분이 조금 별로라고 생각할 수도 있겠지만, 기본적으로 GitOps까지의 연결을 제공하려는 툴이다보니, 전 그러려니 했습니다. 여기서 Git 계정을 등록하고 나면 어디서 이미지를 갖고 올 건지를 선택합니다. DockerHub를 사용할 수도 있겠지만, 사내에서 사용하고, AWS를 사용한다면 ECR도 등록가능합니다. 그 다음 Dockerfile의 위치를 골라주고 Configuration을 저장해주면 됩니다. 그 다음은 Docker Build Config, Secret 등 세부 설정입니다.</p><p>Rancher에서는 Deployment를 하나 짜주고 배포하면 끝입니다. 물론 이게 훨씬 간단해보인다고 생각하시는 분들도 있겠지만, Git으로 관리해준다는 점이 인상적입니다. 사실 로컬에 배포하고 테스트하는 경우가 참 많아서 이미지 어디에 있는지 물어보는 경우도 많거든요. 이렇게 Git으로 꼭 등록을 해줘야 한다면, 이미지를 잃어버리지 않고 관리할 수 있어서 좋을 것 같습니다.</p><p><br></br></p><h3 id="Devtron-3-Workflow"><a href="#Devtron-3-Workflow" class="headerlink" title="Devtron 3. Workflow"></a>Devtron 3. Workflow</h3><p>위에서 설정을 다 했다면 이제 워크플로우 단계입니다. Pipeline을 만들 수 있는 것인데요, Devtron 의 핵심 기능이라고 봐야 할 것 같습니다. 파이프라인은 세 종류로 나뉩니다. 이 글에서는 Continuous Intergration만 살펴보겠습니다. 저는 이것만 사용하거든요!</p><p><img src="/images/devtron/7.png" alt="Build Pipeline" style="zoom:50%;" /></p><p><br></br></p><h4 id="Continuous-Integration"><a href="#Continuous-Integration" class="headerlink" title="Continuous Integration"></a>Continuous Integration</h4><ul><li>Github에서 하나의 레포에 연결할 수 있습니다. 어떤 브랜치에 연결할 것인지를 고르고 넘어가면 기본적인 파이프라인이 생성됩니다. </li></ul><p><img src="/images/devtron/8.png" alt=" " style="zoom:50%;" /></p><p><img src="/images/devtron/9.png" alt="build configuration" style="zoom:50%;" /></p><ul><li>자동으로 빌드할 것인지 일일이 마우스 클릭으로 빌드할 것인지를 선택할 수 있고, 파라미터를 넣을 수 있으며, 빌드 전 스테이지, 빌드 후 스테이지에 작업을 추가할 수 있습니다. </li></ul><p><br></br></p><h4 id="Continuous-Deployment"><a href="#Continuous-Deployment" class="headerlink" title="Continuous Deployment"></a>Continuous Deployment</h4><p>위의 사진에서 Build파이프라인 옆에 +가 보이실 겁니다. 이 버튼을 눌러주면 Deployment를 어떻게 할 것인지 선택할 수 있습니다.</p><p> <img src="/images/devtron/10.png" alt="어디에 배포할지 고르자"></p><p><br></br></p><p>어떤 환경에 배포할지, 네임스페이스는 어디인지를 설정할 수 있고, 배포 전략도 고를 수 있습니다. 저는 test환경으로 환경을 4-devtron으로 해놨는데, dev - stage - prod 로 나눠서 안전하게 배포를 할 수도 있습니다. </p><p>production은 예민한 사항이다 보니, 유닛 테스트와 기능적인 테스트들을 추가적으로 설정해줘야겠지만, 가능합니다. 그래서 stage와 prod로 나눠서 관리를 하고 싶다 하면 다음과 같이 설정해서 운영할 수 있습니다. 운영 쪽은 확인을 한 번 해주고 수동으로 승인을 해서 배포하는게 나을 것 같습니다.</p><p><img src="/images/devtron/11.png" alt="staging-production" style="zoom:50%;" /></p><p><br></br></p><p>자 이렇게 설정되면 끝입니다. 그래서 뭐가 좋은거냐구요?  ArgoCD처럼 변경사항을 반영해서 자동으로 배포를 할 수 있습니다. 작업을 하는 git 레포 디렉토리에서 변경사항을 만들고 git add - commit - push를 타겟 브랜치로 보내면, 이를 감지하고 자동으로 devtron이 동작하기 시작합니다. 마치 github action을 활용해서 배포를 하는 것처럼 말이죠. </p><p>위에서 등록한 파이프라인대로 쭉 돌아가기 시작하는데, 이 상황은 아래 그림처럼 확인할 수 있습니다. 성공을 했는지 실패를 했는지, 디테일도 볼 수 있고, 배포 된 이후의 상태체크 까지 가능하며 롤백도 가능합니다. 한 큐에 배포까지 자동으로 되는 게 참 편리하지 않나요?!</p><p><img src="/images/devtron/12.png" alt="배포 상황을 알 수 있다."></p><p><br></br></p><h3 id="이외에도…"><a href="#이외에도…" class="headerlink" title="이외에도…"></a>이외에도…</h3><p>Devtron은 Log Analyzer가 내장되어 있어 pod의 logging을 rancher보다 조금 더 편하게 확인 가능하다는 장점이 있습니다.  Rancher는 쌩 로그가 나와서 알아보기 참 어려운 점이 있지만, Devtron은 좀 나을 것 같습니다. 또 vault, external secret을 연결해서 사용을 할 수 있습니다. Rancher는 Vault를 따로 관리하는 느낌인데, Devtron은 쉽게 vault와도 연동이 가능합니다. </p><p>참 다양한 기능이 있어서 Rancher를 아예 떼버리고 Devtron으로 이사갈까?? 싶었지만, Rancher는 또 Rancher만의 장점이 있습니다. 아쉽게도 Devtron에서는 쉘을 따로 열 수 없습니다. Rancher에서는 답답하면 kubectl 쉘을 열어서 필요한 명령어들을 직접 내릴 수 있었지만, Devtron에서는 따로 쉘을 주지 않습니다. Bulk Edit을 할 수 있는 창은 있지만, Rancher에서 주는 쉘과는 다릅니다.</p><hr><p><strong>따라서</strong> 저희 팀은 같이 써보기로 했습니다. k8s에서 파드에 직접 접근해서 수정하거나, 노드를 늘리거나 줄인다거나, 좀 더 로우 레벨에서 작업할 일이 있으면 Rancher를 사용하고 그 외에 CI/CD 쪽은 Devtron을 사용하면 편하게 작업할 수 있을 것 같습니다. 현재는 POC 중이라 큰 문제는 없는 것 같습니다. GitOps가 된다는 점이 아주 매력적으로 다가오네요. 물론 내부적으로 DevOps팀과 Git Token 사용에 대해서 논의는 필요합니다. Github에서 토큰 발행이 너무 무분별하게 이루어진다면, 관리가 어렵게 되거든요. 잘못해서 유출된 토큰이 있게되면 참 곤란해집니다. 권한을 최소한으로 해서 Token을 발행할 수 있도록 하던지, 관리 주체를 한정해서 최소한으로 토큰을 발행할 수 있게 하던지 등등 내부적인 논의가 꼭 필요할 것 같습니다.</p><p><br></br></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p><a href="https://devtron.ai/">https://devtron.ai/</a></p></li><li><p><a href="https://github.com/devtron-labs/devtron">https://github.com/devtron-labs/devtron</a></p></li><li><a href="https://www.youtube.com/watch?v=ZKcfZC-zSMM">https://www.youtube.com/watch?v=ZKcfZC-zSMM</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/05/29/devtron/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또 7기 시작하기</title>
      <link>http://tkdguq05.github.io/2022/05/15/geultto7/</link>
      <guid>http://tkdguq05.github.io/2022/05/15/geultto7/</guid>
      <pubDate>Sun, 15 May 2022 10:06:01 GMT</pubDate>
      <description>
      
        &lt;p&gt;글또 7기 시작하기!&lt;/p&gt;
&lt;p&gt;다짐글 내지 무엇을 작성할지 정리하기.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>글또 7기 시작하기!</p><p>다짐글 내지 무엇을 작성할지 정리하기.</p><span id="more"></span><h1 id="글또-7기"><a href="#글또-7기" class="headerlink" title="글또 7기"></a>글또 7기</h1><p>글또가 무려 7기이다. 3기였나 4기였나, 아무튼 19년 7월에 공부한 걸 정리하자는 목표로 시작했는데 벌써 거의 3년이 다 되어간다. 물론 3년의 시간동안 코로나가 갑자기 터져서 네트워킹은 좀 힘들었던 것 같다. 4기때 잠깐 데이터 분들이 모여서 얘기를 나눴던 것 이후로 오프라인으로 모임을 거의 할 수 없었다. 드디어 끝이 보이기 시작했고 오프라인 모임을 시작해보려고 한다. 7기에 운영진으로 참여하게 되었는데, 글또 시작부터 너무 하고 싶었던 오프라인 모임을 적극적으로 추진해보려고 한다. 확실히 많이 친해질수록 글을 더 자세히 읽게 되고 피드백 퀄러티도 높아지는 것 같았다. 크고 작은 모임을 통해서 사람들이 더 많이 영향을 주고받았으면 좋겠다.</p><p><br> </br></p><h2 id="뭘-해야할까"><a href="#뭘-해야할까" class="headerlink" title="뭘 해야할까?"></a>뭘 해야할까?</h2><p>글또 7기를 통해 해야할 것은 명확하다. 2주간의 기간동안 글 하나 이상을 제출하는 것이다. 하지만 이 커뮤니티에서 글 이상의 가치를 얻어가고 불어놓고 싶은 생각이 있다. 다양한 채널들이 생겼는데, 나보다 어떤 분야에 대해서 더 잘하시는 분이 있으면 더 배워보고, 내가 더 잘 아는 분야가 있다면 더 잘 알려주고 싶다. 그 전에 무엇을 주제로 글을 작성해야 할지, 6개월간 어떻게 살아볼지 정리를 좀 해봐야겠다.</p><h3 id="새해의-초심"><a href="#새해의-초심" class="headerlink" title="새해의 초심"></a>새해의 초심</h3><p>사실 1월부터 어떤 큰 계획을 세워놓고, 또 월 별로 회고를 해보고 있다. 노션에 정리 중이었는데, 블로그 자체를 노션으로 옮길까도 고민해보고 있다. 아무튼 예전에 정리했던 내용들을 한 번 살펴보자.</p><p>노션에는 Milestone이란 제목으로 작성해놨는데, 이 페이지는 크게 네 부분으로 나뉘어져있다. (생각보다 많네?)</p><p><strong>장기, 년, 월,  Not To Do</strong> 이다.</p><p><br> </br></p><h4 id="장기"><a href="#장기" class="headerlink" title="장기"></a>장기</h4><ul><li>45세 전에 원하는 만큼 일하고 생활할 수 있는 캐시플로우를 확보하기</li><li>몸과 마음의 건강<ul><li>독서와 운동</li><li>독서모임을 통해 월 1권의 독서를 한다</li><li>주 3회 이상 운동을 한다.</li></ul></li><li>인생을 풍족하게 만드는 취미활동<ul><li>요리<ul><li>내가 한 요리를 기록해두자, 노션 페이지를 이용</li><li>요리와 함게 곁들인 와인이나 음료들도 기록해두자<ul><li>인스타 와인 계정, 요리계정?</li></ul></li></ul></li><li>제빵?</li><li>독서</li><li>테니스</li><li>피아노</li></ul></li></ul><p>장기 계획으로 <strong>캐시플로우</strong>를 생각했다. 파이어족이란 단어를 막 들었을 때 였는데,  막상 내 인생 목표가 뭘까, 장기적인 목표를 뭘 세우면 좋을까 하다가 괜찮을 것 같아서 정해봤다. 좀 더 근사한 목표를 잡고 싶기도 했지만, 아직까진 좋아보인다.</p><p>두 번째는 <strong>몸과 마음의 건강</strong>이다. 일을 하면서 잃기 쉬운게 건강인 것 같다. 몸도 몸인데, 정신이 망가지고 있는 걸 깨닫기 참 힘들고 다시 올라오기도 힘든 것 같다. 물론 몸이 무너지면서 정신도 흐트러지는 것 같긴한데, 일단 관리할 대상을 두 가지로 정했다. 몸과 마음 둘 다 중요하다. 몸 건강이야 운동으로 챙기면 되는데 마음 건강은 어떻게 해야할까 하다가 책이 마음의 양식이라는 생각이 들었고 개발 도서 외에 책을 안 읽은지 오래됐다는 것을 느꼈다. 그래서 독서도 하고 독서모임도 해야지 하는 찰나에 6기에서 독서모임이 만들어졌고 지금까지 해오고 있다. 혼자 읽는 것보다 모여서 얘기를 나누니 머리에 더 잘 들어오는 느낌이라 좋다. 꾸준히 하고 싶다.</p><p>세 번째는 <strong>취미생활</strong>이다. 돈만 많이 번다고 행복할까?란 생각을 해봤는데 인생이 풍족해야, 또 즐길거리가 많아야 행복할 수 있을 것 같았다. 그래서 취미 생활도 여러가지를 해봐야겠다고 생각했다. 하고 싶은 취미들과 내가 잘하는 것들을 쭉 써봤다. 다 하기에는 시간이 없을 것 같았지만 최대한 추려봤다. 요리나 와인을 기록해야겠다고 생각한 건, 사진만으로는 기억에 잘 남지 않는 것 같았고 기록을 해놓고 정리를 해놔야 공유하기도 쉬울 것 같았기 때문이다. 특히 와인은 마시고 슥 지나가 버리기 마련인데, 이왕 돈 좀 쓴거 기록이라도 해놔야 돈이 덜 아까울 것 같았다. 그래서 와인 계정하나 만들었고 와인 마실일 있을 때마다 올리고 있다. 술 마시면서 올려야되는데, 나중에 올려야지 하다보니깐 귀찮아진다… 다시 올려야겠다.</p><p><strong>최근에는</strong>, <strong>테니스</strong>를 배우기 시작했다. 너무 배워보고 싶었던 운동이었는데, 생각보다 어렵다. 영상에서는 너무 쉬워보였는데, 축구나 농구, 배드민턴이랑 좀 다른 스포츠인 것 같다. 하지만 어려워서 한 번 잘 맞을때 희열이 있다. 6개월 내에 게임 뛰어보는게 목표다.</p><p><br> </br></p><h4 id="년"><a href="#년" class="headerlink" title="년"></a>년</h4><ul><li>데이터 엔지니어로서 커리어를 강화하기<ul><li>데이터 플랫폼팀 업무에 적응한다</li><li>연동 업무나 새벽 작업을 줄일 수 있도록 만든다</li><li>누구나 원하는 데이터를 쉽게 얻을 수 있는 플랫폼을 구성한다<ul><li>기획자나 데이터 사이언티스트들의 불편함이 무엇일까?</li></ul></li><li>다른 엔지니어들의 고민은 무엇인지 공유하기<ul><li>글또나 다른 커뮤니티를 통해서</li></ul></li></ul></li><li>ML 플랫폼 쪽 일을 같이 하면서 ml을 좀 더 쉽고 빠르게 할 수 있게 만들기<ul><li>Kubernetes 익숙해지기</li><li>MLOps 관련 커뮤니티, 세션 꾸준히 참석</li></ul></li><li>독서 모임 꾸준히 해보자<ul><li>개발</li><li>그 외</li></ul></li></ul><p>22년에 할 것들을 생각해봤는데 이 주제는 다분히 커리어적이다. 21년 12월에 이직을 막 하기도 했고, 새 직장에서 뭘 해보고 싶은지, 어떤 커리어를 쌓아가고 싶은지를 정리해봤다.</p><p>먼저 기존 업무에 적응하는게 1순위였다. 지금은 많이 적응했고 어떻게 돌아가고 있는지 감이 잡힌 상태다. 직접 업무를 하나하나 해보고 싶긴 한데, 그 전에 키워야 할 스킬들이 필요한 것 같다.</p><p> 데이터 플랫폼을 이용하는 다른 분들의 고민이 무엇인지 파악하고 싶었다. 좀 더 편하게 쓰고 불편함을 줄이고, 이런 시도들을 해야 매너리즘에 빠지지 않을 것 같았고, 계속 무엇인가를 배우고 성장시킬 수 있을 것 같았다. 연동업무를 많이 맡아야 이런 고민들을 더 해볼 수 있을 것 같은데, 7-8월 쯤 되면 충분한 기회가 올 것 같다.</p><p>ML플랫폼을 구축해나가고 있다. Feature Store나 FastAPI, Kubernetes 등 MLOps에 필요한 스킬들을 키워나가고 있다. MAB프로젝트에 참여하고 있고 ML Engineer분들과 일할 기회가 많아 이 파트를 맡게 되었다. 조금 더 다듬으면 운영 쪽에 배포할 수 있을 것 같은데, 얼른 마무리 하고 배포하고 싶다. 데이터 엔지니어지만 ML쪽을 놓고 싶지 않았는데, 참 다행이다.</p><p><br> </br></p><h4 id="월"><a href="#월" class="headerlink" title="월"></a>월</h4><p>월 별에서는 <strong>직장과 개인</strong>으로 나눠서 작성했다. 월 별로 직장에서 이루고 싶은 것과, 개인적으로 이루고 싶은 것을 나눠봤다. 나중에 지나고 나서 다시보면 참 재밌다. <code>&#39;와 이거 고민많이 했었는데, 이젠 어느정도 할 수 있게 됐네ㅎㅎ&#39;</code>, <code>&#39;아 이건 아직도 못했네...&#39;</code> 란 생각이 교차하는데 다음 계획 세우기에 좋은 것 같다. 근데 3월까지밖에 못했다… 4월부터 좀 게을러진 것 같아서 반성하고 글또 시작과 함께 다시 습관을 들여야겠다.</p><p>(직장 관련 내용이 있어 1월달 내용에 일부 수정)</p><ul><li>직장 - 성공적으로 온보딩 끝내고 실무투입<ul><li>EKS 개발계 구성하기<ul><li>JupyterHub 및 기타 컴포넌트 최적화(Subnet IP, MEM, CPU 등)</li><li>업무 우선 순위에서 밀림, 일단 AA나 운영 airflow에 더 신경쓰기</li><li>개선점 없을지 고민<ul><li>Airflow PodOperator로 띄웠을 때 IP부족해지지는 않는지 확인</li><li>연산최적화 - Ray, Faiss 등</li></ul></li></ul></li><li>연동 작업 파악 및 실제 연동<ul><li>모니터링</li><li>기타 다른 방안 있을지, 개선점은 없을지 고민</li></ul></li><li>SageMaker 테스트</li></ul></li><li>개인 - EKS, 쿠버네티스에 익숙해지기, EKS로 airflow 자유롭게 띄워보기, config 설정<ul><li>EKS 관련된 블로그 글 작성하기 - 하나도 못씀<ul><li>쿠버네티스 개념 글</li><li>EKS로 Airflow 띄우기</li><li>mlFlow 띄우기</li><li>JupyterHub 띄우기<ul><li>문제점은 있음, helm chart 배포 필요</li></ul></li></ul></li><li>CDC 개념 파악</li><li>Deview, Kakao 세션에서 CDC, MLOps 관련 세션 정리</li></ul></li></ul><p>대충 이런 내용들이 있었다. Kubernetes를 써보고 싶었고 내가 운영하는 서비스를 하나 만들어보고 싶었는데, 2월 3월달 쯤 다시보니 너무 뿌듯했다. (4월 5월 내용도 회고를 했다. 요건 업무 적인 내용이 많아서 비밀)</p><p><br> </br></p><h4 id="Not-To-Do"><a href="#Not-To-Do" class="headerlink" title="Not To Do"></a>Not To Do</h4><ul><li>유튜브만 보고 있기</li><li>하염없이 웹 서핑 및 핸드폰</li></ul><p>얼마전에 대나무 숲 채널에도 올라온 것 같았는데, 너무 공감가는 내용이었다. 핸드폰을 너무 많이 보고 있다. Not To Do로 써놓고 나서 유튜브를 보다가 끄고 다른 할걸 해나갔었는데, 애플워치를 사고 나니… 폰을 많이 보게 되는 것 같다. 간만에 Not To Do를 확인 했으니, 자제 해야겠다. </p><p><br> </br></p><h2 id="다시-글또로…-뭘-써볼까"><a href="#다시-글또로…-뭘-써볼까" class="headerlink" title="다시 글또로… 뭘 써볼까?"></a>다시 글또로… 뭘 써볼까?</h2><p>이렇게 글또 7기에 들어가기 전에 어떻게 살아볼까하고 이전에 세워본 계획들을 살펴봤고 남은 6개월도 어떻게 살지 생각해봤다. 이제 뭘 작성할지 정리를 해봤다.</p><ul><li>스터디</li><li>Kubernetes</li><li>MLOps</li><li>데이터 연동</li></ul><p>크게 세 가지 내용이 주가 될 것 같다. 먼저 스터디는 회사에서 조직해보고 싶었다. 이전 회사에서도 스터디나 공유하는 문화를 통해 참 빠르게 성장할 수 있었던 것 같았다. 뭔가 같이 해보고 싶어하는 분들이 있어보였는데, 총대메고 싶지 않아하는? 총대메면 더 열심히 참여할 수 밖에 없다는 걸 알기에 같이 해보자고 했고, 스터디 리딩을 하면서 끝까지 가보려고 한다. Data Intensive Application, 데이터 중심 애플리케이션 책으로 시작을 했는데 첫 스터디를 마쳤고 시작이 좋은 것 같다. 이 내용들도 정리해서 공유할 예정이고, 다른 스터디원들 분에게도 공유해서 글 작성을 권유해볼 생각이다.</p><p><strong>Kubernetes</strong> 는 써야지 써야지 하고 하나도 안썼다. 개념적인 내용을 공부하면서 노션에 작성하긴 했지만 글로 쓸 정도의 완성도가 아니라 메모 정도의 수준이라 다시 정리를 해야할 것 같다. 전체적인 이론 내용을 쭉 보고 업무를 하면서 사용하는 개념만 계속 쓰는데, 그래서 낯선 개념이 등장하면 다시 기억이 잘 나지 않고 있다. 대표적으로 3월인가 4월쯤에 K8S에 장애가 났는데 Pod Description에 Cordon이라는게 나왔다. 어디서 봤는데 하다가 이전에 정리한 걸 보니 정리한 게 있었다. 역시 글로 남기지 않으니 머리에 잘 남지 않는다. 처음부터 다시 보고 완성도 있게 정리를 해봐야겠다.</p><p><strong>MLOps</strong>는 최근에 많이 맡아서 하고 있는 부분이다. Kubernetes 클러스터를 구성해서 다양한 것들을 올려보고 있는데, 이런 앱들을 어떻게 올려야 하는지, 어떻게 잘 사용해야 하는지를 작성해보고 싶다. 기본적으로 Airflow나 MLflow, JupyterHub을 다뤄보고 싶고, Feature Store의 필요성과 Feast를 정리해보고 싶다. 아마 다른 회사들에서도 비슷한 고민을 갖는 엔지니어들이 있는데, 설치 방법이 잘 나와있는 곳이 없어서 시간을 많이 소모하는 것 같았다. 엔지니어 특징 중 하나가 정리를 귀찮아 한다는 건데, 역시 한 분야에 깊게 갈수록 잘 정리된 내용이 별로 없다. 내가 인터넷을 통해 도움을 받았듯, 나도 베풀 차례이지 않을까?</p><p><strong>데이터 연동</strong> 은 데이터 엔지니어로서 가장 많이 다뤄야 하는 부분이다. 이 연동 작업을 하는데에 있어 부족한 점이 있는 게 느껴져서 이걸 보완하는 기술적인 내용을 주로 다뤄볼 생각이다. Java를 공부한 내용을 써봐야 되나 싶긴한데, 너무 재미없을 것 같아서… Java를 통해 재밌는 사이드 프로젝트를 하나 만드는 게 더 좋아보인다. 뭐든 하나 만들어봐야 내 것이 되는 것 같다. 회사의 자원을 빌려서 하고 싶은 걸 좀 해봐야겠다ㅎㅎ.</p><p>이렇게 쓰고 보니 글 쓸게 너무 많다. 2주에 하나? 한 네 개를 작성해야 목표한 걸 다 쓸 수 있을 것 같은데… 적당히 몸과 마음의 건강을 생각하면서 차근차근 써봐야겠다. 사실 글 작성 외에도 글또에 정말 훌륭하신 분들이 많아서 그 분들의 글을 보고 좋은 내용은 팀원들에게 공유하면서 이건 어때요 저건 어때요 하면서 회사에서든 글또에서든 많은 토론을 이끌어내고 싶다. 가만히 앉아서 수동적으로 정보를 수용하려고 하면 배움의 한계가 있는 것을 느꼈다. 처음이었던 3기 4기때 아는 사람도 없고 낯을 가리면서 적극적으로 정보를 공유하지 못한 것 같은 아쉬움이 있다. 다른 분들의 글을, 다른 채널에 있는 분들의 글도 잘 챙겨보고 더 많은 참여를 통해 더 많은 걸 얻어나가고 싶다. 이번 7기, 6개월간 더 많이 성장해보자!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/05/15/geultto7/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2021년 회고</title>
      <link>http://tkdguq05.github.io/2022/01/02/2021-goodbye/</link>
      <guid>http://tkdguq05.github.io/2022/01/02/2021-goodbye/</guid>
      <pubDate>Sun, 02 Jan 2022 06:14:07 GMT</pubDate>
      <description>
      
        &lt;p&gt;다사다난한 2021년 한 해를 보내면서, 회고하기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>다사다난한 2021년 한 해를 보내면서, 회고하기</p><span id="more"></span><h2 id="2022년이-되었다"><a href="#2022년이-되었다" class="headerlink" title="2022년이 되었다."></a>2022년이 되었다.</h2><p>해가 지날수록, 나이를 먹을수록 시간이 더 빠르게 지나가고 있다. 21년은 사실 힘든 일이 더 많이 기억에 남았던 달이었던 것 같은데, 정리가 되려니까 벌써 2022년이 되어버렸다. 뭘 해왔었는지 기억이 잘 안나는데, 블로그나 기록하는 일에 대해서 이 맘 때쯤 다시 한번 고마움을 느낀다. 겨우겨우 글또를 통해서 반 강제적으로 글을 쓰고 있는데, 이를 통해서 어떤 생각을 해왔고 어떤 걸 하고 있었는지 기억이 난다. 이번 글또에서는 좀 더 글을 많이 작성하고 싶었지만, 여러 일이 겹치다 보니… 계획을 실천할 수 없었다. 패스권 2장을 최대한 안써보려고 했지만, 빠르게 소진해버렸고? 다행히도 예치금 차감없이 마무리 지을 수 있을 것 같다. 정말 빠르게 21년이 흘러왔는데, 무슨 일이 있었는지 한 번 생각해 봐야겠다.</p><h2 id="2021년에-대하여"><a href="#2021년에-대하여" class="headerlink" title="2021년에 대하여"></a>2021년에 대하여</h2><ul><li>21년에 작성한 글 목록<ul><li>2021-12-19 <a href="https://tkdguq05.github.io/2021/12/19/kafka-101/">kafka 첫번째, 개념 정리</a></li><li>2021-12-04 <a href="https://tkdguq05.github.io/2021/12/04/linux-top10/">linux top 10 명령어들을 정리해봤다</a></li><li>2021-11-18 <a href="https://tkdguq05.github.io/2021/11/18/Moving/">이직의 과정, 그리고 얻은 것</a></li><li>2021-11-03 <a href="https://tkdguq05.github.io/2021/11/03/EKS-workshop/">EKS-workshop</a></li><li>2021-10-07 <a href="https://tkdguq05.github.io/2021/10/07/docker-hub-limit/">Docker hub pull limit이 발생했다면?</a></li><li>2021-09-12 <a href="https://tkdguq05.github.io/2021/09/12/work-overtime-with-airflow/">Airflow와 야근하기</a></li><li>2021-09-04 <a href="https://tkdguq05.github.io/2021/09/04/sparton/">sparton</a></li><li>2021-08-28 <a href="https://tkdguq05.github.io/2021/08/28/airflow-ecs/">Airflow Workers on ECS Fargate</a></li><li>2021-08-11 <a href="https://tkdguq05.github.io/2021/08/11/yaml-break/">yml 파일을 잘 다뤄보자</a></li><li>2021-08-01 <a href="https://tkdguq05.github.io/2021/08/01/geultto6/">글을 왜 써야 할까? - 고민과 정리의 시간</a></li><li>2021-05-02 <a href="https://tkdguq05.github.io/2021/05/02/geultto5-end/">글또 5기를 끝내고, 회고하기</a></li><li>2021-04-18 <a href="https://tkdguq05.github.io/2021/04/18/AWS-Immersion-DAY/">AWS_Immersion_DAY, 추천 파이프라인</a></li><li>2021-04-04 <a href="https://tkdguq05.github.io/2021/04/04/airflow-clusterization/">Airflow Clusterization</a></li><li>2021-03-07 <a href="https://tkdguq05.github.io/2021/03/07/apriori-FP/">Apriori와 FP-Growth. 추천 시스템 시리즈</a></li><li>2021-02-21 <a href="https://tkdguq05.github.io/2021/02/21/airflow-basic2/">Airflow Basic. 두 번째</a></li><li>2021-01-24 <a href="https://tkdguq05.github.io/2021/01/24/cost-mgt-emr/">AWS 비용관리의 서막, EMR</a></li></ul></li></ul><p>글또 5기부터 시작해서 6기까지 쭉 흘러오다 보니 21년이 다 가버렸다. 작성한 글들을 보니 주로 업무에서 발생한 일들을 작성한 것 같다. 그 외에 이직을 한 내용, 워크샵에서 배운 내용, 회고 글, 하루를 열심히 살았던 스파르톤에 대해서 글을 작성했다. </p><h3 id="21년-초"><a href="#21년-초" class="headerlink" title="21년 초"></a>21년 초</h3><p>21년 초에는 데이터 엔지니어로 전향하고 인수인계 받은 일에 대해서 적응하는 기간이었다. 그 기간에 갑자기 EMR비용이 상승한 일이 있어서 이를 위해서 Cloud FinOps개념을 공부했고, 적용하기 위해서 노력했다. 덕분에 데이터 사이언스 팀에서 발생하는 비용을 측정하고, 관리함으로써 비용을 어느정도 줄일 수 있었다. 하지만 동시에 한계도 느껴졌다. 결국 AWS를 쓰는 모든 사람들이 합의를 하는 어떤 룰을 만들고 지켜나가야할 것 같은데, 혼자 열심히 해봤자 전체적인 관리가 안될 것이 너무 뻔했다. 체계를 잡고 싶었지만 이런 체계를 어디서부터 잡아야 할지, 어떤 자료를 찾아봐야할지, 그리고 실제 팀에서 따라올 수 있을지 감이 잡히질 않았다. 가뜩이나 과중한 업무가 치고들어오는 마당에, 전 조직을 위한 체계는 결국 잡지 못했다. 결국 다른 일을 할 수 밖에 없었다.</p><h3 id="21년-중"><a href="#21년-중" class="headerlink" title="21년 중"></a>21년 중</h3><p>한 차례 전쟁을 치룬 후에는 Airflow 고도화 작업에 착수했다. 특정 작업을 하는 Airflow는 버전도 낮았고, 싱글구조로 돌아가고 있었는데, 워커에 부하가 계속 걸리고 있는 상태였기 때문에 Clsuter화 시켜주는 작업이 필요하다고 생각했다. 그렇게 하다보니, 이 구조를 Kubernetes에 옮겨서 깔끔하게 관리하고 싶다는 생각도 하게 되었다. 하지만 이 당시에는 학습이 많이 부족해서 아이디어만 갖고 있다가 8월 쯤부터 ECS로 옮기고 나서 이것을 Kubernetes로 옮겨보자 라는 목표를 세우게 되었다. ECS까지는 성공했는데, kubernetes는 조금 아쉽다. 이직생각이 없었고 회사에 열점이 더 남아있었다면, 데이터 사이언스 팀 인프라를 모두 Kubernetes상에 옮겨놓고 장에 없이 운영할 수 있을 것 같았는데, 조금 아쉽다. 7-8월쯤 회사에 정이 떨어지니 어떤 작업도 하기 싫어졌다. </p><p>이 맘 때쯤 BigQuery도입을 위해서 설득을 하고 있었다. 의사결정권자분들께 이게 왜 필요하고, 어떻게 확장이 가능한지를 설명했다. 결국 POC를 승인 받았고 작업을 이제 막 하려고 했다. 그런데 데이터를 받는 부분은 개발팀 소관이었고, Pub/sub으로 데이터를 받아야 하기에 도움을 청했다. 자바 소스에 publisher하나 붙이면 되는 간단한 작업으로 생각했는데, 감감 무소식이다가 결국은 할 수 없다는 답변을 받았다. 그리고 이걸 시작으로 다양한 일들이 일어나기 시작하면서 더 이상 회사를 위해 어떤 일도 스스로 나서서 하기 싫어졌다. 2년동안 설득에 설득을 하면서 필요한 것들을 발전시키고 있었는데, 바닥을 보고 나니 어떤 열정이 다 사그러지는 느낌이었다. 조직은 계속 같은 일만 반복하고, 발전이 없어보였고, 이 잔잔한 물결에 나조차도 고여버리고 있는 느낌이 들기 시작했다. </p><p>회사에 데이터 관련 인력으로 맨 처음 들었을때는 참 재미있었다. 아무것도 없었기 때문에 물론 힘들었지만, 하나하나씩 만들어내고, 발전해나가는 모습이 너무 좋았었다. 다른 조직에서 하고 있는 기술들도 가져와서 써보고 토론하면서, 나름 데이터 조직이 제대로 구성되고 있다는 생각을 했었다. 하지만 여기까진 것 같았다. 발전과 성장에 목마른 사람들은 몇 명 남아있지 않았고, 그 불씨도 다 꺼지고 있었다. 힘을 받아서 해야했던 시기에 불씨가 다 꺼져버렸었다. </p><p>그래서 이때부터 이직준비를 했던 것 같다. 예전과 같이 빠르게 성장하는 조직에서 전 처럼 치고 받으면서 모르는 부분을 헤쳐나가고 싶었다. 이직준비를 하게 되면서 글또에 올리는 글이 퀄러티가 급 떨어지는 게 너무 느껴졌었다… 항상 완성도 높은 글을 작성하고 싶었는데, 이직에 회사에 글까지 신경쓰면 부서질 거 같아서 조금 놓아버렸다…ㅠㅠ</p><p>추가로 이때 처음으로 강의를 해보게 됐다. 이직시기와 맞물려서 할까말까 고민했지만, 거절할 수 없는 금액이었다… 엘리스쪽에서 제안을 해주셨는데, 가이드도 잘해주셔서 무리없이 잘 끝낼 수 있었다. 마이크까지 제공받아서 녹음을 해봤는데, 나름 재밌었다. 다음에 기회가 된다면 또 해보고 싶다. </p><h3 id="21년-말"><a href="#21년-말" class="headerlink" title="21년 말"></a>21년 말</h3><p>10월과 11월에는 이직 면접과 그 과정의 마무리에 있었다. 6개정도의 회사와 컨택을 했고 최종 두 회사에 합격을 했다. 주위에 여러 분들이 도와주신 덕에 신중하게 회사를 선택할 수 있었고 그 결과에 만족을 하고있다. 이직도 이직이지만, 회사를 고르는 것도 쉽지 않다는 것을 느꼈다. 새 회사에 합격을 해서 마음은 놓았지만… 남은 다른 분들이 걱정이 되어 마냥 좋지만은 않았다. 능력에 대해서 다시 한번 생각하게 되었는데, 만약 내가 능력이 정말 출중했다면 팀 단위로도 회사를 옮길 수 도 있지 않았을까? 이런 생각도 해봤다. 정말 다행히도 남은 분들도 컬리 근처의 회사에 합격을 하셔서 자주 볼 수 있을 것 같다. 이직하는 시간이 길어질수록 이직이 쉽지 않았을 것 같다고 느꼈어서, 내가 나간 후에 바로 이직을 하게 되셔서 참 다행이라고 생각했다. 물론 전 회사는 많이 착잡하겠지만, 그 동안 계속 이야기해온 걸 조금이라도 들어줬더라면… 조금이라도 더 있지 않았을까 했지만…</p><h3 id="최근"><a href="#최근" class="headerlink" title="최근"></a>최근</h3><p>새 회사에 적응 중이다. 전 보다 규모가 엄청나게 크다보니, 적응이 잘 되지 않고, 보안 규칙도 너무 어색하고, 처음 써보는 것들이 너무 많아서 어렵다. 특히 개발 규모와 하루에 가용하는 데이터 양이 예전에 작업하던 거에 비해, 아니 비교할 수 없을 정도라 작업 실수에 대한 부담이 좀 있다. 예전에는 DB에 직접 접근하고, 권한을 나누지 않고 사용했었는데, DBA님께 쿼리에 대한 허가를 받고 작업을 하는 구조가 이제야 이해가 가기 시작했다. 가파른 성장을 해가고 있고, 다양한 시도를 하고 있는 회사에 입사하게 되어서 참 좋다고 생각하고 있다. 일단 가장 먼저는 EKS와 Rancher 사용법을 빠르게 익혀야 될 것 같다. 1월까지 기한인 일이 몇 개 되어서…(입사한 지 2주밖에 안됐는데!ㅠㅠ) 빠르게 EKS쪽 일에 익숙해지고, CDC쪽을 살펴봐야 할 것 같다. 이쪽에서 사용하는 게 처음이라 또 적응하는데 고생할 것 같지만, 차근차근 테스트를 하면서 몸에 익혀야겠다. 카프카는 아마 CDC쪽 일을 하면서 더 공부할 것 같다. 일단은 EKS를 공부하느라… 카프카 글을 1편 써놔서 빨리 2편을 쓰고 싶지만, 당분간은 업무와 관련된 것을 더 보고 작성해봐야지!</p><p>또 요즘 느끼는 것은, 전체 아키텍쳐를 크게크게 보는 법을 배워야 되겠다는 것이다. 거대한 서비스가 돌아가는 것을 처음봐서 어떻게 뭐가 돌아가는지 이해하는데 오래걸리는 것 같다. 시니어 분들은 어떤 문제를 콕 집어서 이게 이렇게 문제가 되니, 개선이 필요하다고 하시는데 아직 그게 어떤 문제가 되는지 잘 이해가 가지 않는다. 이와 관련된 개발서적을 좀 찾아보고 공부해야겠다. 2022년도 열심히 성장해보자!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/01/02/2021-goodbye/#disqus_thread</comments>
    </item>
    
    <item>
      <title>kafka 첫번째, 개념 정리</title>
      <link>http://tkdguq05.github.io/2021/12/19/kafka-101/</link>
      <guid>http://tkdguq05.github.io/2021/12/19/kafka-101/</guid>
      <pubDate>Sun, 19 Dec 2021 10:26:46 GMT</pubDate>
      <description>
      
        &lt;p&gt;카프카 첫 번째, 상세 개념 다루기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>카프카 첫 번째, 상세 개념 다루기</p><span id="more"></span><h1 id="카프카"><a href="#카프카" class="headerlink" title="카프카"></a>카프카</h1><p>데이터 엔지니어를 하면서 꼭 다뤄봐야할 것 중에 하나가 실시간 처리입니다. 이 실시간 처리에 다양한 기술들이 사용되지만, 카프카가 특히 많이 다뤄지고 있습니다. 카프카는 LinkedIn의 점차 복잡해지고 거대해지는 아키텍쳐에 따라 개발되었습니다. 가면 갈수록 LinkedIn의 데이터 파이프라인이 복잡해지면서 소스와 타깃의 연결을 정리할 필요성이 생겼고, 데이터를 한 곳에 모아 처리할 수 있도록 중앙집중화 해, 관리할 수 있도록 만들었던 것이 카프카 입니다.</p><p><img src="https://media.vlpt.us/images/king3456/post/43c19a96-28fe-45bb-873b-376f8088312c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-02-14%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.57.16.png" alt="Kafak가 적용되면 한 곳에 데이터를 모아 처리할 수 있다"></p><p>이런 식으로 한 번 정리가 된다면, 복잡성을 떨어트릴 수 있고, 시스템간 의존도를 줄일 수 있겠습니다. 이런 구조를 통해서 카프카는 대용량 데이터를 수집하고, 이 데이터를 실시간 스트림으로 소비할 수 있도록 만들어주게 되었고, 이외에도 다양한 기능을 통해서 카프카는 실시간 처리 분야에서 중요한 플랫폼으로서 기능하게 되었습니다.</p><p><br></br></p><h2 id="카프카의-기본-개념"><a href="#카프카의-기본-개념" class="headerlink" title="카프카의 기본 개념"></a>카프카의 기본 개념</h2><p>카프카는 발행-구독(Publish &amp; Subscribe)의 구조로 이루어져 있습니다. 쉽게 말해서 한 쪽에서 어떤 주제에 대해서 메세지를 보내면 이것을 구독했을때, 관련 주제의 메세지만을 받는 구조인 것입니다. 따라서 이 구조에서는 세 컴포넌트가 등장합니다. 바로 <strong>Producer, Consumer, Broker</strong> 입니다.</p><p><img src="https://www.cloudkarafka.com/img/blog/apache-kafka-partition.png" alt="카프가 기본 구조"></p><p>위 그림에서처럼, 프로듀서가 메세지를 브로커에 보내면, 주제에 맞게 이 메세지들을 보관하고 있다가 컨슈머 그룹에서 이 메세지를 가져가는 구조입니다. 프로듀서와 컨슈머는 대충 이름만 보고도 어떤 일을 하는지는 알 수 있을 것 같습니다. 그런데 브로커는 정확히 어떤 일을 하는 것일까요?</p><p><br></br></p><h3 id="카프카-브로커-클러스터-주키퍼"><a href="#카프카-브로커-클러스터-주키퍼" class="headerlink" title="카프카 브로커, 클러스터, 주키퍼"></a>카프카 브로커, 클러스터, 주키퍼</h3><p>카프카 브로커는 데이터를 주고받기 위한 주체입니다. 이 브로커를 통해서 데이터를 분산 저장하며, 이를 통해 장애가 발생하더라도 안정적으로 서비스를 운영할 수 있습니다. 하나의 서버에는 한 개의 카프카 브로커 프로세스가 실행됩니다. 카프카 브로커 서버는 1대로도 기능을 할 수 있지만, 안전성을 위해서 3대 이상의 서버를 1개의 클러스터로 묶어서 운영하는 것을 권장합니다. 이 클러스터 안에 있는 브로커들은 프로듀서가 보낸 데이터를 분산해서 저장해서 안전하게 서비스를 운영할 수 있도록 합니다.</p><p><img src="https://www.tutorialspoint.com/apache_kafka/images/cluster_architecture.jpg" alt="브로커, 클러스터, 주키퍼"></p><p>브로커가 데이터를 프로듀서로부터 전달받으면 프로듀서가 요청한 토픽의 파티션에 데이터를 저장하고, 컨슈머가 데이터를 요청하게되면 파티션에서 저장된 데이터를 전달합니다. 이렇게 전달된 데이터는 파일 시스템에 저장되고 이는 <code>/tmp/kafka-logs</code>에서 확인할 수 있습니다. 이 설정은 <code>config/server.properties</code>의 <code>log.dir</code>옵션에 정의된 디렉토리기 때문에, 원한다면 설정을 변경해도 무방합니다. log에는 메시지와 메타데이터를 저장하고 index에는 오프셋을 인덱싱한 정보를 담았습니다. timeindex에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담겨있는데, timestamp는 브로커가 적재한 데이터를 삭제하거나 압축하는 데 사용합니다. </p><p>카프카를 좀 더 살펴보다 보면 속도 이슈에 대한 의문이 들 수 있습니다. 왜냐하면 메세지가 저장된 다음 다시 읽어야 하기 때문입니다. 보통 디스크 IO는 속도 이슈에 치명적인 요소로 보이곤 합니다. 하지만 카프카는 <strong>페이지 캐시</strong>를 사용해 디스크 입출력 속도를 높였습니다. 이 페이지 캐시는 OS에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역으로, 한 번 읽은 것은 페이지 캐시 영역에 저장하고 추후에 동일 파일 접근이 일어나면 여기에서 읽게 됩니다. 그래서 브로커를 실행하는데 힙 메모리 사이즈를 크게 가져갈 필요가 없게되는 것입니다.</p><p><strong>replication</strong>은 장애 허용 시스템으로 동작하게 하는 원동력으로 데이터 복제는 파티션 단위로 이루어집니다. 만약 토픽 생성할 옵션을 직접 선택하지 않으면 브로커에 있는 설정대로 구동하게 됩니다. 이 복제의 개수의 최대 값은 브로커 개수만큼 주는 것이 좋은데, 복제 개수만큼 저장용량이 증가하는 단점이 있습니다. 하지만 그 만큼 안정성이 증가하는 장점이 있습니다.</p><p>파티션에는 <strong>리더와 팔로워</strong> 개념이 등장합니다. 복제된 파티션의 구분으로 둘을 나눠놓았는데, 여기서 리더란 프로듀서 또는 컨슈머와 직접 통신하는 파티션을 뜻하는 것이고, 나머지가 팔로워 파티션이 됩니다. 팔로워는 리더 파티션의 오프셋을 확인해서 현재 자신이 갖고 있는 오프셋과 차이가 나게 된다면, 리더 파티션으로부터 데이터를 갖고 와서 자신의 파티션에 저장합니다. 이렇게 데이터를 갖고와서 자기 파티션에 저장하는 것이 <strong>복제</strong>라 하는 것입니다. 만약 리더가 죽는다면 어떻게 될까요? 카프카의 장점은 안정성입니다. 만약에 리더가 다운되면 팔로워 파티션 중 하나가 리더 지위를 넘겨받게되어 정상적으로 기능하게 됩니다.</p><p>여기서, 브로커 중에 한 대는 <strong>컨트롤러</strong>의 역할을 맡게 됩니다. 이 컨트롤러는 중요한 요소로서, 다른 브로커들의 상태를 체크하고 브로커가 클러스터에서 빠지게 되었을 때, 해당 브로커에 있는 리더 파티션을 재분배하는 기능을 하게 됩니다. 카프카는 지속적으로 데이터를 처리해야 해서 브로커의 상태가 비정상이라면 빠르게 클러스터에서 빼내고, 새로운 브로커를 지정해 문제를 해결합니다. 만약 컨트롤러 브로커에 장애가 생기면, 다른 브로커가 컨트롤러를 맡게됩니다.</p><p>카프카 토픽의 데이터는 참고로 삭제가 되지 않습니다. <strong>브로커만 데이터를 삭제할 수 있습니다.</strong> 또한 데이터 삭제는 파일 단위로 이루어지는데, 이 단위를 <strong>로그 세그먼트라고</strong> 부릅니다. 이 세그먼트에는 다수의 데이터가 들어있어서, 데이터를 구분해서 삭제할 수는 없습니다. 세그먼트에 데이터가 쌓이는 동안 파일이 열려있는 상태가 되고, log.segment.bytes,  log.segment.ms 옵션에 따라 조건이 충족되면, 세그먼트 파일이 닫히게 됩니다. log.segment.bytes의 기본값은 1GB입니다. 이 용량 설정에서 너무 작은 용량을 설정하게 되면, 세그먼트 파일을 자주 여닫게 되어 부하가 발생할 수 있습니다.</p><p>컨슈머 오프셋 저장을 할때, log.dir 옵션에 지정된 경로로 가면 consumer_offset 디렉토리가 있는 것을 볼 수 있습니다. 컨슈머 그룹은 토픽이 특정 파티션으로 부터 데이터를 가져가서 처리하고 나서 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋합니다. 커밋한 오프셋이 바로 consumer_offset 디렉토리에 저장됩니다. 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리하게 됩니다.</p><p>아까 브로커 중 한 대는 컨트롤러를 맡는다고 했는데, 다수 브로커 중 한 대는 또 코디네이터 역할을 맡습니다. 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배합니다. 컨슈머가 컨슈머 그룹에서 빠지게 되면, 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 데이터 처리가 이어지도록 하는 거입니다. 파티션을 컨슈머로 재 할당하는 이 과정을 <strong>리밸런스</strong>라고 합니다.</p><p>참고로, <strong>주키퍼는</strong> 카프카의 메타데이터를 관리합니다. <code>bin/zookeeper-shell.sh</code> 을 통해서 주키퍼로 직접 접속할 수 있습니다. <code>ls /</code>를 통해서 원하는 정보를 탐색해서 볼 수 있다. 주키퍼에서 다수의 카프카 클러스터를 사용하려면 주키퍼의 서로 다른 znode에 카프카 클러스터들을 설정하면 됩니다. <code>znode</code>란 주키퍼가 사용하는 데이터 저장 단위입니다. znode는 znode간에 tree구조와 같은 계층 구조를 가집니다. 이 구조에서 root znode가 아닌 한 단계 아래의 znode를 카프카 브로커 옵션으로 지정하도록 합니다. 이렇게 설정되었을 때, 각기 다른 하위 znode로 설정된 서로 다른 카프카 클러스터는 각 클러스터의 데이터에 영향을 미치지 않고 정상 동작하게 됩니다.</p><p><br></br></p><h2 id="카프카-토픽-파티션-컨슈머"><a href="#카프카-토픽-파티션-컨슈머" class="headerlink" title="카프카 토픽, 파티션, 컨슈머"></a>카프카 토픽, 파티션, 컨슈머</h2><p><strong>토픽</strong>과 <strong>파티션</strong>은 카프카에서 데이터를 구분하기 위해 사용하는 단위로, 토픽은 1개 이상의 파티션을 소유하고 있습니다. 프로듀서가 보낸 데이터들이 파티션에 저장되는데 이것을 <strong>레코드</strong>라고 부릅니다. 결국 <strong>파티션</strong>에는 오프셋, 메세지 키, 메세지 값으로 되어있는 레코드가 저장되는 것입니다. 이렇게 레코드가 저장된 파티션을 여러개로 나눔으로써 카프카의 병렬처리의 핵심으로 기능하게 됩니다. 파티션은 큐와 비슷한 구조로, 먼저 들어간 레코드를 컨슈머가 먼저 가져갑니다. pop이 되지 않는 것이 큐와의 차이점이다. 데이터가 삭제되지 않기 때문에 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러 번 가져갈 수 있는 것입니다.</p><p><strong>컨슈머그룹</strong>은 레코드를 병렬로 처리할 수 있도록 파티션과 매칭됩니다. 만약 컨슈머 처리량이 제한되어 있다면, 최대한 많은 레코드를 병렬로 처리하는 가장 좋은 방법 중 하나는 컨슈머 개수를 늘려 스케일 아웃 하는 것입니다. 모니터링을 하며 컨슈머 개수를 늘리면서 동시에 파티션 개수도 늘리게 되면 처리량이 증가합니다.</p><p><strong>토픽 이름</strong>을 지을 때는 이름을 잘 지어야 합니다. 이해하기 어려운 토픽이름은 기술 부채가 되기도 하며, 토픽 이름 변경은 카프카에서 지원이 되지 않으므로 변경이 필요하면 삭제 후 다시 생성해야 하기 때문입니다. Jira 티켓 번호를 넣는 것도 괜찮은 방법 중 하나이고,  &lt;환경&gt;.&lt;팀명&gt;.&lt;애플리케이션명&gt;.&lt;메시지타입&gt;등 카프카를 관리하고 사용하는 그룹에서 규칙을 정하고 잘 따라야 합니다.</p><p><strong>레코드</strong>는 타임스탬프, 메세지 키, 값, 오프셋, 헤더로 구성되어 있습니다. 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장됩니다. 브로커에 한 번 적재된 레코드는 수정이 불가능하고 로그 리텐션 기간 또는 용량에 따라서 자동으로 삭제됩니다. timestamp는 프로듀서에서 해당 레코드가 생성된 시점의 유닉스 타임이 설정됩니다. 여기서 프로듀서가 레코드를 생성할 때 임의의 타임스탬프 값을 설정할 수 있고, 토픽 설정에 따라 브로커에 적재된 시간으로 설정될 수 있습니다. 메세지 키는 메시지 값을 순서대로 처리하거나 메세지 값의 종류를 나타내기 위해 사용됩니다. 키의 해시값을 토대로 파티션을 지정하게 되는데, 동일 메세지 키는 동일 파티션에 들어가고 파티션 개수가 변경되면 메세지 키와 파티션 매칭이 달라집니다. 만약 키를 설정 하지 않으면 <code>null</code>로 설정됩니다. 기본 설정 파티셔너에 따라 파티션에 분배되며, 메세지 키와 값은 직렬화되어 브로커로 전송되어 컨슈머가 이용할 때는 역직렬화를 수행합니다.</p><p><strong>컨슈머</strong> 운영 방법은 크게 2가지입니다. 1개 이상의 컨슈머로 이루어진 컨슈머 그룹 운영하거나, 토픽의 특정 파티션만 구독하는 컨슈머 운영하는 것입니다.</p><p><strong>컨슈머 그룹</strong>으로 운영</p><p>컨슈머 그룹으로 운영하면, 각 컨슈머 그룹으로 부터 격리된 환경에서 안전하게 운영할 수 있습니다. 이렇게 컨슈머 그룹으로 묶인 컨슈머들은 토픽의 1개이상 파티션들에 할당되어 데이터를 가져갈 수 있게 됩니다. 1개의 파티션은 최대 1개의 컨슈머에 할당될 수 있으며 1개 컨슈머는 여러 파티션에 할당 될 수 있습니다. 컨슈머 그룹의 컨슈머 개수는 가져가고자 하는 토픽의 파티션 개수보다 같거나 작아야 하는데 만약 컨슈머가 더 많다면, 남는 컨슈머는 파티션을 할당받지 못하고 유휴상태로 남게 되는 것을 주의해야 합니다. 이렇게 되면 스레드만 차지하고 실질적인 처리를 못하므로, 불필요한 스레드로 남게됩니다. 하지만, 컨슈머 그룹으로 운영하면 다른 컨슈머 그룹의 영향을 받지 않고 처리할 수 있습니다. 컨슈머 그룹이 아니라면, 동기적으로 처리를 해야할 수 있는데, 이렇게 되면 어떤 한 적재 파이프라인에 장애가 발생했을때 다른 곳에 적재가 불가능해집니다. 따라서 컨슈머 그룹으로 나눌 수 있는 것은 최대한 나누는 것이 좋습니다.</p><p>그런데 만약, 컨슈머 그룹 컨슈머에 장애가 발생한다면 어떻게 될까요? 그렇게 되면, 장애가 발생한 컨슈머에 할당된 파티션은 장애가 없는 컨슈머에 소유권이 넘어가게 됩니다. 이렇게 되면 리밸런싱이 일어나게 됩니다. 리밸런싱이 일어나는 경우는 크게 2가지 경우입니다.</p><ul><li>컨슈머가 추가 될 때</li><li>컨슈머가 제외될 때</li></ul><p>다만, 리밸런싱은 자주 일어나서는 안됩니다. 왜냐하면 리밸런싱이 발생할 때 재할당하는 과정에서 해당 컨슈머 그룹의 컨슈머들이 토픽의 데이터를 읽을 수 없기 때문입니다. group coordinator는 리밸런싱을 발동시키는데 컨슈머 그룹의 컨슈머가 추가되고 삭제되는 것을 감지합니다. 카프카 브로커 중 한 대가 그룹 조정자 역할을 수행하며 <code>__consumer_offsets</code> 에 기록된 커밋을 통해서 레코드의 어디 까지 읽었는지 파악해서 데이터 처리 중복을 피합니다. </p><p><br></br></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://www.yes24.com/Product/Goods/99122569">http://www.yes24.com/Product/Goods/99122569</a></li><li><a href="https://velog.io/@king3456/Apache-Kafka-%EA%B8%B0%EB%B3%B8%EA%B0%9C%EB%85%90">https://velog.io/@king3456/Apache-Kafka-%EA%B8%B0%EB%B3%B8%EA%B0%9C%EB%85%90</a></li><li><a href="https://jhleed.tistory.com/180">https://jhleed.tistory.com/180</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/12/19/kafka-101/#disqus_thread</comments>
    </item>
    
    <item>
      <title>linux top 10 명령어들을 정리해봤다</title>
      <link>http://tkdguq05.github.io/2021/12/04/linux-top10/</link>
      <guid>http://tkdguq05.github.io/2021/12/04/linux-top10/</guid>
      <pubDate>Sat, 04 Dec 2021 12:58:45 GMT</pubDate>
      <description>
      
        &lt;p&gt;자주 사용하지만 잘 까먹게 되는 명령어들을 정리해봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>자주 사용하지만 잘 까먹게 되는 명령어들을 정리해봤습니다.</p><span id="more"></span><h1 id="Top-10-Linux-명령어-정리"><a href="#Top-10-Linux-명령어-정리" class="headerlink" title="Top 10. Linux 명령어 정리"></a>Top 10. Linux 명령어 정리</h1><p>유튜브 영상을 보다가 자주 사용하는 명령어에 대한 것을 보게 되었고, 그 동안 사용했던 명려어와 관련 내용들을 정리해보면 좋겠다는 생각에 글을 작성하게 되었습니다.</p><p>원본 유튜브 영상 주소는 <a href="https://www.youtube.com/watch?v=u9RukvKZJZM">https://www.youtube.com/watch?v=u9RukvKZJZM</a> 여기 입니다!</p><p><br></br></p><h2 id="1-Server-접속하기"><a href="#1-Server-접속하기" class="headerlink" title="1. Server 접속하기"></a>1. Server 접속하기</h2><p>보통 쉘을 통해 서버로 접속할 때, ssh을 많이 사용합니다. 단순히 ssh를 사용해 서버에 접속하는 것, 그 이상으로 ssh가 무엇인지, 또 어떤 도구로까지 활용할 수 있는지 안다면, ssh를 더 잘 사용한다고 할 수 있을 것입니다.</p><p>ssh는 <code>secured shell</code> 로 암호화 된 채널이기 때문에 보안 상 사용하는 것이 좋습니다. ssh는 사용할 때 붙일 수 있는 다양한 옵션들이 있습니다. AWS에서 특정 서버로 접속할때, pem파일을 사용해서 접속을 한다면 항상 붙이는 <code>-i</code> 옵션이 있습니다. 이것은 무엇에 관한 옵션일까요?</p><ul><li><p>-i 옵션?</p><p><code>-i</code> 는 id_rsa 옵션으로 어떤 암호키가 있다면 이것을 넘겨서 사용하겠다는 의미입니다. 그래서 pem파일 등 암호화된 파일을 사용해서 인증을 하다면 이 옵션을 통해서 인증에 사용할 파일을 지정하는 것입니다.</p></li><li><p>rsa키 인증 방식의 원리?</p><p>잠깐 rsa키 인증 방식에 대해서 짚고 넘어가겠습니다. rsa가 가끔 등장하곤 하는데, 어떤 것인지 제대로 확인하고 넘어간 적이 없었거든요. </p><p>rsa키 인증 방식은 private, public key 두 키를 이용해서 인증 작업을 합니다. user/.ssh 에 있는 authorized_keys를 보면 pem키(private key)의 pair인 public key가 서버에 저장되어 있는 것을 볼 수 있습니다. 그래서 private키를 갖고와서 서버에 저장되어 있는 public key와 대조해, 일치한다면 인증이 된 것입니다. 즉, 서버에 접속이 가능해지는 것입니다.</p><p>이를 <code>key - pair</code> 방식 이라고 합니다. 일반적으로 패스워드 인증방식을 많이 사용하고 친숙하겠지만, 패스워드 방식보다 좀 더 안정한 방식이라고 볼 수 있습니다.</p></li><li><p>(MAC) ssh-add</p><p>이것은 키를 미리 입력해주는 명렁어입니다. pem 키를 포워딩 하는 것으로서, -i옵션을 굳이 붙여 접속하지 않아도 됩니다. 귀찮은 작업이 좀 덜어지겠죠?, -A를 사용한다면, 다음 접속지에서도 같은 키로 그대로 접속할 수 있어, 편하게 사용할 수 있습니다.</p></li><li><p>포트 넣기</p><p>만약 포트를 지정하고 싶다면 <code>-p</code> 를 사용 하면 됩니다. 일반적으로 ssh는 기본 포트를 사용합니다. 22이번이죠. 하지만 보안 이론에는  well known port를 사용하지 말아야 한다는 것이 있습니다. 그래서 /etc/ssh/sshd_config를 통해서 ssh 포트 정보를 변경해서 사용하는 경우가 있습니다. 나만 알고있는 접속가능한 포트를 만들어서 접근을 하겠다는 것입니다. 이렇게 설정을 한다면, 다른 사람은 일반적인 접근 경로가 22번이니까, 자연스럽게 들어오게 되었을 때, 모두 차단할 수 있게 되겠습니다.</p><p><br></br></p></li></ul><h2 id="2-IP를-확인하는-리눅스-명령어-자신의-public-ip-확인"><a href="#2-IP를-확인하는-리눅스-명령어-자신의-public-ip-확인" class="headerlink" title="2. IP를 확인하는 리눅스 명령어, 자신의 public ip 확인?"></a>2. IP를 확인하는 리눅스 명령어, 자신의 public ip 확인?</h2><p>현재 사용하고 있는 서버의 IP를 알고 싶은 경우가 은근히 자주 있습니다. Private IP 확인하고 싶다면 ifconfig를 사용해서 알ㅇ아낼 수 있습니다. 그런데 이 서버의 Public IP를 알고싶다면 어떻게 해야할까요?</p><p>이 서버의 Public IP를 알고싶다면 <code>curl ifconfig.co</code> 를 사용하면 됩니다. </p><p><br></br></p><h2 id="3-웹-사이트가-잘-동작하는지-체크할때"><a href="#3-웹-사이트가-잘-동작하는지-체크할때" class="headerlink" title="3. 웹 사이트가 잘 동작하는지 체크할때?"></a>3. 웹 사이트가 잘 동작하는지 체크할때?</h2><p>띄워놓은 웹 사이트나 서비스를 체크하는 데 어떤 명령어를 사용해야할지 모르는 경우가 있습니다. 주로 curl을 사용하게 되는데, curl에는 다양한 옵션들이 있습니다.</p><ul><li>curl의 다양한 옵션<ul><li>X : 사용할 방식 메소드 선택하기</li><li>d : 함께 전달할 파라미터값 설정하기</li><li>G : 전송할 사이트 url 및 ip 주소</li><li>H : 헤더 정보를 전달하기</li><li>I : 사이트의 Header 정보만 가져오기</li><li>i : 사이트의 Header와 바디 정보를 함께 가져오기</li><li>u : 사용자 정보</li></ul></li></ul><p>이것들을 잘 활용하면 API서버에 GET, POST방식을 선택하고 헤더를 더해서 데이터를 넣어 던지는 것을 만들어 볼 수 있습니다.</p><p><code>curl -X POST -H &quot;Content-Type: application/json&quot;  -d &#39;&#123;&quot;data&quot;:1&#125;&#39; &#123;IP Address&#125;</code> 이런식으로 명령어를 보내서 API가 잘 동작하는지 점검할 수 있습니다.</p><p><br></br></p><h2 id="4-도메인의-IP를-조회하는-방법"><a href="#4-도메인의-IP를-조회하는-방법" class="headerlink" title="4. 도메인의 IP를 조회하는 방법"></a>4. 도메인의 IP를 조회하는 방법</h2><p>도메인의 IP를 확인하고 싶다면 네임서버에 등록된 정보를 조회할 수 있어야 합니다. <code>nslookup</code> 명령어를 잘 사용해야 관련 정보를 갖고 올 수 있을 것입니다. 기본적으로 <code>nslookup &#123;IP Address&#125;</code> 를 하면 도메인의 IP 정보를 알 수 있습니다.</p><p>이를 넘어서 도메인 서버의 동작 원리를 한 번 알아보고 넘어가 보겠습니다.</p><ol><li><p>웹 브라우저에 www.naver.com을 입력하면 먼저 Local DNS에게 “www.naver.com”이라는 hostname”에 대한 IP 주소를 질의하여 Local DNS에 없으면 다른 DNS name 서버 정보를 받음(Root DNS 정보 전달 받음)</p></li><li><p>Root DNS 서버에 “www.naver.com” 질의</p></li><li><p>Root DNS 서버로 부터 “com 도메인”을 관리하는 TLD (Top-Level Domain) 이름 서버 정보 전달 받음</p></li><li><p>TLD에 “www.naver.com” 질의</p></li><li><p>TLD에서 “name.com” 관리하는 DNS 정보 전달</p></li><li><p>“naver.com” 도메인을 관리하는 DNS 서버에 “www.naver.com” 호스트네임에 대한 IP 주소 질의</p></li><li><p>Local DNS 서버에게 “응! www.naver.com에 대한 IP 주소는 222.122.195.6 응답 </p></li><li><p>Local DNS는 www.naver.com에 대한 IP 주소를 캐싱을 하고 IP 주소 정보 전달 </p></li></ol><p><br></br></p><h2 id="5-웹-서버-혹은-DB같은-서버들을-확인하는-방법"><a href="#5-웹-서버-혹은-DB같은-서버들을-확인하는-방법" class="headerlink" title="5. 웹 서버 혹은 DB같은 서버들을 확인하는 방법?"></a>5. 웹 서버 혹은 DB같은 서버들을 확인하는 방법?</h2><p>특정 프로세스들의 동작여부를 확인하고 싶을 때, 일반적으로 알고 있는 ping을 사용하게 됩니다. 그런데, ping으로는 이게 실제로 살아있는지, 동작하는지를 알기는 어렵습니다. 물론 그에 앞서서 웹서버가 뜨면 어떤 tcp로 올라오는지, 어떤 포트로 뜨는지는 미리 알고 있어야겠습니다. </p><p>이런 경우에는 telnet을 주로 사용하게 됩니다.</p><ul><li><p>ping? telnet?</p><p>ping은 단순 체크라 mysql, redis등이 살아있는지 알기가 어렵습니다. 반면에 telnet에 ip와 port를 넣어서 체크를 한다면 적합한 포트넘버로 신호를 보냈을 때, 관련된 프로세스가 살아있는지 여부를 알 수 있습니다.</p><p><code>telnet [ip] [port]</code> 로 명령어를 보낼 수 있습니다. telnet은 udp 지원을 안하는 것을 유념하시기 바랍니다.</p></li></ul><p><br></br></p><h2 id="6-서버가-잘-떠있는지-DB커넥션-확인"><a href="#6-서버가-잘-떠있는지-DB커넥션-확인" class="headerlink" title="6. 서버가 잘 떠있는지, DB커넥션 확인"></a>6. 서버가 잘 떠있는지, DB커넥션 확인</h2><p>포트가 잘 열려있는지 확인을 해야할 일이 자주 발생합니다. 저의 경우에는 airflow를 예로 들자면, 8793포트나 3306, 6379 등의 포트를 확인하는 경우가 많았습니다. 특히 로그의 전송을 담당하는 8793포트를 자주 확인했었습니다. airflow를 사용하면서 UI에서 로그를 확인하려고 하는데, 연결이 되지 않아 로그를 볼 수 없는 경우가 종종 있었기 때문입니다. 그렇다면, 포트가 열려있는지를 봤었어야 했는데, 어떻게 확인하면 좋을까요?</p><ul><li><p>netstat 명령어</p><p>nltp 명령어를 사용해서 원하는 포트의 상태를 확인할 수 있습니다. 주로 <code>netstat -nltp|grep &#123;port&#125;</code>를 통해서 포트가 열려있는지를 봤습니다.</p></li><li><p>netstat 옵션</p><p>-a : 현재다른PC와 연결(Established)되어 있거나 대기(Listening)중인 모든 포트 번호를 확인 </p><p>-r : 라우팅 테이블 확인 및 커넥션되어 있는 포트번호를 확인 </p><p>-n : 현재 다른PC와 연결되어 있는 포트번호를 확인</p><p>-e : 랜카드에서 송수한 패킷의 용량 및 종류를 확인 </p><p>-s : IP, ICMP, UDP프로토콜별의 상태 확인</p><p>-t : tcp protocol </p><p>-u : udp protocol </p><p>-p : 프로토콜 사용 Process ID 노출</p><p>-c : 1초 단위로 보여줌</p><p><br></br></p></li></ul><h2 id="7-process-확인"><a href="#7-process-확인" class="headerlink" title="7. process 확인?"></a>7. process 확인?</h2><p>아까 airflow의 예를 다시 한번 사용해보자면, 현재 서버에 워커가 죽었는지 살았는지를 보고 싶을 때가 있습니다. 이런 경우에는 워커 프로세스의 상태를 확인하면 될 것입니다. 이럴 때 사용하는 명령어가 <code>ps</code> 입니다.</p><ul><li><p>ps 명령어</p><p>ps명령어는 기본적으로 해당 사용자 소유의 프로세스만 보여줍니다. ps를 치게되면 나오는 정보는 다음과 같습니다.</p><ul><li>PID: 프로세스 아이디</li><li>TTY: 프로세스와 연결된 터미널 포트</li><li>TIME: 프로세스에서 사용한 CPU시간</li><li>CMD: 명령어</li></ul></li><li><p><code>ps -ef |grep &#123;PROCESS&#125;</code></p><p>아마 가장 많이 사용하는 명령어이지 않을까 싶습니다. e는 모든 프로세스 상태 출력한다는 의미이고, f는 full format으로 보여달라는 옵션입니다.</p></li><li><p><code>ps -aux|grep</code></p><p>이 역시 많이 사용하는 명령어 입니다.</p><ul><li><p>a: 모든 사용자의 프로세스를 출력하는 옵션</p></li><li><p>u: 특정 사용자의 프로세스 정보 확인, 지정하지 않으면 현재 사용자 기준</p></li><li><p>x: 자신의 터미널이 없는 프로세스도 출력하는 옵션</p><p><img src="/images/linux_top10/스크린샷_2021-11-18_오후_2.56.20.png" alt=""></p></li></ul></li></ul><p><br></br></p><h2 id="8-Linux-CPU-MEM-DISK-시스템-정보-확인"><a href="#8-Linux-CPU-MEM-DISK-시스템-정보-확인" class="headerlink" title="8. Linux CPU, MEM, DISK 시스템 정보 확인"></a>8. Linux CPU, MEM, DISK 시스템 정보 확인</h2><p>서버의 리소스를 보고 모니터링을 하거나, 상태가 어떤지 보고 싶을 때는 보통 <code>top</code>을 많이 사용합니다. 이외에도 htop, sar도 자주 사용합니다.</p><ul><li><p>top</p><p><img src="/images/linux_top10/Untitled.png" alt=""></p><ul><li><p>Load Average 는 <strong>CPU Load의 이동 평균를 표시</strong>합니다.</p></li><li><p>Tasks는 현재 프로세스들의 상태를 보여줍니다.</p><ul><li>실행(Runnable) - CPU에 의해서 명령어가 실행중인 Process</li><li>준비(Ready) - CPU의 명령어 실행을 기다리는 Process</li><li>대기(Waiting) - I/O operation이 끝나기를 기다리는 Process</li><li>종료(Terminated) - Ctrl + Z 등의 signal로 종료된 Process</li><li>Zombie - Process는 root Process로 부터 뿌리내린 자식 Process의 형식으로 트리구조를 형성합니다. 이 때 부모가 먼저 종료된 다면 root process로 부터 닿을 수 없는 Process가 생깁니다. 이를 zombie process라고 부릅니다.</li></ul></li><li><p>CPU 사용량은 <strong>CPU가 어떻게 사용되고 있는지 그 사용율을 보여주는 영역</strong>입니다.</p><ul><li>us : 프로세스의 유저 영역에서의 CPU 사용률</li><li>sy : 프로세스의 커널 영역에서의 CPU 사용률</li><li>ni : 프로세스의 우선순위(priority) 설정에 사용하는 CPU 사용률</li><li>id : 사용하고 있지 않는 비율</li><li>wa : IO가 완료될때까지 기다리고 있는 CPU 비율</li><li>hi : 하드웨어 인터럽트에 사용되는 CPU 사용률</li><li>si : 소프트웨어 인터럽트에 사용되는 CPU 사용률</li><li>st : CPU를 VM에서 사용하여 대기하는 CPU 비율</li></ul></li><li><p>메모리 사용량, MEM</p><ul><li>total : 총 메모리 양</li><li>free : 사용가능한 메모리 양</li><li>used : 사용중인 메모리 양</li></ul><p><br></br></p></li></ul></li><li><p>htop (top보다 더 자세한 정보, 편하게 확인 가능)</p><p><img src="/images/linux_top10/스크린샷_2021-11-18_오후_2.59.58.png" alt=""></p><p><br></br></p></li><li><p>sar (System Activity Reporter)</p><ul><li>user : 사용자 레벨(application level) 에서 실행중일때의 CPU 사용률 (%)</li><li>nice : 사용자 레벨(appliaction level) 에서 nice 가중치를 준 CPU 사용률(%)</li><li>system : 시스템레벨(kernel) 에서 실행중일때의 CPU 사용률(%)</li><li>iowait : system이 I/O요청을 처리하지 못한 상태에서의 CPU의 idle 상태인 시간의 비율(%)</li><li>steal : virtual processer에 의한 작업이 진행되는 동안 virtual CPU에 의해 뜻하지 않는 대기시간이 생기는 시간의 비율(%)</li><li>idle : CPU가 쉬고있는 시간의 %</li><li>Average : 마지막 라인에 출력되며, 각 값의 평균치 이다.</li></ul><p><br></br></p></li><li><p>free</p><ul><li><strong>[total] :</strong> 설치된 총 메모리 크기 / 설정된 스왑 총 크기</li><li><strong>[used] :</strong> total에서 free, buff/cache를 뺀 사용중인 메모리. / 사용중인 스왑 크기</li><li><strong>[free] :</strong> total에서 used와 buff/cahce를 뺀 실제 사용 가능한 여유 있는 메모리량 / 사용되지 않은 스왑 크기</li><li><strong>[shared] :</strong> tmpfs(메모리 파일 시스템), ramfs 등으로 사용되는 메모리. 여러 프로세스에서 사용할 수 있는 공유 메모리</li><li><strong>[buffers] :</strong> 커널 버퍼로 사용중인 메모리</li><li><strong>[cache] :</strong> 페이지 캐시와 slab으로 사용중인 메모리</li><li><strong>[buff/cache] :</strong> 버퍼와 캐시를 더한 사용중인 메모리</li><li><strong>[available] :</strong> swapping 없이 새로운 프로세스에서 할당 가능한 메모리의 예상 크기. (예전의 -/+ buffers/cache이 사라지고 새로 생긴 컬럼)</li></ul></li></ul><p><code>/proc/meminfo</code>를 사용하는 경우도 있는데 free 자체가 /proc/meminfo를 갖고 오는 것이기 때문에 따로 작성하지 않았습니다.</p><p><br></br></p><ul><li><p>df는 사용 가능한 디스크 공간의 양을 알 수 있습니다.</p><p>| 옵션 | Long옵션            | 설명                                                         |<br>| — | ————————— | —————————————————————————————- |<br>|  -a  | —all               | 0 블록의 파일 시스템을 포함하여, 모든 파일시스템을 출력      |<br>|  -B  | —block-size=SIZE   | 지정한 크기(SIZE)를 블록 단위로 정하여 용량을 표시(예:—block-size=1m) |<br>|      | —total             | 총계를 출력                                                  |<br>|  -h  | —human-readable    | 사람이 읽을 수 있는 형태의 크기로 출력(예:1K, 512M, 4G)      |<br>|  -H  | —si                | 1KB는 1,024Byte지만 사용자가 보기 편하도록 1,000단위로 용량을 표시 |<br>|  -i  | —inodes            | inode의 남은 공간, 사용 공간, 사용 퍼센트를 출력             |<br>|  -k  |                     | —block-size=1K와 같은 의미                                  |<br>|  -l  | —local             | 출력하는 목록을 로컬 파일 시스템으로 제한                    |<br>|      | —no-sync           | 사용 정보를 얻기 전에 싱크를 하지 않음(디폴트 값)            |<br>|  -P  | —portability       | POSIX에서 사용되는 형태로 출력                               |<br>|      | —sync              | 사용 정보를 얻기 전에 싱크                                   |<br>|  -t  | —type=TYPE         | 보여주는 목록을 파일 시스템의 타입(TYPE)으로 제한            |<br>|  -T  | —print-type        | 파일 시스템의 형태를 추가하여 각각의 파티션 정보를 출력      |<br>|  -x  | —exclude-type=TYPE | 지정한 형태(TYPE)를 제외하고 나머지 모든 파일 시스템 정보를 출력 |</p></li></ul><p><br></br></p><h2 id="9-service-관리"><a href="#9-service-관리" class="headerlink" title="9. service 관리"></a>9. service 관리</h2><p>서버가 시작할 때 뜨는 프로세스를 관리하고 싶을 때가 있습니다. 그럴때는 daemon이나 service를 살펴보면 됩니다.</p><p>리눅스 서비스 관리하는 명령어는 service와 systemctl이 있습니다.</p><p>서비스를 등록할 때는 이런 파일을 하나 작성합니다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Service Desceiption</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/path/to/shellscript.sh</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Unit</span><br><span class="line">  - Description: 해당 유닛에 대한 설명</span><br><span class="line">  - Requires : 상위 의존성 구성, 포함하는 유닛이 정상적이어야 실행</span><br><span class="line">  - RequiresOverridable : 상위 의존성 구성이며 이것이 실패하더라도 무시하고 유닛을 시작</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">Service</span><br><span class="line">  - Type: [simple | forking | oneshot | notify | dbus] 유닛의 타입</span><br><span class="line">  - Environment: 해당 유닛에서 사용할 환경 변수 선언</span><br><span class="line">  - ExecStart: 시작 명령을 정의</span><br><span class="line">  - ExecStop: 중지 명령을 정의</span><br><span class="line"></span><br><span class="line">Install</span><br><span class="line">  - WantedBy: 유닛을 등록하기 위한 종속성 검사. 유닛을 등록할 때 등록에 필요한 유닛을 지정</span><br></pre></td></tr></table></figure><ul><li>Before=, After=<ul><li>유닛 시작의 전후 관계를 설정합니다. 해당 설정은 “Requires=” 설정과는 독립적입니다. “Before=” 에 나열된 유닛이 시작되기 전에 실행하고 “After=” 은 해당 유닛이 시작된 이후 나열된 유닛이 실행합니다.</li><li>이 설정은 시스템이 종료(shutdown) 될때는 역으로 작동하게 됩니다.</li></ul></li><li>[Install] 섹션<ul><li>systemctl enable [service name] 으로 VM 구동시 서비스가 자동으로 구동되도록 할 때 이용하는 섹션입니다.</li><li>[Install]은 해당 서비스를 등록할 때 사용되는 설정이다. 같이 등록/해지할 서비스나 필요한 서비스등을 지정해줄 수 있습니다.</li></ul></li><li>[WantedBy]<ul><li>서비스가 어떤 전제조건 하에서 실행되는 지를 결정하는 프로퍼티입니다.</li><li>WantedBy는 해당 install한 서비스가 설치되어 있어야 본 서비스를 설치한다는 뜻입니다.</li></ul></li></ul><p>이렇게 만들어진 파일을 /usr/lib/systemd/system에 넣어주고(리눅수 버전이나 종류별로 상이할 수 있음)  서비스를 start해주고 실행이 되고 enable까지 해주면 서버가 재시작 되더라도, 서비스가 실행됩니다. </p><ul><li>service start : <code>systemctl start servicename</code> </li><li>service enable :  <code>systemctl start servicename</code> </li></ul><p><br></br></p><h2 id="10-리눅스-파일-권한-체계"><a href="#10-리눅스-파일-권한-체계" class="headerlink" title="10. 리눅스 파일 권한 체계"></a>10. 리눅스 파일 권한 체계</h2><p>어떤 파일을 실행하고 사용할 때, 권한이 없다는 에러가 나와서 실행이 되지 않는 경우가 아주 많은 것을 아실 겁니다. 이런 경우에는 파일의 권한을 풀어주면 해결이 되곤 합니다. 이럴 때 주로 사용하는 명령어는 <code>chmod</code>그리고 <code>chown</code>입니다.</p><ul><li><p>chmod</p><ul><li><p>파일이나 폴더의 권한(읽기, 쓰기, 실행)를 변경</p></li><li><p>권한들을 <strong>mode</strong>라고 부른다</p><p><img src="/images/linux_top10/스크린샷_2021-11-18_오후_3.11.55.png" alt=""></p><p><img src="/images/linux_top10/스크린샷_2021-11-18_오후_3.12.17.png" alt=""></p></li></ul></li><li><p>chown</p><ul><li>파일, 또는 폴더의 소유권을 변경하는 명령어</li></ul></li><li><p>ls -l을 했을때 나오는 정보에 대한 이해</p><ul><li>long listing format</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">user@user_group:~/exp$</span> <span class="string">ls</span> <span class="string">-l</span></span><br><span class="line"><span class="string">drwxrwxr-x</span>  <span class="number">2</span> <span class="string">user</span> <span class="string">user_group</span> <span class="number">4096 </span><span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">dir1</span></span><br><span class="line"><span class="string">drwxrwxr-x</span>  <span class="number">2</span> <span class="string">user</span> <span class="string">user_group</span> <span class="number">4096 </span><span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">dir2</span></span><br><span class="line"><span class="string">-rw-rw-r--</span>  <span class="number">1</span> <span class="string">user</span> <span class="string">user_group</span>    <span class="number">0</span> <span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">file1</span></span><br></pre></td></tr></table></figure><ul><li>첫번째 블락이 <strong>권한 문자열(Permission string)</strong></li><li>두번째 블락이 해당 디렉토리 내부의 파일과 디렉토리 갯수,</li><li>세번째 블락이 소유주,</li><li>네번째가 소유주가 속한 그룹,</li><li>다섯번째가 크기,</li><li>6,7,8번째는 마지막으로 파일/디렉토리에 접근한 시각,</li><li>아홉번째가 파일/디렉토리의 이름을 의미한다.</li></ul></li></ul><p>root와 chmod 777 는 full 권한을 부여하는 것이므로 사용을 지양합니다.</p><p><br></br></p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://www.whatap.io/ko/blog/37/">https://www.whatap.io/ko/blog/37/</a></li><li><a href="https://sabarada.tistory.com/146">https://sabarada.tistory.com/146</a></li><li><a href="https://happist.com/557995/%EC%84%9C%EB%B2%84-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-htop-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95-ubuntu">https://happist.com/557995/서버-모니터링-프로그램-htop-사용-방법-ubuntu</a></li><li><a href="https://csjung.tistory.com/50">https://csjung.tistory.com/50</a></li><li><a href="https://jhnyang.tistory.com/268">https://jhnyang.tistory.com/268</a></li><li><a href="https://muckycode.blogspot.com/2016/09/linux-chown-vs-chmod.html">https://muckycode.blogspot.com/2016/09/linux-chown-vs-chmod.html</a></li><li><a href="https://ijbgo.tistory.com/27">https://ijbgo.tistory.com/27</a></li><li><a href="https://blog.voidmainvoid.net/201">https://blog.voidmainvoid.net/201</a></li><li><a href="https://springboot.cloud/16">https://springboot.cloud/16</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/12/04/linux-top10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>이직의 과정, 그리고 얻은 것</title>
      <link>http://tkdguq05.github.io/2021/11/18/Moving/</link>
      <guid>http://tkdguq05.github.io/2021/11/18/Moving/</guid>
      <pubDate>Thu, 18 Nov 2021 06:54:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;인수인계서를 작성하면서 쓰는 이직 후기.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>인수인계서를 작성하면서 쓰는 이직 후기.</p><span id="more"></span><h2 id="이직에-성공하다"><a href="#이직에-성공하다" class="headerlink" title="이직에 성공하다"></a>이직에 성공하다</h2><p>2년 4개월. 첫 입사한 회사에서 벌써 2년이 넘는 시간이 흘렀습니다. 참 우여곡절이 많았고 재밌었던 일도 많았던 것 같습니다. 이직을 해야겠다고 마음을 먹은건 대략 5-6개월 정도 된 것 같은데,  결실을 맺을 수 있어서 다행이라고 생각합니다. 준비도 준비지만 이직과정에서 느낀 건 실력도 중요하지만, 운이 있어야한다, 즉 운칠기삼이라는 것이었습니다. 아마 이직 과정에서 운이 없었다면 이직에 도전하는 데에도 시간이 더 걸렸을지 모르고, 면접에서 탈락했었을 지도 모르겠습니다. 앞으로도 항상 겸손하게, 묵묵히 할 일을 해 나가야 할 것 같다는 생각입니다.</p><p><br></br></p><h2 id="이직-과정"><a href="#이직-과정" class="headerlink" title="이직 과정"></a>이직 과정</h2><p>저의 첫 면접은 7월이었습니다. 7월초에 링크드인을 통해서 N사의 데이터 엔지니어 직무를 제안을 받게되었습니다. 당시에는 이직에 대한 생각정도만 갖고 있어서, 이력서나 포트폴리오가 제대로 준비가 되어있지 않은 상태였습니다. 그래서 3일 안에 이력서를 채우고 포트폴리오를 구성해서 조금 급하다는 느낌이 들긴했지만, 이력서와 포트폴리오를 전달했습니다. 서류는 통과가 되었고 1차 전화면접을 준비했습니다. 아무래도 대기업이다 보니 기본적인 내용이 나올 것 같아, API나 네트워크, CS관련 내용들, 파이썬 기본 개념들을 다시 봤습니다. 몇몇 질문에는 대답하지 못했지만, 전화면접까지 통과가 되었습니다. 사실상 거의 마지막 관문이라고 볼 수 있는 기술 면접 일정이 잡혔고, 제가 했었던 프로젝트와 그 프로젝트에 들어가 있는 기술 내용들을 더 깊이 공부했습니다. 사실 경력직 이직 면접이 처음이라 어디까지 질문이 나올지 몰라 시간을 많이 썼습니다. 유일한 취미였던 운동마저 포기하고 건강이 점점 안좋아지는 걸 느끼면서 나름 준비를 마쳤습니다. 기술면접은 자그마치 3시간이었습니다. 면접관 세 분이서 한시간씩 사용하셨고, 약 30-40분간의 라이브 코딩시간이 있었습니다. 라이브 코딩은 한 번만 할 줄 알았는데, 체력과 정신력의 한계를 느낄 수 있었던 시간이었습니다. 라이브 코딩 외에 질문들은 대개 프로젝트에 대한 내용은 일부분이었고, 하둡의 어떤 개념을 아는지? 스파크의 어떤 걸 아는지? 문제는 어떻게 해결했는지 등의 문제가 대부분이었던 것 같습니다. 물론 면접 결과는 불합격이었습니다. 그술적으로 부족한 부분이 있긴 했지만, 이력서나 포트폴리오에 없는 기술들에 대한 질문이 있어서 난처했었습니다. ‘’관련 기술이 없는데 왜 기숢면접까지 부른거지…?’’란 생각이 들었지만, 어찌 됐든 저의 부족한 점을 느낄 수 있었고, 대기업의 프로세스를 경험해 볼 수 있어서 좋은 시간이었습니다.</p><p><img src="https://www.desktopbackground.org/download/1600x900/2015/03/14/917047_depression-sad-mood-sorrow-dark-people-wallpapers_1920x1080_h.jpg" alt=""></p><p>많은 생각이 드는 면접이었지만, 느낀 것은 항상 준비가 되어있어야 한다는 것이었습니다. 급하게 2주일정도 준비를 하느라 신체적으로나 정신적으로나 면접 전에 너무 지쳐있었고, 기술 내용들을 다시 숙지하기에도 벅찼던 것 같습니다. 그래서 본격적으로 준비를 해야겠다고 마음먹고 준비해야할 리스트들을 작성하기 시작했습니다.</p><p><br></br></p><h2 id="이직을-위해-필요한-것"><a href="#이직을-위해-필요한-것" class="headerlink" title="이직을 위해 필요한 것"></a>이직을 위해 필요한 것</h2><p>저의 부족한 점을 곰곰히 생각해보며 리스트화 해봤습니다.</p><ul><li>코딩 테스트</li><li>알고리즘</li><li>이전 프로젝트 기술 숙지<ul><li>Docker</li><li>Spark 등등</li></ul></li></ul><p>라이브코딩로 한 번 시달리고 나니, 코딩 테스트로 시간 뺏기기 싫다는 생각이 들었습니다. 그래서 막 문제를 풀까? 생각을 해봤는데, 한 번도 제대로 코딩 테스트 교육을 받은 적이나, 자료구조를 정리해 본적이 없다는 것을 알게 되었습니다. 그래서 맘을 독하게 먹고 약 40?만원 정도 하는 강의를 꼼꼼히 살펴보고 담당 매니저와 통화까지 한 다음 바로 결제를 해버렸습니다. 그리고 약 5-6주동안 열심히 이론을 공부하고 실습 문제를 풀었습니다. 문제를 풀면서 어떤 문제에 대해서 어떻게 접근을 할 것인지를 많이 고민했던 것 같습니다. 특히 스택, 큐에 대한 문제가 잘 풀리지 않았는데, 이럴 때는 관련 문제들을 리스트업하고 내가 세운 문제 해결 전략이 잘 들어 맞는지, 안 맞았다면 어떤 게 문제가 되었던 것인지를 정리했습니다. </p><p>그리고 이전 프로젝트 기술 숙지를 하는 부분에서, 사실 한 게 너무 많아서 건드린 기술 스택이 너무 많았습니다. 그래서 대답을 하지 못할 것 같거나, 너무 얕게 본 기술, 그리고 서비스화하려다 실패한 프로젝트는 생략하기로 했습니다. 그리고 기술을 다시 보면서 이해가 되지 않고, 왜 이렇게 했는지 이해가 안가는 부분은 고민해보고 다시 고쳐보고 고도화 하면서 체화되도록 했습니다. 이렇게 준비를 하는 도중에 또 하나의 메세지가 오게됩니다.</p><p><br></br></p><h2 id="준비한-걸-쏟아내기"><a href="#준비한-걸-쏟아내기" class="headerlink" title="준비한 걸 쏟아내기"></a>준비한 걸 쏟아내기</h2><p>참 신기했던 건 거의 한 날에 이직관련 메세지가 동시에 전달되었다는 것이였습니다. 물론 그 중에 제일 처음으로 받은 메세지는 바로 이직하기로 결정한 회사였습니다. 이 회사에 대한 정체는 글 마지막에 알려드리도록 하겠습니다(<del>스크롤 드르륵</del>). 맨 처음 이직 준비를 하던 때와는 다르게 당황스럽거나 어떻게 하지? 이런 생각은 들지 않았습니다. <code>&#39;해보지 뭐&#39;</code>, <code>&#39;문제없어&#39;</code> 이런 생각이 먼저 들었던 것 같습니다. </p><p>한 번 준비를 해놓고 나니 그 이후의 준비는 훨씬 수월했습니다. 정리를 해놨기 때문에 다시 보기에도 편했고 머리에 이미 정리되어 있어 다른 곳에 시간을 더 쓸 수 있었습니다. 그래서 기술이나 저에 대한 이야기를 다 정리해 놓은 다음, 회사에 대해서 더 자세히 공부하고 면접에 들어갔습니다. 회사에 대해서, 사용하는 기술에 대해 공부를 하고 가면 할 얘기가 훨씬 많아지는 것 같습니다. 약 4-5개의 회사에 대해서 면접을 봤는데, 저의 프로젝트에 대한 얘기에서는 한 회사를 제외하고는 크게 막히는 부분은 없었고 수월하게 진행이 되었습니다. 그리고 면접 마무리 때 궁금한 게 있으면 질문을 해달라라고 하셨을 때, 파이프라인에 대해 궁금한 부분들을 물어봤고, 고도화 방향이 어떻게 되는지, 오픈소스나 클라우드 제품을 어떤 것을 쓰는지를 질문했습니다. 기대와 다른 부분도 있었고 기대대로 좋은 스택을 지닌 회사들도 있었습니다. 이 질문에서는 잘 보이기 위해 질문을 한다기 보다는, ‘’나도 면접에 참여하는 사람이다’’, ‘’나도 회사에 대한 정보를 얻고싶다’라는 생각으로 접근했습니다. </p><p>면접때 잘 풀리지 않았던 한 회사는 중고거래 회사였습니다. 세션을 보다가 너무 멋진 데이터 엔지니어가 있어서 같이 일해보고 싶은마음에 지원을 했습니다. 면접에서 막혔던 부분은, 저도 회사에서 이해가 안가는 부분이었습니다. 로그 처리에 관한 것이었는데 한 번 면접관이 이해가 가지 않기 시작하면, 다른 부분에서도 의문을 품기 시작하는 것을 알게 되었습니다. 물론 저도 회사에서 면접관으로 참여했었던 적이 있는지라, 논리적으로 설득하려고 했지만, 저조차 이해가 안가는 부분에 대해서는 답변을 하지 못했었습니다. 아쉽지만 불합격!</p><p><br></br></p><h2 id="결정"><a href="#결정" class="headerlink" title="결정"></a>결정</h2><p> 면접이 모두 마무리되었고, 최종적으로 두 회사에 합격하게 되었습니다. 사실 두 회사의 성격이 완전 달라서 한 회사를 선택하면 길이 달라지는 느낌이 있었습니다.</p><p><br></br></p><ul><li>A 회사<ul><li>새로오신 팀장님 (같이 일한 동료를 통해 들었을 때 훌륭한 분!)</li><li>DE팀이지만 DS일도 같이 할 수 있음</li><li>잡플래닛 등 평점 안 좋음</li><li>복지는 조금 부족…</li></ul></li><li>B 회사<ul><li>코로나 이후에 폭발적인 성장이 기대됨</li><li>엔지니어링 + 개발만 할 것 같음</li><li>평점 좋음</li><li>복지 괜찮음</li></ul></li></ul><p><br></br></p><p>두 회사를 간단하게 정리하면 위와 같았습니다. 제시된 연봉은 비슷했기 때문에 돈, 사람, 일 중 사람과 일을 갖고 선택을 해야겠다고 생각했습니다. 정보가 너무 없어서 주위에 도움을 많이 요청했습니다. 회사의 분위기가 어떤지 기술이 어떤지… 등등 그러던 중에 글또 모임의 장이신 성윤님께 상담을 받아보고 싶었습니다. 예전 성윤님이 회사를 선택하실때 글에서 관련내용을 본 적이 있어서 도움을 받을 수 있을 것 같았습니다. </p><p>바로 연락을 드렸고, 흔쾌히 상담을 해주셨습니다. 바쁘신 와중에 많은 얘기들을 해주셨는데, 주된 얘기는 다음과 같았습니다.</p><ul><li>잡플래닛 별점? 별로 신경쓰지 마라<ul><li>볼거면 개발직군의 별점이 어떤지 따로 볼 것</li><li>새로 만들어진 팀일 경우, 별점은 더욱 더 의미가 없다</li></ul></li><li>연봉보다는 앞으로의 성장성에 무게를 두는 게 좋을 것 같다<ul><li>연봉은 나중에 일을 잘하게 되면 알아서 따라올 거다</li><li>어디에서 능력을 키울 수 있는지가 중요</li></ul></li><li>돈, 사람, 일 중에 돈이 가장 숫자가 명확하다 하지만<ul><li>사람과 일은 부딪혀보면 나오는 정보가 많다</li><li>회사에 인터뷰를 요청해서 사람과 일에 대해 후회없이 물어봐라</li><li>힘든 점에 대해서 솔직하게 말을 해주는 지 알아봐라, 힘든 포인트를 예상해서 어떻게 답변하는지 체크해라<ul><li>팀원들이 힘들어하면 어떤 조치를 하시는지, 실례가 있었는지 등</li></ul></li><li>사람보다는 회사의 문화가 더 중요하다. 사람은 나가면 그만이지만, 문화가 잘 만들어져 있으면, 좋은 사람은 다시 나온다.</li></ul></li><li>내가 정말 좋아하는 일, 어떤 회사가 좋은 회사인지를 더 생각해보자<ul><li>어떤 회사가 좋은 회사인지를 바탕으로 회사를 비교해보자</li><li>이직할 회사에 찜찜한 점이 있다면 질문으로 바꿔서 질문해보자</li></ul></li></ul><p><br></br></p><p>상담을 하고 나니 너무 소극적으로 임하고 있다는 생각이 들었습니다. 하루의 반 이상을 일하면서 지내는데, 그냥 하나 선택해서 될대로 되라! 하는 것 같았습니다. 바로 두 회사에 티 타임을 요청했고, 질문리스트를 정리했습니다. 그리고 공교롭게도 두 회사는 같은 날에 티 타임 미팅이 잡히게 되었습니다.</p><p><br></br></p><h3 id="첫-번째-티타임"><a href="#첫-번째-티타임" class="headerlink" title="첫 번째 티타임"></a>첫 번째 티타임</h3><p><img src="https://lordofghent.com/wp-content/uploads/2017/05/121014063411-coffee-meeting-table-story-top.jpg" alt=""></p><p>첫 번째 회사(위에서 정리한 B 회사)는 인사팀 분의 안내가 좋았습니다. 친절히 맞아주셨고, 엔지니어 팀분들과 얘기를 나누기 전에 회사를 한 번 구경시켜주시는 점이 인상깊었습니다. 회사 구경을 쭉 한 다음 미팅룸에서 얘기를 나누기 시작했습니다. </p><p><br></br></p><p>질문은 다음과 같았습니다.</p><ul><li><p>데이터 플랫폼팀은 어떤 일을 하는 조직인지?</p></li><li><p>나는 어떤 일을 하게 되는지</p></li><li><ul><li>주로 어떤 사람과 일을 하게 되는지</li><li>내가 주로 의사소통하는 대상은 누가되는지</li></ul></li><li><p>팀 조직 구성은?</p></li><li><p>업무하시면서 재밌으셨던 일은</p></li><li><p>리드분은 어떤 비전을 갖고 계시는지, 어떤 일을 하고 계시고 계속 하고 싶으신지</p></li><li><p>어떤 문화를 만드실 계획인지</p></li><li><ul><li>스터디나 성장하는 문화 등</li></ul></li><li><p>팀에서 힘들어하는 팀원을 어떻게 캐치하시는지?</p></li><li><ul><li>그런 팀원이 있다면 어떻게 케어하시는지</li></ul></li><li><p>저에게 부족한 부분이 있다면?</p></li></ul><p><br></br></p><p>질문에 대해서 하나 하나 얘기해주셨고, 실제로 동작하는 파이프라인까지 보여주셔서 감명이 깊었습니다. 그리고 제가 맡게되는 일에 대해서도 자세히 알려주셔서 제가 어떤 일을 하게되는지 구체적으로 알 수 있었습니다. 그리고 대화를 해보면서 팀에서 어떻게 일이 수행되는지, 사람들의 스타일은 어떤지 간접적으로 알 수 있었습니다. 확실한 건 역시 개발-엔지니어링 쪽으로 특화되서 성장하게 될 것이라는 것이었습니다.</p><p>다음 회사로 가기 전 잠시 카페에 와서 내용들을 정리하고 이 회사에서 일하면 어떨지를 곰곰히 생각해 봤습니다. 나에게 부족한 부분이 무엇인지, 그 부분을 이 회사에서 채워나가고 잘 성장해나갈 수 있을지, 그리고 현재 회사의 팀원들이나 분위기들을 한 번 비교해 봤습니다.</p><p><br></br></p><h3 id="두-번째-티타임"><a href="#두-번째-티타임" class="headerlink" title="두 번째 티타임"></a>두 번째 티타임</h3><p>생각을 정리하고, 다음회사에 대한 질문을 다시 한번 살펴본 뒤 두 번째 회사(A 회사)로 향했습니다. 라운지에는 팀장님이 나와계셨고 인사를 하며 자리에 앉았습니다. 굉장히 캐주얼하게 얘기를 했습니다. 질문 리스트에 있는 얘기부터, 일과 아카데미, 취미 등등 자연스럽게 얘기가 흘러갔습니다. 일도 일이었지만, 이전 회사에 비해 자연스럽게 대화가 흐른 다는 것을 느낄 수 있었습니다. 동시에 제가 가장 중요하게 생각하는 것이 무엇인지 깨닫게 되었습니다. <code>커뮤니케이션</code> . 개발이나 데이터 엔지니어링이나 코드를 많이 사용한다고 하더라도, 어쨌든 사람이 하는 일이기 때문에 의사소통이 필수적입니다. 여러 경험으로 봤을 때, 커뮤니케이션이 어려웠을 때 일이 진도도 안나가고 스트레스를 많이 받았던 기억이 많았었습니다. 팀장님과 무려 한 시간 정도 이야기(거의 수다에 가까운)를 하고 HR분과 다시 이야기를 한 후에 집으로 돌아왔습니다. </p><p>두 회사를 갔다오고 나서 느낀 건 내가 진짜 원하는 회사가 무엇인지 알게 되었다는 것입니다.  <code>&quot;대화가 되는 팀에서 일하고 싶다&quot;</code> 이것이 이직을 하면서 얻은 것 중 하나였습니다. 생각이 정리된 후, 저는 두 번째 티타임을 가진 회사로 간다는 의사를 해당 회사에 전달했습니다.</p><p><br></br></p><h2 id="결정-후"><a href="#결정-후" class="headerlink" title="결정 후"></a>결정 후</h2><p>연봉협상이 마무리되고 현 회사, 전 직장이 될 회사에 이직 사실을 밝혔습니다. 팀장님은 많이 놀라신 것 같았지만, 축하한다고 해주셨습니다. 이후에 이사님과 면담이 잡히고 그래도 1주일 정도는 생각해 줄 수 있냐고 물어보셨습니다. 바로 아니라고는 말을 못할 것 같아서 그 다음주에 제 생각에는 변합이 없다고 알려드렸습니다. 제가 들어오고 데이터 팀이 꾸려지기 시작해서, 많이 아쉬워 하셨습니다. 그리고 이제 인수인계서를 작성하고, 대표님 면담을 기다리고 있습니다. </p><p>팀을 꾸리면서 시작했던 슬랙과 봇들, 그리고 노션에 정리한 글들을 다시한번 보면서 아쉬운 감정이 들었습니다. 동시에 내가 한게 별로 없다고 생각했는데, 생각보다 많은 걸 해왔구나 되뇌었습니다. </p><p><br></br></p><p><img src="https://blog.kakaocdn.net/dn/bjjrHJ/btqBPXNC9tF/Sibyn4CJ4fWFTMaqvUYt01/img.jpg" style="zoom:24%;" /></p><p>오늘 부로 퇴사일이 정리되고 출근날이 정해졌습니다. 12월 중순부터 이제 새 회사인 컬리의 데이터 엔지니어로 출근을 하게 되었습니다. 아쉬움이 많이 남은 만큼, 후회하지 않도록 더 열심히 일하고 성장해야겠다고 다짐했습니다. 성공적인 스타트를 한 회사가 어떻게 성장할 수 있었는지, 폭발적인 성장에 기술적으로 어떻게 대응했고, 대응하고 있는지를 경험해보고 싶네요. 앞으로 남은 시간동안 잘 마무리하고 재충전을 하면서, 어떻게 팀에 기여를 할 수 있고 어떤 방향으로 성장해야할지를 좀 더 정리해봐야겠습니다. 도움과 응원을 보내 주신 글또 여러분들과 성윤님, 그리고 데이터 사이언스 팀원들에게 고마운 마음을 글을 통해 전합니다. 감사합니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/11/18/Moving/#disqus_thread</comments>
    </item>
    
    <item>
      <title>EKS-workshop</title>
      <link>http://tkdguq05.github.io/2021/11/03/EKS-workshop/</link>
      <guid>http://tkdguq05.github.io/2021/11/03/EKS-workshop/</guid>
      <pubDate>Wed, 03 Nov 2021 05:40:24 GMT</pubDate>
      <description>
      
        &lt;p&gt;EKS를 다뤄보고 GitOps까지 실습해봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>EKS를 다뤄보고 GitOps까지 실습해봤습니다.</p><span id="more"></span><ul><li><p>실습 링크 : <a href="https://aws-eks-web-application.workshop.aws/ko/10-intro.html">https://aws-eks-web-application.workshop.aws/ko/10-intro.html</a></p><p>실습 영상 : <a href="https://www.youtube.com/watch?v=kb6s0Tmp2CA&amp;ab_channel=AWS%ED%95%9C%EA%B5%AD%EC%82%AC%EC%9A%A9%EC%9E%90%EB%AA%A8%EC%9E%84-AWSKRUG">https://www.youtube.com/watch?v=kb6s0Tmp2CA&amp;ab_channel=AWS한국사용자모임-AWSKRUG</a></p><blockquote><p>AWS 워크샵에서는 다음 내용을 다룹니다.</p></blockquote><ul><li>AWS Cloud9을 통한 실습 환경 구축</li><li>도커를 이용하여 컨테이너 이미지 생성</li><li>컨테이너 이미지를 ECR에 업로드</li><li>Amazon EKS 클러스터 구축 및 서비스 배포</li><li>Container Insights 사용해보기</li><li>파드 및 클러스터 오토 스케일링</li><li>AWS Fargate로 서비스 올리기</li></ul><p><br></br></p><h2 id="Kubernetes-k8s"><a href="#Kubernetes-k8s" class="headerlink" title="Kubernetes(k8s)"></a>Kubernetes(k8s)</h2><ul><li>쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있고, 확장가능한 오픈소스 플랫폼입니다. 쿠버네티스는 선언적 구성과 자동화를 모두 용이하게 해주는 컨테이너 오케스트레이션 툴입니다.</li></ul><blockquote><p>🌐 쿠버네티스에 대해 더 자세히 알고 싶다면 <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">여기</a> 를 클릭하세요.</p></blockquote><p><img src="/images/eks-workshop/kuber_architecture.png" alt="쿠버네티스_아키텍쳐"></p><ul><li>쿠버네티스를 배포하면 <strong>클러스터</strong>를 얻습니다. 그리고 이 클러스터는 <strong>노드들의 집합</strong>입니다. 노드들은 크게 두 가지 유형으로 나눠지는데, 각각이 <strong>컨트롤 플레인</strong>과 <strong>데이터 플레인</strong>입니다.<ul><li>컨트롤 플레인(Control Plane)은 워커 노드와 클러스터 내 파드를 관리하고 제어합니다.</li><li>데이터 플레인(Data Plane)은 <strong>워커 노드들</strong>로 구성되어 있으며 컨테이너화된 애플리케이션의 구성 요소인 <strong>파드</strong>를 호스트합니다.</li></ul></li></ul><p><br></br></p><h2 id="Kubernetes-Objects"><a href="#Kubernetes-Objects" class="headerlink" title="Kubernetes Objects"></a>Kubernetes Objects</h2><ul><li>쿠버네티스의 오브젝트는 <strong>바라는 상태(desired state)를 담은 레코드</strong>입니다. 오브젝트를 생성하면 쿠버네티스의 컨트롤 플레인에서 오브젝트의 <strong>현재 상태(current state)</strong> 와 바라는 상태를 일치시키기 위해 끊임없이 관리합니다.</li><li>쿠버네티스의 오브젝트에는 파드(pod), 서비스(service), 디플로이먼트(Deployment) 등이 있습니다.</li></ul></li></ul><p>  <br></br></p><h2 id="EKS"><a href="#EKS" class="headerlink" title="EKS"></a>EKS</h2><ul><li><a href="https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html">Amazon EKS</a>는 Kubernetes를 쉽게 실행할 수 있는 관리형 서비스입니다. Amazon EKS를 사용하시면 AWS 환경에서 Kubernetes 컨트롤 플레인 또는 노드를 직접 설치, 운영 및 유지할 필요가 없습니다.</li><li>Amazon EKS는 여러 가용 영역에서 Kubernetes 컨트롤 플레인 인스턴스를 실행하여 고가용성을 보장합니다. 또한, 비정상 컨트롤 플레인 인스턴스를 자동으로 감지하고 교체하며 자동화된 버전 업그레이드 및 패치를 제공합니다.</li><li>Amazon EKS는 다양한 AWS 서비스들과 연동하여 애플리케이션에 대한 확장성 및 보안을 제공하는 서비스를 제공합니다.<ul><li>컨테이너 이미지 저장소인 <strong>Amazon ECR(Elastic Container Registry)</strong></li><li>로드 분산을 위한 <strong>AWS ELB(Elastic Load Balancing)</strong></li><li>인증을 위한 <strong>AWS IAM</strong></li><li>격리된 <strong>Amazon VPC</strong></li></ul></li><li>Amazon EKS는 오픈 소스 Kubernetes 소프트웨어의 최신 버전을 실행하므로 <strong>Kubernetes 커뮤니티에서 사용되는 플러그인과 툴을 모두 사용</strong>할 수 있습니다. 온프레미스 데이터 센터에서 실행 중인지 퍼블릭 클라우드에서 실행 중인지에 상관없이, Amazon EKS에서 실행 중인 애플리케이션은 표준 Kubernetes 환경에서 실행 중인 애플리케이션과 완벽하게 호환됩니다. 즉, 코드를 수정하지 않고 표준 Kubernetes 애플리케이션을 Amazon EKS로 손쉽게 마이그레이션할 수 있습니다.</li><li><p>이 워크샵을 위해서는 administrator IAM  권한이 필요합니다.</p><p><br></br></p><h2 id="Cloud9"><a href="#Cloud9" class="headerlink" title="Cloud9"></a>Cloud9</h2></li><li><p>AWS Cloud9은 브라우저만으로도 코드를 작성, 실행 및 디버깅할 수 있는 IDE입니다. 코드 편집기, 디버거 및 터미널이 포함되어 있으며 많이 사용되는 프로그래밍 언어를 위한 필수 도구가 사전에 패키징되어 제공되므로, 새로운 프로젝트를 시작하기 위해 파일을 설치하거나 개발 머신을 구성할 필요가 없다는 특징을 가지고 있습니다.</p><p><br></br></p><h3 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h3></li><li><p>쿠버네티스 클러스터에 명령을 내리는 CLI입니다.</p></li><li><p>쿠버네티스는 오브젝트 생성, 수정 혹은 삭제와 관련한 동작을 수행하기 위해 <strong>쿠버네티스 API</strong>를 사용합니다. 이때, kubectl CLI를 사용하면 해당 명령어가 쿠버네티스 API를 호출해 관련 동작을 수행합니다.</p><p><br></br></p><h3 id="eksctl"><a href="#eksctl" class="headerlink" title="eksctl"></a>eksctl</h3></li><li><p><a href="https://eksctl.io/">eksctl</a>이란 EKS 클러스터를 쉽게 생성 및 관리하는 CLI 툴입니다. Go 언어로 쓰여 있으며 CloudFormation 형태로 배포됩니다.</p><h3 id="eksctl로-클러스터-생성하기"><a href="#eksctl로-클러스터-생성하기" class="headerlink" title="eksctl로 클러스터 생성하기"></a>eksctl로 클러스터 생성하기</h3></li><li><p>eksctl을 사용하여 아무 설정 값을 주지 않고 이 명령어(<code>eksctl create cluster</code>)를 실행하면 default parameter로 클러스터가 배포됩니다.</p></li><li><p>원하는 설정이 있다면 yaml파일을 만들어서 배포하면 됩니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;</span> <span class="string">EOF</span> <span class="string">&gt;</span> <span class="string">eks-demo-cluster.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">eksctl.io/v1alpha5</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfig</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eks-demo</span> <span class="comment"># 생성할 EKS 클러스터명</span></span><br><span class="line">  <span class="attr">region:</span> <span class="string">$&#123;AWS_REGION&#125;</span> <span class="comment"># 클러스터를 생성할 리젼</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">&quot;1.21&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="attr">vpc:</span></span><br><span class="line">  <span class="attr">cidr:</span> <span class="string">&quot;192.168.0.0/16&quot;</span> <span class="comment"># 클러스터에서 사용할 VPC의 CIDR</span></span><br><span class="line"></span><br><span class="line"><span class="attr">managedNodeGroups:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">node-group</span> <span class="comment"># 클러스터의 노드 그룹명</span></span><br><span class="line">    <span class="attr">instanceType:</span> <span class="string">m5.large</span> <span class="comment"># 클러스터 워커 노드의 인스턴스 타입</span></span><br><span class="line">    <span class="attr">desiredCapacity:</span> <span class="number">3</span> <span class="comment"># 클러스터 워커 노드의 갯수</span></span><br><span class="line">    <span class="attr">volumeSize:</span> <span class="number">10</span>  <span class="comment"># 클러스터 워커 노드의 EBS 용량 (단위: GiB)</span></span><br><span class="line">    <span class="attr">ssh:</span></span><br><span class="line">      <span class="attr">enableSsm:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">iam:</span></span><br><span class="line">      <span class="attr">withAddonPolicies:</span></span><br><span class="line">        <span class="attr">imageBuilder:</span> <span class="literal">true</span> <span class="comment"># AWS ECR에 대한 권한 추가</span></span><br><span class="line">        <span class="comment"># albIngress: true  # albIngress에 대한 권한 추가</span></span><br><span class="line">        <span class="attr">cloudWatch:</span> <span class="literal">true</span> <span class="comment"># cloudWatch에 대한 권한 추가</span></span><br><span class="line">        <span class="attr">autoScaler:</span> <span class="literal">true</span> <span class="comment"># auto scaling에 대한 권한 추가</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cloudWatch:</span></span><br><span class="line">  <span class="attr">clusterLogging:</span></span><br><span class="line">    <span class="attr">enableTypes:</span> [<span class="string">&quot;*&quot;</span>]</span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><code>eksctl create cluster -f eks-demo-cluster.yaml</code> 로 배포</p></li><li><p>노드 확인</p><ul><li><p><code>kubectl get nodes</code></p><p><img src="/images/eks-workshop/스크린샷_2021-10-29_오후_1.17.55.png" alt="EKS클러스터 구성"></p><ul><li>기본적인 EKS 클러스터 구성은 끝났고, 본격적으로 서비스를 배포하고, 유저가 들어오는 통로인 인그레스를 설정해 봅시다.</li></ul></li></ul></li><li><p>이 상태에서는 클러스터의 노드를 UI에서 확인할 수 없습니다. Console Crediential을 더해줘야 합니다.</p></li><li><p>Cloud9의 IAM credential을 통해, 클러스터를 생성하였기 때문에 <a href="https://console.aws.amazon.com/eks">Amazon EKS 콘솔창</a>에서 해당 클러스터 정보를 확인하기 위해서는 실제 콘솔에 접근할 IAM entity(사용자 또는 역할)의 AWS Console credential을 클러스터에 추가하는 작업이 필요합니다.</p><ul><li><code>rolearn=$(aws cloud9 describe-environment-memberships --environment-id=$C9_PID | jq -r &#39;.memberships[].userArn&#39;)</code></li><li><code>assumedrolename=$(echo $&#123;rolearn&#125; | awk -F/ &#39;&#123;print $(NF-1)&#125;&#39;)rolearn=$(aws iam get-role --role-name $&#123;assumedrolename&#125; --query Role.Arn --output text)</code> rolearn에 자신의 arn을 넣어주도록 합시다.</li><li>echo ${rolearn}</li><li><code>eksctl create iamidentitymapping --cluster eks-demo --arn $&#123;rolearn&#125; --group system:masters --username admin</code> identity 맵핑</li><li>kubectl describe configmap -n kube-system aws-auth 적용 확인</li></ul><p><br></br></p><h2 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h2></li><li><p><strong>AWS Load Balancer Controller</strong>는 <strong>구 AWS ALB Ingress Controller</strong>에서 리브랜드된 개념입니다.</p></li><li><strong>인그레스(Ingress)</strong> 는 주로 <strong>클러스터 외부에서 쿠버네티스 내부</strong>로 접근할 때, 요청들을 어떻게 처리할지 정의해놓은 규칙이자 리소스 오브젝트입니다. 한마디로 외부의 요청이 내부로 접근하기 위한 관문의 역할을 하는 것이죠. 외부 요청에 대한 로드 밸런싱, TLS/SSL 인증서 처리, HTTP 경로에 대한 라우팅 등을 설정할 수 있습니다. 인그레스는 L7 영역의 요청을 처리합니다.</li><li><p>쿠버네티스에서 서비스 타입 중, NodePort 혹은 LoadBalancer로도 외부로 노출할 수 있지만 인그레스 없이 서비스를 사용할 경우, 모든 서비스에게 라우팅 규칙 및 TLS/SSL 등의 상세한 옵션들을 적용해야 되죠. 그래서 인그레스가 필요합니다.</p><p><img src="/images/eks-workshop/Untitled.png" alt="Ingress"></p></li><li><p>인그레스는 외부 요청 처리에 대한 규칙들을 설정해놓은 것을 의미하며, 이런 설정이 동작하기 위해서 필요한 것이 <strong>인그레스 컨트롤러</strong>입니다. kube-controller-manager의 일부로 실행되는 다른 컨트롤러와 달리 인그레스 컨트롤러는 클러스터와 함께 생성되진 않습니다. 따라서 직접 구현해야 합니다.</p><h3 id="AWS-Load-Balancer-만들기"><a href="#AWS-Load-Balancer-만들기" class="headerlink" title="AWS Load Balancer 만들기"></a>AWS Load Balancer 만들기</h3></li><li><p><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">Amazon EKS의 Application Load Balancing</a>이란 클러스터에 인그레스 자원이 생성될 때에 ALB(Application Load Balancer) 및 필요한 자원이 생성되도록 트리거하는 컨트롤러입니다. 인그레스 자원들은 ALB를 구성하여 HTTP 또는 HTTPS 트래픽을 클러스터 내 파드로 라우팅합니다.</p><ul><li>쿠버네티스의 <code>Ingress</code>의 경우, <code>Application Load Balancers</code>으로 프로비저닝됩니다.</li><li>쿠버네티스의 <code>Service</code>의 경우, <code>Network Load Balancers</code>으로 프로비저닝됩니다.</li></ul></li><li><p>AWS Load Balancer 컨트롤러에서 지원하는 <strong>트래픽 모드</strong>는 아래의 두 가지입니다.</p><ul><li>Instance(default): 클러스터 내 노드를 ALB의 대상으로 등록합니다. ALB에 도달하는 트래픽은 NodePort로 라우팅된 다음 파드로 프록시됩니다.</li><li>IP: 파드를 ALB 대상으로 등록합니다. ALB에 도달하는 트래픽은 파드로 <strong>직접</strong> 라우팅됩니다. 해당 트래픽 모드를 사용하기 위해선 <strong>ingress.yaml 파일에 주석을 사용하여 명시적으로 지정해야</strong> 합니다.</li></ul><p><img src="/images/eks-workshop/Untitled 1.png" alt="Instance Mode, IP Mode"></p><ul><li>각종 배포들에 사용할 manifests 디렉토리를 만들어놓고 관리하는 것을 추천합니다.</li></ul></li><li><p>AWS Load Balancer 컨트롤러를 배포하기 전, 우리는 몇 가지 작업을 수행해야 합니다. controller가 워커 노드 위에서 동작되기 때문에 IAM permissions를 통해, AWS ALB/NLB 리소스에 접근할 수 있도록 만들어야 합니다. IAM permissions는 ServiceAccount를 위한 IAM roles를 설치하거나 워커 노드의 IAM roles에 직접적으로 붙일 수 있습니다.</p></li><li><p>먼저, 클러스터에 대한 <strong>IAM OIDC(OpenID Connect) identity Provider</strong>를 생성합니다. Pod와 같은 클러스터 내 쿠버네티스가 생성한 항목이 API Server 또는 외부 서비스에 인증하는데 사용되는 <strong><a href="https://kubernetes.io/ko/docs/reference/access-authn-authz/service-accounts-admin/">service account</a></strong>에 IAM role을 사용하기 위해, 생성한 클러스터(현재 실습에서의 <em>eks-demo</em>)에 <strong>IAM OIDC provider</strong>가 존재해야 합니다.</p><ul><li><p>eksctl utils associate-iam-oidc-provider \<br>—region ${AWS_REGION} \<br>—cluster eks-demo \<br>—approve</p></li><li><p>OIDC provider URL</p><ul><li>aws eks describe-cluster —name eks-demo —query “cluster.identity.oidc.issuer” —output text</li><li>output: <a href="https://oidc.eks.ap-northeast-2.amazonaws.com/id/8A6E78112D7F1C4DC352B1B511DD13CF">https://oidc.eks.ap-northeast-2.amazonaws.com/id/8A6E78112D7F1C4DC352B1B511DD13CF</a><ul><li>뒤에 id 부분을 복사해서 다음과 같이 입력합니다.</li><li>aws iam list-open-id-connect-providers | grep 8A6E78112D7F1C4DC352B1B511DD13CF</li><li>결과 값이 출력되면 <strong>IAM OIDC identity provider</strong>가 클러스터에 생성이 된 것이고, 아무 값도 나타나지 않으면 생성 작업을 수행해야 합니다.</li></ul></li></ul></li><li><p>AWS Load Balancer Controller에 부여할 IAM Policy를 생성하는 작업을 수행합니다.</p><ul><li>aws iam create-policy \<br>—policy-name AWSLoadBalancerControllerIAMPolicy \<br>—policy-document <a href="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json">https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json</a></li></ul></li><li><p>AWS Load Balancer Controller를 위한 ServiceAccount를 생성합니다.</p><ul><li>eksctl create iamserviceaccount \<br>—cluster eks-demo \<br>—namespace kube-system \<br>—name aws-load-balancer-controller \<br>—attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy \<br>—override-existing-serviceaccounts \<br>—approve</li></ul></li><li><p>AWS Load Balancer controller를 클러스터에 추가하는 작업을 수행합니다. 먼저, 인증서 구성을 웹훅에 삽입할 수 있도록 <strong><a href="https://github.com/jetstack/cert-manager">cert-manager</a></strong>를 설치합니다. <strong>Cert-manager</strong>는 쿠버네티스 클러스터 내에서 TLS인증서를 자동으로 프로비저닝 및 관리하는 오픈 소스입니다.</p><ul><li>kubectl apply —validate=false -f <a href="https://github.com/jetstack/cert-manager/releases/download/v1.4.1/cert-manager.yaml">https://github.com/jetstack/cert-manager/releases/download/v1.4.1/cert-manager.yaml</a></li></ul></li><li><p>load balancer controller yaml</p><ul><li>wget <a href="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/v2_2_1_full.yaml">https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/v2_2_1_full.yaml</a></li></ul></li><li><p>yaml 파일에서 클러스터의 <code>cluster-name</code>을 편집합니다. 본 실습에서는 <strong>eks-demo</strong>로 설정합니다.</p><ul><li>spec:<br>containers:</li><li>args:</li><li>—cluster-name=eks-demo # 생성한 클러스터 이름을 입력</li><li>—ingress-class=alb<br>image: amazon/aws-alb-ingress-controller:v2.2.0</li></ul></li><li><p>ServiceAccount yaml spec 삭제</p><ul><li>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>labels:<br><a href="http://app.kubernetes.io/component:">app.kubernetes.io/component:</a> controller<br><a href="http://app.kubernetes.io/name:">app.kubernetes.io/name:</a> aws-load-balancer-controller<br>name: aws-load-balancer-controller<br>namespace: kube-system</li></ul><hr></li></ul></li><li><p>배포</p><ul><li><code>kubectl apply -f v2_2_1_full.yaml</code></li><li>확인 kubectl get deployment -n kube-system aws-load-balancer-controller</li><li>서비스 어카운드 확인 kubectl get sa aws-load-balancer-controller -n kube-system -o yaml</li></ul></li><li><p>클러스터 내부에서 필요한 기능들을 위해 실행되는 파드들을 <strong>애드온(Addon)</strong> 이라고 합니다. 애드온에 사용되는 파드들은 디플로이먼트, 리플리케이션 컨트롤러 등에 의해 관리됩니다. 그리고 이 애드온이 사용하는 네임스페이스가 <strong>kube-system</strong>입니다. Yaml 파일에서 네임스페이스를 kube-system으로 명시했기에 위의 명령어로 파드 이름이 도출되면 정상적으로 배포된 것입니다. 또한, 아래의 명령어로 관련 로그를 확인할 수 있습니다.</p><ul><li>kubectl logs -n kube-system $(kubectl get po -n kube-system | egrep -o “aws-load-balancer[a-zA-Z0-9-]+”)</li><li>ALBPOD=$(kubectl get pod -n kube-system | egrep -o “aws-load-balancer[a-zA-Z0-9-]+”)</li><li>kubectl describe pod -n kube-system ${ALBPOD}</li></ul><p><br></br></p><h2 id="서비스-배포"><a href="#서비스-배포" class="headerlink" title="서비스 배포"></a>서비스 배포</h2></li><li><p>서비스 배포하는 순서는 다음과 같습니다.</p><p><img src="/images/eks-workshop/Untitled 2.png" alt="서비스 배포 순서"></p></li><li><p>소스 코드 다운로드</p></li><li>Amazon ECR에 각 서비스에 대한 리포지토리 생성</li><li>Dockerfile을 포함한 소스 코드 위치에서 컨테이너 이미지 빌드 후, 리포지토리에 푸시</li><li>각 서비스에 대한 Deployment, Service, Ingress 매니페스트 파일 생성 및 배포</li><li><p>사용자가 실제 서비스를 접근하는 순서</p><p><img src="/images/eks-workshop/Untitled 3.png" alt="사용자가 접근하는 순서"></p></li><li><p>우리의 서비스는 두 개의 백엔드가 존재합니다.</p><ul><li>Flask</li><li>Node.js</li></ul></li><li>각 백엔드의 API가 잘 동작하는지 확인하고 프론트엔드 배포로 넘어가봅시다.</li></ul><p>  <br></br></p><h3 id="Flask-배포"><a href="#Flask-배포" class="headerlink" title="Flask 배포"></a>Flask 배포</h3><ul><li><p>cd ~/environment/manifests/</p></li><li><p>deploy manifest</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">flask-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-flask-backend:latest</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">flask-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">&quot;/contents/aws&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Ingress manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;backend-ingress&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/contents</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-flask-backend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>순서대로 배포</p></li><li><p><code>kubectl apply -f flask-deployment.yamlkubectl apply -f flask-service.yamlkubectl apply -f ingress.yaml</code></p></li><li><p>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/contents/aws 여기서 나온 주소로 확인</p><ul><li>ingress object가 배포되는 동안 기다리고 주소로 접속</li></ul><p><img src="/images/eks-workshop/Untitled 4.png" alt="flask 백엔드 배포"></p><p><br></br></p><h3 id="node-js-배포"><a href="#node-js-배포" class="headerlink" title="node.js 배포"></a>node.js 배포</h3></li><li><p><strong>deploy manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">nodejs-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">public.ecr.aws/y7c9e1d2/joozero-repo:latest</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">3000</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">nodejs-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">&quot;/services/all&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">     <span class="attr">app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">3000</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>ingress manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;backend-ingress&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/contents</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-flask-backend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">8080</span></span><br><span class="line"><span class="comment"># 추가된 부분 확인</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/services</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-nodejs-backend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>순서대로 배포</p><ul><li><p><code>kubectl apply -f nodejs-deployment.yamlkubectl apply -f nodejs-service.yamlkubectl apply -f ingress.yaml</code></p></li><li><p>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/services/all</p><p><img src="/images/eks-workshop/Untitled 5.png" alt="node.js 백엔드 배포"></p></li></ul><p><br></br></p><h3 id="프론트엔드-배포"><a href="#프론트엔드-배포" class="headerlink" title="프론트엔드 배포"></a>프론트엔드 배포</h3></li><li><p>프론트엔드 소스 다운</p><p>cd /home/ec2-user/environment<br>git clone <a href="https://github.com/joozero/amazon-eks-frontend.git">https://github.com/joozero/amazon-eks-frontend.git</a></p></li><li><p>ecr repository 생성</p><p>aws ecr create-repository \<br>—repository-name demo-frontend \<br>—image-scanning-configuration scanOnPush=true \<br>—region ${AWS_REGION}</p></li><li><p>프론트엔드 소스에서 url을 백엔드의 ingress 주소로 변경\</p><ul><li>App.js<ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/contents/‘${search}’</li></ul></li><li>page/UpperPage.js<ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/services/all</li></ul></li></ul></li><li><p>cd /home/ec2-user/environment/amazon-eks-frontend<br>npm install<br>npm run build</p></li><li><p>npm audit fix를 해도 안된다면</p><ul><li><code>export NODE_OPTIONS=--openssl-legacy-provider</code></li></ul></li><li><p>ECR demo-fronted 로 네이밍 한 뒤, 푸쉬</p><ul><li>docker build -t demo-frontend .</li><li>docker tag demo-frontend:latest$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</li><li>docker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</li></ul></li><li><p><strong>fronted배포 manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifests</span></span><br><span class="line"></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-frontend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demo-frontend</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demo-frontend</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo-frontend</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</span></span><br><span class="line">          <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-frontend</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">&quot;/&quot;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demo-frontend</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>ingress 배포</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">&quot;backend-ingress&quot;</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="attr">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/contents</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-flask-backend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">8080</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/services</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span>  </span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-nodejs-backend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">8080</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">            <span class="attr">pathType:</span> <span class="string">Prefix</span></span><br><span class="line">            <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">service:</span></span><br><span class="line">                <span class="attr">name:</span> <span class="string">&quot;demo-frontend&quot;</span></span><br><span class="line">                <span class="attr">port:</span></span><br><span class="line">                  <span class="attr">number:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>배포</p><ul><li><code>kubectl apply -f frontend-deployment.yamlkubectl apply -f frontend-service.yamlkubectl apply -f ingress.yaml</code></li></ul><p><img src="/images/eks-workshop/Untitled 6.png" alt="프론트까지 배포 완료"></p><p><img src="/images/eks-workshop/Untitled 7.png" alt="모든 배포가 완료된 모습" style="zoom:80%;" /></p><p><br></br></p><h2 id="Fargate"><a href="#Fargate" class="headerlink" title="Fargate"></a>Fargate</h2><p><strong>AWS Fargate</strong>는 컨테이너에 적합한 서버리스 컴퓨팅 엔진으로 Amazon Elastic Container Service(ECS) 및 Amazon Elastic Kubernetes Service(EKS)에서 모두 작동합니다. Fargate는 애플리케이션을 빌드하는 데 보다 쉽게 초점을 맞출 수 있도록 해줍니다. Fargate에서는 서버를 프로비저닝하고 관리할 필요가 없어 애플리케이션별로 리소스를 지정하고 관련 비용을 지불할 수 있으며, 계획적으로 애플리케이션을 격리함으로써 보안 성능을 향상시킬 수 있습니다.</p></li><li><p>클러스터에 Fargate로 pod를 배포하기 위해서는 pod가 실행될 때 사용하는 하나 이상의 fargate profile을 정의해야 합니다. 즉, fargate profile이란 fargate로 pod를 생성하기 위한 조건을 명시해놓은 프로파일이라고 보시면 됩니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifestscat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="attr">eks-demo-fargate-profile.yaml---apiVersion: eksctl.io/v1alpha5kind: ClusterConfigmetadata:  name: eks-demo  region:</span> <span class="string">$&#123;AWS_REGION&#125;fargateProfiles:</span>  <span class="bullet">-</span> <span class="attr">name: frontend-fargate-profile    selectors:      - namespace: default        labels:          app:</span> <span class="string">frontend-fargateEOF</span></span><br></pre></td></tr></table></figure><ul><li>yaml 파일에서 <strong>selectors</strong>에 기재된 조건에 부합하는 pod의 경우, fargate로 배포됩니다.</li></ul></li><li><p>아래의 명령어를 통해, fargate profile을 프로비저닝합니다.</p><ul><li>eksctl create fargateprofile -f eks-demo-fargate-profile.yaml</li><li>정상 동작 확인<ul><li>eksctl get fargateprofile —cluster eks-demo -o json</li></ul></li></ul></li><li><p>배포한 3개의 pod 중, 프론트앤드 pod를 fargate로 프로비저닝하는 작업을 수행하겠습니다. 먼저, 기존의 pod를 삭제하는 작업을 수행합니다. 아래의 명령어를 yaml 파일이 위치한 폴더에서 작업합니다.</p><ul><li>kubectl delete -f frontend-deployment.yaml</li></ul></li><li><p>그리고 frontend-deployment.yaml 파일을 수정합니다. 이 yaml파일에서 미리 등록한 fargate profile을 넣어주겠습니다. label값을 frontend-fargate로 변경해야 합니다. 1번에서 key 값이 app이고 value 값이 frontend-fargate이며 namespace가 default일 때, pod를 fargate로 배포하겠다는 조건을 맞추기 위해 값을 변경하였습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifestscat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="attr">frontend-deployment.yaml---apiVersion: apps/v1kind: Deploymentmetadata:  name: demo-frontend  namespace: defaultspec:  replicas: 3  selector:    matchLabels:      app: frontend-fargate  template:    metadata:      labels:        app: frontend-fargate    spec:      containers:        - name: demo-frontend          image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</span>          <span class="attr">imagePullPolicy: Always          ports:            - containerPort:</span> <span class="string">80EOF</span></span><br></pre></td></tr></table></figure></li><li><p>frontend-service.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="attr">frontend-service.yaml---apiVersion: v1kind: Servicemetadata:  name: demo-frontend  annotations:    alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">&quot;/&quot;</span><span class="attr">spec:  selector:    app: frontend-fargate  type: NodePort  ports:    - protocol: TCP      port: 80      targetPort:</span> <span class="string">80EOF</span></span><br></pre></td></tr></table></figure></li><li><p>매니페스트 배포</p><p><code>kubectl apply -f frontend-deployment.yamlkubectl apply -f frontend-service.yaml</code></p></li><li><p>배포 상태 확인</p><ul><li>kubectl get po -o wide</li></ul></li><li><p>이전과 같은 상태인지 웹 확인</p><ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)</li></ul><p><br></br></p><h2 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps?"></a>GitOps?</h2></li><li><p>쿠버네티스 환경에서 CI/CD를 위해서는 Git과 워크플로우 툴의 연결이 필요하고, 이를 통해서 자동화 하는 방안이 필요합니다.</p></li><li>일일이 수정하는 것은 너무나 번거롭습니다.</li><li>그래서 등장한 것이 GitOps입니다.</li><li><p>Github Action, Kustomize, Helm Chart, Argo CD를 활용해 GitOps를 체험해보겠습니다.</p><p><br></br></p><h2 id="GitOps-전에-알아야-할-개념들"><a href="#GitOps-전에-알아야-할-개념들" class="headerlink" title="GitOps 전에 알아야 할 개념들"></a>GitOps 전에 알아야 할 개념들</h2></li><li><p>앞서 소개한 Kustomize나 Argo CD등은 조금 생소해 보입니다.</p></li><li><p>이해를 위해 미리 한번 정리를 하고 넘어가보겠습니다.</p><h3 id="Kustomize"><a href="#Kustomize" class="headerlink" title="Kustomize"></a>Kustomize</h3></li><li><p>Kubernetes에서 app을 배포를 할때 manifest 파일을 작성한다</p></li><li><p>staging 환경에 배포하는 요구사항이 발생하고, 수정사항이 계속 발생한다면, 작업을 반복해야 하는 이슈가 생긴다</p></li><li><p>관리해야할 manifest파일이 3배가 되어버림</p></li><li><p>겹치는 내용이 대부분인데, 이것을 base라고 하고, 환경마다 차이가 나는 부분을 overlay로 관리해보자</p></li><li><p>kustomize는 이렇게, 공통 부분과 차이가 나는 부분을 분리하는 것에서 시작한다.</p></li><li><p>base와 overlay를 머지하면서 환경마다 다른 spec의 리소스를 생성하게 된다.</p></li><li><p>개발 환경마다 소스 구분하여 configmap, secret생성이 가능하다</p></li><li><p>resource이름에 prefix를 추가하는 기능도 제공한다</p></li><li><p>Kustomize를 적용해서 직접 수동으로 배포할 수도 있다</p><ul><li>GitOps로 ArgoCD나 Flux등을 사용해서 배포과정을 자동화 한다.</li><li>Flux 는 쿠버네티스 클러스터가 설정 소스(git..)와 동기화된 상태를 유지하고 새 코드가 추가되면 자동으로 업데이트 해주는 도구</li></ul><p><br></br></p><h3 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h3></li><li><p>쿠버네티스 패키지 관리를 도와줌</p><ul><li>npm, pip와 비슷한 역할</li></ul></li><li><p>Chart: 헬름 패키지</p><ul><li>kubernetes를 설명하는 파일들의 집합</li><li>kubernetes에서 app이 동작하기 위한 모든 리소스들이 포함되어 있음</li></ul></li><li><p>Repository</p><ul><li>차트 저장소, 차트르 모아두고 공유함</li></ul></li><li><p>Release</p><ul><li>kubernetes 클러스터에서 구동되는 차트 인스턴스</li><li>동일한 차트를 여러번 설치할 수 있고 이는 새 릴리즈로 관리됩니다.</li><li>릴리즈 될 때 패키지된 차트와 config가 결합되어 정상 실행 됩니다</li></ul></li><li><p>작업 순서</p><ul><li>Helm 차트를 원하는 레포에서 검색 후 설치 → 각 설치에 따른 새로운 릴리즈 생성</li></ul><p><br></br></p><h3 id="Argo-CD"><a href="#Argo-CD" class="headerlink" title="Argo CD"></a>Argo CD</h3></li><li><p>Continuous Delivery</p><ul><li>지속적 통합을 통해 테스트 되고 빌드된 코드를 지속적으로 전달하여 제품의 질적 향상을 향하는 것</li></ul></li><li>쿠버네티스 운영과 관련된 manifest 파일 관리하고 있는 원격 레포지토리를 조회</li><li>변경 내역이 감지되면 이를 반영하여 배포함(Auto Sync 옵션)</li><li><p>히스토리를 저장하고, 롤백이 가능함</p><p><img src="/images/eks-workshop/스크린샷_2021-11-03_오전_11.46.28.png" alt="Dev, Ops Pipeline"></p></li><li><p>argo의 장점</p><ul><li>실행 단위가 컨테이너기 때문에 고립성이 높다<ul><li>개별 작업마다 실행환경이 다양한 경우 실행환경이 섞이지 않고 단독적인 환경을 제공할 수 있다.</li><li>하나의 역할을 담당하는 Job을 단일하게 개발할 수 있어 재사용성을 높일 수 있다.<ul><li>데이터 입출만 잘 맞춰 놓으면 단일 역할을 하는 Job을 여러개 만들고, 블록처럼 쌓을 수 있다.</li></ul></li></ul></li></ul></li><li>argo의 단점<ul><li>Pod를 생성하고 삭제하는 비용이 크다.<ul><li>간단한 작업이라면 프로세스 또는 스레드 레벨에서 처리하는 게 효율적일 때가 있다</li></ul></li><li>각 스텝마다 개별적인 컨테이너를 실행해서 Job간에 데이터를 빠르게 공유하는 것이 힘들다.<ul><li>Pod 내부 컨테이너 간에만 volume공유가 가능하다</li></ul></li></ul></li><li><p>데이터 파이프라인 및 기계학습 모델 훈련에 활용 가능</p><ul><li>데이터 타입에 따라 추출하는 소스에 따라 상이한 실행환경<ul><li>데이터 추출 - Apache sqoop(Java runtime, hadoop 라이브러리 필요)<ul><li>S3 - aws cli, boto3…</li></ul></li><li>ML<ul><li>python</li><li>R</li></ul></li></ul></li><li>고립성이 굉장히 좋기 때문에 ML워크플로우 툴로도 활용이 가능함</li></ul><p><br></br></p><h2 id="GitOps-실습"><a href="#GitOps-실습" class="headerlink" title="GitOps 실습"></a>GitOps 실습</h2></li><li><p>목표하는 CI/CD 파이프라인은 다음과 같습니다.</p><p><img src="/images/eks-workshop/Untitled 8.png" alt="CI/CD" style="zoom:50%;" /></p></li><li><p>개발에서 작업한 내용은 Github Action을 통해 ECR에 업로드 되고, 이것을 클러스터 Ops쪽에서 Pull을 받아 k8s에 argo CD를 사용해 배포하는 방식입니다.</p></li><li><p>필요한 레포는 두 가지 입니다. application용과 k8s 메니페스트 관리용 레포입니다.</p><ul><li><strong><em>front-app-repo</em></strong>: Frontend 소스가 위치한 레파지토리</li><li><strong><em>k8s-manifest-repo</em></strong>: K8S 관련 메니페스트가 위치한 레파지토리</li></ul></li><li><p>git remote를 위해서 cloud9에 있는 amazon-eks-frontend 디렉토리 git을 초기화 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>rm -rf .git</li><li>소스 파일들을 푸시합니다.<ul><li>cd ~/environment/amazon-eks-frontend<br>git init<br>git add .<br>git commit -m “first commit”<br>git branch -M main<br>git remote add origin <a href="https://github.com/jinseo-jang/front-app-repo.git">https://github.com/jinseo-jang/front-app-repo.git</a><br>git push -u origin main</li></ul></li></ul></li><li><p>CI/CD 파이프라인을 위해서는 권한이 필요합니다.</p><ul><li><p>front app을 빌드하고 docker image로 만들어지면, 이것을 ECR로 푸시해야 합니다.</p></li><li><p>이 과정은 github Action으로 이루어지기 때문에 최소한의 권한을 넣어줍니다.</p><ul><li><p>IAM  user 생성</p><ul><li>aws iam create-user —user-name github-action</li></ul></li><li><p>ECR policy 생성</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environmentcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ecr-policy.json&#123;</span>    <span class="attr">&quot;Version&quot;:</span> <span class="string">&quot;2012-10-17&quot;</span><span class="string">,</span>    <span class="attr">&quot;Statement&quot;:</span> [        &#123;            <span class="attr">&quot;Sid&quot;:</span> <span class="string">&quot;AllowPush&quot;</span>,            <span class="attr">&quot;Effect&quot;:</span> <span class="string">&quot;Allow&quot;</span>,            <span class="attr">&quot;Action&quot;:</span> [                <span class="string">&quot;ecr:GetDownloadUrlForLayer&quot;</span>,                <span class="string">&quot;ecr:BatchGetImage&quot;</span>,                <span class="string">&quot;ecr:BatchCheckLayerAvailability&quot;</span>,                <span class="string">&quot;ecr:PutImage&quot;</span>,                <span class="string">&quot;ecr:InitiateLayerUpload&quot;</span>,                <span class="string">&quot;ecr:UploadLayerPart&quot;</span>,                <span class="string">&quot;ecr:CompleteLayerUpload&quot;</span>            ],            <span class="attr">&quot;Resource&quot;:</span> <span class="string">&quot;arn:aws:ecr:ap-northeast-2:$&#123;ACCOUNT_ID&#125;:repository/demo-frontend&quot;</span>        &#125;,        &#123;            <span class="attr">&quot;Sid&quot;:</span> <span class="string">&quot;GetAuthorizationToken&quot;</span>,            <span class="attr">&quot;Effect&quot;:</span> <span class="string">&quot;Allow&quot;</span>,            <span class="attr">&quot;Action&quot;:</span> [                <span class="string">&quot;ecr:GetAuthorizationToken&quot;</span>            ],            <span class="attr">&quot;Resource&quot;:</span> <span class="string">&quot;*&quot;</span>        &#125;    ]<span class="string">&#125;EOF</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>이 파일을 사용해 IAM 정책을 생성합니다. 이름은 <code>ecr-policy</code> 입니다.</p><ul><li>aws iam create-policy —policy-name ecr-policy —policy-document file://ecr-policy.json</li></ul></li><li><p>만든 정책을 IAM 유저에게 넣어줍니다.</p><ul><li>aws iam attach-user-policy —user-name github-action —policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/ecr-policy</li></ul></li></ul></li><li><p>Github Secret 생성</p><ul><li>이제 github action에서 사용할 AWS credential, github token을 설정해줍니다.</li><li>AWS credential 생성<ul><li>이 또한 최소한의 권한을 갖는 유저를 만들어 줍니다. 이름은 <code>github-action</code>입니다.</li><li>aws iam create-access-key —user-name github-action</li><li>생성된 accesskey와 secretkey를 저장해놓습니다.</li></ul></li><li>Github으로 들어가서 PAT를 생성해주고 저장해놓습니다.</li><li>front-app-repo로 들어가 Settings에 Secret을 선택합니다.<ul><li>이 레포의 secret을 넣어줍니다.</li><li>Name은 <code>ACTION_TOKEN</code></li><li>Value는 PAT를 넣어줍니다.</li><li>마찬가지로 <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> 도 넣어줍니다.</li></ul></li></ul></li><li><p>Github Action을 위한 workflow 스크립트를 생성합니다.</p><ul><li><p>github action을 생성하면 .github/workflows에 만들어지기 때문에 같은 디렉토리를 먼저 만들어줍니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>mkdir -p ./.github/workflows</li></ul></li><li><p>실제 build 코드를 작성합니다. <code>build.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/amazon-eks-frontend/.github/workflowscat</span> <span class="string">&gt;</span> <span class="string">build.yaml</span> <span class="string">&lt;&lt;EOFname:</span> <span class="attr">Build Fronton:  push:    branches:</span> [ <span class="string">main</span> ]<span class="attr">jobs:  build:    runs-on: ubuntu-latest    steps:      - name: Checkout source code        uses:</span> <span class="string">actions/checkout@v2</span>      <span class="bullet">-</span> <span class="attr">name: Check Node v        run: node -v      - name: Build front        run:</span> <span class="string">|</span>          <span class="attr">npm install          npm run build      - name: Configure AWS credentials        uses:</span> <span class="string">aws-actions/configure-aws-credentials@v1</span>        <span class="attr">with:          aws-access-key-id:</span> <span class="string">\$&#123;&#123;</span> <span class="string">secrets.AWS_ACCESS_KEY_ID</span> <span class="string">&#125;&#125;</span>          <span class="attr">aws-secret-access-key:</span> <span class="string">\$&#123;&#123;</span> <span class="string">secrets.AWS_SECRET_ACCESS_KEY</span> <span class="string">&#125;&#125;</span>          <span class="attr">aws-region:</span> <span class="string">$AWS_REGION</span>      <span class="bullet">-</span> <span class="attr">name: Login to Amazon ECR        id: login-ecr        uses:</span> <span class="string">aws-actions/amazon-ecr-login@v1</span>      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Get</span> <span class="string">image</span> <span class="string">tag(verion)</span>        <span class="attr">id: image        run:</span> <span class="string">|</span>          <span class="string">VERSION=\$(echo</span> <span class="string">\$&#123;&#123;</span> <span class="string">github.sha</span> <span class="string">&#125;&#125;</span> <span class="string">|</span> <span class="string">cut</span> <span class="string">-c1-8)</span>          <span class="string">echo</span> <span class="string">VERSION=\$VERSION</span>          <span class="string">echo</span> <span class="string">&quot;::set-output name=version::\$VERSION&quot;</span>      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Build,</span> <span class="string">tag,</span> <span class="attr">and push image to Amazon ECR        id: image-info        env:          ECR_REGISTRY:</span> <span class="string">\$&#123;&#123;</span> <span class="string">steps.login-ecr.outputs.registry</span> <span class="string">&#125;&#125;</span>          <span class="attr">ECR_REPOSITORY: demo-frontend          IMAGE_TAG:</span> <span class="string">\$&#123;&#123;</span> <span class="string">steps.image.outputs.version</span> <span class="string">&#125;&#125;</span>        <span class="attr">run:</span> <span class="string">|</span>          <span class="string">echo</span> <span class="string">&quot;::set-output name=ecr_repository::\$ECR_REPOSITORY&quot;</span>          <span class="string">echo</span> <span class="string">&quot;::set-output name=image_tag::\$IMAGE_TAG&quot;</span>          <span class="string">docker</span> <span class="string">build</span> <span class="string">-t</span> <span class="string">\$ECR_REGISTRY/\$ECR_REPOSITORY:\$IMAGE_TAG</span> <span class="string">.</span>          <span class="string">docker</span> <span class="string">push</span> <span class="string">\$ECR_REGISTRY/\$ECR_REPOSITORY:\$IMAGE_TAGEOF</span></span><br></pre></td></tr></table></figure><ul><li>IMAGE_TAG값은 랜덤으로 생성되어 ECR로 Push됩니다.<ul><li>$</li></ul></li></ul></li><li><p>github action이 잘 동작하는지 테스트 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add github action build script”<br>git push origin main</li><li></li></ul></li></ul></li><li><p>Kustomize 사용을 위한 k8s manifest 구조화</p><ul><li><p>앞서 설명한 대로, kustomize는 manifest를 base와 overlays로 나뉘어 관리합니다.</p></li><li><p>이 구조를 만들어줍니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environmentmkdir</span> <span class="string">-p</span> <span class="string">./k8s-manifest-repo/basemkdir</span> <span class="string">-p</span> <span class="string">./k8s-manifest-repo/overlays/devcd</span> <span class="string">~/environment/manifestscp</span> <span class="string">*.yaml</span> <span class="string">../k8s-manifest-repo/basecd</span> <span class="string">../k8s-manifest-repo/basels</span> <span class="string">-rlt</span></span><br></pre></td></tr></table></figure></li><li><p><em><code>base</code></em> : kubernetes manifest 원본이 위치한 디렉토리 입니다. 이 안에 위치한 manifest 들은 <em><code>overlays</code></em> 아래에 위치한 <strong>kustomize.yaml</strong> 파일에 담긴 <strong>사용자 지정 설정</strong> 내용에 따라 변경됩니다.</p></li><li><p><em><code>overlays</code></em> : <strong>사용자 입맛에 맞는</strong> 설정 값이 위치한 디렉토리 입니다. 이 설정은 <strong>kustomize.yaml</strong> 에 담습니다. 이 하위에 있는 <em><code>dev</code></em> 디렉토리는 실습을 위해 만든 것으로, 개발 환경에 적용할 설정 파일을 모아 두기 위함 입니다.</p></li></ul></li><li><p>Kustomize manifest 생성</p><ul><li><p>frontend app에 대한 배포 구성을 할 것이기 때문에 frontend부분만 작업 하겠습니다.</p><ul><li>frontend-deployment.yaml 과 frontend-service.yaml 파일을 kustomize 를 통해 배포 시점에 의도한 값(e.g. Image Tag)을 반영 할겁니다</li><li>반영될 값<ul><li><strong><code>metadata.labels</code>:</strong> <code>&quot;env: dev&quot;</code>을 frontend-deployment.yaml, frontend-service.yaml 에 일괄 반영 합니다.</li><li><strong><code>spec.selector</code></strong> : <code>&quot;select.app: frontend-fargate&quot;</code> 를 frontend-deployment.yaml, frontend-service.yaml 에 일괄 반영 합니다.</li><li><strong><code>spec.template.spec.containers.image</code></strong> : <code>&quot;image: &quot;</code> 값을 새롭게 변경된 Image Tag 정보로 업데이트 합니다.</li></ul></li></ul></li><li><p>kustomize.yaml 파일을 만들어서 관리 변경할 manifest대상을 정의합니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/base</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">kustomization.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">frontend-deployment.yaml</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">frontend-service.yaml</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>overlays/dev 부분에 바꿀 부분을 정의합니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/dev</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">front-deployment-patch.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demo-frontend</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">frontend-fargate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">frontend-fargate</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/devcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="attr">front-service-patch.yamlapiVersion: v1kind: Servicemetadata:  name: demo-frontend  annotations:    alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">&quot;/&quot;</span>  <span class="attr">labels:    env: devspec:  selector:    app:</span> <span class="string">frontend-fargateEOF</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>마지막으로 위에서 설정 한 파일들(값)을 사용하고 frontend app 빌드에 따라 만들어진 새로운 <strong>Image Tag</strong> 를 사용 하겠다고 정의 하겠습니다. 구체적으로는, <code>name</code> 에 지정된 image는 <code>newName</code>의 image와 <code>newTag</code>의 값으로 사용 하겠다는 의미 입니다.</p></li><li><p>이를 활용해 <code>newTag</code> 값을 변경해 새로운 배포가 이루어질 때 마다 이를 kubernetes 클러스터까지 변경 할 수 있습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/devcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="attr">kustomization.yamlapiVersion: kustomize.config.k8s.io/v1beta1kind: Kustomizationimages:- name:</span> <span class="string">$&#123;ACCOUNT_ID&#125;.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend</span>  <span class="attr">newName:</span> <span class="string">$&#123;ACCOUNT_ID&#125;.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend</span>  <span class="attr">newTag:</span> <span class="string">abcdefgresources:-</span> <span class="string">../../basepatchesStrategicMerge:-</span> <span class="string">front-deployment-patch.yaml-</span> <span class="string">front-service-patch.yamlEOF</span></span><br></pre></td></tr></table></figure><ul><li>이상 -patch.yaml 파일에 정의한 내용들은 배포 과정에서 kustomize 에 의해 자동으로 kubernetes manifest 에 반영 됩니다.</li><li>이미지의 태그명이 abcdefg로 나오면 성공입니다.</li></ul></li><li><p>kubernetes manifest 용 github repo를 만들어줍니다.</p><ul><li>k8s-manifefst-repo <strong>**</strong>생성</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/git</span> <span class="string">initgit</span> <span class="string">add</span> <span class="string">.git</span> <span class="string">commit</span> <span class="string">-m</span> <span class="string">&quot;first commit&quot;</span><span class="string">git</span> <span class="string">branch</span> <span class="string">-M</span> <span class="string">maingit</span> <span class="string">remote</span> <span class="string">add</span> <span class="string">origin</span> <span class="string">https://github.com/jinseo-jang/k8s-manifest-repo.gitgit</span> <span class="string">push</span> <span class="string">-u</span> <span class="string">origin</span> <span class="string">main</span></span><br></pre></td></tr></table></figure></li><li><p>Argo CD 설치</p><ul><li>kubectl create namespace argocd<br>kubectl apply -n argocd -f <a href="https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml">https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</a></li></ul></li><li><p>Argo CD cli 설치</p><ul><li>cd ~/environment<br>VERSION=$(curl —silent “<a href="https://api.github.com/repos/argoproj/argo-cd/releases/latest">https://api.github.com/repos/argoproj/argo-cd/releases/latest</a>“ | grep ‘“tag_name”‘ | sed -E ‘s/.<em>“(<sup><a href="#fn_"" id="reffn_"">"</a></sup>+)”.</em>/\1/‘)</li><li>sudo curl —silent —location -o /usr/local/bin/argocd <a href="https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64">https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64</a></li><li>sudo chmod +x /usr/local/bin/argocd</li></ul></li><li><p>Argo CD는 퍼블릭하게 노출되지 않지만, ELB를 통해 접속 가능하도록 만들겠습니다.</p><ul><li>kubectl patch svc argocd-server -n argocd -p ‘{“spec”: {“type”: “LoadBalancer”}}’</li></ul></li><li><p>접속할 uri를 얻습니다.</p><ul><li>export ARGOCD_SERVER=<code>kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname</code><br>echo $ARGOCD_SERVER</li></ul></li><li><p>기본 username은 admin입니다. password는 다음과 같습니다.</p><ul><li>ARGO_PWD=<code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;&#123;.data.password&#125;&quot; | base64 -d</code><br>echo $ARGO_PWD</li></ul></li><li><p>argo login</p><p><img src="/images/eks-workshop/Untitled 9.png" alt="login화면" style="zoom:60%;" /></p></li><li><p>로그인 후 좌 상단 애플리케이션 설정 메뉴를 클릭하고 새 application을 만들어줍니다.</p></li><li><p><strong>Application Name</strong> 은 <code>eksworkshop-cd-pipeline</code>, <strong>Project</strong>는 <code>default</code>를 입력 합니다.</p></li><li><p><strong>SOURCE</strong> 섹션의 <strong>Repository URL</strong> 에는 앞서 생성한 <strong><code>k8s-manifest-repo</code>의 git 주소</strong>, <strong>Revision</strong> 에는 <code>main</code>, <strong>Path</strong> 에는 <code>overlays/dev</code>를 입력 합니다.</p></li><li><p><strong>DESTINATION</strong> 섹션의 <strong>Cluster URL</strong>에는 <code>https://kubernetes.default.svc</code>, <strong>Namespace</strong> 에는 <code>default</code>를 입력 하고 상단의 <strong>Create</strong> 를 클릭 합니다.</p></li><li><p>eksworkshop-cd-pipeline이 만들어졌습니다.</p></li><li><p>Kustomize 빌드 단계 추가</p><ul><li>github action에서 kustomize를 이용하여 image tag를 업데이트 한 후 k8s-manifest-repo에 커밋 푸시하는 단계를 추가 해줘야 합니다.</li><li>이 단계가 동작하면, argo CD가 k8s-manifest-repo를 센싱 하다가 새로운 변경사항이 감지되면 Kustomize build 작업을 수행해 새로운 kubernetes manifest를 eks클러스터에 배포합니다.</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/amazon-eks-frontend/.github/workflows</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;&gt;</span> <span class="string">build.yaml</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Setup</span> <span class="string">Kustomize</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">imranismail/setup-kustomize@v1</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Checkout</span> <span class="string">kustomize</span> <span class="string">repository</span></span><br><span class="line">        <span class="attr">uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">repository:</span> <span class="string">jinseo-jang/k8s-manifest-repo</span></span><br><span class="line">          <span class="attr">ref:</span> <span class="string">main</span></span><br><span class="line">          <span class="attr">token:</span> <span class="string">\$&#123;&#123;</span> <span class="string">secrets.ACTION_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">k8s-manifest-repo</span></span><br><span class="line"></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Update</span> <span class="string">Kubernetes</span> <span class="string">resources</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          echo \$&#123;&#123; steps.login-ecr.outputs.registry &#125;&#125;</span></span><br><span class="line"><span class="string">          echo \$&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;</span></span><br><span class="line"><span class="string">          echo \$&#123;&#123; steps.image-info.outputs.image_tag &#125;&#125;</span></span><br><span class="line"><span class="string">          cd k8s-manifest-repo/overlays/dev/</span></span><br><span class="line"><span class="string">          kustomize edit set image \$&#123;&#123; steps.login-ecr.outputs.registry&#125;&#125;/\$&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;=\$&#123;&#123; steps.login-ecr.outputs.registry&#125;&#125;/\$&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;:\$&#123;&#123; steps.image-info.outputs.image_tag &#125;&#125;</span></span><br><span class="line"><span class="string">          cat kustomization.yaml</span></span><br><span class="line"><span class="string"></span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Commit</span> <span class="string">files</span></span><br><span class="line">        <span class="attr">run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          cd k8s-manifest-repo</span></span><br><span class="line"><span class="string">          git config --global user.email &quot;github-actions@github.com&quot;</span></span><br><span class="line"><span class="string">          git config --global user.name &quot;github-actions&quot;</span></span><br><span class="line"><span class="string">          git commit -am &quot;Update image tag&quot;</span></span><br><span class="line"><span class="string">          git push -u origin main</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>소스를 만들었으면, 커밋 푸시 해줍니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add kustomize image edit”<br>git push -u origin main</li></ul></li><li><p>github action이 잘 동작하는지를 확인하고, k8s-manifest-repo에 새 manifest가 커밋 되는지 확인합니다.</p></li><li><p>이렇게 새 manifest가 배포되면, argo CD에서 감지하여, Sync Status가 업데이트 됩니다.</p><ul><li>아무 설정을 하지 않았다면, <strong>CURRENT SYNC STATUS</strong>의 값이 <strong>Out of Synced</strong> 입니다.</li><li>git repository 가 변경되면 자동으로 sync 작업이 수행 하도록 하려면 <strong>Auto-Sync</strong> 를 활성화 해야 합니다. 이를 위해 <strong>APP DETAILS</strong> 로 이동 하여 <strong>ENABLE AUTO-SYNC</strong> 버튼을 눌러 활성화 합니다.</li><li>활성화 되었다면, ArgoCD에 의해 k8s-manifest-repo의 커밋 내용이 ArgoCD에 의해 eks클러스터에 반영됩니다.</li><li>마지막으로 정상적으로 새 manifest가 배포되었는지를 확인하기 위해 k8s-manifest-repo의 커밋 히스토리를 통해 image tag를 살펴봅니다.</li><li>abcdefg가 아니라 새로운 태그값으로 들어갔다면 성공입니다.</li></ul></li><li><p>frontend application에 코드를 변경해서, GitOps 파이프라인이 정상 동작하는지를 최종 점검합니다.</p></li><li><p><strong>amazon-eks-frontend/src/</strong> 로 이동하여 <strong><code>App.js</code></strong> 더블 클릭하여 파일을 오픈 합니다.</p><ul><li><p><strong>line 67</strong>의 값을 <strong><code>EKS DEMO Blog version Hyuby</code></strong> 로 변경 하고 저장 합니다. 저장은 <strong>ctrl+s</strong> 를 누릅니다.</p></li><li><p>변경된 소스를 커밋 푸시 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add new blog version”<br>git push -u origin main</li></ul></li><li><p>Sync작업이 모두 끝나면, url로 접속하여 변경사항을 확인합니다.</p><ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)</li><li>짠!</li></ul><p><img src="/images/eks-workshop/demo_hyuby.png" alt="정상동작 확인!"></p></li></ul><p><br></br></p></li></ul><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://daddyprogrammer.org/post/14102/argocd-kubernetes-cluster-deploy/">https://daddyprogrammer.org/post/14102/argocd-kubernetes-cluster-deploy/</a></li><li><a href="https://junghyeonsu.tistory.com/65">https://junghyeonsu.tistory.com/65</a></li><li><a href="https://wookiist.dev/159">https://wookiist.dev/159</a></li><li><a href="https://cwal.tistory.com/23">https://cwal.tistory.com/23</a></li><li>[<a href="https://coffeewhale.com/kubernetes/workflow/argo/2020/02/14/argo-wf/](">https://coffeewhale.com/kubernetes/workflow/argo/2020/02/14/argo-wf/](</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/11/03/EKS-workshop/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
