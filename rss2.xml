<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Unreasonable Effectiveness</title>
    <link>http://tkdguq05.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sat, 20 Aug 2022 08:31:16 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>karpenter에 대한 설명, eks에 적용하기</title>
      <link>http://tkdguq05.github.io/2022/08/20/karpenter/</link>
      <guid>http://tkdguq05.github.io/2022/08/20/karpenter/</guid>
      <pubDate>Sat, 20 Aug 2022 08:09:19 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubernetes 노드 관리를 편리하게. Karpenter&lt;/p&gt;
&lt;p&gt;(Thumbnail image is generated from Dall-e(&lt;a href=&quot;https://labs.openai.com/&quot;&gt;https://labs.openai.com/&lt;/a&gt;))&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Kubernetes 노드 관리를 편리하게. Karpenter</p><p>(Thumbnail image is generated from Dall-e(<a href="https://labs.openai.com/">https://labs.openai.com/</a>))</p><a id="more"></a><h1 id="Karpenter"><a href="#Karpenter" class="headerlink" title="Karpenter"></a>Karpenter</h1><h2 id="🪚-Intro"><a href="#🪚-Intro" class="headerlink" title="🪚 Intro."></a>🪚 <strong>Intro.</strong></h2><hr><p><strong>AWS</strong>에서 만들고 운영중인 프로젝트로 빠르게 버전이 올라가고 있습니다. 글을 작성하는 와중에도 0.14에서 0.15버전이 업데이트 되었습니다. </p><p>AWS re:Invent 2021 행사에서 Karpenter v0.5이 드디어 정식으로 오픈되었고 GA(Generally Available)로 릴리스 되었습니다. 정식으로 오픈되었다 보니, 이것을 가져다가 운영환경에서도 사용을 하는 것도 가능합니다.</p><p>Karpenter는 간단히 말하면, Kubernetes의 worker node 자동 확장 기능을 담당하는 오픈소스 프로젝트라고 할 수 있는데, 기존의 Node Auto Scaling과 비교해서 어떤 장점이 있는지를 알아보도록 하겠습니다.</p><p><br></br></p><h1 id="Kubernetes의-장점-Node-Auto-Scaling"><a href="#Kubernetes의-장점-Node-Auto-Scaling" class="headerlink" title="Kubernetes의 장점, Node Auto Scaling"></a><strong>Kubernetes의 장점, Node Auto Scaling</strong></h1><p>먼저 쿠버네티스에 대해서 간략하게 알아보도록 하겠습니다. 쿠버네티스는 컨테이너 관리 플랫폼으로 빠른 배포와 뛰어난 확장성을 가졌습니다. 쿠버네티스를 활용해서 배포 관리를 하면 정말정말 편한 부분이 많지만, 운영 부담은 줄여야 하는게 중요합니다. 리소스가 모자라서 파드가 안 뜬다던가 하면 곤란해지거든요. 그래서 AWS를 사용하는 회사에서는 Managed Kubernetes 서비스인 EKS를 통해서 노드 관리의 부담을 줄이려고 하고 있습니다.</p><p>결국 EKS 통해서 <a href="https://kubernetes.io/ko/docs/concepts/overview/components/">Control Plane</a> 운영 부담을 줄여야 하는 게 중요한데, 특히 컨테이너 사용량에 따라 지속적으로 Computing Node를 빠르게 추가하고 줄일 수 있는 Elasticity가 필요하게 됩니다. 이 개념에 딱 맞는게 AWS에 이미 존재하는데 바로 EC2 인스턴스입니다. 추가적인 노드가 필요할 때 Elastic한 성질을 지닌 AWS 컴퓨팅의 기본 단위인 EC2(Amazon Elastic Compute Cloud)를 불러와서 이것을 노드로 붙이는 것입니다. 그리고 여기에 Auto Scaling을 적극적으로 활용하면서 Worker node의 운영 부담을 줄일 수 있게 되는 것입니다.</p><p>EKS에서는 Auto Scaling이 있는데 왜 그럼 Karpenter를 굳이 사용하려고 하는 건가요? 이제 그 이유를 차근차근 알아보겠습니다.</p><p><br></br></p><h2 id="Karpenter에-대한-짧막한-소개"><a href="#Karpenter에-대한-짧막한-소개" class="headerlink" title="Karpenter에 대한 짧막한 소개."></a><strong>Karpenter에 대한 짧막한 소개.</strong></h2><p>Karpenter는 신규 배포될 pod를 지속적으로 체크하고 Worker Node가 부족하면 자동으로 Worker node를 추가배포하고 확장하는 역할을 담당합니다. 추가적인 노드를 확장하는 것 뿐만 아니라, 불필요한 Worker node도 정리하기도 합니다. 따라서, 노드의 비용 효율화와 운영 부담을 최소화 하기 위한 자동화 도구로 Karpenter를 사용할 수 있게 되는 것입니다. 자세한 설명은 아래에서 계속 하도록 하고 이제 기존의 Auto Scaling 방식은 어떤 것이 있는지, 어떻게 이루어지는 지에 대해서 알아보겠습니다.</p><p><br></br></p><h2 id="기존의-Node-Auto-Scaling"><a href="#기존의-Node-Auto-Scaling" class="headerlink" title="기존의 Node Auto Scaling"></a><strong>기존의 Node Auto Scaling</strong></h2><p>기존의 노드 오토스케일링 방법은 <strong>CA</strong>(Cluster Auto Scaler)가 대표적입니다.</p><p>CA는 Cloud Provider(AWS, GCP)마다 각자 다른 방법으로 지원합니다. AWS의 경우에는 EC2 Auto Scaling Group을 사용하여 CA를 구현하는데, EKS에서 제공되는 Node Group기능은 다음과 같습니다.</p><p><strong>Node Group</strong></p><ul><li>Worker Node를 그룹핑하여 관리하는 기능 제공</li><li>Auto Scaling Group과 Launch Template으로 구현됨<ul><li>Launch Template에 어떤 스펙의 Instance, AMI를 사용할 것인지 작성되어 있음.</li></ul></li><li>Worker Node들은 ASG를 통해 확장하는 구조</li><li>ASG를 Cluster Autoscaler가 컨트롤</li></ul><p><img src="/images/karpenter/ng.png" alt="Node Group"></p><p><br></br></p><h3 id="Node-Group의-자동-확장-시나리오"><a href="#Node-Group의-자동-확장-시나리오" class="headerlink" title="Node Group의 자동 확장 시나리오"></a><strong>Node Group의 자동 확장 시나리오</strong></h3><p>그렇다면 이 Node Groupb이 어떻게 Node를 자동으로 확장하는 것인지 알아보겠습니다. 가상의 상황을 가정해보겠습니다. pod가 가득 찼고 더 이상 pod를 배포할 수 없는 상태입니다. 이런 상태에서 신규 pod 생성 요청이 왔습니다. 대략 Airflow에서 KubernetesPodOperator를 통해서 새 파드를 통해 작업을 하려는 것으로 상상해보겠습니다(이 Operator는 Airflow를 통해 작업을 하려고 할때, 작업을 워커에서 수행하지 않고 새 파드를 띄워서 해당 파드에서 작업하도록 합니다).  Kube-Scheduler는 신규 Pod를 배치할 적절한 Node를 선정하려고 합니다. </p><aside>💡 **kube-scheduler**는[노드](https://kubernetes.io/ko/docs/concepts/architecture/nodes/)가 배정되지 않은 새로 생성된 [파드](https://kubernetes.io/ko/docs/concepts/workloads/pods/) 를 감지하고, 실행할 노드를 선택하는 컨트롤 플레인의 컴포넌트입니다. 스케줄링 결정을 위해서 고려되는 요소는 리소스에 대한 개별 및 총체적 요구 사항, 하드웨어/소프트웨어/정책적 제약, 어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세, 데이터 지역성, 워크로드-간 간섭, 데드라인을 포함합니다.</aside><p>이제 다양한 Label 정보, 가용량 등을 통해서 pod가 배치될 대상 노드를 필터링 하고 최종으로 자리잡을 노드를 선정하려고 합니다. 이 노드 선정 전까지는 pod는 Pending상태로 나오게 됩니다(Unschedulable pod). 이 상태에서 머물다가 적당한 노드가 없다면 pod의 프로비저닝은 실패하게 됩니다. pod의 상태를 확인하고 노드 선정에 실패하게 된다면 Node Group의 ASG값 중 Desired Capacity 값을 수정하면서 워커 노드의 개수를 증가시도록 설정합니다. </p><p><img src="/images/karpenter/ng_1.png" alt=""></p><p>ASG는 수정된 Desired Capacity값을 읽고 EC2 워커 노드를 추가로 배포하게 됩니다. 이것은 Kubernetes와는 무관하고, AWS의 ASG에 의해 실행된 작업입니다. EC2 AMI, Instance Type은 AWS에 정의된 Launch Template에 있는 내용을 따르게 됩니다. 배포가 완료되어 노드가 Ready 상태가 되면 kube-scheduler는 배포되고 있지 못하던 pod를 새로운 워커 노드로 할당시킵니다. kube-apiserver에 해당 정보를 전달하고 kube-apiserver는 워커 노드에서 실행중인 kubelet에게 pod 배포 명령을 보내게 되고 파드가 해당노드에 프로비저닝 됩니다.</p><p><img src="/images/karpenter/ng_2.png" alt=""></p><p><br></br></p><h2 id="Karpenter의-동작방식"><a href="#Karpenter의-동작방식" class="headerlink" title="Karpenter의 동작방식"></a><strong>Karpenter의 동작방식</strong></h2><p>Karpenter는 ASG와는 조금 다른 구조로 동작하게 됩니다. Karpenter는 AWS에서 개발은 했지만 Cloud Provider와 무관하게 동작 가능한 구조로 설계되었습니다. Karpenter는 지속적으로 신규 pod의 상태를 확인하고 필요하다면 워커노드의 배포와 삭제도 직접 수행하고 kube-scheduler를 대신해서 pod를 특정 워커 노드쪽으로 바인딩 되도록 하는 요청도 수행합니다.</p><p><strong>Karpenter의 자동 확장 시나리오</strong></p><p>위에서 살펴본 상황과 같이 가용공간이 없는 상태에서 신규 pod생성 요청이 왔습니다. kube-scheduler가 신규 pod 배치할 적정 노드를 선정하는 것과 필터링 하는 과정은 같습니다. Unschedulabel 한 pod를 프로비저닝 해주려고 하는 상황인 것입니다. </p><p><img src="/images/karpenter/karpenter_1.png" alt=""></p><p>워커 노드를 생성할 때 어떤 인스턴스 타입을 선택해서 배포할 것인지를 정하는 과정에서 Node Group을 사용하는 Auto Scaling과 차이점이 있습니다. Karpenter는 자체 Custom Resource인 Provisioner를 등록하고 이것에 의해서 새 노드가 어떤 스펙일지를 결정합니다. </p><p><img src="/images/karpenter/karpenter_2.png" alt=""></p><p>Karpenter는 결국 이 Provisioner를 통해 모든 워커 노드의 lifecycle을 결정할 수 있게 됩니다. Karpenter 역시 노드가 배포된 이후에 Ready상태가 되면 직접 pod를 새로운 워커 노드에 배포될 수 있도록 바인딩 요청을 하게 됩니다.</p><p><br></br></p><hr><h3 id="둘의-차이점"><a href="#둘의-차이점" class="headerlink" title="둘의 차이점?"></a>둘의 차이점?</h3><p>둘의 차이점을 본다면, CA는 ASG와 같이 노드확장 같은 작업을 할때 Cloud Provider가 제공하는 기능들과 연계할 수 밖에 없습니다. 그렇기 때문에 훨씬 더 많은 단계를 거쳐야 하게 되고, 이 과정 때문에 노드의 배포과정이 느리고 번거롭게 됩니다.</p><p>Karpenter는 Auto Scaling 할때 일어나는 많은 부분을 Karpenter가 직접처리 합니다. 기존 ASG대비 훨씬 심플한 구조로 빠르게 처리가 가능해지는 것입니다.</p><p><br></br></p><h2 id="🪚-Karpenter의-주요-개념"><a href="#🪚-Karpenter의-주요-개념" class="headerlink" title="🪚 Karpenter의 주요 개념"></a>🪚 <strong>Karpenter의 주요 개념</strong></h2><p>이제 이유를 알았고 둘의 차이를 대충 파악했으니, Karpenter의 주요 개념을 살펴보도록 하겠습니다. Karpenter는 크게 4가지 특성을 갖고 설명할 수 있습니다.</p><ul><li><strong>Watching :</strong> unschedulable한 pod를 계속 보고 있습니다.(파드 자체를 계속 체크)</li><li><strong>Evaluating :</strong> 스케쥴링 하는데 제약이 없는지를 확인합니다.</li><li><strong>Provisioning :</strong> 요구사항에 맞는 노드에 파드를 배포합니다.</li><li><strong>Removing :</strong> 더 이상 노드가 필요없다면 삭제합니다.</li></ul><p>그렇다면 무엇에 의해 노드가 생성되고 어떤 종류의 인스턴스일 것인지, 언제 노드를 삭제할 것인지 등등을 결정할 무엇인가가 필요합니다. 그 설정이 바로 Provisioner입니다.</p><p><br></br></p><h3 id="Provisioner"><a href="#Provisioner" class="headerlink" title="Provisioner"></a><strong>Provisioner</strong></h3><p>Provisioner는 Custom Resource에 대해 작성한 것으로서, 노드 프로비저닝에 대한 제약사항이나 노드가 필요없다고 판단할 설정들, 예를 들어 timeout과 같은 것들을 설정합니다. 여러 설정들이 많지만 주요 설정들을 살펴보겠습니다. </p><ul><li>taints : 프로비저닝 된 노드들에 테인트를 정합니다. 파드가 taint에 대한 toleration이 맞지 않다면 taint에 설정된 대로 이벤트가 발생합니다.<ul><li>NoSchedule, PreferNoSchedule, or NoExecute</li></ul></li><li>labels : 파드에 매칭될 수 있는, 임의의 key-value를 노드에 붙입니다.</li><li>requirements : Acceptable한 것에 In, Unacceptable한 것에 Out을 설정합니다.<ul><li><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/">Well-Known Labels, Annotations and Taints</a> 를 참조해 설정합니다.</li><li><a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#nodekubernetesioinstance-type">instance types</a>, <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#topologykubernetesiozone">zones</a>, <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-arch">computer architecture</a>, <a href="https://karpenter.sh/v0.15.0/provisioner/#capacity-type">capacity type</a></li></ul></li><li>limits : 클러스터에서 사용할 전체 CPU와 메모리의 제한을 설정합니다.<ul><li>이 설정을 통해 노드 프로비저닝을 효과적으로 중지할 수 있습니다.</li></ul></li></ul><hr><p><br></br></p><h2 id="Karpenter-Demo-시나리오"><a href="#Karpenter-Demo-시나리오" class="headerlink" title="Karpenter Demo 시나리오"></a><strong>Karpenter Demo 시나리오</strong></h2><p>이제 Karpenter가 얼마나 효과적이고 빠른지를 살펴보기 위해 Karpenter를 직접 세팅해 보겠습니다. 설치 과정은 <a href="https://karpenter.sh/v0.15.0/getting-started/getting-started-with-eksctl/">공식 문서</a>를 참조하시는 게 가장 깔끔합니다. 꼭 최신 버전인지를 한 번 더 확인하시기 바랍니다. 설치가 완료되었다면 샘플 Provisioner를 넣어보겠습니다. </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">karpenter.sh/v1alpha5</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Provisioner</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  requirements:</span></span><br><span class="line"><span class="attr">    - key:</span> <span class="string">karpenter.sh/capacity-type</span>         <span class="comment"># optional, set to on-demand by default, spot if both are listed</span></span><br><span class="line"><span class="attr">      operator:</span> <span class="string">In</span></span><br><span class="line"><span class="attr">      values:</span> <span class="string">["spot"]</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    resources:</span></span><br><span class="line"><span class="attr">      cpu:</span> <span class="number">1000</span>                               <span class="comment"># optional, recommended to limit total provisioned CPUs</span></span><br><span class="line"><span class="attr">      memory:</span> <span class="number">1000</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  ttlSecondsAfterEmpty:</span> <span class="number">30</span>                    <span class="comment"># optional, but never scales down if not set</span></span><br><span class="line"><span class="attr">  ttlSecondsUntilExpired:</span> <span class="number">2592000</span>             <span class="comment"># optional, but never expires if not set</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">karpenter.k8s.aws/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">AWSNodeTemplate</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  subnetSelector:</span>                             <span class="comment"># required</span></span><br><span class="line">    <span class="string">karpenter.sh/discovery:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br><span class="line"><span class="attr">  securityGroupSelector:</span>                      <span class="comment"># required, when not using launchTemplate</span></span><br><span class="line">    <span class="string">karpenter.sh/discovery:</span> <span class="string">$&#123;CLUSTER_NAME&#125;</span></span><br></pre></td></tr></table></figure><p>이제 테스트를 할 새로운 namespace를 만들어주겠습니다(<code>kubectl create namespace test</code>). 그리고 해당 네임스페이스에 파드를 생성할 deployment를 만들고 파드를 10개로 늘려보겠습니다. (<code>kubectl scale deployment inflate --replicas 10 -n test</code>)</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">inflate</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">inflate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">inflate</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      terminationGracePeriodSeconds:</span> <span class="number">0</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">inflate</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">public.ecr.aws/eks-distro/kubernetes/pause:3.2</span></span><br><span class="line"><span class="attr">          resources:</span></span><br><span class="line"><span class="attr">            requests:</span></span><br><span class="line"><span class="attr">              cpu:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>이 설정에서 cpu를 굉장히 작게 요청했기 때문에 pod를 갑자기 늘렸을 때, 파드는 제대로 생성되지 못하고 Pending이 되어 있을 것입니다. </p><p><code>kubectl get po -n test</code>를 통해 확인해보면 다음과 같습니다.</p><p><img src="/images/karpenter/get_pod.png" alt="파드가 제대로 생성되지 못하고 Pending되어 있다."></p><p>Provisioner에서는 어떤일이 일어나고 있을까요?(<code>kubectl logs -f deployment/karpenter -c controller -n karpenter</code>) Provisioner에서는 어떤 리소스가 부족한지를 파악하고 어떤 클러스터의 어떤 서브넷에 어떤 스펙의 노드를 추가할 지를 정하고 있습니다. 노드를 올리게 되고 Ready상태와 함께 pod들은 프로비저닝이 완료됩니다.</p><p><img src="/images/karpenter/provisioner_log.png" alt=""></p><p><br></br></p><h3 id="🤯-Trouble-Shooting"><a href="#🤯-Trouble-Shooting" class="headerlink" title="🤯 Trouble Shooting"></a>🤯 Trouble Shooting</h3><p>karpenter v0.14.0를 사용하면서 provisioner의 requirement에 <code>topology.kubernetes.io/zone</code> 를 설정하고 파드를 프로비저닝 할 때 다음과 같은 에러를 만날 수 있습니다.</p><p><code>2022-08-15T21:16:31.881Z        ERROR   controller.provisioning Could not schedule pod, incompatible with provisioner &quot;default&quot;, incompatible requirements, key topology.kubernetes.io/region does not have known values; incompatible with provisioner &quot;labeltest&quot;, incompatible requirements, key topology.kubernetes.io/region does not have known values {&quot;commit&quot;: &quot;5edcce3&quot;, &quot;pod&quot;: &quot;monitoring/prometheus-helm-kube-prometheus-stack-prometheus-1&quot;}</code> (From Kuberentes Slack, karpenter channel. visokoo’s thread) </p><p>그래서 열심히 구글 검색도 해보고 하다가 슬랙방에 들어가서 비슷한 사례가 없는지 봤는데, 8월 16일에 비슷한 문제를 겪는 사람이 있었습니다. 결국 이유는 버그였습니다. 프로비저닝이 안되어서 2-3일 정도를 허비했는데 조금 허무 했습니다. 하지만 빠르게 버그 픽스가 되었고 v0.15.0이 올라오면서 이 문제는 해결 되었습니다. 꼭 최신 버전을 사용하시기 바랍니다!</p><hr><p><br></br></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a><strong>Reference</strong></h2><ul><li><a href="https://karpenter.sh/">https://karpenter.sh/</a></li><li><a href="https://karpenter.sh/v0.15.0/">https://karpenter.sh/v0.15.0/</a></li><li><a href="https://youtu.be/EsmG7nCD_hI">https://youtu.be/EsmG7nCD_hI</a></li><li><a href="https://kubernetes.io/ko/docs/concepts/overview/components/">https://kubernetes.io/ko/docs/concepts/overview/components/</a></li><li><a href="https://kubernetes.io/ko/docs/concepts/architecture/nodes/">https://kubernetes.io/ko/docs/concepts/architecture/nodes/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/08/20/karpenter/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Nifi with Jolt</title>
      <link>http://tkdguq05.github.io/2022/08/06/jolt/</link>
      <guid>http://tkdguq05.github.io/2022/08/06/jolt/</guid>
      <pubDate>Sat, 06 Aug 2022 04:06:47 GMT</pubDate>
      <description>
      
        &lt;p&gt;Nifi를 잘 활용하는 방법! Jolt&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Nifi를 잘 활용하는 방법! Jolt</p><a id="more"></a><h1 id="Jolt"><a href="#Jolt" class="headerlink" title="Jolt"></a>Jolt</h1><ul><li><p><a href="https://github.com/bazaarvoice/jolt">Jolt</a>?</p><p>Jolt(<strong><strong>JsOn Language for Transform</strong></strong>) 는 JSON을 JSON으로 변환하는 데 사용할 수 있는 Java 라이브러리입니다. Jolt 변환 사양 자체도 JSON 파일이기 때문에, Apache NiFi 및 Apache Camel과 같은 제품에서 사용할 수 있습니다.</p><p>Jolt는 전체 JSON을 JSON으로 변환하기 위해 함께 연결할 수 있는 변환 셋들을 제공합니다. Jolt의 특징은 특정 값을 조작하는 것이 아니라 JSON 데이터의 구조를 변환하는 데 중점을 두고 있다는 것입니다.</p><p>Jolt의 공식 github에서는 다음과 같이 설명하고 있습니다.</p><blockquote><p>JSON to JSON transformation library written in Java where the “specification” for the transform is itself a JSON document.</p></blockquote></li></ul><p><br></br></p><p>Jolt 는 다음과 같이 사용할 수 있습니다.</p><ol><li>Transforming JSON data from ElasticSearch, MongoDb, Cassandra, etc before sending it off to the world</li><li>Extracting data from a large JSON documents for your own consumption</li></ol><p><br></br></p><p>JSON 데이터를 변환하는 데 특화되었다는 것을 설명을 통해 알 수 있을 것입니다. Jolt가 갖고 있는 변환 셋들은 5가지가 있습니다.</p><ul><li>shift       : copy data from the input tree and put it the output tree</li><li>default     : apply default values to the tree</li><li>remove      : remove data from the tree</li><li>sort        : sort the Map key values alphabetically ( for debugging and human readability )</li><li>cardinality : “fix” the cardinality of input data.  Eg, the “urls” element is usually a List, but if there is only one, then it is a String</li></ul><p>이 글에서는 shift를 사용해서 nested array로 이루어진 Json을 Flatten하는 것을 예시로 살펴보겠습니다.</p><p><br></br></p><h2 id="Jolt의-간단한-문법"><a href="#Jolt의-간단한-문법" class="headerlink" title="Jolt의 간단한 문법"></a>Jolt의 간단한 문법</h2><p>그 전에 참고할 내용을 먼저 알려드리겠습니다. <strong>LHS (Left Hand Side)</strong>와 <strong>RHS (Right Hand Side)</strong>입니다. LHS는 콜론, 즉, : 를 기준으로 왼쪽을 뜻하고, RHS는 :를 기준으로 오른쪽을 뜻합니다.</p><p>예를 들자면 이렇습니다.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"operation"</span>: <span class="string">"shift"</span>,</span><br><span class="line">    <span class="attr">"spec"</span>: &#123;</span><br><span class="line">LHS -&gt; "customer": &#123;</span><br><span class="line">  LHS -&gt;  "name": "client.fullName",  &lt;- RHS</span><br><span class="line">  LHS -&gt;  "birhtDate": "client.dateOfBirth",  &lt;- RHS</span><br><span class="line">  LHS -&gt;  "address": "client.address.street",  &lt;- RHS</span><br><span class="line">  LHS -&gt;  "country": "client.address.country"  &lt;- RHS</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>이제 기본 구조에 대해서 알아보겠습니다. Jolt는 다음과 같은 기본적인 구조를 갖고 있습니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;operation&quot;: &quot;&quot;,</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><ul><li><strong>“operations”</strong>: 적용할 변환 유형을 정의합니다.</li><li><strong>“spec”</strong>: 어떤 변환을 넣을지 적는 필드입니다.</li><li><strong>“[]”</strong>: Jolt의 기본 구조도 JSON이므로 목록 내에서 여러 작업을 연결할 수 있습니다.</li></ul><p>위에서 변환 셋들에 대해서 간단히 다루긴 했지만, 더 자세한 설명을 보고 싶다면 <a href="https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt">여기</a>에서 확인할 수 있습니다.</p><p><br></br></p><h2 id="Jolt를-활용한-Flatten-작업"><a href="#Jolt를-활용한-Flatten-작업" class="headerlink" title="Jolt를 활용한 Flatten 작업"></a>Jolt를 활용한 Flatten 작업</h2><p>Json 데이터가 nested로 이루어졌을때  Flatten이 필요한 경우가 있습니다. 이럴 때는 AWS라면 EMR, GCP라면 Dataproc에 Spark를 사용해서 처리를 하곤 합니다. 물론 대규모로 이루어진 데이터에 대해서 작업하려면 어쩔 수 없이 사용하겠지만, 미리 Flatten해서 저장을 하면 어떨까요? 그렇다면 비싼 EMR을 사용할 일도 없어질 듯 합니다.</p><p>먼저 이 작업을 위한 방식은 두 가지입니다. <code>JoltTransformJSON</code>, <code>JoltTransformRecord</code> 입니다. </p><p>둘의 차이는 간단합니다. <code>JoltTransformRecord</code>는 레코드 판독기와 레코드 작성기를 사용하는 반면 <code>JoltTransformJSON</code>은 작업할 JSON 흐름 파일 콘텐츠를 찾을 것으로 예상합니다. 두 경우 모두 출력은 플로우 파일 내용으로 끝나고 변환 구성은 동일합니다.</p><p>이제 작업할 대상 데이터를 보겠습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"data"</span>: &#123;</span><br><span class="line"><span class="attr">"_id"</span>: <span class="number">1000009195</span>,</span><br><span class="line"><span class="attr">"code"</span>: <span class="string">"*******"</span>,</span><br><span class="line"><span class="attr">"name"</span>: <span class="string">"************"</span>,</span><br><span class="line"><span class="attr">"array_deal_products"</span>: <span class="string">"[ &#123; \"deal_product_no\" : ***, \"code\" : \"***\", \"name\" : \"***\", \"seq\" : 1, \"master_product_code\" : \"***\", ]"</span>,</span><br><span class="line"><span class="attr">"short_description"</span>: <span class="string">"*******"</span>,</span><br><span class="line"><span class="attr">"array_site_attributes"</span>: <span class="string">"[ \"MARKET\" ]"</span>,</span><br><span class="line"><span class="attr">"array_product_tags"</span>: <span class="string">"[ 2 ]"</span>,</span><br><span class="line">&#125;,</span><br><span class="line"><span class="attr">"metadata"</span>: &#123;</span><br><span class="line"><span class="attr">"timestamp"</span>: <span class="string">"2022-**-**T01:12:05.790692Z"</span>,</span><br><span class="line"><span class="attr">"record-type"</span>: <span class="string">"data"</span>,</span><br><span class="line"><span class="attr">"operation"</span>: <span class="string">"load"</span>,</span><br><span class="line"><span class="attr">"partition-key-type"</span>: <span class="string">"attribute-name"</span>,</span><br><span class="line"><span class="attr">"schema-name"</span>: <span class="string">"commerce_product"</span>,</span><br><span class="line"><span class="attr">"table-name"</span>: <span class="string">"contents_products"</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>여기서 작업할 내용을 미리 정의하면 </p><ol><li>data부분과 metadata에서 필요한 것만 가져올 것입니다.</li><li>필요한 것만 가져와서 합칠 것입니다.</li><li>그리고 <strong>array_deal_products</strong>에 싸여있는 데이터를 풀어줄 것입니다.</li></ol><p><br></br></p><h3 id="data-metadata-특정-키-값만-합치기"><a href="#data-metadata-특정-키-값만-합치기" class="headerlink" title="data, metadata 특정 키 값만 합치기"></a>data, metadata 특정 키 값만 합치기</h3><p>미리 만들어 둔 Nifi Task에서 properties로 들어갑니다. Jolt Transformation DSL을 Chain으로 설정하고 Jolt Specification을 작성해줍니다.</p><ul><li><p>Jolt Specification</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"operation"</span>: <span class="string">"shift"</span>,</span><br><span class="line">    <span class="attr">"spec"</span>: &#123;</span><br><span class="line">      <span class="attr">"data"</span>: &#123;</span><br><span class="line">        <span class="attr">"_id"</span>: <span class="string">"&amp;"</span>,</span><br><span class="line">        <span class="attr">"array_deal_products"</span>: <span class="string">"&amp;"</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">"metadata"</span>: &#123;</span><br><span class="line">        <span class="attr">"operation"</span>: <span class="string">"&amp;"</span>,</span><br><span class="line">        <span class="attr">"timestamp"</span>: <span class="string">"&amp;"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li></ul><p>이제 spec 부분에 어떤 작업을 원하는지를 넣어주면 됩니다.</p><ul><li>위의 specification은 “data” 부분과 “metadata” 부분에 대해서 작업을 할 것이라는 뜻입니다.<ul><li>data에서는 id와 array_deal_products를 가져올 것이고, metedata에서는 operation과 timestamp를 가져올 것입니다.</li></ul></li></ul><p>이렇게 사용했을때 나오는 결과는 다음과 같습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"_id"</span> : <span class="number">1000009195</span>,</span><br><span class="line">  <span class="attr">"array_deal_products"</span> : <span class="string">"[ &#123; \"deal_product_no\" : ***, \"code\" : \"***\", \"name\" : \"***\", \"seq\" : 1, \"master_product_code\" : \"***\", ]"</span>,</span><br><span class="line">  <span class="attr">"operation"</span> : <span class="string">"load"</span>,</span><br><span class="line">  <span class="attr">"timestamp"</span> : <span class="string">"2022-**-**T01:12:05.790692Z"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><br></br></p><h3 id="List로-묶인-값-풀어주기"><a href="#List로-묶인-값-풀어주기" class="headerlink" title="List로 묶인 값 풀어주기"></a>List로 묶인 값 풀어주기</h3><ul><li><strong>SplitJson</strong>을 사용합니다.<ul><li><code>$.[*]</code></li><li><a href="https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624">https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624</a></li></ul></li></ul><p>SplitJson 프로세서를 만들어주고 위에서 작성한 <code>$.[*]</code> 을 써주면, 원하는 값을 가져와서 리스트로 묶인 값을 풀어줄 수 있습니다.</p><p>파이프라인으로 확인한다면 이렇게 되겠네요</p><p><img src="images/Jolt/jolt1.png" alt=""></p><p><img src="images/Jolt/jolt2.png" alt="EvaluateJsonPath"></p><p>중간에 <strong>EvaluateJsonPath</strong>는 Json을 파싱하는 프로세서입니다. 이 설정에 Destination을 <code>flowfile-content</code>로 설정해두시면 파싱 결과를 content에 저장하고 하나의 JSON path expression만 가질 수 있습니다. 반면 <code>flowfile-attribute</code> 라면 일단 attribute로 저장하겠죠? 그리고 각 JSON path가 명명된 속성 값으로 추출됩니다. 미리 지정한 ****attribute에 저장해둔 array_deal_products에 대해서 SplitJson을 해주는 것이라고 보면 되겠습니다.</p><p><br></br></p><h3 id="을-로-변환하기"><a href="#을-로-변환하기" class="headerlink" title=".을 _로 변환하기"></a>.을 _로 변환하기</h3><p>DB에서 데이터 연동을 하다보면 <code>~~~.~~~.~~</code>로 된 컬럼들이 있습니다. 해당 DB에서야 문제가 없겠는데, 다른 DB로 넣을 때면 문제가 발생하곤 합니다. 보통 _로 처리해서 넣으면 별 일 없기에 이렇게 변환해서 저장을 하거나 연동을 합니다. 하지만 이 간단한 작업을 하는데 Spark로 처리하기에는 너무 아까운 것 같습니다. 별다른 데이터 변환 없이 컬럼 명만 수정하면 되기 때문입니다. 그렇다고 손으로 일일이 하기엔 너무 힘이 들 것 같습니다. 이런 경우에 Jolt를 활용하면 쉽게 .이 들어간 컬럼(또는 키)을 _로 변환할 수 있습니다. </p><ul><li>데이터 예시</li></ul><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"_id"</span>: <span class="number">1234</span>,</span><br><span class="line">  <span class="attr">"purchase_policy.min"</span>: <span class="number">1</span>,</span><br><span class="line">  <span class="attr">"product_notice.type"</span>: <span class="string">"***"</span>,</span><br><span class="line">  <span class="attr">"product_notice.template_id"</span>: <span class="string">"****"</span>,</span><br><span class="line">  <span class="attr">"product_notice.is_free_template"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">"normal_order_type_policy"</span>: <span class="string">"DEFAULT"</span>,</span><br><span class="line">  <span class="attr">"is_search_enabled"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">"is_expose_product_list"</span>: <span class="literal">true</span>,</span><br><span class="line">  <span class="attr">"version"</span>: <span class="number">8</span>,</span><br><span class="line">  <span class="attr">"thumbnail.original.service_type"</span>: <span class="string">"***"</span>,</span><br><span class="line">  <span class="attr">"thumbnail.share.service_type"</span>: <span class="string">"***"</span>,</span><br><span class="line">  <span class="attr">"operation"</span>: <span class="string">"update"</span>,</span><br><span class="line">  <span class="attr">"timestamp"</span>: <span class="string">"***************"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>이런 데이터가 있다고 하겠습니다. 이 Json데이터의 키를 잘 보면 중간 중간에 .이 들어간 것을 볼 수 있습니다. <code>thumbnail.original.service_type</code> 처럼 처음, 중간에 .이 들어간 키가 있는 반면에, <code>purchase_policy.min</code> 처럼 중간에 .이 있는 것처럼 보이는 키도 있습니다. 이런 경우에 .을 _로 변경하기 위한 Jolt Spec은 다음과 같습니다.</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line"></span><br><span class="line">  &#123;</span><br><span class="line">    <span class="attr">"operation"</span>: <span class="string">"shift"</span>,</span><br><span class="line">    <span class="attr">"spec"</span>: &#123;</span><br><span class="line">      <span class="attr">"*.*"</span>: <span class="string">"&amp;(0,1)_&amp;(0,2)"</span>,</span><br><span class="line">      <span class="attr">"*.*.*"</span>: <span class="string">"&amp;(0,1)_&amp;(0,2)_&amp;(0,3)"</span>,</span><br><span class="line">      <span class="attr">"*"</span>: <span class="string">"&amp;"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>이번에도 shift operation을 사용할 것입니다. 자세히 봐야할 부분은 <code>*</code> 가 있는 부분입니다. 인풋으로 들어오는 데이터의 키를 쭉 보다보니처음에 .이 붙는 경우와 처음과 그 다음 부분에 .이 붙는 경우가 있습니다. 그래서 spec에 <code>&quot;*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)&quot;</code>,    <code>&quot;*.*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)_&amp;(0,3)&quot;</code> 이렇게 넣어줬습니다. 괄호에 있는 숫자는 depth의 level을 뜻합니다. level 0이라면 첫 번째 {}안에 있는 키들을 의미하고 1이라면 키의 첫번째 값을 의미합니다. 예를 들어 <code>purchase_policy.min</code> 자체는 level 0이겠고, purchase_policy는 level 1이 되겠습니다. 그렇다면 min은 level2가 되겠습니다.</p><p>따라서 <code>thumbnail.original.service_type</code> 를 <code>thumbnail_original_service_type</code> 로 바꾸겠다 하면 <code>&quot;*.*.*&quot;: &quot;&amp;(0,1)_&amp;(0,2)_&amp;(0,3)&quot;</code> 이렇게 Spec을 작성하면 되는 것입니다. </p><p>그리고 마지막으로 나머지 값들을 다 사용할 것이므로 <code>&quot;*&quot;: &quot;&amp;&quot;</code> 를 통해 가져오면 끝입니다.</p><hr><p><br></br></p><p>이렇게 해서 Jolt에 대해서 간단하게 알아봤습니다. 생각보다 간단하게 데이터를 Transform할 수 있는데, 이를 통해서 대대적인 상품개편 시에 큰 도움을 받았었습니다. 이것을 일일이 내려서 EMR을 돌리고 하는 작업들을 했었으면 너무 파이프라인이 복잡해지고, 모니터링 할 것도 많아졌을 것 같네요. Nifi를 사용하시는 분들이나 Spark를 사용하시는 분들이라면 Jolt를 한 번쯤 살펴보면 좋을 것 같습니다.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://github.com/bazaarvoice/jolt#stock-transforms">https://github.com/bazaarvoice/jolt#stock-transforms</a></li><li><a href="https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt">https://intercom.help/godigibee/en/articles/4044359-transformer-getting-to-know-jolt</a></li><li>[<a href="https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624]">https://community.cloudera.com/t5/Support-Questions/Nifi-SplitJson-how-to-split-json-array-to-individual-recods/td-p/210624]</a>(</li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/08/06/jolt/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2022년 상반기를 되돌아보기</title>
      <link>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/</link>
      <guid>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/</guid>
      <pubDate>Sat, 09 Jul 2022 08:51:20 GMT</pubDate>
      <description>
      
        &lt;p&gt;2022년 상반기에 대한 회고&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>2022년 상반기에 대한 회고</p><a id="more"></a><h2 id="2022년-요즘"><a href="#2022년-요즘" class="headerlink" title="2022년, 요즘"></a>2022년, 요즘</h2><p>7월 2번째 주가 흘러가고 있다. 매우 무덥고 습하지만 기분이 나쁘지만은 않다. 나름 하는 일을 계속 하고 있고 계획했던 일을 해나가고 있다. 상반기 인사평가도 지났고, 리더님을 통해서 결과도 전달을 받았다. 나름 잘 해나가고 있는 것 같았다. 회사에 입사하면서 맡고 싶었던 일이었고 이 일의 한 축이 되면서도 나름의 인정을 받은 것 같아 기뻤다. 하지만 내가 어떤 일을 해왔고 어떤 고민 때문에 지금의 일을 하고 있는지에 대해서는 회고 기록이 없어서 알기 어려 웠다. 사실 2022년을 시작하면서 월 회고를 하고 있었다. 하지만 글또 운영진 활동과 본격적인 회사 업무, 루틴화된 일들과 더불어 나의 게으름 덕분에 회고를 최근까지 하지 못했었다. 귀찮아서 미뤄두었는데 지나고 나니 한 달에 대한 기억이 없어지고 내가 무엇을 했는지 알 수가 없어서 너무 너무 아쉬웠다. 다시금 회고하는 습관을 들여서 남은 5개월은 잘 적어둬야겠다.(이렇게 보니 2022년이 얼마남지 않았다…)</p><p>먼저 가장 최근인 6월에 대한 회고를 시작해본다.</p><hr><p><br></br></p><h3 id="2022년-6월"><a href="#2022년-6월" class="headerlink" title="2022년 6월"></a>2022년 6월</h3><p>이미 지나가서 잘 기억이 나지 않는 달에 대한 회고를 위해서 가장 먼저 확인한 것은 캘린더였다. 캘린더를 열어서 어떤 미팅이 잡혔는지를 확인했고, Google Task에 적어둔, 완료된 To Do List들을 체크했다. 그리고 <a href="https://tkdguq05.github.io/2022/05/15/geultto7/#more">이전에 하던 회고의 형식</a>을 사용해서 작성해봤다. </p><p>나는 보통 평일, 주말, 직장, 개인 파트로 나누고 이에 대해서 회고를 진행한다. 사실 평일, 주말에 대해서는 잘 기억이 나지 않는다.. . 아마 글 주제 잡고 스터디 준비하고 운동하지 않았을까…? 이전 달에 뭘 계획하고 계획한 것에 대해서 얼마나 잘 진행되고 있는지를 파악해야 했는데, 아쉽다. </p><p>아쉬운대로 직장, 개인 파트로 넘어가서(개인도 잘 생각나지 않아 직장에만 집중하자면…) 회사에서는 6월에 굉장히 다양한 일을 진행하고 있었다. 기존에 하던 DE업무와 Kubernetes 활용도 계속하고 있고, ML업무에도 투입이 되었다. 여기서 자세하게 작성하지는 못하겠지만 적용하고 싶었던 것들도 적용해보고 하고 싶었던 ML쪽에도 일을 할 수 있게 됐다. 물론 회고를 해보니 보완해야 할점들이 수두룩하게 나왔지만… 덕분에 뭘 더 해야될지 더 명확해진 것 같았다. 외부 연동 프로젝트를 하면서 회사의 연동시스템과 프로세스를 더 익혀야겠다고 생각했고, 카프카를 더 공부하면 연동업무에 더 좋을 것 같다고 느꼈다. 쿠버네티스 쪽에서는 글또에서 활동하고 계시는 <a href="https://swalloow.github.io/eks-karpenter-groupless-autoscaling/">준영님의 Karpenter 글</a> 을 회사 팀 채널에 공유했고, 이걸 적용하고 싶어하셨던 분이 이미 계셔서 얘기를 해보다가 Dev쪽에 먼저 적용을 해보게 되었다. 다만 Provisioner까지 적용를 해보고 테스트까지 해보고 싶었는데 다른 일 때문에 여기까지는 해보지 못했다. </p><p>ML프로젝트를 하면서 자연스레 MLOps에 대한 관심이 커졌다. 사실 MLOps에 대해 고민할 단계가 아니라는 것을 알고는 있지만, 이것을 플랫폼화 하고, 사용하기 쉽게 만드려면 지금 논의해보는게 나쁘지 않을 것 같았다. 여러 자료들을 보고는 있는데 이제 시작되고 있는 분야라 Best Practice에 대한 자료가 거의 없다. 다른 회사의 사례를 참고해 보면서 우리 회사만의 MLOps를 구상해봐야겠다. 그래서 현재 사용하고 있는 구조를 활용해서 아키텍쳐를 그려보고 있는데, DS나 ML쪽 팀원들이 어떤 MLOps환경을 원하는지 - 예를 들어 “빠른 연산이 필요해요”, “배포를 쉽게하고 싶어요”, “실험 환경이 필요해요”… 등등을 잘 들어보고 결정해야겠다.</p><p><br></br></p><p>개인적으로는 테니스를 쭉 해오고 있는데 백핸드를 원핸드로 배우면서 스텝이 꼬여버려서 이걸 극복해보고 싶어졌다. 그래서 랠리가 참 안되는데, 같이하시는 분이 투핸드 백핸드를 하시는데 안정적으로 포핸드 - 백핸드 랠리를 하셔서 투핸드로 바꿀까 고민을 했었다. 하지만! 페더러가 백핸드를 하는 것을 보고 다시 열심히 해보기로!</p><p>그리고 글또 런닝 채널에서 양평 마라톤을 올려주신 분이 있어서 10km 마라톤을 등록했다. 덕분에 축구로 뛴 걸 제외하고 인생에서 최초로 10km를 뛰어보게 되었다. 7월 9일에 결국 완주했는데, 진짜 힘들지만 보람찼다. 왜 하는지 알 것 같았고 약간 슬럼프가 올려던 때였는데 잘 극복할 수 있게 되었다. 생각하면 할 수록 뿌듯했고, 다음엔 한 시간 안쪽으로 들어오고 싶은 욕심도 조금 났다…!</p><p><br></br></p><h3 id="2022년-5월-4월"><a href="#2022년-5월-4월" class="headerlink" title="2022년 5월 4월"></a>2022년 5월 4월</h3><p>사실 4월 5월은 기간이 너무 지나서 세세한 것 까지는 기억이 나지 않는다. 캘린더를 봐도 뭘 했는지도 잘 생각이 안나서 메모들을 통해서 떠오른 과거의 기억들을 조합해서 회고를 해봤다.</p><p>회사에서는 Kubernetes를 다루기 위해 Rancher에 대해서 살펴보고 공유를 받았다. 대충 어떻게 쓰는 건지는 알고 있었는데 어떻게 클러스터와 연결이 되는건지 이때에 알게 되었었다. 회사에서 Airflow관련 세션도 진행을 했었고, 하고싶었던 스터디도 이때에 진행을 하게 되었다. <strong>데이터 중심 어플리케이션</strong> 이라는 책으로 번역의 퀄이… 그리 좋지는 않아서 혼자 읽기는 매우 힘들고 중도 포기했었을 것 같은 책이었는데, 스터디를 통해서 7월에 결국 완주를 하게 되었다. 최근에 책걸이로 맛있는 점심을 먹었는데 기분이 참 좋았다. 다른 책들이나 주제들로 스터디를 만들어보고도 싶고, 다른 분들도 스터디를 만드시는 것을 지원해드리고도 싶다.</p><p>또 회사에 최근에 입사하시는 분이 있는데, 이 분을 회사에 추천을 했다. 전 직장에서 같이 일했던 분인데 회사에서 고통을 받고 계시길래, 그리고 같이 일하면 시너지가 잘 나는 분이라 모셔오고 싶었다. 7월에 입사하시는데, 회사에 MLE로 계시는 분이 퇴사를 하시게 되면서 이 분이 안왔으면 큰일날 뻔 했다… 하는 생각도 요즘 하고 있다. 여러 고민이 참 많으셔서 내가 있는 회사에서 어떤 것을 할 수 있는지를 설명해드리고 선택은 본인이 하셔야 된다고 말씀드렸는데, 참 감사히도 여기를 선택해주셨다. 온보딩 잘해드리고 가이드 잘 해드려야지…! 다른 분들과도 시너지를 잘 내셨으면 좋겠고, 그 시너지 효과가 기대가 된다.</p><p><br></br></p><p>개인적으로는 부업?이라고 해야할까 모 회사의 멘토링에 참여하게 되었다. 그 분들의 고민이 무엇인지 알게 되었고, 대략적으로 어떤 시스템을 가지고 있는지 유추할 수 있었다. 굉장히 재밌었던 경험이었고, 보수도 넉넉히 받아서 여러 번 참여하고 싶은 활동이었다.</p><p>캘린터를 보니 5월에는 테니스를 시작했다. 2022년에 하기로 한 일 중에 가장 잘한 일인 것 같다. 페더러와 나달을 좋아해서 테니스를 꼭 배워보고 싶었는데, 뜻이 맞는 분이 있어서 시작하게 되었다. 덕분에 요즘에는 원핸드 백핸드도 하고 나름 폼이 조금 나는 것 같다. 코트 한 번 나가서 시원하게 쭉쭉 쳐보고 싶다… 얼른 실력을 키워봐야지.</p><p><br></br></p><hr><h3 id="Outro"><a href="#Outro" class="headerlink" title="Outro"></a>Outro</h3><p>지나간 달에 대한 세밀한 회고가 잘 되지는 않았지만, 그래도 의미가 있었던 것 같다. 오랜만에 회고를 해보니 재미도 있었고 어떻게 살아가야 할지에 대해서 방향성이 좀 더 명확해지는 느낌이다. 일주일 단위로도 하면 어떨까도 잠깐 생각해봤는데, 너무 스스로를 옭아매는 것 같아 한 달 단위로만 진행하는 게 좋을 것 같다. 한 달간의 마일스톤 내지는 To-Do-List를 통해 시간을 잘 활용해봐야겠다. 지나가버린 달들에 대해 회고를 제대로 하지 못해 아쉽긴하지만, 남은 달들을 더 잘 살아보고 스스로 회고를 잘 해봐야겠다. 7월도 잘 즐겨보자.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/07/09/retrospect-2022-half/#disqus_thread</comments>
    </item>
    
    <item>
      <title>분산시스템의 문제점, zookeeper &amp; etcd</title>
      <link>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/</link>
      <guid>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/</guid>
      <pubDate>Sun, 26 Jun 2022 01:29:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;코디네이션 시스템, 왜 이런 도구들을 사용해야 하는 걸일까요?&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>코디네이션 시스템, 왜 이런 도구들을 사용해야 하는 걸일까요?</p><a id="more"></a><h1 id="분산시스템의-문제점-zookeeper-amp-etcd"><a href="#분산시스템의-문제점-zookeeper-amp-etcd" class="headerlink" title="분산시스템의 문제점, zookeeper &amp; etcd"></a>분산시스템의 문제점, zookeeper &amp; etcd</h1><p>데이터 엔지니어링 관련 애플리케이션을 다루다보면 심심치 않게 등장하는 것이 Zookeper입니다. Kafka를 다룰때도 등장하고, 하둡-스파크를 다룰 때도 역시 등장합니다. 쿠버네티스를 다룰때도 비슷하게 등장하는 것이 etcd입니다. 가끔 개발 세션을 볼 때나 블로그 글을 살펴볼때에도 etcd가 나오는데 정확히 어떤 역할을 하는지 이해가 가지 않았습니다. 둘의 공통점이라고 하면 분산 환경에서 사용되는 코디네이션 시스템인데, 왜 이런 도구들을 사용해야 하는 걸일까요?</p><p><br></br></p><h2 id="분산-시스템에서-코디네이션의-필요성"><a href="#분산-시스템에서-코디네이션의-필요성" class="headerlink" title="분산 시스템에서 코디네이션의 필요성"></a>분산 시스템에서 코디네이션의 필요성</h2><p>요즘의 대부분의 데이터 엔지니어링 시스템은 분산 시스템을 사용하고 있습니다. 아파치 카프카, 하둡, 스파크 등등 많은 애플리케이션들이 고가용성과 내결함성, 짧은 지연 등의 이유로 이를 활용하고 있습니다. 하지만 이러한 분산 시스템을 잘 유지하기 위해서는 몇가지 조건들이 필요합니다. 분산 시스템이 가진 문제점들이 있기 때문입니다. 여러 노드들로 나누어진 스토리지가 있다고 해봅시다. 이 노드들이 데이터를 write하고 클라이언트는 이 데이터를 읽을 것입니다. 노드들 중 한 대가 리더 노드로 write를 담당할 것이고 이 노드를 통해 모든 클라이언트의 write명령을 받을 것입니다. 명령은 리더가 아닌 다른 팔로워 노드들이 차차 처리하겠죠. 시스템이 완벽해 보이나요? 리더 노드가 죽으면 어떻게 될까요? 데이터를 한 노드에 write하는 동안 클라이언트가 다른 노드에서 데이터를 읽으려고 하면 어떻게 될까요?</p><p>분산시스템은 멀리서 보면 안정적이고 성능도 굉장히 뛰어나보이지만, 가까이에서 보면 처리해야될 문제들이 이처럼 상당합니다. 이러한 문제들을 잘 다루기 위해 사용하는 것이 코디네이션 시스템이고 대표적인 것이 바로 zookeeper와 etcd인 것입니다.</p><p>분산 시스템의 대표적인 문제 몇 가지를 살펴보고 Zookeeper와 etcd가 왜 필요한지, 어떻게 이런 문제들을 관리하는지 살펴보겠습니다.</p><p><img src="https://80000hours.org/wp-content/uploads/2016/02/Screen-Shot-2016-02-09-at-12.26.20-AM.png" alt="힙을 합칠땐 신뢰가 필요해..!" style="zoom:80%;" /></p><p><br></br></p><hr><p><br></br></p><h2 id="분산-시스템의-문제점"><a href="#분산-시스템의-문제점" class="headerlink" title="분산 시스템의 문제점"></a>분산 시스템의 문제점</h2><p>우리가 잘 사용하고 있는 분산시스템은 사실 고장날 것을 가정하고 설계하고 있습니다. 여러 노드를 통해 시스템을 구축하고 있기 때문에, 고장날 가능성이 단일 노드보다 훨씬 큰 것이죠. 그래서 장애가 났을 때는 잘못된 결과를 반환하는 것보다 아예 동작하지 않기를 바랍니다. 장애에는 여러 종류가 있는데 부분 장애는 시스템의 어떤 부분은 잘 동작하지만 다른 부분은 예측할 수 없는 방식으로 실패 하는 것을 말합니다. 이러한 비결정성과 부분 장애 가능성이 분산 시스템을 어렵게 만드는 것입니다.</p><p><br></br></p><h3 id="1-신뢰성-없는-네트워크"><a href="#1-신뢰성-없는-네트워크" class="headerlink" title="1. 신뢰성 없는 네트워크"></a>1. 신뢰성 없는 네트워크</h3><p>분산 시스템은 어떻게 연결되어있을까요? 분산 시스템은 비공유 시스템, 네트워크로 다수의 장비를 연결합니다. 네트워크는 장비들이 통신하는 유일한 수단이기에 굉장히 주의깊게 봐야합니다. 장비들간의 네트워크는 <code>비동기 패킷 네트워크</code>를 주로 사용합니다. 왜냐면 순간 몰리는 데이터 전송에 특화되어야 하기 때문입니다. 패킷 전송 지연시간 최대치가 고정되어 있고 유실되는 데이터가 없는 극단적인 신뢰성을 지닌 네트워크를 구성하면 안될까요? 이렇게 하기 위해서는 고정된 대역폭을 설정해야 합니다. 이렇게 되면 정확히 몇 초에 얼마의 데이터를 보낼 수 있는 것이 보장됩니다. 이런 방식을 <code>회선</code> 이라고 합니다. 회선은 데이터 전송량이 예상 가능한 경우에 적절하겠습니다. 우리가 사용하는 일반적인 상황에서는 데이터 전송량이 예상이 가능할까요? 아마 불가능할 것입니다. 또한 우리가 원하는 시스템의 목적은 가능한 빨리 완료되는 것입니다. 그러니까 순간적으로 몰리는 트래픽에 최적화된 방식이 필요한 것입니다. 그래서 데이터센터 네트워크와 인터넷은 <code>패킷 교환</code> 방식을 사용하는 것입니다. 결국 이 방식을 사용하다보니 어쩔 수 없이 네트워크에 신뢰성이 떨어질 수 밖에 없습니다. 타임아웃등의 방식을 사용할 수 있지만, 완벽하지는 않습니다.</p><p><br></br></p><h3 id="2-신뢰성-없는-시계"><a href="#2-신뢰성-없는-시계" class="headerlink" title="2. 신뢰성 없는 시계"></a>2. 신뢰성 없는 시계</h3><p>분산 시스템에서는 통신이 즉각적이지 않아, 시간은 다루기 까다로운 개념 중 하나입니다. 시간은 절대적인 것이라고 생각할 수 있지만, 유감스럽게도 시계가 정확한 시간을 알려주게 하는 방법은 기대만큼 신뢰성이 있거나 정확하지 않습니다. 각 장비는 사실 자신만의 시계를 갖고 있지만 완벽하게 정확하지 않습니다. 이를 위한 동기화 매커니즘 중 하나는 서버 그룹에서 보고한 시간에 따라 컴퓨터 시계를 조정할 수 있게 합니다.</p><p>이렇게 동기화된 시계에 의종할 수 있습니다. 보통 대부분의 시간에 잘 동작하지만, 견고하게 소프트웨어를 설계하고자 한다면, 잘못된 시계에 대비할 필요가 있습니다. 하루는 정확히 86,400초가 아닐 수도 있고, 일 기준 시계가 시간이 거꾸로 갈 수도 있으며, 노드의 시간이 다른 노드의 시간과 차이가 많이 날 수도 있습니다. 문제는 시계가 잘못된다는 것을 눈치채기 쉽지 않다는 것입니다. 천천히 조금씩 실제 시간으로부터 차이가 나기 시작하지만, 대부분 잘 동작하는 것처럼 보입니다. 이렇게 되면 극적인 고장보다는 조용하고 미묘한 데이터 손실이 발생할 가능성이 높을 것입니다.</p><p><br></br></p><h3 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h3><p>이외에도 네트워크에 비대칭적인 결함이 있어 노드가 메세지는 받지만 밖으로 보내지 못한다면, 노드에 타임아웃을 통해 죽었다고 잘못 선언해버리는 일이 발생할 수도 있고, 죽은 상태인 줄 알았던 노드가 갑자기 되돌아와서 리더처럼 역할을 수행하기도 합니다.</p><p>결국 위와 같은 문제들로 인해서 분산시스템의 균형이 무너져버립니다. 리더와 팔로워의 구조를 통해서 명령을 리더로 부터 내려받고 실행된 내용을 리더에게 보고해서 완료처리를 해야하지만, 갑자기 리더가 사라진다던가 리더가 둘 이상이 되어 버린다던가, 결국 쓰기가 충돌되고 파일이 오염되기 시작합니다. 더 자세하게는 리더와 락의 문제를 더 설명해야하지만, 너무 길어질 것 같아서 생략하겠습니다. 결론은 네트워크나 시계등의 문제로 신뢰성이 깨지면서 분산시스템이 제대로 동작하지 않는다는 것입니다.</p><p>하지만 엔지니어로서의 우리의 임무는 모든 게 잘못되더라도 제 역할을 해내도록 만드는 것입니다. 그래서 부분 장애 가능성을 받아들이고 소프트웨어에 내결함성 메커니즘을 넣어서 신뢰성 있는 시스템을 만들려고 하고 있습니다. 이런 상황에서 모든 노드가 어떤 것에 동의하도록 만드는 것이 중요한데, 이런 <code>합의</code> 에 신뢰성있게 도달하는 것은 또 다른 까다로운 문제입니다. 다만, Zookeeper라던가 etcd 등의 코디네이션 도구들을 도움을 통해 좀 더 쉽게 합의에 도달할 수 있습니다.</p><p><br></br></p><hr><p><br></br></p><h2 id="Part1-Zookeeper"><a href="#Part1-Zookeeper" class="headerlink" title="Part1. Zookeeper"></a>Part1. Zookeeper</h2><p>Zookeeper는 분산 애플리케이션을 위한 코디네이션 시스템입니다. 분산 애플리케이션이 안정적인 서비스를 할 수 있도록 분산되어 있는 각 애플리케이션의 정보를 중앙에 집중하고 구성 관리, 그룹 관리 네이밍, 동기화 등의 서비스를 제공합니다.</p><p>분산시스템에 여러 문제들이 있기 때문에, 시스템에서 정보를 어떻게 공유할 것이고 상태를 어떻게 체크할 것이며, 분산 서버들간의 잠금을 처리하는 방법에 대한 해결방법이 필요해졌고 여기서 주키퍼는 해결책을 일부 제시해주고 있습니다.</p><p>분산 시스템의 코디네이션에서도 마찬가지로, 코디네이션 시스템에 장애가 발생하면 전체 시스템에 장애가 발생해 버리게 됩니다. 따라서 코디네이션 시스템 역시 이중화 등 고가용성을 위한 시스템을 갖고 있습니다.</p><p>엄청나게 대단한 일을 하는 것처럼 보이긴 하는데, 기능은 단순한 편입니다. <code>Znode</code> 라는 Key-value로 이루어진 데이터 저장 객체를 제공하고, 여기에 데이터를 넣고 빼는 기능만을 제공하는 것입니다. 아키텍쳐를 살펴보면서 Znode까지 살펴보겠습니다.</p><p><br></br></p><h3 id="Zookeeper-아키텍쳐"><a href="#Zookeeper-아키텍쳐" class="headerlink" title="Zookeeper 아키텍쳐"></a>Zookeeper 아키텍쳐</h3><p>고가용성을 위해 zookeeper는 클러스터링화 해서 최대한 정상 동작을 보장하려고 합니다. 이 클러스터를 <code>앙상블</code>이라고 부릅니다. 앙상블로 묶인 서버 중 한대가 쓰기 명령을 담당하는 리더 역할을 맡고, 나머지는 팔로어가 되는 구조입니다. 클라이언트가 쓰기 명령을 내리면 앙상블 중 리더 역할을 맡는 zookeeper서버로 바로 전달되고, 리더는 팔로어들에게 쓰기를 수행할 수 있는지 확인합니다. 만약 팔로어 중 과반 수의 팔로우로부터 쓸 수 있다는 응답이 오면 리더는 팔로어에게 Write하도록 지시합니다.</p><p><img src="/images/zookeeper_etcd/Untitled.png" alt="Zookeeper Architecture"></p><p><br></br></p><p>이런 이유로 앙상블을 이루기 위해서는 최소 3대의 서버가 필요합니다. 만약 5대의 서버가 있다면 3대의 서버가 살아있어야겠습니다. </p><p>리더는 업데이트 명령을 받으면 트랜잭션의 순서를 반영하는 번호로 업데이트를 스탬프 처리합니다(번호 붙이기). 그리고 리더는 이 번호와 함께 업데이트 요청을 브로드캐스트하고 다음서버의 다수가 메세지에 응답할 때까지 기다립니다. 다음 서버는 업데이트 요청을 받으면 스탬핑된 번호를 확인하고 이 숫자가 자신의 로그에 기록된 트랜잭션보다 크면 리더에게 동의한다고 응답합니다. 리더는 앙상블의 서버로부터 응답을 받고 앙상블에 있는 팔로워들이 승인을 한 경우에만 리더는 요청된 트랜잭션을 자체 로그에 입력하고 앙상블을 통해 복제합니다. 이제 쿼리의 업데이트 실행이 발생하고 응답이 클라이언트 쪽으로 다시 전송됩니다. 이를 통해 <code>합의</code>가 유지되는 것입니다.</p><p><br></br></p><h3 id="Zookeeper의-데이터-모델"><a href="#Zookeeper의-데이터-모델" class="headerlink" title="Zookeeper의 데이터 모델"></a>Zookeeper의 데이터 모델</h3><p>Zookeeper는 Znode라는 노드의 계층구조를 유지합니다. 네임스페이스의 각 znode에는 연관된 데이터가 있습니다. </p><p><img src="/images/zookeeper_etcd/Untitled1.png" alt="Znode"></p><p><br></br></p><p>Znode의 이름은 슬래시로 구분되어있는 경로의 집합이고, 모든 znode는 경로로 식별됩니다. znode에는 두 유형이 있습니다.</p><ol><li>Persistent znode: 이 znode는 znode를 생성한 세션이 살아있지 않는 경우에도 디스크에 유지됩니다.</li><li>Ephermeral znode: znode를 생성한 세션이 종료될 때 까지 존재합니다. 세션이 끝나면 삭제됩니다. </li></ol><p><br></br></p><h3 id="Quorum-정족수"><a href="#Quorum-정족수" class="headerlink" title="Quorum(정족수)"></a>Quorum(정족수)</h3><p>리더가 새 트랜잭션을 수행하기 위해서는 자신을 포함해 과반수 이상의 서버의 합의를 얻어야 한다. 과반수의 합의를 위해 필요한 서버들이 바로 Quorum이다. 앙상블을 구성하는 서버 수가 5개라면 quorum은 3개로 구성된다. 앙상블로 구성되어 있는 주키퍼는 과반수 방식에 따라 살아 있는 노드 수가 과반 수 이상 유지되기만 하면 지속적인 서비스가 가능하다.</p><p>결론적으로 Zookeeper 서비스는 이런 방식들을 통해 클라이언트의 업데이트가 전송된 순서대로 적용되고 연결된 서버에 관계없이 클라이언트가 동일한 데이터를 읽을 수  있도록 보장합니다.</p><p><br></br></p><hr><p><br></br></p><h2 id="Part2-etcd"><a href="#Part2-etcd" class="headerlink" title="Part2. etcd"></a>Part2. etcd</h2><p>머신의 분산된 시스템 또는 클러스터의 설정 공유, 서비스 검색, 스케쥴러 조정을 위한 오픈소스 분산형 키-값 저장소입니다. 사실상 쿠버네티스의 기본 데이터 저장소로 사용되는데(컨트롤 플레인 컴포넌트로 채택됨), 클라우드 네이티브 애플리케이션은 etcd 사용을 통해 일관성 있는 가동시간을 유지하고 개별 서버에 장애가 발생하더라도 작동 상태를 유지할 수 있게 되었습니다. 애플리케이션은 이 etcd에서 데이터를 읽고 쓸 수 있고, 이를 통해 설정 데이터를 배포해 노드 설정에 대한 이중화 및 복구 능력을 제공할 수 있습니다. 이 etcd는 쿠버네티스에서 마스터노드에 구성되어 있습니다.</p><p>만약 etcd에 문제가 발생해 데이터가 유실된다면, 컨테이너 뿐 아니라 클러스터가 사용하는 모든 리소스가 길을 읽게 되어버립니다. 따라서 etcd에는 높은 신뢰성이 꼭 필요합니다.</p><p><br></br></p><h3 id="RSM-Replicated-State-Machine"><a href="#RSM-Replicated-State-Machine" class="headerlink" title="RSM(Replicated State Machine)"></a>RSM(Replicated State Machine)</h3><p>분산 환경에서 서버가 몇 개 죽더라도 잘 동작하는 시스템을 만들 수 있는 방법 중 하나입니다. 똑같은 데이터를 여러서버에 복제하는 것인데, 이 역할을 수행하는 머신을 RSM이라고 부릅니다. 데이터를 여러 서버에 복제하면 모든게 해결될까요? 오히려 이 상황때문에 문제가 발생하기도 합니다. 데이터 복제과정에서는 합의가 꼭 필요한데, 합의란 RSM이 4가지 속성을 만족한다는 의미입니다. etcd는 이 조건을 만족하기 위해 Raft를 사용합니다.</p><ul><li>Safety : 항상 올바른, 의도하는 결과를 리턴해야 합니다.</li><li>Available : 서버가 몇 대 다운되더라도 항상 응답해야 합니다.</li><li>Independent from Timing : 네트워크 지연이 발생해도 로그의 일관성이 깨져서는 안됩니다.</li><li>Reactivity : 모든 서버에 복제되지 않았더라도 조건을 만족하면 빠르게 요청에 응답해야 합니다.</li></ul><p><br></br></p><h3 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h3><p>Raft는 분산 시스템에서 특히 etcd에서 합의를 도출하는 프로토콜입니다. Follower, Candidate, Leader로 State가 나눠져있습니다.</p><p><strong>Leader Election</strong></p><p>Follower가 리더로부터 정보를 받지 못하고 있으면 후보자가 됩니다. 그러니까 시작상태에도 적용이 됩니다. 이 후보자들은 다른 노드들에 투표를합니다. 그러면 다른 노드들은 투표에 대한 응답을 보내게 되고, 과반수에 따라 한 노드가 리더로 승격됩니다.</p><p><strong><strong>Log Replication.</strong></strong></p><p>이렇게 리더가 선정되면 시스템의 모든 변화들은 리더에게 전달되고 각 변화는 노드의 로그 항목에 추가됩니다. 하지만 커밋이 되지 않은 상태이기 때문에 노드의 value를 업데이트 하지는 않습니다. 항목을 커밋하기 위해 다른 팔로워 노드에 복제를 합니다. 이제 리더가 다른 과반수의 노드들에 항목이 쓰여질 때까지 기다립니다.</p><p>이제 항목이 커밋되었고 리더노드들과 다른 노드들의 상태가 업데이트 되었습니다. 리더는 팔로워들에게 항목이 커밋되었다고 공지하는데, 이 때 클러스터는 합의가 되었다고 말할 수 있습니다.</p><p>간단하게는 이렇게 설명할 수 있는데 알고리즘의 자세한 동작을 알아보고 싶다면, 아래 링크 애니메이션을 통해 확인할 수 있습니다. etcd가 이루는 합의에 대해서 제대로 이해할 수 있을 것입니다.</p><p><a href="http://thesecretlivesofdata.com/raft/">http://thesecretlivesofdata.com/raft/</a></p><p><br></br></p><h3 id="참고-Consul"><a href="#참고-Consul" class="headerlink" title="참고, Consul"></a>참고, Consul</h3><p>서비스 디스커버리 측면에서 Consul도 여기에 낄 수 있습니다. Consul는 서비스 디스커버리(Service discovery)와 설정을 관리하는 툴입니다. Consule는 <strong>분산&amp;클라우드</strong> 환경에 적응하기 위한 고가용성, 유연한 스케일링, 분산시스템의 특징을 가집니다. 리더 선출을 위해 서버는 역시 Raft 알고리즘을 사용합니다.</p><p>Consul의 핵심 기능은 아래와 같습니다.</p><ul><li>서비스 디스커버리 : DNS 나 HTTP 인터페이스를 통해서 서비스를 찾을 수 있게 합니다. 외부의 SaaS 서비스업체도 등록할 수 있습니다.</li><li>Health Checking : 클러스터의 건강 상태를 모니터링하며 문제가 생길 경우 신속하게 전파합니다. 헬스체크는 서비스 디스커버리와 함께 작동하며, 문제가 생긴 호스트로 서비스 요청이 흐르는 걸 막습니다. 이를 이용해서 서비스레벨에서의 서킷 브레이커(circuit breaker)를 구현 할 수 있습니다.</li><li>KV(Key/Value) 저장소(Store) : Consul은 계층적으로 구성 할 수 있는 KV 저장소를 제공합니다. 애플리케이션은 설정, 플래그, 리더 선출 등 다양한 목적을 위해서 이 저장소를 사용 할 수 있습니다. 이 저장소는 HTTP API를 이용해서 간단하게 사용 할 수 있습니다.</li><li>멀티 데이터센터 대응 : 데이터센터 규모에서 사용 할 수 있으며, 복잡한 구성없이 여러 리전(region)을 지원 할 수 있습니다.</li><li>Service Segmentation : Consul Connect는 TLS를 이용 서비스와 서비스사이에 안전한 통신이 가능하게 합니다.</li></ul><p><br></br></p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Designing Data-Intensive Applications(데이터 중심 어플리케이션 설계)</p><p><a href="https://www.bizety.com/2019/01/17/service-discovery-consul-vs-zookeeper-vs-etcd/">https://www.bizety.com/2019/01/17/service-discovery-consul-vs-zookeeper-vs-etcd/</a></p><p><a href="https://cornswrold.tistory.com/523">https://cornswrold.tistory.com/523</a></p><p><a href="https://bcho.tistory.com/1016">https://bcho.tistory.com/1016</a></p><p><a href="https://d2.naver.com/helloworld/583580">https://d2.naver.com/helloworld/583580</a></p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/06/26/zookeeper-etcd/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Kubernetes CI CD devtron, GitOps</title>
      <link>http://tkdguq05.github.io/2022/05/29/devtron/</link>
      <guid>http://tkdguq05.github.io/2022/05/29/devtron/</guid>
      <pubDate>Sun, 29 May 2022 00:52:49 GMT</pubDate>
      <description>
      
        &lt;p&gt;Kubernetes를 편리하게, Devtron&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Kubernetes를 편리하게, Devtron</p><a id="more"></a><h2 id="Devtron"><a href="#Devtron" class="headerlink" title="Devtron"></a>Devtron</h2><p><img src="/images/devtron/isthisyou.png" alt=""></p><p><a href="https://devtron.ai/">Devtron</a>은 Kubernetes를 위한 오픈소스 딜리버리 워크플로우 툴입니다. 쿠버네티스에서 CI/CD 뿐 아니라 GitOps와도 연동이 되어 굉장히 편리하게 Kubernetes를 관리할 수 있습니다. 이런 비슷한 툴로 Rancher를 많이 사용하곤 합니다. 저희 팀에서도 Rancher를 이용해 쿠버네티스를 살펴보고 있습니다. Rancher의 좋은 점은 쿠버네티스 관리를 눈으로 보면서 할 수 있다는 것인데, 일일이 <code>kubectl</code> 을 사용하지 않더라도, 마우스 클릭으로 Pod나 Service 등등의 상태를 확인할 수 있고 조정할 수 있어서 참 편합니다.</p><p><br></br></p><p><img src="/images/devtron/1.png" alt="Devtron 메인 화면"></p><p>Devtron은 이러한 Rancher의 장점에 GitOps까지 추가된 툴이라고 보면 되겠습니다. ArgoCD를 사용해보신 분이 있다면, 마치 Rancher + ArgoCD의 느낌입니다. 기본적인 쿠버네티스의 리소스 확인이나 관리는 Rancher의 기능을 거의 다 갖고 있고, 여기에 Git을 연결하면, ArgoCD처럼 GitOps도 가능합니다. 즉, 배포 관리를 한 번에 처리할 수 있다는 것입니다.</p><p><br></br></p><h3 id="Devtron-1-차트관리"><a href="#Devtron-1-차트관리" class="headerlink" title="Devtron 1. 차트관리"></a>Devtron 1. 차트관리</h3><p>Rancher에서도 마찬가지지만 레포지토리에 차트 주소만 넣어주면, 해당 주소에서 helm chart를 가져올 수 있습니다.  차트를 등록하는 방법은 Global Configurations에서 Chart Repository로 들어가면 됩니다. </p><p><img src="/images/devtron/2.png" alt="차트 등록"></p><p>이름과 차트의 URL, 인증방식을 넣어주고 Save를 해주면 끝입니다! Rancher와 거의 비슷한 방식입니다. 다만, 랜쳐는 Chart페이지에서 등록과 차트확인을 같이 할 수 있지만, Devtron은 차트 등록은 Configuration에서 하고, 차트 관리는 Chart Store에서 하네요.</p><p><br></br></p><img src="/images/devtron/3.png" alt="왼쪽 바를 살펴보면 차트스토어가 있다" style="zoom:50%;" /><p>차트 스토어에 들어가면 아까 등록한 차트들을 볼 수 있습니다. 정말 간단하게 원하는 차트를 누르고 Deploy하면 됩니다. Deploy를 누르게 되면 앱의 이름과 프로젝트, 환경 등을 선택할 수 있습니다. 차트 버전과 values.yaml도 선택가능하며, 하단에는 values.yaml이 나와서 수정을 할 수 있습니다.</p><p><br></br></p><p><img src="/images/devtron/4.png" alt="Airflow 예시"></p><p>이제 배포를 하면 끝!</p><p><br></br></p><h3 id="Devtron-2-App-배포"><a href="#Devtron-2-App-배포" class="headerlink" title="Devtron 2. App 배포"></a>Devtron 2. App 배포</h3><p>Devtron에서 App을 배포하는 방법 역시 간단합니다. 아까 봤었던 왼쪽 바에서 application을 눌러 들어가서 custom app을 만들건지 helm chart기반으로 만들건지 선택하면 됩니다. custom app으로 들어가면 다음과 같은 화면이 등장합니다. <img src="/images/devtron/5.png" alt="App생성 화면"></p><p><br></br></p><p>기본적으로 UI가 너무 깔끔합니다. Rancher는 정말 편하긴 하지만, old한 느낌을 지울 수 없는데 참 깔끔하고 산뜻하네요. 아무튼 화면으로 와서 Git Material을 등록해줍니다. </p><p>여기서 뭔가 의아해 하실 분도 있을겁니다. ‘Git을 꼭 등록해야하는건가?’</p><p> <img src="/images/devtron/6.png" alt="Image세팅"></p><p><br></br></p><p>네 git을 등록해야 사용할 수 있습니다. 이 부분이 조금 별로라고 생각할 수도 있겠지만, 기본적으로 GitOps까지의 연결을 제공하려는 툴이다보니, 전 그러려니 했습니다. 여기서 Git 계정을 등록하고 나면 어디서 이미지를 갖고 올 건지를 선택합니다. DockerHub를 사용할 수도 있겠지만, 사내에서 사용하고, AWS를 사용한다면 ECR도 등록가능합니다. 그 다음 Dockerfile의 위치를 골라주고 Configuration을 저장해주면 됩니다. 그 다음은 Docker Build Config, Secret 등 세부 설정입니다.</p><p>Rancher에서는 Deployment를 하나 짜주고 배포하면 끝입니다. 물론 이게 훨씬 간단해보인다고 생각하시는 분들도 있겠지만, Git으로 관리해준다는 점이 인상적입니다. 사실 로컬에 배포하고 테스트하는 경우가 참 많아서 이미지 어디에 있는지 물어보는 경우도 많거든요. 이렇게 Git으로 꼭 등록을 해줘야 한다면, 이미지를 잃어버리지 않고 관리할 수 있어서 좋을 것 같습니다.</p><p><br></br></p><h3 id="Devtron-3-Workflow"><a href="#Devtron-3-Workflow" class="headerlink" title="Devtron 3. Workflow"></a>Devtron 3. Workflow</h3><p>위에서 설정을 다 했다면 이제 워크플로우 단계입니다. Pipeline을 만들 수 있는 것인데요, Devtron 의 핵심 기능이라고 봐야 할 것 같습니다. 파이프라인은 세 종류로 나뉩니다. 이 글에서는 Continuous Intergration만 살펴보겠습니다. 저는 이것만 사용하거든요!</p><img src="/images/devtron/7.png" alt="Build Pipeline" style="zoom:50%;" /><p><br></br></p><h4 id="Continuous-Integration"><a href="#Continuous-Integration" class="headerlink" title="Continuous Integration"></a>Continuous Integration</h4><ul><li>Github에서 하나의 레포에 연결할 수 있습니다. 어떤 브랜치에 연결할 것인지를 고르고 넘어가면 기본적인 파이프라인이 생성됩니다. </li></ul><img src="/images/devtron/8.png" alt=" " style="zoom:50%;" /><img src="/images/devtron/9.png" alt="build configuration" style="zoom:50%;" /><ul><li>자동으로 빌드할 것인지 일일이 마우스 클릭으로 빌드할 것인지를 선택할 수 있고, 파라미터를 넣을 수 있으며, 빌드 전 스테이지, 빌드 후 스테이지에 작업을 추가할 수 있습니다. </li></ul><p><br></br></p><h4 id="Continuous-Deployment"><a href="#Continuous-Deployment" class="headerlink" title="Continuous Deployment"></a>Continuous Deployment</h4><p>위의 사진에서 Build파이프라인 옆에 +가 보이실 겁니다. 이 버튼을 눌러주면 Deployment를 어떻게 할 것인지 선택할 수 있습니다.</p><p> <img src="/images/devtron/10.png" alt="어디에 배포할지 고르자"></p><p><br></br></p><p>어떤 환경에 배포할지, 네임스페이스는 어디인지를 설정할 수 있고, 배포 전략도 고를 수 있습니다. 저는 test환경으로 환경을 4-devtron으로 해놨는데, dev - stage - prod 로 나눠서 안전하게 배포를 할 수도 있습니다. </p><p>production은 예민한 사항이다 보니, 유닛 테스트와 기능적인 테스트들을 추가적으로 설정해줘야겠지만, 가능합니다. 그래서 stage와 prod로 나눠서 관리를 하고 싶다 하면 다음과 같이 설정해서 운영할 수 있습니다. 운영 쪽은 확인을 한 번 해주고 수동으로 승인을 해서 배포하는게 나을 것 같습니다.</p><img src="/images/devtron/11.png" alt="staging-production" style="zoom:50%;" /><p><br></br></p><p>자 이렇게 설정되면 끝입니다. 그래서 뭐가 좋은거냐구요?  ArgoCD처럼 변경사항을 반영해서 자동으로 배포를 할 수 있습니다. 작업을 하는 git 레포 디렉토리에서 변경사항을 만들고 git add - commit - push를 타겟 브랜치로 보내면, 이를 감지하고 자동으로 devtron이 동작하기 시작합니다. 마치 github action을 활용해서 배포를 하는 것처럼 말이죠. </p><p>위에서 등록한 파이프라인대로 쭉 돌아가기 시작하는데, 이 상황은 아래 그림처럼 확인할 수 있습니다. 성공을 했는지 실패를 했는지, 디테일도 볼 수 있고, 배포 된 이후의 상태체크 까지 가능하며 롤백도 가능합니다. 한 큐에 배포까지 자동으로 되는 게 참 편리하지 않나요?!</p><p><img src="/images/devtron/12.png" alt="배포 상황을 알 수 있다."></p><p><br></br></p><h3 id="이외에도…"><a href="#이외에도…" class="headerlink" title="이외에도…"></a>이외에도…</h3><p>Devtron은 Log Analyzer가 내장되어 있어 pod의 logging을 rancher보다 조금 더 편하게 확인 가능하다는 장점이 있습니다.  Rancher는 쌩 로그가 나와서 알아보기 참 어려운 점이 있지만, Devtron은 좀 나을 것 같습니다. 또 vault, external secret을 연결해서 사용을 할 수 있습니다. Rancher는 Vault를 따로 관리하는 느낌인데, Devtron은 쉽게 vault와도 연동이 가능합니다. </p><p>참 다양한 기능이 있어서 Rancher를 아예 떼버리고 Devtron으로 이사갈까?? 싶었지만, Rancher는 또 Rancher만의 장점이 있습니다. 아쉽게도 Devtron에서는 쉘을 따로 열 수 없습니다. Rancher에서는 답답하면 kubectl 쉘을 열어서 필요한 명령어들을 직접 내릴 수 있었지만, Devtron에서는 따로 쉘을 주지 않습니다. Bulk Edit을 할 수 있는 창은 있지만, Rancher에서 주는 쉘과는 다릅니다.</p><hr><p><strong>따라서</strong> 저희 팀은 같이 써보기로 했습니다. k8s에서 파드에 직접 접근해서 수정하거나, 노드를 늘리거나 줄인다거나, 좀 더 로우 레벨에서 작업할 일이 있으면 Rancher를 사용하고 그 외에 CI/CD 쪽은 Devtron을 사용하면 편하게 작업할 수 있을 것 같습니다. 현재는 POC 중이라 큰 문제는 없는 것 같습니다. GitOps가 된다는 점이 아주 매력적으로 다가오네요. 물론 내부적으로 DevOps팀과 Git Token 사용에 대해서 논의는 필요합니다. Github에서 토큰 발행이 너무 무분별하게 이루어진다면, 관리가 어렵게 되거든요. 잘못해서 유출된 토큰이 있게되면 참 곤란해집니다. 권한을 최소한으로 해서 Token을 발행할 수 있도록 하던지, 관리 주체를 한정해서 최소한으로 토큰을 발행할 수 있게 하던지 등등 내부적인 논의가 꼭 필요할 것 같습니다.</p><p><br></br></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><p><a href="https://devtron.ai/">https://devtron.ai/</a></p></li><li><p><a href="https://github.com/devtron-labs/devtron">https://github.com/devtron-labs/devtron</a></p></li><li><p><a href="https://www.youtube.com/watch?v=ZKcfZC-zSMM">https://www.youtube.com/watch?v=ZKcfZC-zSMM</a></p></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/05/29/devtron/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또 7기 시작하기</title>
      <link>http://tkdguq05.github.io/2022/05/15/geultto7/</link>
      <guid>http://tkdguq05.github.io/2022/05/15/geultto7/</guid>
      <pubDate>Sun, 15 May 2022 10:06:01 GMT</pubDate>
      <description>
      
        &lt;p&gt;글또 7기 시작하기!&lt;/p&gt;
&lt;p&gt;다짐글 내지 무엇을 작성할지 정리하기.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>글또 7기 시작하기!</p><p>다짐글 내지 무엇을 작성할지 정리하기.</p><a id="more"></a><h1 id="글또-7기"><a href="#글또-7기" class="headerlink" title="글또 7기"></a>글또 7기</h1><p>글또가 무려 7기이다. 3기였나 4기였나, 아무튼 19년 7월에 공부한 걸 정리하자는 목표로 시작했는데 벌써 거의 3년이 다 되어간다. 물론 3년의 시간동안 코로나가 갑자기 터져서 네트워킹은 좀 힘들었던 것 같다. 4기때 잠깐 데이터 분들이 모여서 얘기를 나눴던 것 이후로 오프라인으로 모임을 거의 할 수 없었다. 드디어 끝이 보이기 시작했고 오프라인 모임을 시작해보려고 한다. 7기에 운영진으로 참여하게 되었는데, 글또 시작부터 너무 하고 싶었던 오프라인 모임을 적극적으로 추진해보려고 한다. 확실히 많이 친해질수록 글을 더 자세히 읽게 되고 피드백 퀄러티도 높아지는 것 같았다. 크고 작은 모임을 통해서 사람들이 더 많이 영향을 주고받았으면 좋겠다.</p><p><br> </br></p><h2 id="뭘-해야할까"><a href="#뭘-해야할까" class="headerlink" title="뭘 해야할까?"></a>뭘 해야할까?</h2><p>글또 7기를 통해 해야할 것은 명확하다. 2주간의 기간동안 글 하나 이상을 제출하는 것이다. 하지만 이 커뮤니티에서 글 이상의 가치를 얻어가고 불어놓고 싶은 생각이 있다. 다양한 채널들이 생겼는데, 나보다 어떤 분야에 대해서 더 잘하시는 분이 있으면 더 배워보고, 내가 더 잘 아는 분야가 있다면 더 잘 알려주고 싶다. 그 전에 무엇을 주제로 글을 작성해야 할지, 6개월간 어떻게 살아볼지 정리를 좀 해봐야겠다.</p><h3 id="새해의-초심"><a href="#새해의-초심" class="headerlink" title="새해의 초심"></a>새해의 초심</h3><p>사실 1월부터 어떤 큰 계획을 세워놓고, 또 월 별로 회고를 해보고 있다. 노션에 정리 중이었는데, 블로그 자체를 노션으로 옮길까도 고민해보고 있다. 아무튼 예전에 정리했던 내용들을 한 번 살펴보자.</p><p>노션에는 Milestone이란 제목으로 작성해놨는데, 이 페이지는 크게 네 부분으로 나뉘어져있다. (생각보다 많네?)</p><p><strong>장기, 년, 월,  Not To Do</strong> 이다.</p><p><br> </br></p><h4 id="장기"><a href="#장기" class="headerlink" title="장기"></a>장기</h4><ul><li>45세 전에 원하는 만큼 일하고 생활할 수 있는 캐시플로우를 확보하기</li><li>몸과 마음의 건강<ul><li>독서와 운동</li><li>독서모임을 통해 월 1권의 독서를 한다</li><li>주 3회 이상 운동을 한다.</li></ul></li><li>인생을 풍족하게 만드는 취미활동<ul><li>요리<ul><li>내가 한 요리를 기록해두자, 노션 페이지를 이용</li><li>요리와 함게 곁들인 와인이나 음료들도 기록해두자<ul><li>인스타 와인 계정, 요리계정?</li></ul></li></ul></li><li>제빵?</li><li>독서</li><li>테니스</li><li>피아노</li></ul></li></ul><p>장기 계획으로 <strong>캐시플로우</strong>를 생각했다. 파이어족이란 단어를 막 들었을 때 였는데,  막상 내 인생 목표가 뭘까, 장기적인 목표를 뭘 세우면 좋을까 하다가 괜찮을 것 같아서 정해봤다. 좀 더 근사한 목표를 잡고 싶기도 했지만, 아직까진 좋아보인다.</p><p>두 번째는 <strong>몸과 마음의 건강</strong>이다. 일을 하면서 잃기 쉬운게 건강인 것 같다. 몸도 몸인데, 정신이 망가지고 있는 걸 깨닫기 참 힘들고 다시 올라오기도 힘든 것 같다. 물론 몸이 무너지면서 정신도 흐트러지는 것 같긴한데, 일단 관리할 대상을 두 가지로 정했다. 몸과 마음 둘 다 중요하다. 몸 건강이야 운동으로 챙기면 되는데 마음 건강은 어떻게 해야할까 하다가 책이 마음의 양식이라는 생각이 들었고 개발 도서 외에 책을 안 읽은지 오래됐다는 것을 느꼈다. 그래서 독서도 하고 독서모임도 해야지 하는 찰나에 6기에서 독서모임이 만들어졌고 지금까지 해오고 있다. 혼자 읽는 것보다 모여서 얘기를 나누니 머리에 더 잘 들어오는 느낌이라 좋다. 꾸준히 하고 싶다.</p><p>세 번째는 <strong>취미생활</strong>이다. 돈만 많이 번다고 행복할까?란 생각을 해봤는데 인생이 풍족해야, 또 즐길거리가 많아야 행복할 수 있을 것 같았다. 그래서 취미 생활도 여러가지를 해봐야겠다고 생각했다. 하고 싶은 취미들과 내가 잘하는 것들을 쭉 써봤다. 다 하기에는 시간이 없을 것 같았지만 최대한 추려봤다. 요리나 와인을 기록해야겠다고 생각한 건, 사진만으로는 기억에 잘 남지 않는 것 같았고 기록을 해놓고 정리를 해놔야 공유하기도 쉬울 것 같았기 때문이다. 특히 와인은 마시고 슥 지나가 버리기 마련인데, 이왕 돈 좀 쓴거 기록이라도 해놔야 돈이 덜 아까울 것 같았다. 그래서 와인 계정하나 만들었고 와인 마실일 있을 때마다 올리고 있다. 술 마시면서 올려야되는데, 나중에 올려야지 하다보니깐 귀찮아진다… 다시 올려야겠다.</p><p><strong>최근에는</strong>, <strong>테니스</strong>를 배우기 시작했다. 너무 배워보고 싶었던 운동이었는데, 생각보다 어렵다. 영상에서는 너무 쉬워보였는데, 축구나 농구, 배드민턴이랑 좀 다른 스포츠인 것 같다. 하지만 어려워서 한 번 잘 맞을때 희열이 있다. 6개월 내에 게임 뛰어보는게 목표다.</p><p><br> </br></p><h4 id="년"><a href="#년" class="headerlink" title="년"></a>년</h4><ul><li>데이터 엔지니어로서 커리어를 강화하기<ul><li>데이터 플랫폼팀 업무에 적응한다</li><li>연동 업무나 새벽 작업을 줄일 수 있도록 만든다</li><li>누구나 원하는 데이터를 쉽게 얻을 수 있는 플랫폼을 구성한다<ul><li>기획자나 데이터 사이언티스트들의 불편함이 무엇일까?</li></ul></li><li>다른 엔지니어들의 고민은 무엇인지 공유하기<ul><li>글또나 다른 커뮤니티를 통해서</li></ul></li></ul></li><li>ML 플랫폼 쪽 일을 같이 하면서 ml을 좀 더 쉽고 빠르게 할 수 있게 만들기<ul><li>Kubernetes 익숙해지기</li><li>MLOps 관련 커뮤니티, 세션 꾸준히 참석</li></ul></li><li>독서 모임 꾸준히 해보자<ul><li>개발</li><li>그 외</li></ul></li></ul><p>22년에 할 것들을 생각해봤는데 이 주제는 다분히 커리어적이다. 21년 12월에 이직을 막 하기도 했고, 새 직장에서 뭘 해보고 싶은지, 어떤 커리어를 쌓아가고 싶은지를 정리해봤다.</p><p>먼저 기존 업무에 적응하는게 1순위였다. 지금은 많이 적응했고 어떻게 돌아가고 있는지 감이 잡힌 상태다. 직접 업무를 하나하나 해보고 싶긴 한데, 그 전에 키워야 할 스킬들이 필요한 것 같다.</p><p> 데이터 플랫폼을 이용하는 다른 분들의 고민이 무엇인지 파악하고 싶었다. 좀 더 편하게 쓰고 불편함을 줄이고, 이런 시도들을 해야 매너리즘에 빠지지 않을 것 같았고, 계속 무엇인가를 배우고 성장시킬 수 있을 것 같았다. 연동업무를 많이 맡아야 이런 고민들을 더 해볼 수 있을 것 같은데, 7-8월 쯤 되면 충분한 기회가 올 것 같다.</p><p>ML플랫폼을 구축해나가고 있다. Feature Store나 FastAPI, Kubernetes 등 MLOps에 필요한 스킬들을 키워나가고 있다. MAB프로젝트에 참여하고 있고 ML Engineer분들과 일할 기회가 많아 이 파트를 맡게 되었다. 조금 더 다듬으면 운영 쪽에 배포할 수 있을 것 같은데, 얼른 마무리 하고 배포하고 싶다. 데이터 엔지니어지만 ML쪽을 놓고 싶지 않았는데, 참 다행이다.</p><p><br> </br></p><h4 id="월"><a href="#월" class="headerlink" title="월"></a>월</h4><p>월 별에서는 <strong>직장과 개인</strong>으로 나눠서 작성했다. 월 별로 직장에서 이루고 싶은 것과, 개인적으로 이루고 싶은 것을 나눠봤다. 나중에 지나고 나서 다시보면 참 재밌다. <code>&#39;와 이거 고민많이 했었는데, 이젠 어느정도 할 수 있게 됐네ㅎㅎ&#39;</code>, <code>&#39;아 이건 아직도 못했네...&#39;</code> 란 생각이 교차하는데 다음 계획 세우기에 좋은 것 같다. 근데 3월까지밖에 못했다… 4월부터 좀 게을러진 것 같아서 반성하고 글또 시작과 함께 다시 습관을 들여야겠다.</p><p>(직장 관련 내용이 있어 1월달 내용에 일부 수정)</p><ul><li>직장 - 성공적으로 온보딩 끝내고 실무투입<ul><li>EKS 개발계 구성하기<ul><li>JupyterHub 및 기타 컴포넌트 최적화(Subnet IP, MEM, CPU 등)</li><li>업무 우선 순위에서 밀림, 일단 AA나 운영 airflow에 더 신경쓰기</li><li>개선점 없을지 고민<ul><li>Airflow PodOperator로 띄웠을 때 IP부족해지지는 않는지 확인</li><li>연산최적화 - Ray, Faiss 등</li></ul></li></ul></li><li>연동 작업 파악 및 실제 연동<ul><li>모니터링</li><li>기타 다른 방안 있을지, 개선점은 없을지 고민</li></ul></li><li>SageMaker 테스트</li></ul></li><li>개인 - EKS, 쿠버네티스에 익숙해지기, EKS로 airflow 자유롭게 띄워보기, config 설정<ul><li>EKS 관련된 블로그 글 작성하기 - 하나도 못씀<ul><li>쿠버네티스 개념 글</li><li>EKS로 Airflow 띄우기</li><li>mlFlow 띄우기</li><li>JupyterHub 띄우기<ul><li>문제점은 있음, helm chart 배포 필요</li></ul></li></ul></li><li>CDC 개념 파악</li><li>Deview, Kakao 세션에서 CDC, MLOps 관련 세션 정리</li></ul></li></ul><p>대충 이런 내용들이 있었다. Kubernetes를 써보고 싶었고 내가 운영하는 서비스를 하나 만들어보고 싶었는데, 2월 3월달 쯤 다시보니 너무 뿌듯했다. (4월 5월 내용도 회고를 했다. 요건 업무 적인 내용이 많아서 비밀)</p><p><br> </br></p><h4 id="Not-To-Do"><a href="#Not-To-Do" class="headerlink" title="Not To Do"></a>Not To Do</h4><ul><li>유튜브만 보고 있기</li><li>하염없이 웹 서핑 및 핸드폰</li></ul><p>얼마전에 대나무 숲 채널에도 올라온 것 같았는데, 너무 공감가는 내용이었다. 핸드폰을 너무 많이 보고 있다. Not To Do로 써놓고 나서 유튜브를 보다가 끄고 다른 할걸 해나갔었는데, 애플워치를 사고 나니… 폰을 많이 보게 되는 것 같다. 간만에 Not To Do를 확인 했으니, 자제 해야겠다. </p><p><br> </br></p><h2 id="다시-글또로…-뭘-써볼까"><a href="#다시-글또로…-뭘-써볼까" class="headerlink" title="다시 글또로… 뭘 써볼까?"></a>다시 글또로… 뭘 써볼까?</h2><p>이렇게 글또 7기에 들어가기 전에 어떻게 살아볼까하고 이전에 세워본 계획들을 살펴봤고 남은 6개월도 어떻게 살지 생각해봤다. 이제 뭘 작성할지 정리를 해봤다.</p><ul><li>스터디</li><li>Kubernetes</li><li>MLOps</li><li>데이터 연동</li></ul><p>크게 세 가지 내용이 주가 될 것 같다. 먼저 스터디는 회사에서 조직해보고 싶었다. 이전 회사에서도 스터디나 공유하는 문화를 통해 참 빠르게 성장할 수 있었던 것 같았다. 뭔가 같이 해보고 싶어하는 분들이 있어보였는데, 총대메고 싶지 않아하는? 총대메면 더 열심히 참여할 수 밖에 없다는 걸 알기에 같이 해보자고 했고, 스터디 리딩을 하면서 끝까지 가보려고 한다. Data Intensive Application, 데이터 중심 애플리케이션 책으로 시작을 했는데 첫 스터디를 마쳤고 시작이 좋은 것 같다. 이 내용들도 정리해서 공유할 예정이고, 다른 스터디원들 분에게도 공유해서 글 작성을 권유해볼 생각이다.</p><p><strong>Kubernetes</strong> 는 써야지 써야지 하고 하나도 안썼다. 개념적인 내용을 공부하면서 노션에 작성하긴 했지만 글로 쓸 정도의 완성도가 아니라 메모 정도의 수준이라 다시 정리를 해야할 것 같다. 전체적인 이론 내용을 쭉 보고 업무를 하면서 사용하는 개념만 계속 쓰는데, 그래서 낯선 개념이 등장하면 다시 기억이 잘 나지 않고 있다. 대표적으로 3월인가 4월쯤에 K8S에 장애가 났는데 Pod Description에 Cordon이라는게 나왔다. 어디서 봤는데 하다가 이전에 정리한 걸 보니 정리한 게 있었다. 역시 글로 남기지 않으니 머리에 잘 남지 않는다. 처음부터 다시 보고 완성도 있게 정리를 해봐야겠다.</p><p><strong>MLOps</strong>는 최근에 많이 맡아서 하고 있는 부분이다. Kubernetes 클러스터를 구성해서 다양한 것들을 올려보고 있는데, 이런 앱들을 어떻게 올려야 하는지, 어떻게 잘 사용해야 하는지를 작성해보고 싶다. 기본적으로 Airflow나 MLflow, JupyterHub을 다뤄보고 싶고, Feature Store의 필요성과 Feast를 정리해보고 싶다. 아마 다른 회사들에서도 비슷한 고민을 갖는 엔지니어들이 있는데, 설치 방법이 잘 나와있는 곳이 없어서 시간을 많이 소모하는 것 같았다. 엔지니어 특징 중 하나가 정리를 귀찮아 한다는 건데, 역시 한 분야에 깊게 갈수록 잘 정리된 내용이 별로 없다. 내가 인터넷을 통해 도움을 받았듯, 나도 베풀 차례이지 않을까?</p><p><strong>데이터 연동</strong> 은 데이터 엔지니어로서 가장 많이 다뤄야 하는 부분이다. 이 연동 작업을 하는데에 있어 부족한 점이 있는 게 느껴져서 이걸 보완하는 기술적인 내용을 주로 다뤄볼 생각이다. Java를 공부한 내용을 써봐야 되나 싶긴한데, 너무 재미없을 것 같아서… Java를 통해 재밌는 사이드 프로젝트를 하나 만드는 게 더 좋아보인다. 뭐든 하나 만들어봐야 내 것이 되는 것 같다. 회사의 자원을 빌려서 하고 싶은 걸 좀 해봐야겠다ㅎㅎ.</p><p>이렇게 쓰고 보니 글 쓸게 너무 많다. 2주에 하나? 한 네 개를 작성해야 목표한 걸 다 쓸 수 있을 것 같은데… 적당히 몸과 마음의 건강을 생각하면서 차근차근 써봐야겠다. 사실 글 작성 외에도 글또에 정말 훌륭하신 분들이 많아서 그 분들의 글을 보고 좋은 내용은 팀원들에게 공유하면서 이건 어때요 저건 어때요 하면서 회사에서든 글또에서든 많은 토론을 이끌어내고 싶다. 가만히 앉아서 수동적으로 정보를 수용하려고 하면 배움의 한계가 있는 것을 느꼈다. 처음이었던 3기 4기때 아는 사람도 없고 낯을 가리면서 적극적으로 정보를 공유하지 못한 것 같은 아쉬움이 있다. 다른 분들의 글을, 다른 채널에 있는 분들의 글도 잘 챙겨보고 더 많은 참여를 통해 더 많은 걸 얻어나가고 싶다. 이번 7기, 6개월간 더 많이 성장해보자!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/05/15/geultto7/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2021년 회고</title>
      <link>http://tkdguq05.github.io/2022/01/02/2021-goodbye/</link>
      <guid>http://tkdguq05.github.io/2022/01/02/2021-goodbye/</guid>
      <pubDate>Sun, 02 Jan 2022 06:14:07 GMT</pubDate>
      <description>
      
        &lt;p&gt;다사다난한 2021년 한 해를 보내면서, 회고하기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>다사다난한 2021년 한 해를 보내면서, 회고하기</p><a id="more"></a><h2 id="2022년이-되었다"><a href="#2022년이-되었다" class="headerlink" title="2022년이 되었다."></a>2022년이 되었다.</h2><p>해가 지날수록, 나이를 먹을수록 시간이 더 빠르게 지나가고 있다. 21년은 사실 힘든 일이 더 많이 기억에 남았던 달이었던 것 같은데, 정리가 되려니까 벌써 2022년이 되어버렸다. 뭘 해왔었는지 기억이 잘 안나는데, 블로그나 기록하는 일에 대해서 이 맘 때쯤 다시 한번 고마움을 느낀다. 겨우겨우 글또를 통해서 반 강제적으로 글을 쓰고 있는데, 이를 통해서 어떤 생각을 해왔고 어떤 걸 하고 있었는지 기억이 난다. 이번 글또에서는 좀 더 글을 많이 작성하고 싶었지만, 여러 일이 겹치다 보니… 계획을 실천할 수 없었다. 패스권 2장을 최대한 안써보려고 했지만, 빠르게 소진해버렸고? 다행히도 예치금 차감없이 마무리 지을 수 있을 것 같다. 정말 빠르게 21년이 흘러왔는데, 무슨 일이 있었는지 한 번 생각해 봐야겠다.</p><h2 id="2021년에-대하여"><a href="#2021년에-대하여" class="headerlink" title="2021년에 대하여"></a>2021년에 대하여</h2><ul><li>21년에 작성한 글 목록<ul><li>2021-12-19 <a href="https://tkdguq05.github.io/2021/12/19/kafka-101/">kafka 첫번째, 개념 정리</a></li><li>2021-12-04 <a href="https://tkdguq05.github.io/2021/12/04/linux-top10/">linux top 10 명령어들을 정리해봤다</a></li><li>2021-11-18 <a href="https://tkdguq05.github.io/2021/11/18/Moving/">이직의 과정, 그리고 얻은 것</a></li><li>2021-11-03 <a href="https://tkdguq05.github.io/2021/11/03/EKS-workshop/">EKS-workshop</a></li><li>2021-10-07 <a href="https://tkdguq05.github.io/2021/10/07/docker-hub-limit/">Docker hub pull limit이 발생했다면?</a></li><li>2021-09-12 <a href="https://tkdguq05.github.io/2021/09/12/work-overtime-with-airflow/">Airflow와 야근하기</a></li><li>2021-09-04 <a href="https://tkdguq05.github.io/2021/09/04/sparton/">sparton</a></li><li>2021-08-28 <a href="https://tkdguq05.github.io/2021/08/28/airflow-ecs/">Airflow Workers on ECS Fargate</a></li><li>2021-08-11 <a href="https://tkdguq05.github.io/2021/08/11/yaml-break/">yml 파일을 잘 다뤄보자</a></li><li>2021-08-01 <a href="https://tkdguq05.github.io/2021/08/01/geultto6/">글을 왜 써야 할까? - 고민과 정리의 시간</a></li><li>2021-05-02 <a href="https://tkdguq05.github.io/2021/05/02/geultto5-end/">글또 5기를 끝내고, 회고하기</a></li><li>2021-04-18 <a href="https://tkdguq05.github.io/2021/04/18/AWS-Immersion-DAY/">AWS_Immersion_DAY, 추천 파이프라인</a></li><li>2021-04-04 <a href="https://tkdguq05.github.io/2021/04/04/airflow-clusterization/">Airflow Clusterization</a></li><li>2021-03-07 <a href="https://tkdguq05.github.io/2021/03/07/apriori-FP/">Apriori와 FP-Growth. 추천 시스템 시리즈</a></li><li>2021-02-21 <a href="https://tkdguq05.github.io/2021/02/21/airflow-basic2/">Airflow Basic. 두 번째</a></li><li>2021-01-24 <a href="https://tkdguq05.github.io/2021/01/24/cost-mgt-emr/">AWS 비용관리의 서막, EMR</a></li></ul></li></ul><p>글또 5기부터 시작해서 6기까지 쭉 흘러오다 보니 21년이 다 가버렸다. 작성한 글들을 보니 주로 업무에서 발생한 일들을 작성한 것 같다. 그 외에 이직을 한 내용, 워크샵에서 배운 내용, 회고 글, 하루를 열심히 살았던 스파르톤에 대해서 글을 작성했다. </p><h3 id="21년-초"><a href="#21년-초" class="headerlink" title="21년 초"></a>21년 초</h3><p>21년 초에는 데이터 엔지니어로 전향하고 인수인계 받은 일에 대해서 적응하는 기간이었다. 그 기간에 갑자기 EMR비용이 상승한 일이 있어서 이를 위해서 Cloud FinOps개념을 공부했고, 적용하기 위해서 노력했다. 덕분에 데이터 사이언스 팀에서 발생하는 비용을 측정하고, 관리함으로써 비용을 어느정도 줄일 수 있었다. 하지만 동시에 한계도 느껴졌다. 결국 AWS를 쓰는 모든 사람들이 합의를 하는 어떤 룰을 만들고 지켜나가야할 것 같은데, 혼자 열심히 해봤자 전체적인 관리가 안될 것이 너무 뻔했다. 체계를 잡고 싶었지만 이런 체계를 어디서부터 잡아야 할지, 어떤 자료를 찾아봐야할지, 그리고 실제 팀에서 따라올 수 있을지 감이 잡히질 않았다. 가뜩이나 과중한 업무가 치고들어오는 마당에, 전 조직을 위한 체계는 결국 잡지 못했다. 결국 다른 일을 할 수 밖에 없었다.</p><h3 id="21년-중"><a href="#21년-중" class="headerlink" title="21년 중"></a>21년 중</h3><p>한 차례 전쟁을 치룬 후에는 Airflow 고도화 작업에 착수했다. 특정 작업을 하는 Airflow는 버전도 낮았고, 싱글구조로 돌아가고 있었는데, 워커에 부하가 계속 걸리고 있는 상태였기 때문에 Clsuter화 시켜주는 작업이 필요하다고 생각했다. 그렇게 하다보니, 이 구조를 Kubernetes에 옮겨서 깔끔하게 관리하고 싶다는 생각도 하게 되었다. 하지만 이 당시에는 학습이 많이 부족해서 아이디어만 갖고 있다가 8월 쯤부터 ECS로 옮기고 나서 이것을 Kubernetes로 옮겨보자 라는 목표를 세우게 되었다. ECS까지는 성공했는데, kubernetes는 조금 아쉽다. 이직생각이 없었고 회사에 열점이 더 남아있었다면, 데이터 사이언스 팀 인프라를 모두 Kubernetes상에 옮겨놓고 장에 없이 운영할 수 있을 것 같았는데, 조금 아쉽다. 7-8월쯤 회사에 정이 떨어지니 어떤 작업도 하기 싫어졌다. </p><p>이 맘 때쯤 BigQuery도입을 위해서 설득을 하고 있었다. 의사결정권자분들께 이게 왜 필요하고, 어떻게 확장이 가능한지를 설명했다. 결국 POC를 승인 받았고 작업을 이제 막 하려고 했다. 그런데 데이터를 받는 부분은 개발팀 소관이었고, Pub/sub으로 데이터를 받아야 하기에 도움을 청했다. 자바 소스에 publisher하나 붙이면 되는 간단한 작업으로 생각했는데, 감감 무소식이다가 결국은 할 수 없다는 답변을 받았다. 그리고 이걸 시작으로 다양한 일들이 일어나기 시작하면서 더 이상 회사를 위해 어떤 일도 스스로 나서서 하기 싫어졌다. 2년동안 설득에 설득을 하면서 필요한 것들을 발전시키고 있었는데, 바닥을 보고 나니 어떤 열정이 다 사그러지는 느낌이었다. 조직은 계속 같은 일만 반복하고, 발전이 없어보였고, 이 잔잔한 물결에 나조차도 고여버리고 있는 느낌이 들기 시작했다. </p><p>회사에 데이터 관련 인력으로 맨 처음 들었을때는 참 재미있었다. 아무것도 없었기 때문에 물론 힘들었지만, 하나하나씩 만들어내고, 발전해나가는 모습이 너무 좋았었다. 다른 조직에서 하고 있는 기술들도 가져와서 써보고 토론하면서, 나름 데이터 조직이 제대로 구성되고 있다는 생각을 했었다. 하지만 여기까진 것 같았다. 발전과 성장에 목마른 사람들은 몇 명 남아있지 않았고, 그 불씨도 다 꺼지고 있었다. 힘을 받아서 해야했던 시기에 불씨가 다 꺼져버렸었다. </p><p>그래서 이때부터 이직준비를 했던 것 같다. 예전과 같이 빠르게 성장하는 조직에서 전 처럼 치고 받으면서 모르는 부분을 헤쳐나가고 싶었다. 이직준비를 하게 되면서 글또에 올리는 글이 퀄러티가 급 떨어지는 게 너무 느껴졌었다… 항상 완성도 높은 글을 작성하고 싶었는데, 이직에 회사에 글까지 신경쓰면 부서질 거 같아서 조금 놓아버렸다…ㅠㅠ</p><p>추가로 이때 처음으로 강의를 해보게 됐다. 이직시기와 맞물려서 할까말까 고민했지만, 거절할 수 없는 금액이었다… 엘리스쪽에서 제안을 해주셨는데, 가이드도 잘해주셔서 무리없이 잘 끝낼 수 있었다. 마이크까지 제공받아서 녹음을 해봤는데, 나름 재밌었다. 다음에 기회가 된다면 또 해보고 싶다. </p><h3 id="21년-말"><a href="#21년-말" class="headerlink" title="21년 말"></a>21년 말</h3><p>10월과 11월에는 이직 면접과 그 과정의 마무리에 있었다. 6개정도의 회사와 컨택을 했고 최종 두 회사에 합격을 했다. 주위에 여러 분들이 도와주신 덕에 신중하게 회사를 선택할 수 있었고 그 결과에 만족을 하고있다. 이직도 이직이지만, 회사를 고르는 것도 쉽지 않다는 것을 느꼈다. 새 회사에 합격을 해서 마음은 놓았지만… 남은 다른 분들이 걱정이 되어 마냥 좋지만은 않았다. 능력에 대해서 다시 한번 생각하게 되었는데, 만약 내가 능력이 정말 출중했다면 팀 단위로도 회사를 옮길 수 도 있지 않았을까? 이런 생각도 해봤다. 정말 다행히도 남은 분들도 컬리 근처의 회사에 합격을 하셔서 자주 볼 수 있을 것 같다. 이직하는 시간이 길어질수록 이직이 쉽지 않았을 것 같다고 느꼈어서, 내가 나간 후에 바로 이직을 하게 되셔서 참 다행이라고 생각했다. 물론 전 회사는 많이 착잡하겠지만, 그 동안 계속 이야기해온 걸 조금이라도 들어줬더라면… 조금이라도 더 있지 않았을까 했지만…</p><h3 id="최근"><a href="#최근" class="headerlink" title="최근"></a>최근</h3><p>새 회사에 적응 중이다. 전 보다 규모가 엄청나게 크다보니, 적응이 잘 되지 않고, 보안 규칙도 너무 어색하고, 처음 써보는 것들이 너무 많아서 어렵다. 특히 개발 규모와 하루에 가용하는 데이터 양이 예전에 작업하던 거에 비해, 아니 비교할 수 없을 정도라 작업 실수에 대한 부담이 좀 있다. 예전에는 DB에 직접 접근하고, 권한을 나누지 않고 사용했었는데, DBA님께 쿼리에 대한 허가를 받고 작업을 하는 구조가 이제야 이해가 가기 시작했다. 가파른 성장을 해가고 있고, 다양한 시도를 하고 있는 회사에 입사하게 되어서 참 좋다고 생각하고 있다. 일단 가장 먼저는 EKS와 Rancher 사용법을 빠르게 익혀야 될 것 같다. 1월까지 기한인 일이 몇 개 되어서…(입사한 지 2주밖에 안됐는데!ㅠㅠ) 빠르게 EKS쪽 일에 익숙해지고, CDC쪽을 살펴봐야 할 것 같다. 이쪽에서 사용하는 게 처음이라 또 적응하는데 고생할 것 같지만, 차근차근 테스트를 하면서 몸에 익혀야겠다. 카프카는 아마 CDC쪽 일을 하면서 더 공부할 것 같다. 일단은 EKS를 공부하느라… 카프카 글을 1편 써놔서 빨리 2편을 쓰고 싶지만, 당분간은 업무와 관련된 것을 더 보고 작성해봐야지!</p><p>또 요즘 느끼는 것은, 전체 아키텍쳐를 크게크게 보는 법을 배워야 되겠다는 것이다. 거대한 서비스가 돌아가는 것을 처음봐서 어떻게 뭐가 돌아가는지 이해하는데 오래걸리는 것 같다. 시니어 분들은 어떤 문제를 콕 집어서 이게 이렇게 문제가 되니, 개선이 필요하다고 하시는데 아직 그게 어떤 문제가 되는지 잘 이해가 가지 않는다. 이와 관련된 개발서적을 좀 찾아보고 공부해야겠다. 2022년도 열심히 성장해보자!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2022/01/02/2021-goodbye/#disqus_thread</comments>
    </item>
    
    <item>
      <title>kafka 첫번째, 개념 정리</title>
      <link>http://tkdguq05.github.io/2021/12/19/kafka-101/</link>
      <guid>http://tkdguq05.github.io/2021/12/19/kafka-101/</guid>
      <pubDate>Sun, 19 Dec 2021 10:26:46 GMT</pubDate>
      <description>
      
        &lt;p&gt;카프카 첫 번째, 상세 개념 다루기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>카프카 첫 번째, 상세 개념 다루기</p><a id="more"></a><h1 id="카프카"><a href="#카프카" class="headerlink" title="카프카"></a>카프카</h1><p>데이터 엔지니어를 하면서 꼭 다뤄봐야할 것 중에 하나가 실시간 처리입니다. 이 실시간 처리에 다양한 기술들이 사용되지만, 카프카가 특히 많이 다뤄지고 있습니다. 카프카는 LinkedIn의 점차 복잡해지고 거대해지는 아키텍쳐에 따라 개발되었습니다. 가면 갈수록 LinkedIn의 데이터 파이프라인이 복잡해지면서 소스와 타깃의 연결을 정리할 필요성이 생겼고, 데이터를 한 곳에 모아 처리할 수 있도록 중앙집중화 해, 관리할 수 있도록 만들었던 것이 카프카 입니다.</p><p><img src="https://media.vlpt.us/images/king3456/post/43c19a96-28fe-45bb-873b-376f8088312c/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-02-14%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%201.57.16.png" alt="Kafak가 적용되면 한 곳에 데이터를 모아 처리할 수 있다"></p><p>이런 식으로 한 번 정리가 된다면, 복잡성을 떨어트릴 수 있고, 시스템간 의존도를 줄일 수 있겠습니다. 이런 구조를 통해서 카프카는 대용량 데이터를 수집하고, 이 데이터를 실시간 스트림으로 소비할 수 있도록 만들어주게 되었고, 이외에도 다양한 기능을 통해서 카프카는 실시간 처리 분야에서 중요한 플랫폼으로서 기능하게 되었습니다.</p><p><br></br></p><h2 id="카프카의-기본-개념"><a href="#카프카의-기본-개념" class="headerlink" title="카프카의 기본 개념"></a>카프카의 기본 개념</h2><p>카프카는 발행-구독(Publish &amp; Subscribe)의 구조로 이루어져 있습니다. 쉽게 말해서 한 쪽에서 어떤 주제에 대해서 메세지를 보내면 이것을 구독했을때, 관련 주제의 메세지만을 받는 구조인 것입니다. 따라서 이 구조에서는 세 컴포넌트가 등장합니다. 바로 <strong>Producer, Consumer, Broker</strong> 입니다.</p><p><img src="https://www.cloudkarafka.com/img/blog/apache-kafka-partition.png" alt="카프가 기본 구조"></p><p>위 그림에서처럼, 프로듀서가 메세지를 브로커에 보내면, 주제에 맞게 이 메세지들을 보관하고 있다가 컨슈머 그룹에서 이 메세지를 가져가는 구조입니다. 프로듀서와 컨슈머는 대충 이름만 보고도 어떤 일을 하는지는 알 수 있을 것 같습니다. 그런데 브로커는 정확히 어떤 일을 하는 것일까요?</p><p><br></br></p><h3 id="카프카-브로커-클러스터-주키퍼"><a href="#카프카-브로커-클러스터-주키퍼" class="headerlink" title="카프카 브로커, 클러스터, 주키퍼"></a>카프카 브로커, 클러스터, 주키퍼</h3><p>카프카 브로커는 데이터를 주고받기 위한 주체입니다. 이 브로커를 통해서 데이터를 분산 저장하며, 이를 통해 장애가 발생하더라도 안정적으로 서비스를 운영할 수 있습니다. 하나의 서버에는 한 개의 카프카 브로커 프로세스가 실행됩니다. 카프카 브로커 서버는 1대로도 기능을 할 수 있지만, 안전성을 위해서 3대 이상의 서버를 1개의 클러스터로 묶어서 운영하는 것을 권장합니다. 이 클러스터 안에 있는 브로커들은 프로듀서가 보낸 데이터를 분산해서 저장해서 안전하게 서비스를 운영할 수 있도록 합니다.</p><p><img src="https://www.tutorialspoint.com/apache_kafka/images/cluster_architecture.jpg" alt="브로커, 클러스터, 주키퍼"></p><p>브로커가 데이터를 프로듀서로부터 전달받으면 프로듀서가 요청한 토픽의 파티션에 데이터를 저장하고, 컨슈머가 데이터를 요청하게되면 파티션에서 저장된 데이터를 전달합니다. 이렇게 전달된 데이터는 파일 시스템에 저장되고 이는 <code>/tmp/kafka-logs</code>에서 확인할 수 있습니다. 이 설정은 <code>config/server.properties</code>의 <code>log.dir</code>옵션에 정의된 디렉토리기 때문에, 원한다면 설정을 변경해도 무방합니다. log에는 메시지와 메타데이터를 저장하고 index에는 오프셋을 인덱싱한 정보를 담았습니다. timeindex에는 메시지에 포함된 timestamp값을 기준으로 인덱싱한 정보가 담겨있는데, timestamp는 브로커가 적재한 데이터를 삭제하거나 압축하는 데 사용합니다. </p><p>카프카를 좀 더 살펴보다 보면 속도 이슈에 대한 의문이 들 수 있습니다. 왜냐하면 메세지가 저장된 다음 다시 읽어야 하기 때문입니다. 보통 디스크 IO는 속도 이슈에 치명적인 요소로 보이곤 합니다. 하지만 카프카는 <strong>페이지 캐시</strong>를 사용해 디스크 입출력 속도를 높였습니다. 이 페이지 캐시는 OS에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역으로, 한 번 읽은 것은 페이지 캐시 영역에 저장하고 추후에 동일 파일 접근이 일어나면 여기에서 읽게 됩니다. 그래서 브로커를 실행하는데 힙 메모리 사이즈를 크게 가져갈 필요가 없게되는 것입니다.</p><p><strong>replication</strong>은 장애 허용 시스템으로 동작하게 하는 원동력으로 데이터 복제는 파티션 단위로 이루어집니다. 만약 토픽 생성할 옵션을 직접 선택하지 않으면 브로커에 있는 설정대로 구동하게 됩니다. 이 복제의 개수의 최대 값은 브로커 개수만큼 주는 것이 좋은데, 복제 개수만큼 저장용량이 증가하는 단점이 있습니다. 하지만 그 만큼 안정성이 증가하는 장점이 있습니다.</p><p>파티션에는 <strong>리더와 팔로워</strong> 개념이 등장합니다. 복제된 파티션의 구분으로 둘을 나눠놓았는데, 여기서 리더란 프로듀서 또는 컨슈머와 직접 통신하는 파티션을 뜻하는 것이고, 나머지가 팔로워 파티션이 됩니다. 팔로워는 리더 파티션의 오프셋을 확인해서 현재 자신이 갖고 있는 오프셋과 차이가 나게 된다면, 리더 파티션으로부터 데이터를 갖고 와서 자신의 파티션에 저장합니다. 이렇게 데이터를 갖고와서 자기 파티션에 저장하는 것이 <strong>복제</strong>라 하는 것입니다. 만약 리더가 죽는다면 어떻게 될까요? 카프카의 장점은 안정성입니다. 만약에 리더가 다운되면 팔로워 파티션 중 하나가 리더 지위를 넘겨받게되어 정상적으로 기능하게 됩니다.</p><p>여기서, 브로커 중에 한 대는 <strong>컨트롤러</strong>의 역할을 맡게 됩니다. 이 컨트롤러는 중요한 요소로서, 다른 브로커들의 상태를 체크하고 브로커가 클러스터에서 빠지게 되었을 때, 해당 브로커에 있는 리더 파티션을 재분배하는 기능을 하게 됩니다. 카프카는 지속적으로 데이터를 처리해야 해서 브로커의 상태가 비정상이라면 빠르게 클러스터에서 빼내고, 새로운 브로커를 지정해 문제를 해결합니다. 만약 컨트롤러 브로커에 장애가 생기면, 다른 브로커가 컨트롤러를 맡게됩니다.</p><p>카프카 토픽의 데이터는 참고로 삭제가 되지 않습니다. <strong>브로커만 데이터를 삭제할 수 있습니다.</strong> 또한 데이터 삭제는 파일 단위로 이루어지는데, 이 단위를 <strong>로그 세그먼트라고</strong> 부릅니다. 이 세그먼트에는 다수의 데이터가 들어있어서, 데이터를 구분해서 삭제할 수는 없습니다. 세그먼트에 데이터가 쌓이는 동안 파일이 열려있는 상태가 되고, log.segment.bytes,  log.segment.ms 옵션에 따라 조건이 충족되면, 세그먼트 파일이 닫히게 됩니다. log.segment.bytes의 기본값은 1GB입니다. 이 용량 설정에서 너무 작은 용량을 설정하게 되면, 세그먼트 파일을 자주 여닫게 되어 부하가 발생할 수 있습니다.</p><p>컨슈머 오프셋 저장을 할때, log.dir 옵션에 지정된 경로로 가면 consumer_offset 디렉토리가 있는 것을 볼 수 있습니다. 컨슈머 그룹은 토픽이 특정 파티션으로 부터 데이터를 가져가서 처리하고 나서 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋합니다. 커밋한 오프셋이 바로 consumer_offset 디렉토리에 저장됩니다. 여기에 저장된 오프셋을 토대로 컨슈머 그룹은 다음 레코드를 가져가서 처리하게 됩니다.</p><p>아까 브로커 중 한 대는 컨트롤러를 맡는다고 했는데, 다수 브로커 중 한 대는 또 코디네이터 역할을 맡습니다. 코디네이터는 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배합니다. 컨슈머가 컨슈머 그룹에서 빠지게 되면, 매칭되지 않은 파티션을 정상 동작하는 컨슈머로 할당하여 데이터 처리가 이어지도록 하는 거입니다. 파티션을 컨슈머로 재 할당하는 이 과정을 <strong>리밸런스</strong>라고 합니다.</p><p>참고로, <strong>주키퍼는</strong> 카프카의 메타데이터를 관리합니다. <code>bin/zookeeper-shell.sh</code> 을 통해서 주키퍼로 직접 접속할 수 있습니다. <code>ls /</code>를 통해서 원하는 정보를 탐색해서 볼 수 있다. 주키퍼에서 다수의 카프카 클러스터를 사용하려면 주키퍼의 서로 다른 znode에 카프카 클러스터들을 설정하면 됩니다. <code>znode</code>란 주키퍼가 사용하는 데이터 저장 단위입니다. znode는 znode간에 tree구조와 같은 계층 구조를 가집니다. 이 구조에서 root znode가 아닌 한 단계 아래의 znode를 카프카 브로커 옵션으로 지정하도록 합니다. 이렇게 설정되었을 때, 각기 다른 하위 znode로 설정된 서로 다른 카프카 클러스터는 각 클러스터의 데이터에 영향을 미치지 않고 정상 동작하게 됩니다.</p><p><br></br></p><h2 id="카프카-토픽-파티션-컨슈머"><a href="#카프카-토픽-파티션-컨슈머" class="headerlink" title="카프카 토픽, 파티션, 컨슈머"></a>카프카 토픽, 파티션, 컨슈머</h2><p><strong>토픽</strong>과 <strong>파티션</strong>은 카프카에서 데이터를 구분하기 위해 사용하는 단위로, 토픽은 1개 이상의 파티션을 소유하고 있습니다. 프로듀서가 보낸 데이터들이 파티션에 저장되는데 이것을 <strong>레코드</strong>라고 부릅니다. 결국 <strong>파티션</strong>에는 오프셋, 메세지 키, 메세지 값으로 되어있는 레코드가 저장되는 것입니다. 이렇게 레코드가 저장된 파티션을 여러개로 나눔으로써 카프카의 병렬처리의 핵심으로 기능하게 됩니다. 파티션은 큐와 비슷한 구조로, 먼저 들어간 레코드를 컨슈머가 먼저 가져갑니다. pop이 되지 않는 것이 큐와의 차이점이다. 데이터가 삭제되지 않기 때문에 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러 번 가져갈 수 있는 것입니다.</p><p><strong>컨슈머그룹</strong>은 레코드를 병렬로 처리할 수 있도록 파티션과 매칭됩니다. 만약 컨슈머 처리량이 제한되어 있다면, 최대한 많은 레코드를 병렬로 처리하는 가장 좋은 방법 중 하나는 컨슈머 개수를 늘려 스케일 아웃 하는 것입니다. 모니터링을 하며 컨슈머 개수를 늘리면서 동시에 파티션 개수도 늘리게 되면 처리량이 증가합니다.</p><p><strong>토픽 이름</strong>을 지을 때는 이름을 잘 지어야 합니다. 이해하기 어려운 토픽이름은 기술 부채가 되기도 하며, 토픽 이름 변경은 카프카에서 지원이 되지 않으므로 변경이 필요하면 삭제 후 다시 생성해야 하기 때문입니다. Jira 티켓 번호를 넣는 것도 괜찮은 방법 중 하나이고,  &lt;환경&gt;.&lt;팀명&gt;.&lt;애플리케이션명&gt;.&lt;메시지타입&gt;등 카프카를 관리하고 사용하는 그룹에서 규칙을 정하고 잘 따라야 합니다.</p><p><strong>레코드</strong>는 타임스탬프, 메세지 키, 값, 오프셋, 헤더로 구성되어 있습니다. 프로듀서가 생성한 레코드가 브로커로 전송되면 오프셋과 타임스탬프가 지정되어 저장됩니다. 브로커에 한 번 적재된 레코드는 수정이 불가능하고 로그 리텐션 기간 또는 용량에 따라서 자동으로 삭제됩니다. timestamp는 프로듀서에서 해당 레코드가 생성된 시점의 유닉스 타임이 설정됩니다. 여기서 프로듀서가 레코드를 생성할 때 임의의 타임스탬프 값을 설정할 수 있고, 토픽 설정에 따라 브로커에 적재된 시간으로 설정될 수 있습니다. 메세지 키는 메시지 값을 순서대로 처리하거나 메세지 값의 종류를 나타내기 위해 사용됩니다. 키의 해시값을 토대로 파티션을 지정하게 되는데, 동일 메세지 키는 동일 파티션에 들어가고 파티션 개수가 변경되면 메세지 키와 파티션 매칭이 달라집니다. 만약 키를 설정 하지 않으면 <code>null</code>로 설정됩니다. 기본 설정 파티셔너에 따라 파티션에 분배되며, 메세지 키와 값은 직렬화되어 브로커로 전송되어 컨슈머가 이용할 때는 역직렬화를 수행합니다.</p><p><strong>컨슈머</strong> 운영 방법은 크게 2가지입니다. 1개 이상의 컨슈머로 이루어진 컨슈머 그룹 운영하거나, 토픽의 특정 파티션만 구독하는 컨슈머 운영하는 것입니다.</p><p><strong>컨슈머 그룹</strong>으로 운영</p><p>컨슈머 그룹으로 운영하면, 각 컨슈머 그룹으로 부터 격리된 환경에서 안전하게 운영할 수 있습니다. 이렇게 컨슈머 그룹으로 묶인 컨슈머들은 토픽의 1개이상 파티션들에 할당되어 데이터를 가져갈 수 있게 됩니다. 1개의 파티션은 최대 1개의 컨슈머에 할당될 수 있으며 1개 컨슈머는 여러 파티션에 할당 될 수 있습니다. 컨슈머 그룹의 컨슈머 개수는 가져가고자 하는 토픽의 파티션 개수보다 같거나 작아야 하는데 만약 컨슈머가 더 많다면, 남는 컨슈머는 파티션을 할당받지 못하고 유휴상태로 남게 되는 것을 주의해야 합니다. 이렇게 되면 스레드만 차지하고 실질적인 처리를 못하므로, 불필요한 스레드로 남게됩니다. 하지만, 컨슈머 그룹으로 운영하면 다른 컨슈머 그룹의 영향을 받지 않고 처리할 수 있습니다. 컨슈머 그룹이 아니라면, 동기적으로 처리를 해야할 수 있는데, 이렇게 되면 어떤 한 적재 파이프라인에 장애가 발생했을때 다른 곳에 적재가 불가능해집니다. 따라서 컨슈머 그룹으로 나눌 수 있는 것은 최대한 나누는 것이 좋습니다.</p><p>그런데 만약, 컨슈머 그룹 컨슈머에 장애가 발생한다면 어떻게 될까요? 그렇게 되면, 장애가 발생한 컨슈머에 할당된 파티션은 장애가 없는 컨슈머에 소유권이 넘어가게 됩니다. 이렇게 되면 리밸런싱이 일어나게 됩니다. 리밸런싱이 일어나는 경우는 크게 2가지 경우입니다.</p><ul><li>컨슈머가 추가 될 때</li><li>컨슈머가 제외될 때</li></ul><p>다만, 리밸런싱은 자주 일어나서는 안됩니다. 왜냐하면 리밸런싱이 발생할 때 재할당하는 과정에서 해당 컨슈머 그룹의 컨슈머들이 토픽의 데이터를 읽을 수 없기 때문입니다. group coordinator는 리밸런싱을 발동시키는데 컨슈머 그룹의 컨슈머가 추가되고 삭제되는 것을 감지합니다. 카프카 브로커 중 한 대가 그룹 조정자 역할을 수행하며 <code>__consumer_offsets</code> 에 기록된 커밋을 통해서 레코드의 어디 까지 읽었는지 파악해서 데이터 처리 중복을 피합니다. </p><p><br></br></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ul><li><a href="http://www.yes24.com/Product/Goods/99122569">http://www.yes24.com/Product/Goods/99122569</a></li><li><a href="https://velog.io/@king3456/Apache-Kafka-%EA%B8%B0%EB%B3%B8%EA%B0%9C%EB%85%90">https://velog.io/@king3456/Apache-Kafka-%EA%B8%B0%EB%B3%B8%EA%B0%9C%EB%85%90</a></li><li><a href="https://jhleed.tistory.com/180">https://jhleed.tistory.com/180</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/12/19/kafka-101/#disqus_thread</comments>
    </item>
    
    <item>
      <title>linux top 10 명령어들을 정리해봤다</title>
      <link>http://tkdguq05.github.io/2021/12/04/linux-top10/</link>
      <guid>http://tkdguq05.github.io/2021/12/04/linux-top10/</guid>
      <pubDate>Sat, 04 Dec 2021 12:58:45 GMT</pubDate>
      <description>
      
        &lt;p&gt;자주 사용하지만 잘 까먹게 되는 명령어들을 정리해봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>자주 사용하지만 잘 까먹게 되는 명령어들을 정리해봤습니다.</p><a id="more"></a><h1 id="Top-10-Linux-명령어-정리"><a href="#Top-10-Linux-명령어-정리" class="headerlink" title="Top 10. Linux 명령어 정리"></a>Top 10. Linux 명령어 정리</h1><p>유튜브 영상을 보다가 자주 사용하는 명령어에 대한 것을 보게 되었고, 그 동안 사용했던 명려어와 관련 내용들을 정리해보면 좋겠다는 생각에 글을 작성하게 되었습니다.</p><p>원본 유튜브 영상 주소는 <a href="https://www.youtube.com/watch?v=u9RukvKZJZM">https://www.youtube.com/watch?v=u9RukvKZJZM</a> 여기 입니다!</p><p><br></br></p><h2 id="1-Server-접속하기"><a href="#1-Server-접속하기" class="headerlink" title="1. Server 접속하기"></a>1. Server 접속하기</h2><p>보통 쉘을 통해 서버로 접속할 때, ssh을 많이 사용합니다. 단순히 ssh를 사용해 서버에 접속하는 것, 그 이상으로 ssh가 무엇인지, 또 어떤 도구로까지 활용할 수 있는지 안다면, ssh를 더 잘 사용한다고 할 수 있을 것입니다.</p><p>ssh는 <code>secured shell</code> 로 암호화 된 채널이기 때문에 보안 상 사용하는 것이 좋습니다. ssh는 사용할 때 붙일 수 있는 다양한 옵션들이 있습니다. AWS에서 특정 서버로 접속할때, pem파일을 사용해서 접속을 한다면 항상 붙이는 <code>-i</code> 옵션이 있습니다. 이것은 무엇에 관한 옵션일까요?</p><ul><li><p>-i 옵션?</p><p><code>-i</code> 는 id_rsa 옵션으로 어떤 암호키가 있다면 이것을 넘겨서 사용하겠다는 의미입니다. 그래서 pem파일 등 암호화된 파일을 사용해서 인증을 하다면 이 옵션을 통해서 인증에 사용할 파일을 지정하는 것입니다.</p></li><li><p>rsa키 인증 방식의 원리?</p><p>잠깐 rsa키 인증 방식에 대해서 짚고 넘어가겠습니다. rsa가 가끔 등장하곤 하는데, 어떤 것인지 제대로 확인하고 넘어간 적이 없었거든요. </p><p>rsa키 인증 방식은 private, public key 두 키를 이용해서 인증 작업을 합니다. user/.ssh 에 있는 authorized_keys를 보면 pem키(private key)의 pair인 public key가 서버에 저장되어 있는 것을 볼 수 있습니다. 그래서 private키를 갖고와서 서버에 저장되어 있는 public key와 대조해, 일치한다면 인증이 된 것입니다. 즉, 서버에 접속이 가능해지는 것입니다.</p><p>이를 <code>key - pair</code> 방식 이라고 합니다. 일반적으로 패스워드 인증방식을 많이 사용하고 친숙하겠지만, 패스워드 방식보다 좀 더 안정한 방식이라고 볼 수 있습니다.</p></li><li><p>(MAC) ssh-add</p><p>이것은 키를 미리 입력해주는 명렁어입니다. pem 키를 포워딩 하는 것으로서, -i옵션을 굳이 붙여 접속하지 않아도 됩니다. 귀찮은 작업이 좀 덜어지겠죠?, -A를 사용한다면, 다음 접속지에서도 같은 키로 그대로 접속할 수 있어, 편하게 사용할 수 있습니다.</p></li><li><p>포트 넣기</p><p>만약 포트를 지정하고 싶다면 <code>-p</code> 를 사용 하면 됩니다. 일반적으로 ssh는 기본 포트를 사용합니다. 22이번이죠. 하지만 보안 이론에는  well known port를 사용하지 말아야 한다는 것이 있습니다. 그래서 /etc/ssh/sshd_config를 통해서 ssh 포트 정보를 변경해서 사용하는 경우가 있습니다. 나만 알고있는 접속가능한 포트를 만들어서 접근을 하겠다는 것입니다. 이렇게 설정을 한다면, 다른 사람은 일반적인 접근 경로가 22번이니까, 자연스럽게 들어오게 되었을 때, 모두 차단할 수 있게 되겠습니다.</p><p><br></br></p></li></ul><h2 id="2-IP를-확인하는-리눅스-명령어-자신의-public-ip-확인"><a href="#2-IP를-확인하는-리눅스-명령어-자신의-public-ip-확인" class="headerlink" title="2. IP를 확인하는 리눅스 명령어, 자신의 public ip 확인?"></a>2. IP를 확인하는 리눅스 명령어, 자신의 public ip 확인?</h2><p>현재 사용하고 있는 서버의 IP를 알고 싶은 경우가 은근히 자주 있습니다. Private IP 확인하고 싶다면 ifconfig를 사용해서 알ㅇ아낼 수 있습니다. 그런데 이 서버의 Public IP를 알고싶다면 어떻게 해야할까요?</p><p>이 서버의 Public IP를 알고싶다면 <code>curl ifconfig.co</code> 를 사용하면 됩니다. </p><p><br></br></p><h2 id="3-웹-사이트가-잘-동작하는지-체크할때"><a href="#3-웹-사이트가-잘-동작하는지-체크할때" class="headerlink" title="3. 웹 사이트가 잘 동작하는지 체크할때?"></a>3. 웹 사이트가 잘 동작하는지 체크할때?</h2><p>띄워놓은 웹 사이트나 서비스를 체크하는 데 어떤 명령어를 사용해야할지 모르는 경우가 있습니다. 주로 curl을 사용하게 되는데, curl에는 다양한 옵션들이 있습니다.</p><ul><li>curl의 다양한 옵션<ul><li>X : 사용할 방식 메소드 선택하기</li><li>d : 함께 전달할 파라미터값 설정하기</li><li>G : 전송할 사이트 url 및 ip 주소</li><li>H : 헤더 정보를 전달하기</li><li>I : 사이트의 Header 정보만 가져오기</li><li>i : 사이트의 Header와 바디 정보를 함께 가져오기</li><li>u : 사용자 정보</li></ul></li></ul><p>이것들을 잘 활용하면 API서버에 GET, POST방식을 선택하고 헤더를 더해서 데이터를 넣어 던지는 것을 만들어 볼 수 있습니다.</p><p><code>curl -X POST -H &quot;Content-Type: application/json&quot;  -d &#39;{&quot;data&quot;:1}&#39; {IP Address}</code> 이런식으로 명령어를 보내서 API가 잘 동작하는지 점검할 수 있습니다.</p><p><br></br></p><h2 id="4-도메인의-IP를-조회하는-방법"><a href="#4-도메인의-IP를-조회하는-방법" class="headerlink" title="4. 도메인의 IP를 조회하는 방법"></a>4. 도메인의 IP를 조회하는 방법</h2><p>도메인의 IP를 확인하고 싶다면 네임서버에 등록된 정보를 조회할 수 있어야 합니다. <code>nslookup</code> 명령어를 잘 사용해야 관련 정보를 갖고 올 수 있을 것입니다. 기본적으로 <code>nslookup {IP Address}</code> 를 하면 도메인의 IP 정보를 알 수 있습니다.</p><p>이를 넘어서 도메인 서버의 동작 원리를 한 번 알아보고 넘어가 보겠습니다.</p><ol><li><p>웹 브라우저에 <a href="http://www.naver.com을">www.naver.com을</a> 입력하면 먼저 Local DNS에게 “<a href="http://www.naver.com&quot;이라는">www.naver.com&quot;이라는</a> hostname”에 대한 IP 주소를 질의하여 Local DNS에 없으면 다른 DNS name 서버 정보를 받음(Root DNS 정보 전달 받음)</p></li><li><p>Root DNS 서버에 “<a href="http://www.naver.com&quot;">www.naver.com&quot;</a> 질의</p></li><li><p>Root DNS 서버로 부터 “com 도메인”을 관리하는 TLD (Top-Level Domain) 이름 서버 정보 전달 받음</p></li><li><p>TLD에 “<a href="http://www.naver.com&quot;">www.naver.com&quot;</a> 질의</p></li><li><p>TLD에서 “name.com” 관리하는 DNS 정보 전달</p></li><li><p>“naver.com” 도메인을 관리하는 DNS 서버에 “<a href="http://www.naver.com&quot;">www.naver.com&quot;</a> 호스트네임에 대한 IP 주소 질의</p></li><li><p>Local DNS 서버에게 “응! <a href="http://www.naver.com에">www.naver.com에</a> 대한 IP 주소는 222.122.195.6 응답 </p></li><li><p>Local DNS는 <a href="http://www.naver.com에">www.naver.com에</a> 대한 IP 주소를 캐싱을 하고 IP 주소 정보 전달 </p></li></ol><p><br></br></p><h2 id="5-웹-서버-혹은-DB같은-서버들을-확인하는-방법"><a href="#5-웹-서버-혹은-DB같은-서버들을-확인하는-방법" class="headerlink" title="5. 웹 서버 혹은 DB같은 서버들을 확인하는 방법?"></a>5. 웹 서버 혹은 DB같은 서버들을 확인하는 방법?</h2><p>특정 프로세스들의 동작여부를 확인하고 싶을 때, 일반적으로 알고 있는 ping을 사용하게 됩니다. 그런데, ping으로는 이게 실제로 살아있는지, 동작하는지를 알기는 어렵습니다. 물론 그에 앞서서 웹서버가 뜨면 어떤 tcp로 올라오는지, 어떤 포트로 뜨는지는 미리 알고 있어야겠습니다. </p><p>이런 경우에는 telnet을 주로 사용하게 됩니다.</p><ul><li><p>ping? telnet?</p><p>ping은 단순 체크라 mysql, redis등이 살아있는지 알기가 어렵습니다. 반면에 telnet에 ip와 port를 넣어서 체크를 한다면 적합한 포트넘버로 신호를 보냈을 때, 관련된 프로세스가 살아있는지 여부를 알 수 있습니다.</p><p><code>telnet [ip] [port]</code> 로 명령어를 보낼 수 있습니다. telnet은 udp 지원을 안하는 것을 유념하시기 바랍니다.</p></li></ul><p><br></br></p><h2 id="6-서버가-잘-떠있는지-DB커넥션-확인"><a href="#6-서버가-잘-떠있는지-DB커넥션-확인" class="headerlink" title="6. 서버가 잘 떠있는지, DB커넥션 확인"></a>6. 서버가 잘 떠있는지, DB커넥션 확인</h2><p>포트가 잘 열려있는지 확인을 해야할 일이 자주 발생합니다. 저의 경우에는 airflow를 예로 들자면, 8793포트나 3306, 6379 등의 포트를 확인하는 경우가 많았습니다. 특히 로그의 전송을 담당하는 8793포트를 자주 확인했었습니다. airflow를 사용하면서 UI에서 로그를 확인하려고 하는데, 연결이 되지 않아 로그를 볼 수 없는 경우가 종종 있었기 때문입니다. 그렇다면, 포트가 열려있는지를 봤었어야 했는데, 어떻게 확인하면 좋을까요?</p><ul><li><p>netstat 명령어</p><p>nltp 명령어를 사용해서 원하는 포트의 상태를 확인할 수 있습니다. 주로 <code>netstat -nltp|grep {port}</code>를 통해서 포트가 열려있는지를 봤습니다.</p></li><li><p>netstat 옵션</p><p>-a : 현재다른PC와 연결(Established)되어 있거나 대기(Listening)중인 모든 포트 번호를 확인 </p><p>-r : 라우팅 테이블 확인 및 커넥션되어 있는 포트번호를 확인 </p><p>-n : 현재 다른PC와 연결되어 있는 포트번호를 확인</p><p>-e : 랜카드에서 송수한 패킷의 용량 및 종류를 확인 </p><p>-s : IP, ICMP, UDP프로토콜별의 상태 확인</p><p>-t : tcp protocol </p><p>-u : udp protocol </p><p>-p : 프로토콜 사용 Process ID 노출</p><p>-c : 1초 단위로 보여줌</p><p><br></br></p></li></ul><h2 id="7-process-확인"><a href="#7-process-확인" class="headerlink" title="7. process 확인?"></a>7. process 확인?</h2><p>아까 airflow의 예를 다시 한번 사용해보자면, 현재 서버에 워커가 죽었는지 살았는지를 보고 싶을 때가 있습니다. 이런 경우에는 워커 프로세스의 상태를 확인하면 될 것입니다. 이럴 때 사용하는 명령어가 <code>ps</code> 입니다.</p><ul><li><p>ps 명령어</p><p>ps명령어는 기본적으로 해당 사용자 소유의 프로세스만 보여줍니다. ps를 치게되면 나오는 정보는 다음과 같습니다.</p><ul><li>PID: 프로세스 아이디</li><li>TTY: 프로세스와 연결된 터미널 포트</li><li>TIME: 프로세스에서 사용한 CPU시간</li><li>CMD: 명령어</li></ul></li><li><p><code>ps -ef |grep {PROCESS}</code></p><p>아마 가장 많이 사용하는 명령어이지 않을까 싶습니다. e는 모든 프로세스 상태 출력한다는 의미이고, f는 full format으로 보여달라는 옵션입니다.</p></li><li><p><code>ps -aux|grep</code></p><p>이 역시 많이 사용하는 명령어 입니다.</p><ul><li><p>a: 모든 사용자의 프로세스를 출력하는 옵션</p></li><li><p>u: 특정 사용자의 프로세스 정보 확인, 지정하지 않으면 현재 사용자 기준</p></li><li><p>x: 자신의 터미널이 없는 프로세스도 출력하는 옵션</p><p><img src="/images/linux_top10/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-11-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.56.20.png" alt=""></p></li></ul></li></ul><p><br></br></p><h2 id="8-Linux-CPU-MEM-DISK-시스템-정보-확인"><a href="#8-Linux-CPU-MEM-DISK-시스템-정보-확인" class="headerlink" title="8. Linux CPU, MEM, DISK 시스템 정보 확인"></a>8. Linux CPU, MEM, DISK 시스템 정보 확인</h2><p>서버의 리소스를 보고 모니터링을 하거나, 상태가 어떤지 보고 싶을 때는 보통 <code>top</code>을 많이 사용합니다. 이외에도 htop, sar도 자주 사용합니다.</p><ul><li><p>top</p><p><img src="/images/linux_top10/Untitled.png" alt=""></p><ul><li><p>Load Average 는 <strong>CPU Load의 이동 평균를 표시</strong>합니다.</p></li><li><p>Tasks는 현재 프로세스들의 상태를 보여줍니다.</p><ul><li>실행(Runnable) - CPU에 의해서 명령어가 실행중인 Process</li><li>준비(Ready) - CPU의 명령어 실행을 기다리는 Process</li><li>대기(Waiting) - I/O operation이 끝나기를 기다리는 Process</li><li>종료(Terminated) - Ctrl + Z 등의 signal로 종료된 Process</li><li>Zombie - Process는 root Process로 부터 뿌리내린 자식 Process의 형식으로 트리구조를 형성합니다. 이 때 부모가 먼저 종료된 다면 root process로 부터 닿을 수 없는 Process가 생깁니다. 이를 zombie process라고 부릅니다.</li></ul></li><li><p>CPU 사용량은 <strong>CPU가 어떻게 사용되고 있는지 그 사용율을 보여주는 영역</strong>입니다.</p><ul><li>us : 프로세스의 유저 영역에서의 CPU 사용률</li><li>sy : 프로세스의 커널 영역에서의 CPU 사용률</li><li>ni : 프로세스의 우선순위(priority) 설정에 사용하는 CPU 사용률</li><li>id : 사용하고 있지 않는 비율</li><li>wa : IO가 완료될때까지 기다리고 있는 CPU 비율</li><li>hi : 하드웨어 인터럽트에 사용되는 CPU 사용률</li><li>si : 소프트웨어 인터럽트에 사용되는 CPU 사용률</li><li>st : CPU를 VM에서 사용하여 대기하는 CPU 비율</li></ul></li><li><p>메모리 사용량, MEM</p><ul><li>total : 총 메모리 양</li><li>free : 사용가능한 메모리 양</li><li>used : 사용중인 메모리 양</li></ul><p><br></br></p></li></ul></li><li><p>htop (top보다 더 자세한 정보, 편하게 확인 가능)</p><p><img src="/images/linux_top10/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-11-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_2.59.58.png" alt=""></p><p><br></br></p></li><li><p>sar (System Activity Reporter)</p><ul><li>user : 사용자 레벨(application level) 에서 실행중일때의 CPU 사용률 (%)</li><li>nice : 사용자 레벨(appliaction level) 에서 nice 가중치를 준 CPU 사용률(%)</li><li>system : 시스템레벨(kernel) 에서 실행중일때의 CPU 사용률(%)</li><li>iowait : system이 I/O요청을 처리하지 못한 상태에서의 CPU의 idle 상태인 시간의 비율(%)</li><li>steal : virtual processer에 의한 작업이 진행되는 동안 virtual CPU에 의해 뜻하지 않는 대기시간이 생기는 시간의 비율(%)</li><li>idle : CPU가 쉬고있는 시간의 %</li><li>Average : 마지막 라인에 출력되며, 각 값의 평균치 이다.</li></ul><p><br></br></p></li><li><p>free</p><ul><li><strong>[total] :</strong> 설치된 총 메모리 크기 / 설정된 스왑 총 크기</li><li><strong>[used] :</strong> total에서 free, buff/cache를 뺀 사용중인 메모리. / 사용중인 스왑 크기</li><li><strong>[free] :</strong> total에서 used와 buff/cahce를 뺀 실제 사용 가능한 여유 있는 메모리량 / 사용되지 않은 스왑 크기</li><li><strong>[shared] :</strong> tmpfs(메모리 파일 시스템), ramfs 등으로 사용되는 메모리. 여러 프로세스에서 사용할 수 있는 공유 메모리</li><li><strong>[buffers] :</strong> 커널 버퍼로 사용중인 메모리</li><li><strong>[cache] :</strong> 페이지 캐시와 slab으로 사용중인 메모리</li><li><strong>[buff/cache] :</strong> 버퍼와 캐시를 더한 사용중인 메모리</li><li><strong>[available] :</strong> swapping 없이 새로운 프로세스에서 할당 가능한 메모리의 예상 크기. (예전의 -/+ buffers/cache이 사라지고 새로 생긴 컬럼)</li></ul></li></ul><p><code>/proc/meminfo</code>를 사용하는 경우도 있는데 free 자체가 /proc/meminfo를 갖고 오는 것이기 때문에 따로 작성하지 않았습니다.</p><p><br></br></p><ul><li><p>df는 사용 가능한 디스크 공간의 양을 알 수 있습니다.</p><table><thead><tr><th>옵션</th><th>Long옵션</th><th>설명</th></tr></thead><tbody><tr><td>-a</td><td>–all</td><td>0 블록의 파일 시스템을 포함하여, 모든 파일시스템을 출력</td></tr><tr><td>-B</td><td>–block-size=SIZE</td><td>지정한 크기(SIZE)를 블록 단위로 정하여 용량을 표시(예:–block-size=1m)</td></tr><tr><td></td><td>–total</td><td>총계를 출력</td></tr><tr><td>-h</td><td>–human-readable</td><td>사람이 읽을 수 있는 형태의 크기로 출력(예:1K, 512M, 4G)</td></tr><tr><td>-H</td><td>–si</td><td>1KB는 1,024Byte지만 사용자가 보기 편하도록 1,000단위로 용량을 표시</td></tr><tr><td>-i</td><td>–inodes</td><td>inode의 남은 공간, 사용 공간, 사용 퍼센트를 출력</td></tr><tr><td>-k</td><td></td><td>–block-size=1K와 같은 의미</td></tr><tr><td>-l</td><td>–local</td><td>출력하는 목록을 로컬 파일 시스템으로 제한</td></tr><tr><td></td><td>–no-sync</td><td>사용 정보를 얻기 전에 싱크를 하지 않음(디폴트 값)</td></tr><tr><td>-P</td><td>–portability</td><td>POSIX에서 사용되는 형태로 출력</td></tr><tr><td></td><td>–sync</td><td>사용 정보를 얻기 전에 싱크</td></tr><tr><td>-t</td><td>–type=TYPE</td><td>보여주는 목록을 파일 시스템의 타입(TYPE)으로 제한</td></tr><tr><td>-T</td><td>–print-type</td><td>파일 시스템의 형태를 추가하여 각각의 파티션 정보를 출력</td></tr><tr><td>-x</td><td>–exclude-type=TYPE</td><td>지정한 형태(TYPE)를 제외하고 나머지 모든 파일 시스템 정보를 출력</td></tr></tbody></table></li></ul><p><br></br></p><h2 id="9-service-관리"><a href="#9-service-관리" class="headerlink" title="9. service 관리"></a>9. service 관리</h2><p>서버가 시작할 때 뜨는 프로세스를 관리하고 싶을 때가 있습니다. 그럴때는 daemon이나 service를 살펴보면 됩니다.</p><p>리눅스 서비스 관리하는 명령어는 service와 systemctl이 있습니다.</p><p>서비스를 등록할 때는 이런 파일을 하나 작성합니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Service Desceiption</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">ExecStart&#x3D;&#x2F;path&#x2F;to&#x2F;shellscript.sh</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Unit</span><br><span class="line">  - Description: 해당 유닛에 대한 설명</span><br><span class="line">  - Requires : 상위 의존성 구성, 포함하는 유닛이 정상적이어야 실행</span><br><span class="line">  - RequiresOverridable : 상위 의존성 구성이며 이것이 실패하더라도 무시하고 유닛을 시작</span><br><span class="line">  ...</span><br><span class="line"></span><br><span class="line">Service</span><br><span class="line">  - Type: [simple | forking | oneshot | notify | dbus] 유닛의 타입</span><br><span class="line">  - Environment: 해당 유닛에서 사용할 환경 변수 선언</span><br><span class="line">  - ExecStart: 시작 명령을 정의</span><br><span class="line">  - ExecStop: 중지 명령을 정의</span><br><span class="line"></span><br><span class="line">Install</span><br><span class="line">  - WantedBy: 유닛을 등록하기 위한 종속성 검사. 유닛을 등록할 때 등록에 필요한 유닛을 지정</span><br></pre></td></tr></table></figure><ul><li>Before=, After=<ul><li>유닛 시작의 전후 관계를 설정합니다. 해당 설정은 “Requires=” 설정과는 독립적입니다. “Before=” 에 나열된 유닛이 시작되기 전에 실행하고 “After=” 은 해당 유닛이 시작된 이후 나열된 유닛이 실행합니다.</li><li>이 설정은 시스템이 종료(shutdown) 될때는 역으로 작동하게 됩니다.</li></ul></li><li>[Install] 섹션<ul><li>systemctl enable [service name] 으로 VM 구동시 서비스가 자동으로 구동되도록 할 때 이용하는 섹션입니다.</li><li>[Install]은 해당 서비스를 등록할 때 사용되는 설정이다. 같이 등록/해지할 서비스나 필요한 서비스등을 지정해줄 수 있습니다.</li></ul></li><li>[WantedBy]<ul><li>서비스가 어떤 전제조건 하에서 실행되는 지를 결정하는 프로퍼티입니다.</li><li>WantedBy는 해당 install한 서비스가 설치되어 있어야 본 서비스를 설치한다는 뜻입니다.</li></ul></li></ul><p>이렇게 만들어진 파일을 /usr/lib/systemd/system에 넣어주고(리눅수 버전이나 종류별로 상이할 수 있음)  서비스를 start해주고 실행이 되고 enable까지 해주면 서버가 재시작 되더라도, 서비스가 실행됩니다. </p><ul><li>service start : <code>systemctl start servicename</code> </li><li>service enable :  <code>systemctl start servicename</code> </li></ul><p><br></br></p><h2 id="10-리눅스-파일-권한-체계"><a href="#10-리눅스-파일-권한-체계" class="headerlink" title="10. 리눅스 파일 권한 체계"></a>10. 리눅스 파일 권한 체계</h2><p>어떤 파일을 실행하고 사용할 때, 권한이 없다는 에러가 나와서 실행이 되지 않는 경우가 아주 많은 것을 아실 겁니다. 이런 경우에는 파일의 권한을 풀어주면 해결이 되곤 합니다. 이럴 때 주로 사용하는 명령어는 <code>chmod</code>그리고 <code>chown</code>입니다.</p><ul><li><p>chmod</p><ul><li><p>파일이나 폴더의 권한(읽기, 쓰기, 실행)를 변경</p></li><li><p>권한들을 <strong>mode</strong>라고 부른다</p><p><img src="/images/linux_top10/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-11-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.11.55.png" alt=""></p><p><img src="/images/linux_top10/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-11-18_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_3.12.17.png" alt=""></p></li></ul></li><li><p>chown</p><ul><li>파일, 또는 폴더의 소유권을 변경하는 명령어</li></ul></li><li><p>ls -l을 했을때 나오는 정보에 대한 이해</p><ul><li>long listing format</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">user@user_group:~/exp$</span> <span class="string">ls</span> <span class="bullet">-l</span></span><br><span class="line"><span class="string">drwxrwxr-x</span>  <span class="number">2</span> <span class="string">user</span> <span class="string">user_group</span> <span class="number">4096</span> <span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">dir1</span></span><br><span class="line"><span class="string">drwxrwxr-x</span>  <span class="number">2</span> <span class="string">user</span> <span class="string">user_group</span> <span class="number">4096</span> <span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">dir2</span></span><br><span class="line"><span class="bullet">-</span><span class="string">rw-rw-r--</span>  <span class="number">1</span> <span class="string">user</span> <span class="string">user_group</span>    <span class="number">0</span> <span class="string">Sep</span> <span class="number">17</span> <span class="number">06</span><span class="string">:39</span> <span class="string">file1</span></span><br></pre></td></tr></table></figure><ul><li>첫번째 블락이 <strong>권한 문자열(Permission string)</strong></li><li>두번째 블락이 해당 디렉토리 내부의 파일과 디렉토리 갯수,</li><li>세번째 블락이 소유주,</li><li>네번째가 소유주가 속한 그룹,</li><li>다섯번째가 크기,</li><li>6,7,8번째는 마지막으로 파일/디렉토리에 접근한 시각,</li><li>아홉번째가 파일/디렉토리의 이름을 의미한다.</li></ul></li></ul><p>root와 chmod 777 는 full 권한을 부여하는 것이므로 사용을 지양합니다.</p><p><br></br></p><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://www.whatap.io/ko/blog/37/">https://www.whatap.io/ko/blog/37/</a></li><li><a href="https://sabarada.tistory.com/146">https://sabarada.tistory.com/146</a></li><li><a href="https://happist.com/557995/%EC%84%9C%EB%B2%84-%EB%AA%A8%EB%8B%88%ED%84%B0%EB%A7%81-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8-htop-%EC%82%AC%EC%9A%A9-%EB%B0%A9%EB%B2%95-ubuntu">https://happist.com/557995/서버-모니터링-프로그램-htop-사용-방법-ubuntu</a></li><li><a href="https://csjung.tistory.com/50">https://csjung.tistory.com/50</a></li><li><a href="https://jhnyang.tistory.com/268">https://jhnyang.tistory.com/268</a></li><li><a href="https://muckycode.blogspot.com/2016/09/linux-chown-vs-chmod.html">https://muckycode.blogspot.com/2016/09/linux-chown-vs-chmod.html</a></li><li><a href="https://ijbgo.tistory.com/27">https://ijbgo.tistory.com/27</a></li><li><a href="https://blog.voidmainvoid.net/201">https://blog.voidmainvoid.net/201</a></li><li><a href="https://springboot.cloud/16">https://springboot.cloud/16</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/12/04/linux-top10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>이직의 과정, 그리고 얻은 것</title>
      <link>http://tkdguq05.github.io/2021/11/18/Moving/</link>
      <guid>http://tkdguq05.github.io/2021/11/18/Moving/</guid>
      <pubDate>Thu, 18 Nov 2021 06:54:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;인수인계서를 작성하면서 쓰는 이직 후기.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>인수인계서를 작성하면서 쓰는 이직 후기.</p><a id="more"></a><h2 id="이직에-성공하다"><a href="#이직에-성공하다" class="headerlink" title="이직에 성공하다"></a>이직에 성공하다</h2><p>2년 4개월. 첫 입사한 회사에서 벌써 2년이 넘는 시간이 흘렀습니다. 참 우여곡절이 많았고 재밌었던 일도 많았던 것 같습니다. 이직을 해야겠다고 마음을 먹은건 대략 5-6개월 정도 된 것 같은데,  결실을 맺을 수 있어서 다행이라고 생각합니다. 준비도 준비지만 이직과정에서 느낀 건 실력도 중요하지만, 운이 있어야한다, 즉 운칠기삼이라는 것이었습니다. 아마 이직 과정에서 운이 없었다면 이직에 도전하는 데에도 시간이 더 걸렸을지 모르고, 면접에서 탈락했었을 지도 모르겠습니다. 앞으로도 항상 겸손하게, 묵묵히 할 일을 해 나가야 할 것 같다는 생각입니다.</p><p><br></br></p><h2 id="이직-과정"><a href="#이직-과정" class="headerlink" title="이직 과정"></a>이직 과정</h2><p>저의 첫 면접은 7월이었습니다. 7월초에 링크드인을 통해서 N사의 데이터 엔지니어 직무를 제안을 받게되었습니다. 당시에는 이직에 대한 생각정도만 갖고 있어서, 이력서나 포트폴리오가 제대로 준비가 되어있지 않은 상태였습니다. 그래서 3일 안에 이력서를 채우고 포트폴리오를 구성해서 조금 급하다는 느낌이 들긴했지만, 이력서와 포트폴리오를 전달했습니다. 서류는 통과가 되었고 1차 전화면접을 준비했습니다. 아무래도 대기업이다 보니 기본적인 내용이 나올 것 같아, API나 네트워크, CS관련 내용들, 파이썬 기본 개념들을 다시 봤습니다. 몇몇 질문에는 대답하지 못했지만, 전화면접까지 통과가 되었습니다. 사실상 거의 마지막 관문이라고 볼 수 있는 기술 면접 일정이 잡혔고, 제가 했었던 프로젝트와 그 프로젝트에 들어가 있는 기술 내용들을 더 깊이 공부했습니다. 사실 경력직 이직 면접이 처음이라 어디까지 질문이 나올지 몰라 시간을 많이 썼습니다. 유일한 취미였던 운동마저 포기하고 건강이 점점 안좋아지는 걸 느끼면서 나름 준비를 마쳤습니다. 기술면접은 자그마치 3시간이었습니다. 면접관 세 분이서 한시간씩 사용하셨고, 약 30-40분간의 라이브 코딩시간이 있었습니다. 라이브 코딩은 한 번만 할 줄 알았는데, 체력과 정신력의 한계를 느낄 수 있었던 시간이었습니다. 라이브 코딩 외에 질문들은 대개 프로젝트에 대한 내용은 일부분이었고, 하둡의 어떤 개념을 아는지? 스파크의 어떤 걸 아는지? 문제는 어떻게 해결했는지 등의 문제가 대부분이었던 것 같습니다. 물론 면접 결과는 불합격이었습니다. 그술적으로 부족한 부분이 있긴 했지만, 이력서나 포트폴리오에 없는 기술들에 대한 질문이 있어서 난처했었습니다. ‘’관련 기술이 없는데 왜 기숢면접까지 부른거지…?’’란 생각이 들었지만, 어찌 됐든 저의 부족한 점을 느낄 수 있었고, 대기업의 프로세스를 경험해 볼 수 있어서 좋은 시간이었습니다.</p><p><img src="https://www.desktopbackground.org/download/1600x900/2015/03/14/917047_depression-sad-mood-sorrow-dark-people-wallpapers_1920x1080_h.jpg" alt=""></p><p>많은 생각이 드는 면접이었지만, 느낀 것은 항상 준비가 되어있어야 한다는 것이었습니다. 급하게 2주일정도 준비를 하느라 신체적으로나 정신적으로나 면접 전에 너무 지쳐있었고, 기술 내용들을 다시 숙지하기에도 벅찼던 것 같습니다. 그래서 본격적으로 준비를 해야겠다고 마음먹고 준비해야할 리스트들을 작성하기 시작했습니다.</p><p><br></br></p><h2 id="이직을-위해-필요한-것"><a href="#이직을-위해-필요한-것" class="headerlink" title="이직을 위해 필요한 것"></a>이직을 위해 필요한 것</h2><p>저의 부족한 점을 곰곰히 생각해보며 리스트화 해봤습니다.</p><ul><li>코딩 테스트</li><li>알고리즘</li><li>이전 프로젝트 기술 숙지<ul><li>Docker</li><li>Spark 등등</li></ul></li></ul><p>라이브코딩로 한 번 시달리고 나니, 코딩 테스트로 시간 뺏기기 싫다는 생각이 들었습니다. 그래서 막 문제를 풀까? 생각을 해봤는데, 한 번도 제대로 코딩 테스트 교육을 받은 적이나, 자료구조를 정리해 본적이 없다는 것을 알게 되었습니다. 그래서 맘을 독하게 먹고 약 40?만원 정도 하는 강의를 꼼꼼히 살펴보고 담당 매니저와 통화까지 한 다음 바로 결제를 해버렸습니다. 그리고 약 5-6주동안 열심히 이론을 공부하고 실습 문제를 풀었습니다. 문제를 풀면서 어떤 문제에 대해서 어떻게 접근을 할 것인지를 많이 고민했던 것 같습니다. 특히 스택, 큐에 대한 문제가 잘 풀리지 않았는데, 이럴 때는 관련 문제들을 리스트업하고 내가 세운 문제 해결 전략이 잘 들어 맞는지, 안 맞았다면 어떤 게 문제가 되었던 것인지를 정리했습니다. </p><p>그리고 이전 프로젝트 기술 숙지를 하는 부분에서, 사실 한 게 너무 많아서 건드린 기술 스택이 너무 많았습니다. 그래서 대답을 하지 못할 것 같거나, 너무 얕게 본 기술, 그리고 서비스화하려다 실패한 프로젝트는 생략하기로 했습니다. 그리고 기술을 다시 보면서 이해가 되지 않고, 왜 이렇게 했는지 이해가 안가는 부분은 고민해보고 다시 고쳐보고 고도화 하면서 체화되도록 했습니다. 이렇게 준비를 하는 도중에 또 하나의 메세지가 오게됩니다.</p><p><br></br></p><h2 id="준비한-걸-쏟아내기"><a href="#준비한-걸-쏟아내기" class="headerlink" title="준비한 걸 쏟아내기"></a>준비한 걸 쏟아내기</h2><p>참 신기했던 건 거의 한 날에 이직관련 메세지가 동시에 전달되었다는 것이였습니다. 물론 그 중에 제일 처음으로 받은 메세지는 바로 이직하기로 결정한 회사였습니다. 이 회사에 대한 정체는 글 마지막에 알려드리도록 하겠습니다(<del>스크롤 드르륵</del>). 맨 처음 이직 준비를 하던 때와는 다르게 당황스럽거나 어떻게 하지? 이런 생각은 들지 않았습니다. <code>&#39;해보지 뭐&#39;</code>, <code>&#39;문제없어&#39;</code> 이런 생각이 먼저 들었던 것 같습니다. </p><p>한 번 준비를 해놓고 나니 그 이후의 준비는 훨씬 수월했습니다. 정리를 해놨기 때문에 다시 보기에도 편했고 머리에 이미 정리되어 있어 다른 곳에 시간을 더 쓸 수 있었습니다. 그래서 기술이나 저에 대한 이야기를 다 정리해 놓은 다음, 회사에 대해서 더 자세히 공부하고 면접에 들어갔습니다. 회사에 대해서, 사용하는 기술에 대해 공부를 하고 가면 할 얘기가 훨씬 많아지는 것 같습니다. 약 4-5개의 회사에 대해서 면접을 봤는데, 저의 프로젝트에 대한 얘기에서는 한 회사를 제외하고는 크게 막히는 부분은 없었고 수월하게 진행이 되었습니다. 그리고 면접 마무리 때 궁금한 게 있으면 질문을 해달라라고 하셨을 때, 파이프라인에 대해 궁금한 부분들을 물어봤고, 고도화 방향이 어떻게 되는지, 오픈소스나 클라우드 제품을 어떤 것을 쓰는지를 질문했습니다. 기대와 다른 부분도 있었고 기대대로 좋은 스택을 지닌 회사들도 있었습니다. 이 질문에서는 잘 보이기 위해 질문을 한다기 보다는, ‘’나도 면접에 참여하는 사람이다’’, ‘’나도 회사에 대한 정보를 얻고싶다’라는 생각으로 접근했습니다. </p><p>면접때 잘 풀리지 않았던 한 회사는 중고거래 회사였습니다. 세션을 보다가 너무 멋진 데이터 엔지니어가 있어서 같이 일해보고 싶은마음에 지원을 했습니다. 면접에서 막혔던 부분은, 저도 회사에서 이해가 안가는 부분이었습니다. 로그 처리에 관한 것이었는데 한 번 면접관이 이해가 가지 않기 시작하면, 다른 부분에서도 의문을 품기 시작하는 것을 알게 되었습니다. 물론 저도 회사에서 면접관으로 참여했었던 적이 있는지라, 논리적으로 설득하려고 했지만, 저조차 이해가 안가는 부분에 대해서는 답변을 하지 못했었습니다. 아쉽지만 불합격!</p><p><br></br></p><h2 id="결정"><a href="#결정" class="headerlink" title="결정"></a>결정</h2><p> 면접이 모두 마무리되었고, 최종적으로 두 회사에 합격하게 되었습니다. 사실 두 회사의 성격이 완전 달라서 한 회사를 선택하면 길이 달라지는 느낌이 있었습니다.</p><p><br></br></p><ul><li>A 회사<ul><li>새로오신 팀장님 (같이 일한 동료를 통해 들었을 때 훌륭한 분!)</li><li>DE팀이지만 DS일도 같이 할 수 있음</li><li>잡플래닛 등 평점 안 좋음</li><li>복지는 조금 부족…</li></ul></li><li>B 회사<ul><li>코로나 이후에 폭발적인 성장이 기대됨</li><li>엔지니어링 + 개발만 할 것 같음</li><li>평점 좋음</li><li>복지 괜찮음</li></ul></li></ul><p><br></br></p><p>두 회사를 간단하게 정리하면 위와 같았습니다. 제시된 연봉은 비슷했기 때문에 돈, 사람, 일 중 사람과 일을 갖고 선택을 해야겠다고 생각했습니다. 정보가 너무 없어서 주위에 도움을 많이 요청했습니다. 회사의 분위기가 어떤지 기술이 어떤지… 등등 그러던 중에 글또 모임의 장이신 성윤님께 상담을 받아보고 싶었습니다. 예전 성윤님이 회사를 선택하실때 글에서 관련내용을 본 적이 있어서 도움을 받을 수 있을 것 같았습니다. </p><p>바로 연락을 드렸고, 흔쾌히 상담을 해주셨습니다. 바쁘신 와중에 많은 얘기들을 해주셨는데, 주된 얘기는 다음과 같았습니다.</p><ul><li>잡플래닛 별점? 별로 신경쓰지 마라<ul><li>볼거면 개발직군의 별점이 어떤지 따로 볼 것</li><li>새로 만들어진 팀일 경우, 별점은 더욱 더 의미가 없다</li></ul></li><li>연봉보다는 앞으로의 성장성에 무게를 두는 게 좋을 것 같다<ul><li>연봉은 나중에 일을 잘하게 되면 알아서 따라올 거다</li><li>어디에서 능력을 키울 수 있는지가 중요</li></ul></li><li>돈, 사람, 일 중에 돈이 가장 숫자가 명확하다 하지만<ul><li>사람과 일은 부딪혀보면 나오는 정보가 많다</li><li>회사에 인터뷰를 요청해서 사람과 일에 대해 후회없이 물어봐라</li><li>힘든 점에 대해서 솔직하게 말을 해주는 지 알아봐라, 힘든 포인트를 예상해서 어떻게 답변하는지 체크해라<ul><li>팀원들이 힘들어하면 어떤 조치를 하시는지, 실례가 있었는지 등</li></ul></li><li>사람보다는 회사의 문화가 더 중요하다. 사람은 나가면 그만이지만, 문화가 잘 만들어져 있으면, 좋은 사람은 다시 나온다.</li></ul></li><li>내가 정말 좋아하는 일, 어떤 회사가 좋은 회사인지를 더 생각해보자<ul><li>어떤 회사가 좋은 회사인지를 바탕으로 회사를 비교해보자</li><li>이직할 회사에 찜찜한 점이 있다면 질문으로 바꿔서 질문해보자</li></ul></li></ul><p><br></br></p><p>상담을 하고 나니 너무 소극적으로 임하고 있다는 생각이 들었습니다. 하루의 반 이상을 일하면서 지내는데, 그냥 하나 선택해서 될대로 되라! 하는 것 같았습니다. 바로 두 회사에 티 타임을 요청했고, 질문리스트를 정리했습니다. 그리고 공교롭게도 두 회사는 같은 날에 티 타임 미팅이 잡히게 되었습니다.</p><p><br></br></p><h3 id="첫-번째-티타임"><a href="#첫-번째-티타임" class="headerlink" title="첫 번째 티타임"></a>첫 번째 티타임</h3><p><img src="https://lordofghent.com/wp-content/uploads/2017/05/121014063411-coffee-meeting-table-story-top.jpg" alt=""></p><p>첫 번째 회사(위에서 정리한 B 회사)는 인사팀 분의 안내가 좋았습니다. 친절히 맞아주셨고, 엔지니어 팀분들과 얘기를 나누기 전에 회사를 한 번 구경시켜주시는 점이 인상깊었습니다. 회사 구경을 쭉 한 다음 미팅룸에서 얘기를 나누기 시작했습니다. </p><p><br></br></p><p>질문은 다음과 같았습니다.</p><ul><li><p>데이터 플랫폼팀은 어떤 일을 하는 조직인지?</p></li><li><p>나는 어떤 일을 하게 되는지</p></li><li><ul><li>주로 어떤 사람과 일을 하게 되는지</li><li>내가 주로 의사소통하는 대상은 누가되는지</li></ul></li><li><p>팀 조직 구성은?</p></li><li><p>업무하시면서 재밌으셨던 일은</p></li><li><p>리드분은 어떤 비전을 갖고 계시는지, 어떤 일을 하고 계시고 계속 하고 싶으신지</p></li><li><p>어떤 문화를 만드실 계획인지</p></li><li><ul><li>스터디나 성장하는 문화 등</li></ul></li><li><p>팀에서 힘들어하는 팀원을 어떻게 캐치하시는지?</p></li><li><ul><li>그런 팀원이 있다면 어떻게 케어하시는지</li></ul></li><li><p>저에게 부족한 부분이 있다면?</p></li></ul><p><br></br></p><p>질문에 대해서 하나 하나 얘기해주셨고, 실제로 동작하는 파이프라인까지 보여주셔서 감명이 깊었습니다. 그리고 제가 맡게되는 일에 대해서도 자세히 알려주셔서 제가 어떤 일을 하게되는지 구체적으로 알 수 있었습니다. 그리고 대화를 해보면서 팀에서 어떻게 일이 수행되는지, 사람들의 스타일은 어떤지 간접적으로 알 수 있었습니다. 확실한 건 역시 개발-엔지니어링 쪽으로 특화되서 성장하게 될 것이라는 것이었습니다.</p><p>다음 회사로 가기 전 잠시 카페에 와서 내용들을 정리하고 이 회사에서 일하면 어떨지를 곰곰히 생각해 봤습니다. 나에게 부족한 부분이 무엇인지, 그 부분을 이 회사에서 채워나가고 잘 성장해나갈 수 있을지, 그리고 현재 회사의 팀원들이나 분위기들을 한 번 비교해 봤습니다.</p><p><br></br></p><h3 id="두-번째-티타임"><a href="#두-번째-티타임" class="headerlink" title="두 번째 티타임"></a>두 번째 티타임</h3><p>생각을 정리하고, 다음회사에 대한 질문을 다시 한번 살펴본 뒤 두 번째 회사(A 회사)로 향했습니다. 라운지에는 팀장님이 나와계셨고 인사를 하며 자리에 앉았습니다. 굉장히 캐주얼하게 얘기를 했습니다. 질문 리스트에 있는 얘기부터, 일과 아카데미, 취미 등등 자연스럽게 얘기가 흘러갔습니다. 일도 일이었지만, 이전 회사에 비해 자연스럽게 대화가 흐른 다는 것을 느낄 수 있었습니다. 동시에 제가 가장 중요하게 생각하는 것이 무엇인지 깨닫게 되었습니다. <code>커뮤니케이션</code> . 개발이나 데이터 엔지니어링이나 코드를 많이 사용한다고 하더라도, 어쨌든 사람이 하는 일이기 때문에 의사소통이 필수적입니다. 여러 경험으로 봤을 때, 커뮤니케이션이 어려웠을 때 일이 진도도 안나가고 스트레스를 많이 받았던 기억이 많았었습니다. 팀장님과 무려 한 시간 정도 이야기(거의 수다에 가까운)를 하고 HR분과 다시 이야기를 한 후에 집으로 돌아왔습니다. </p><p>두 회사를 갔다오고 나서 느낀 건 내가 진짜 원하는 회사가 무엇인지 알게 되었다는 것입니다.  <code>&quot;대화가 되는 팀에서 일하고 싶다&quot;</code> 이것이 이직을 하면서 얻은 것 중 하나였습니다. 생각이 정리된 후, 저는 두 번째 티타임을 가진 회사로 간다는 의사를 해당 회사에 전달했습니다.</p><p><br></br></p><h2 id="결정-후"><a href="#결정-후" class="headerlink" title="결정 후"></a>결정 후</h2><p>연봉협상이 마무리되고 현 회사, 전 직장이 될 회사에 이직 사실을 밝혔습니다. 팀장님은 많이 놀라신 것 같았지만, 축하한다고 해주셨습니다. 이후에 이사님과 면담이 잡히고 그래도 1주일 정도는 생각해 줄 수 있냐고 물어보셨습니다. 바로 아니라고는 말을 못할 것 같아서 그 다음주에 제 생각에는 변합이 없다고 알려드렸습니다. 제가 들어오고 데이터 팀이 꾸려지기 시작해서, 많이 아쉬워 하셨습니다. 그리고 이제 인수인계서를 작성하고, 대표님 면담을 기다리고 있습니다. </p><p>팀을 꾸리면서 시작했던 슬랙과 봇들, 그리고 노션에 정리한 글들을 다시한번 보면서 아쉬운 감정이 들었습니다. 동시에 내가 한게 별로 없다고 생각했는데, 생각보다 많은 걸 해왔구나 되뇌었습니다. </p><p><br></br></p><img src="https://blog.kakaocdn.net/dn/bjjrHJ/btqBPXNC9tF/Sibyn4CJ4fWFTMaqvUYt01/img.jpg" style="zoom:24%;" /><p>오늘 부로 퇴사일이 정리되고 출근날이 정해졌습니다. 12월 중순부터 이제 새 회사인 컬리의 데이터 엔지니어로 출근을 하게 되었습니다. 아쉬움이 많이 남은 만큼, 후회하지 않도록 더 열심히 일하고 성장해야겠다고 다짐했습니다. 성공적인 스타트를 한 회사가 어떻게 성장할 수 있었는지, 폭발적인 성장에 기술적으로 어떻게 대응했고, 대응하고 있는지를 경험해보고 싶네요. 앞으로 남은 시간동안 잘 마무리하고 재충전을 하면서, 어떻게 팀에 기여를 할 수 있고 어떤 방향으로 성장해야할지를 좀 더 정리해봐야겠습니다. 도움과 응원을 보내 주신 글또 여러분들과 성윤님, 그리고 데이터 사이언스 팀원들에게 고마운 마음을 글을 통해 전합니다. 감사합니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/11/18/Moving/#disqus_thread</comments>
    </item>
    
    <item>
      <title>EKS-workshop</title>
      <link>http://tkdguq05.github.io/2021/11/03/EKS-workshop/</link>
      <guid>http://tkdguq05.github.io/2021/11/03/EKS-workshop/</guid>
      <pubDate>Wed, 03 Nov 2021 05:40:24 GMT</pubDate>
      <description>
      
        &lt;p&gt;EKS를 다뤄보고 GitOps까지 실습해봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>EKS를 다뤄보고 GitOps까지 실습해봤습니다.</p><a id="more"></a><ul><li><p>실습 링크 : <a href="https://aws-eks-web-application.workshop.aws/ko/10-intro.html">https://aws-eks-web-application.workshop.aws/ko/10-intro.html</a></p><p>실습 영상 : <a href="https://www.youtube.com/watch?v=kb6s0Tmp2CA&ab_channel=AWS%ED%95%9C%EA%B5%AD%EC%82%AC%EC%9A%A9%EC%9E%90%EB%AA%A8%EC%9E%84-AWSKRUG">https://www.youtube.com/watch?v=kb6s0Tmp2CA&amp;ab_channel=AWS한국사용자모임-AWSKRUG</a></p><blockquote><p>AWS 워크샵에서는 다음 내용을 다룹니다.</p></blockquote><ul><li>AWS Cloud9을 통한 실습 환경 구축</li><li>도커를 이용하여 컨테이너 이미지 생성</li><li>컨테이너 이미지를 ECR에 업로드</li><li>Amazon EKS 클러스터 구축 및 서비스 배포</li><li>Container Insights 사용해보기</li><li>파드 및 클러스터 오토 스케일링</li><li>AWS Fargate로 서비스 올리기</li></ul><p><br></br></p><h2 id="Kubernetes-k8s"><a href="#Kubernetes-k8s" class="headerlink" title="Kubernetes(k8s)"></a>Kubernetes(k8s)</h2><ul><li>쿠버네티스는 컨테이너화된 워크로드와 서비스를 관리하기 위한 이식성이 있고, 확장가능한 오픈소스 플랫폼입니다. 쿠버네티스는 선언적 구성과 자동화를 모두 용이하게 해주는 컨테이너 오케스트레이션 툴입니다.</li></ul><blockquote><p>🌐 쿠버네티스에 대해 더 자세히 알고 싶다면 <a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">여기</a> 를 클릭하세요.</p></blockquote><p><img src="/images/eks-workshop/kuber_architecture.png" alt="쿠버네티스_아키텍쳐"></p><ul><li>쿠버네티스를 배포하면 <strong>클러스터</strong>를 얻습니다. 그리고 이 클러스터는 <strong>노드들의 집합</strong>입니다. 노드들은 크게 두 가지 유형으로 나눠지는데, 각각이 <strong>컨트롤 플레인</strong>과 <strong>데이터 플레인</strong>입니다.<ul><li>컨트롤 플레인(Control Plane)은 워커 노드와 클러스터 내 파드를 관리하고 제어합니다.</li><li>데이터 플레인(Data Plane)은 <strong>워커 노드들</strong>로 구성되어 있으며 컨테이너화된 애플리케이션의 구성 요소인 <strong>파드</strong>를 호스트합니다.</li></ul></li></ul><p><br></br></p><h2 id="Kubernetes-Objects"><a href="#Kubernetes-Objects" class="headerlink" title="Kubernetes Objects"></a>Kubernetes Objects</h2><ul><li>쿠버네티스의 오브젝트는 <strong>바라는 상태(desired state)를 담은 레코드</strong>입니다. 오브젝트를 생성하면 쿠버네티스의 컨트롤 플레인에서 오브젝트의 <strong>현재 상태(current state)</strong> 와 바라는 상태를 일치시키기 위해 끊임없이 관리합니다.</li><li>쿠버네티스의 오브젝트에는 파드(pod), 서비스(service), 디플로이먼트(Deployment) 등이 있습니다.</li></ul></li></ul><p>  <br></br></p><h2 id="EKS"><a href="#EKS" class="headerlink" title="EKS"></a>EKS</h2><ul><li><p><a href="https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html">Amazon EKS</a>는 Kubernetes를 쉽게 실행할 수 있는 관리형 서비스입니다. Amazon EKS를 사용하시면 AWS 환경에서 Kubernetes 컨트롤 플레인 또는 노드를 직접 설치, 운영 및 유지할 필요가 없습니다.</p></li><li><p>Amazon EKS는 여러 가용 영역에서 Kubernetes 컨트롤 플레인 인스턴스를 실행하여 고가용성을 보장합니다. 또한, 비정상 컨트롤 플레인 인스턴스를 자동으로 감지하고 교체하며 자동화된 버전 업그레이드 및 패치를 제공합니다.</p></li><li><p>Amazon EKS는 다양한 AWS 서비스들과 연동하여 애플리케이션에 대한 확장성 및 보안을 제공하는 서비스를 제공합니다.</p><ul><li>컨테이너 이미지 저장소인 <strong>Amazon ECR(Elastic Container Registry)</strong></li><li>로드 분산을 위한 <strong>AWS ELB(Elastic Load Balancing)</strong></li><li>인증을 위한 <strong>AWS IAM</strong></li><li>격리된 <strong>Amazon VPC</strong></li></ul></li><li><p>Amazon EKS는 오픈 소스 Kubernetes 소프트웨어의 최신 버전을 실행하므로 <strong>Kubernetes 커뮤니티에서 사용되는 플러그인과 툴을 모두 사용</strong>할 수 있습니다. 온프레미스 데이터 센터에서 실행 중인지 퍼블릭 클라우드에서 실행 중인지에 상관없이, Amazon EKS에서 실행 중인 애플리케이션은 표준 Kubernetes 환경에서 실행 중인 애플리케이션과 완벽하게 호환됩니다. 즉, 코드를 수정하지 않고 표준 Kubernetes 애플리케이션을 Amazon EKS로 손쉽게 마이그레이션할 수 있습니다.</p></li><li><p>이 워크샵을 위해서는 administrator IAM  권한이 필요합니다.</p><p><br></br></p><h2 id="Cloud9"><a href="#Cloud9" class="headerlink" title="Cloud9"></a>Cloud9</h2></li><li><p>AWS Cloud9은 브라우저만으로도 코드를 작성, 실행 및 디버깅할 수 있는 IDE입니다. 코드 편집기, 디버거 및 터미널이 포함되어 있으며 많이 사용되는 프로그래밍 언어를 위한 필수 도구가 사전에 패키징되어 제공되므로, 새로운 프로젝트를 시작하기 위해 파일을 설치하거나 개발 머신을 구성할 필요가 없다는 특징을 가지고 있습니다.</p><p><br></br></p><h3 id="kubectl"><a href="#kubectl" class="headerlink" title="kubectl"></a>kubectl</h3></li><li><p>쿠버네티스 클러스터에 명령을 내리는 CLI입니다.</p></li><li><p>쿠버네티스는 오브젝트 생성, 수정 혹은 삭제와 관련한 동작을 수행하기 위해 <strong>쿠버네티스 API</strong>를 사용합니다. 이때, kubectl CLI를 사용하면 해당 명령어가 쿠버네티스 API를 호출해 관련 동작을 수행합니다.</p><p><br></br></p><h3 id="eksctl"><a href="#eksctl" class="headerlink" title="eksctl"></a>eksctl</h3></li><li><p><a href="https://eksctl.io/">eksctl</a>이란 EKS 클러스터를 쉽게 생성 및 관리하는 CLI 툴입니다. Go 언어로 쓰여 있으며 CloudFormation 형태로 배포됩니다.</p><h3 id="eksctl로-클러스터-생성하기"><a href="#eksctl로-클러스터-생성하기" class="headerlink" title="eksctl로 클러스터 생성하기"></a>eksctl로 클러스터 생성하기</h3></li><li><p>eksctl을 사용하여 아무 설정 값을 주지 않고 이 명령어(<code>eksctl create cluster</code>)를 실행하면 default parameter로 클러스터가 배포됩니다.</p></li><li><p>원하는 설정이 있다면 yaml파일을 만들어서 배포하면 됩니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;</span> <span class="string">EOF</span> <span class="string">&gt; eks-demo-cluster.yaml</span></span><br><span class="line"><span class="string">---</span></span><br><span class="line"><span class="string"></span><span class="attr">apiVersion:</span> <span class="string">eksctl.io/v1alpha5</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfig</span></span><br><span class="line"></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">eks-demo</span> <span class="comment"># 생성할 EKS 클러스터명</span></span><br><span class="line"><span class="attr">  region:</span> <span class="string">$&#123;AWS_REGION&#125;</span> <span class="comment"># 클러스터를 생성할 리젼</span></span><br><span class="line"><span class="attr">  version:</span> <span class="string">"1.21"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">vpc:</span></span><br><span class="line"><span class="attr">  cidr:</span> <span class="string">"192.168.0.0/16"</span> <span class="comment"># 클러스터에서 사용할 VPC의 CIDR</span></span><br><span class="line"></span><br><span class="line"><span class="attr">managedNodeGroups:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">node-group</span> <span class="comment"># 클러스터의 노드 그룹명</span></span><br><span class="line"><span class="attr">    instanceType:</span> <span class="string">m5.large</span> <span class="comment"># 클러스터 워커 노드의 인스턴스 타입</span></span><br><span class="line"><span class="attr">    desiredCapacity:</span> <span class="number">3</span> <span class="comment"># 클러스터 워커 노드의 갯수</span></span><br><span class="line"><span class="attr">    volumeSize:</span> <span class="number">10</span>  <span class="comment"># 클러스터 워커 노드의 EBS 용량 (단위: GiB)</span></span><br><span class="line"><span class="attr">    ssh:</span></span><br><span class="line"><span class="attr">      enableSsm:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">    iam:</span></span><br><span class="line"><span class="attr">      withAddonPolicies:</span></span><br><span class="line"><span class="attr">        imageBuilder:</span> <span class="literal">true</span> <span class="comment"># AWS ECR에 대한 권한 추가</span></span><br><span class="line">        <span class="comment"># albIngress: true  # albIngress에 대한 권한 추가</span></span><br><span class="line"><span class="attr">        cloudWatch:</span> <span class="literal">true</span> <span class="comment"># cloudWatch에 대한 권한 추가</span></span><br><span class="line"><span class="attr">        autoScaler:</span> <span class="literal">true</span> <span class="comment"># auto scaling에 대한 권한 추가</span></span><br><span class="line"></span><br><span class="line"><span class="attr">cloudWatch:</span></span><br><span class="line"><span class="attr">  clusterLogging:</span></span><br><span class="line"><span class="attr">    enableTypes:</span> <span class="string">["*"]</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><code>eksctl create cluster -f eks-demo-cluster.yaml</code> 로 배포</p></li><li><p>노드 확인</p><ul><li><p><code>kubectl get nodes</code></p><p><img src="/images/eks-workshop/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-10-29_%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE_1.17.55.png" alt="EKS클러스터 구성"></p><ul><li>기본적인 EKS 클러스터 구성은 끝났고, 본격적으로 서비스를 배포하고, 유저가 들어오는 통로인 인그레스를 설정해 봅시다.</li></ul></li></ul></li><li><p>이 상태에서는 클러스터의 노드를 UI에서 확인할 수 없습니다. Console Crediential을 더해줘야 합니다.</p></li><li><p>Cloud9의 IAM credential을 통해, 클러스터를 생성하였기 때문에 <a href="https://console.aws.amazon.com/eks">Amazon EKS 콘솔창</a>에서 해당 클러스터 정보를 확인하기 위해서는 실제 콘솔에 접근할 IAM entity(사용자 또는 역할)의 AWS Console credential을 클러스터에 추가하는 작업이 필요합니다.</p><ul><li><code>rolearn=$(aws cloud9 describe-environment-memberships --environment-id=$C9_PID | jq -r &#39;.memberships[].userArn&#39;)</code></li><li><code>assumedrolename=$(echo ${rolearn} | awk -F/ &#39;{print $(NF-1)}&#39;)rolearn=$(aws iam get-role --role-name ${assumedrolename} --query Role.Arn --output text)</code> rolearn에 자신의 arn을 넣어주도록 합시다.</li><li>echo ${rolearn}</li><li><code>eksctl create iamidentitymapping --cluster eks-demo --arn ${rolearn} --group system:masters --username admin</code> identity 맵핑</li><li>kubectl describe configmap -n kube-system aws-auth 적용 확인</li></ul><p><br></br></p><h2 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h2></li><li><p><strong>AWS Load Balancer Controller</strong>는 <strong>구 AWS ALB Ingress Controller</strong>에서 리브랜드된 개념입니다.</p></li><li><p><strong>인그레스(Ingress)</strong> 는 주로 <strong>클러스터 외부에서 쿠버네티스 내부</strong>로 접근할 때, 요청들을 어떻게 처리할지 정의해놓은 규칙이자 리소스 오브젝트입니다. 한마디로 외부의 요청이 내부로 접근하기 위한 관문의 역할을 하는 것이죠. 외부 요청에 대한 로드 밸런싱, TLS/SSL 인증서 처리, HTTP 경로에 대한 라우팅 등을 설정할 수 있습니다. 인그레스는 L7 영역의 요청을 처리합니다.</p></li><li><p>쿠버네티스에서 서비스 타입 중, NodePort 혹은 LoadBalancer로도 외부로 노출할 수 있지만 인그레스 없이 서비스를 사용할 경우, 모든 서비스에게 라우팅 규칙 및 TLS/SSL 등의 상세한 옵션들을 적용해야 되죠. 그래서 인그레스가 필요합니다.</p><p><img src="/images/eks-workshop/Untitled.png" alt="Ingress"></p></li><li><p>인그레스는 외부 요청 처리에 대한 규칙들을 설정해놓은 것을 의미하며, 이런 설정이 동작하기 위해서 필요한 것이 <strong>인그레스 컨트롤러</strong>입니다. kube-controller-manager의 일부로 실행되는 다른 컨트롤러와 달리 인그레스 컨트롤러는 클러스터와 함께 생성되진 않습니다. 따라서 직접 구현해야 합니다.</p><h3 id="AWS-Load-Balancer-만들기"><a href="#AWS-Load-Balancer-만들기" class="headerlink" title="AWS Load Balancer 만들기"></a>AWS Load Balancer 만들기</h3></li><li><p><a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html">Amazon EKS의 Application Load Balancing</a>이란 클러스터에 인그레스 자원이 생성될 때에 ALB(Application Load Balancer) 및 필요한 자원이 생성되도록 트리거하는 컨트롤러입니다. 인그레스 자원들은 ALB를 구성하여 HTTP 또는 HTTPS 트래픽을 클러스터 내 파드로 라우팅합니다.</p><ul><li>쿠버네티스의 <code>Ingress</code>의 경우, <code>Application Load Balancers</code>으로 프로비저닝됩니다.</li><li>쿠버네티스의 <code>Service</code>의 경우, <code>Network Load Balancers</code>으로 프로비저닝됩니다.</li></ul></li><li><p>AWS Load Balancer 컨트롤러에서 지원하는 <strong>트래픽 모드</strong>는 아래의 두 가지입니다.</p><ul><li>Instance(default): 클러스터 내 노드를 ALB의 대상으로 등록합니다. ALB에 도달하는 트래픽은 NodePort로 라우팅된 다음 파드로 프록시됩니다.</li><li>IP: 파드를 ALB 대상으로 등록합니다. ALB에 도달하는 트래픽은 파드로 <strong>직접</strong> 라우팅됩니다. 해당 트래픽 모드를 사용하기 위해선 <strong>ingress.yaml 파일에 주석을 사용하여 명시적으로 지정해야</strong> 합니다.</li></ul><p>![Instance Mode, IP Mode](/images/eks-workshop/Untitled 1.png)</p><ul><li>각종 배포들에 사용할 manifests 디렉토리를 만들어놓고 관리하는 것을 추천합니다.</li></ul></li><li><p>AWS Load Balancer 컨트롤러를 배포하기 전, 우리는 몇 가지 작업을 수행해야 합니다. controller가 워커 노드 위에서 동작되기 때문에 IAM permissions를 통해, AWS ALB/NLB 리소스에 접근할 수 있도록 만들어야 합니다. IAM permissions는 ServiceAccount를 위한 IAM roles를 설치하거나 워커 노드의 IAM roles에 직접적으로 붙일 수 있습니다.</p></li><li><p>먼저, 클러스터에 대한 <strong>IAM OIDC(OpenID Connect) identity Provider</strong>를 생성합니다. Pod와 같은 클러스터 내 쿠버네티스가 생성한 항목이 API Server 또는 외부 서비스에 인증하는데 사용되는 <strong><a href="https://kubernetes.io/ko/docs/reference/access-authn-authz/service-accounts-admin/">service account</a></strong>에 IAM role을 사용하기 위해, 생성한 클러스터(현재 실습에서의 <em>eks-demo</em>)에 <strong>IAM OIDC provider</strong>가 존재해야 합니다.</p><ul><li><p>eksctl utils associate-iam-oidc-provider <br>–region ${AWS_REGION} <br>–cluster eks-demo <br>–approve</p></li><li><p>OIDC provider URL</p><ul><li>aws eks describe-cluster –name eks-demo –query “cluster.identity.oidc.issuer” –output text</li><li>output: <a href="https://oidc.eks.ap-northeast-2.amazonaws.com/id/8A6E78112D7F1C4DC352B1B511DD13CF">https://oidc.eks.ap-northeast-2.amazonaws.com/id/8A6E78112D7F1C4DC352B1B511DD13CF</a><ul><li>뒤에 id 부분을 복사해서 다음과 같이 입력합니다.</li><li>aws iam list-open-id-connect-providers | grep 8A6E78112D7F1C4DC352B1B511DD13CF</li><li>결과 값이 출력되면 <strong>IAM OIDC identity provider</strong>가 클러스터에 생성이 된 것이고, 아무 값도 나타나지 않으면 생성 작업을 수행해야 합니다.</li></ul></li></ul></li><li><p>AWS Load Balancer Controller에 부여할 IAM Policy를 생성하는 작업을 수행합니다.</p><ul><li>aws iam create-policy <br>–policy-name AWSLoadBalancerControllerIAMPolicy <br>–policy-document <a href="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json">https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json</a></li></ul></li><li><p>AWS Load Balancer Controller를 위한 ServiceAccount를 생성합니다.</p><ul><li>eksctl create iamserviceaccount <br>–cluster eks-demo <br>–namespace kube-system <br>–name aws-load-balancer-controller <br>–attach-policy-arn arn:aws:iam::$ACCOUNT_ID:policy/AWSLoadBalancerControllerIAMPolicy <br>–override-existing-serviceaccounts <br>–approve</li></ul></li><li><p>AWS Load Balancer controller를 클러스터에 추가하는 작업을 수행합니다. 먼저, 인증서 구성을 웹훅에 삽입할 수 있도록 <strong><a href="https://github.com/jetstack/cert-manager">cert-manager</a></strong>를 설치합니다. <strong>Cert-manager</strong>는 쿠버네티스 클러스터 내에서 TLS인증서를 자동으로 프로비저닝 및 관리하는 오픈 소스입니다.</p><ul><li>kubectl apply –validate=false -f <a href="https://github.com/jetstack/cert-manager/releases/download/v1.4.1/cert-manager.yaml">https://github.com/jetstack/cert-manager/releases/download/v1.4.1/cert-manager.yaml</a></li></ul></li><li><p>load balancer controller yaml</p><ul><li>wget <a href="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/v2_2_1_full.yaml">https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.2.1/docs/install/v2_2_1_full.yaml</a></li></ul></li><li><p>yaml 파일에서 클러스터의 <code>cluster-name</code>을 편집합니다. 본 실습에서는 <strong>eks-demo</strong>로 설정합니다.</p><ul><li>spec:<br>containers:</li><li>args:</li><li>–cluster-name=eks-demo # 생성한 클러스터 이름을 입력</li><li>–ingress-class=alb<br>image: amazon/aws-alb-ingress-controller:v2.2.0</li></ul></li><li><p>ServiceAccount yaml spec 삭제</p><ul><li>apiVersion: v1<br>kind: ServiceAccount<br>metadata:<br>labels:<br><a href="http://app.kubernetes.io/component:">app.kubernetes.io/component:</a> controller<br><a href="http://app.kubernetes.io/name:">app.kubernetes.io/name:</a> aws-load-balancer-controller<br>name: aws-load-balancer-controller<br>namespace: kube-system</li></ul><hr></li></ul></li><li><p>배포</p><ul><li><code>kubectl apply -f v2_2_1_full.yaml</code></li><li>확인 kubectl get deployment -n kube-system aws-load-balancer-controller</li><li>서비스 어카운드 확인 kubectl get sa aws-load-balancer-controller -n kube-system -o yaml</li></ul></li><li><p>클러스터 내부에서 필요한 기능들을 위해 실행되는 파드들을 <strong>애드온(Addon)</strong> 이라고 합니다. 애드온에 사용되는 파드들은 디플로이먼트, 리플리케이션 컨트롤러 등에 의해 관리됩니다. 그리고 이 애드온이 사용하는 네임스페이스가 <strong>kube-system</strong>입니다. Yaml 파일에서 네임스페이스를 kube-system으로 명시했기에 위의 명령어로 파드 이름이 도출되면 정상적으로 배포된 것입니다. 또한, 아래의 명령어로 관련 로그를 확인할 수 있습니다.</p><ul><li>kubectl logs -n kube-system $(kubectl get po -n kube-system | egrep -o “aws-load-balancer[a-zA-Z0-9-]+”)</li><li>ALBPOD=$(kubectl get pod -n kube-system | egrep -o “aws-load-balancer[a-zA-Z0-9-]+”)</li><li>kubectl describe pod -n kube-system ${ALBPOD}</li></ul><p><br></br></p><h2 id="서비스-배포"><a href="#서비스-배포" class="headerlink" title="서비스 배포"></a>서비스 배포</h2></li><li><p>서비스 배포하는 순서는 다음과 같습니다.</p><p>![서비스 배포 순서](/images/eks-workshop/Untitled 2.png)</p></li><li><p>소스 코드 다운로드</p></li><li><p>Amazon ECR에 각 서비스에 대한 리포지토리 생성</p></li><li><p>Dockerfile을 포함한 소스 코드 위치에서 컨테이너 이미지 빌드 후, 리포지토리에 푸시</p></li><li><p>각 서비스에 대한 Deployment, Service, Ingress 매니페스트 파일 생성 및 배포</p></li><li><p>사용자가 실제 서비스를 접근하는 순서</p><p>![사용자가 접근하는 순서](/images/eks-workshop/Untitled 3.png)</p></li><li><p>우리의 서비스는 두 개의 백엔드가 존재합니다.</p><ul><li>Flask</li><li>Node.js</li></ul></li><li><p>각 백엔드의 API가 잘 동작하는지 확인하고 프론트엔드 배포로 넘어가봅시다.</p></li></ul><p>  <br></br></p><h3 id="Flask-배포"><a href="#Flask-배포" class="headerlink" title="Flask 배포"></a>Flask 배포</h3><ul><li><p>cd ~/environment/manifests/</p></li><li><p>deploy manifest</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">flask-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-flask-backend:latest</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">flask-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">"/contents/aws"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     app:</span> <span class="string">demo-flask-backend</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">      protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Ingress manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"backend-ingress"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">    - http:</span></span><br><span class="line"><span class="attr">        paths:</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/contents</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span></span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-flask-backend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>순서대로 배포</p></li><li><p><code>kubectl apply -f flask-deployment.yamlkubectl apply -f flask-service.yamlkubectl apply -f ingress.yaml</code></p></li><li><p>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/contents/aws 여기서 나온 주소로 확인</p><ul><li>ingress object가 배포되는 동안 기다리고 주소로 접속</li></ul><p>![flask 백엔드 배포](/images/eks-workshop/Untitled 4.png)</p><p><br></br></p><h3 id="node-js-배포"><a href="#node-js-배포" class="headerlink" title="node.js 배포"></a>node.js 배포</h3></li><li><p><strong>deploy manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">nodejs-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">public.ecr.aws/y7c9e1d2/joozero-repo:latest</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">3000</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">nodejs-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">"/services/all"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">     app:</span> <span class="string">demo-nodejs-backend</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">3000</span></span><br><span class="line"><span class="attr">      protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>ingress manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"backend-ingress"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">    - http:</span></span><br><span class="line"><span class="attr">        paths:</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/contents</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span></span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-flask-backend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">8080</span></span><br><span class="line"><span class="comment"># 추가된 부분 확인</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/services</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span></span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-nodejs-backend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">8080</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>순서대로 배포</p><ul><li><p><code>kubectl apply -f nodejs-deployment.yamlkubectl apply -f nodejs-service.yamlkubectl apply -f ingress.yaml</code></p></li><li><p>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/services/all</p><p>![node.js 백엔드 배포](/images/eks-workshop/Untitled 5.png)</p></li></ul><p><br></br></p><h3 id="프론트엔드-배포"><a href="#프론트엔드-배포" class="headerlink" title="프론트엔드 배포"></a>프론트엔드 배포</h3></li><li><p>프론트엔드 소스 다운</p><p>cd /home/ec2-user/environment<br>git clone <a href="https://github.com/joozero/amazon-eks-frontend.git">https://github.com/joozero/amazon-eks-frontend.git</a></p></li><li><p>ecr repository 생성</p><p>aws ecr create-repository <br>–repository-name demo-frontend <br>–image-scanning-configuration scanOnPush=true <br>–region ${AWS_REGION}</p></li><li><p>프론트엔드 소스에서 url을 백엔드의 ingress 주소로 변경\</p><ul><li>App.js<ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/contents/‘${search}’</li></ul></li><li>page/UpperPage.js<ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)/services/all</li></ul></li></ul></li><li><p>cd /home/ec2-user/environment/amazon-eks-frontend<br>npm install<br>npm run build</p></li><li><p>npm audit fix를 해도 안된다면</p><ul><li><code>export NODE_OPTIONS=--openssl-legacy-provider</code></li></ul></li><li><p>ECR demo-fronted 로 네이밍 한 뒤, 푸쉬</p><ul><li>docker build -t demo-frontend .</li><li>docker tag demo-frontend:latest$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</li><li>docker push $ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</li></ul></li><li><p><strong>fronted배포 manifest</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifests</span></span><br><span class="line"></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-deployment.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</span></span><br><span class="line"><span class="attr">          imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line"><span class="attr">          ports:</span></span><br><span class="line"><span class="attr">            - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p><strong>service</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-service.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">"/"</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">    - protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">      port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">      targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>ingress 배포</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ingress.yaml</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">"backend-ingress"</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  annotations:</span></span><br><span class="line">    <span class="string">kubernetes.io/ingress.class:</span> <span class="string">alb</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/scheme:</span> <span class="string">internet-facing</span></span><br><span class="line">    <span class="string">alb.ingress.kubernetes.io/target-type:</span> <span class="string">ip</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  rules:</span></span><br><span class="line"><span class="attr">    - http:</span></span><br><span class="line"><span class="attr">        paths:</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/contents</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span></span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-flask-backend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/services</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span>  </span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-nodejs-backend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">          - path:</span> <span class="string">/</span></span><br><span class="line"><span class="attr">            pathType:</span> <span class="string">Prefix</span></span><br><span class="line"><span class="attr">            backend:</span></span><br><span class="line"><span class="attr">              service:</span></span><br><span class="line"><span class="attr">                name:</span> <span class="string">"demo-frontend"</span></span><br><span class="line"><span class="attr">                port:</span></span><br><span class="line"><span class="attr">                  number:</span> <span class="number">80</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>배포</p><ul><li><code>kubectl apply -f frontend-deployment.yamlkubectl apply -f frontend-service.yamlkubectl apply -f ingress.yaml</code></li></ul><p>![프론트까지 배포 완료](/images/eks-workshop/Untitled 6.png)</p><img src="/images/eks-workshop/Untitled 7.png" alt="모든 배포가 완료된 모습" style="zoom:80%;" /><p><br></br></p><h2 id="Fargate"><a href="#Fargate" class="headerlink" title="Fargate"></a>Fargate</h2></li></ul><p>  <strong>AWS Fargate</strong>는 컨테이너에 적합한 서버리스 컴퓨팅 엔진으로 Amazon Elastic Container Service(ECS) 및 Amazon Elastic Kubernetes Service(EKS)에서 모두 작동합니다. Fargate는 애플리케이션을 빌드하는 데 보다 쉽게 초점을 맞출 수 있도록 해줍니다. Fargate에서는 서버를 프로비저닝하고 관리할 필요가 없어 애플리케이션별로 리소스를 지정하고 관련 비용을 지불할 수 있으며, 계획적으로 애플리케이션을 격리함으로써 보안 성능을 향상시킬 수 있습니다.</p><ul><li><p>클러스터에 Fargate로 pod를 배포하기 위해서는 pod가 실행될 때 사용하는 하나 이상의 fargate profile을 정의해야 합니다. 즉, fargate profile이란 fargate로 pod를 생성하기 위한 조건을 명시해놓은 프로파일이라고 보시면 됩니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifestscat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">eks-demo-fargate-profile.yaml---apiVersion:</span> <span class="string">eksctl.io/v1alpha5kind:</span> <span class="attr">ClusterConfigmetadata:</span>  <span class="attr">name:</span> <span class="string">eks-demo</span>  <span class="attr">region:</span> <span class="string">$&#123;AWS_REGION&#125;fargateProfiles:</span>  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">frontend-fargate-profile</span>    <span class="attr">selectors:</span>      <span class="bullet">-</span> <span class="attr">namespace:</span> <span class="string">default</span>        <span class="attr">labels:</span>          <span class="attr">app:</span> <span class="string">frontend-fargateEOF</span></span><br></pre></td></tr></table></figure><ul><li>yaml 파일에서 <strong>selectors</strong>에 기재된 조건에 부합하는 pod의 경우, fargate로 배포됩니다.</li></ul></li><li><p>아래의 명령어를 통해, fargate profile을 프로비저닝합니다.</p><ul><li>eksctl create fargateprofile -f eks-demo-fargate-profile.yaml</li><li>정상 동작 확인<ul><li>eksctl get fargateprofile –cluster eks-demo -o json</li></ul></li></ul></li><li><p>배포한 3개의 pod 중, 프론트앤드 pod를 fargate로 프로비저닝하는 작업을 수행하겠습니다. 먼저, 기존의 pod를 삭제하는 작업을 수행합니다. 아래의 명령어를 yaml 파일이 위치한 폴더에서 작업합니다.</p><ul><li>kubectl delete -f frontend-deployment.yaml</li></ul></li><li><p>그리고 frontend-deployment.yaml 파일을 수정합니다. 이 yaml파일에서 미리 등록한 fargate profile을 넣어주겠습니다. label값을 frontend-fargate로 변경해야 합니다. 1번에서 key 값이 app이고 value 값이 frontend-fargate이며 namespace가 default일 때, pod를 fargate로 배포하겠다는 조건을 맞추기 위해 값을 변경하였습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">/home/ec2-user/environment/manifestscat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-deployment.yaml---apiVersion:</span> <span class="string">apps/v1kind:</span> <span class="attr">Deploymentmetadata:</span>  <span class="attr">name:</span> <span class="string">demo-frontend</span>  <span class="attr">namespace:</span> <span class="attr">defaultspec:</span>  <span class="attr">replicas:</span> <span class="number">3</span>  <span class="attr">selector:</span>    <span class="attr">matchLabels:</span>      <span class="attr">app:</span> <span class="string">frontend-fargate</span>  <span class="attr">template:</span>    <span class="attr">metadata:</span>      <span class="attr">labels:</span>        <span class="attr">app:</span> <span class="string">frontend-fargate</span>    <span class="attr">spec:</span>      <span class="attr">containers:</span>        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo-frontend</span>          <span class="attr">image:</span> <span class="string">$ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/demo-frontend:latest</span>          <span class="attr">imagePullPolicy:</span> <span class="string">Always</span>          <span class="attr">ports:</span>            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>frontend-service.yaml</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">frontend-service.yaml---apiVersion:</span> <span class="attr">v1kind:</span> <span class="attr">Servicemetadata:</span>  <span class="attr">name:</span> <span class="string">demo-frontend</span>  <span class="attr">annotations:</span>    <span class="string">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">"/"</span><span class="attr">spec:</span>  <span class="attr">selector:</span>    <span class="attr">app:</span> <span class="string">frontend-fargate</span>  <span class="attr">type:</span> <span class="string">NodePort</span>  <span class="attr">ports:</span>    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span>      <span class="attr">port:</span> <span class="number">80</span>      <span class="attr">targetPort:</span> <span class="number">80</span><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>매니페스트 배포</p><p><code>kubectl apply -f frontend-deployment.yamlkubectl apply -f frontend-service.yaml</code></p></li><li><p>배포 상태 확인</p><ul><li>kubectl get po -o wide</li></ul></li><li><p>이전과 같은 상태인지 웹 확인</p><ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)</li></ul><p><br></br></p><h2 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps?"></a>GitOps?</h2></li><li><p>쿠버네티스 환경에서 CI/CD를 위해서는 Git과 워크플로우 툴의 연결이 필요하고, 이를 통해서 자동화 하는 방안이 필요합니다.</p></li><li><p>일일이 수정하는 것은 너무나 번거롭습니다.</p></li><li><p>그래서 등장한 것이 GitOps입니다.</p></li><li><p>Github Action, Kustomize, Helm Chart, Argo CD를 활용해 GitOps를 체험해보겠습니다.</p><p><br></br></p><h2 id="GitOps-전에-알아야-할-개념들"><a href="#GitOps-전에-알아야-할-개념들" class="headerlink" title="GitOps 전에 알아야 할 개념들"></a>GitOps 전에 알아야 할 개념들</h2></li><li><p>앞서 소개한 Kustomize나 Argo CD등은 조금 생소해 보입니다.</p></li><li><p>이해를 위해 미리 한번 정리를 하고 넘어가보겠습니다.</p><h3 id="Kustomize"><a href="#Kustomize" class="headerlink" title="Kustomize"></a>Kustomize</h3></li><li><p>Kubernetes에서 app을 배포를 할때 manifest 파일을 작성한다</p></li><li><p>staging 환경에 배포하는 요구사항이 발생하고, 수정사항이 계속 발생한다면, 작업을 반복해야 하는 이슈가 생긴다</p></li><li><p>관리해야할 manifest파일이 3배가 되어버림</p></li><li><p>겹치는 내용이 대부분인데, 이것을 base라고 하고, 환경마다 차이가 나는 부분을 overlay로 관리해보자</p></li><li><p>kustomize는 이렇게, 공통 부분과 차이가 나는 부분을 분리하는 것에서 시작한다.</p></li><li><p>base와 overlay를 머지하면서 환경마다 다른 spec의 리소스를 생성하게 된다.</p></li><li><p>개발 환경마다 소스 구분하여 configmap, secret생성이 가능하다</p></li><li><p>resource이름에 prefix를 추가하는 기능도 제공한다</p></li><li><p>Kustomize를 적용해서 직접 수동으로 배포할 수도 있다</p><ul><li>GitOps로 ArgoCD나 Flux등을 사용해서 배포과정을 자동화 한다.</li><li>Flux 는 쿠버네티스 클러스터가 설정 소스(git..)와 동기화된 상태를 유지하고 새 코드가 추가되면 자동으로 업데이트 해주는 도구</li></ul><p><br></br></p><h3 id="Helm"><a href="#Helm" class="headerlink" title="Helm"></a>Helm</h3></li><li><p>쿠버네티스 패키지 관리를 도와줌</p><ul><li>npm, pip와 비슷한 역할</li></ul></li><li><p>Chart: 헬름 패키지</p><ul><li>kubernetes를 설명하는 파일들의 집합</li><li>kubernetes에서 app이 동작하기 위한 모든 리소스들이 포함되어 있음</li></ul></li><li><p>Repository</p><ul><li>차트 저장소, 차트르 모아두고 공유함</li></ul></li><li><p>Release</p><ul><li>kubernetes 클러스터에서 구동되는 차트 인스턴스</li><li>동일한 차트를 여러번 설치할 수 있고 이는 새 릴리즈로 관리됩니다.</li><li>릴리즈 될 때 패키지된 차트와 config가 결합되어 정상 실행 됩니다</li></ul></li><li><p>작업 순서</p><ul><li>Helm 차트를 원하는 레포에서 검색 후 설치 → 각 설치에 따른 새로운 릴리즈 생성</li></ul><p><br></br></p><h3 id="Argo-CD"><a href="#Argo-CD" class="headerlink" title="Argo CD"></a>Argo CD</h3></li><li><p>Continuous Delivery</p><ul><li>지속적 통합을 통해 테스트 되고 빌드된 코드를 지속적으로 전달하여 제품의 질적 향상을 향하는 것</li></ul></li><li><p>쿠버네티스 운영과 관련된 manifest 파일 관리하고 있는 원격 레포지토리를 조회</p></li><li><p>변경 내역이 감지되면 이를 반영하여 배포함(Auto Sync 옵션)</p></li><li><p>히스토리를 저장하고, 롤백이 가능함</p><p><img src="/images/eks-workshop/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA_2021-11-03_%E1%84%8B%E1%85%A9%E1%84%8C%E1%85%A5%E1%86%AB_11.46.28.png" alt="Dev, Ops Pipeline"></p></li><li><p>argo의 장점</p><ul><li>실행 단위가 컨테이너기 때문에 고립성이 높다<ul><li>개별 작업마다 실행환경이 다양한 경우 실행환경이 섞이지 않고 단독적인 환경을 제공할 수 있다.</li><li>하나의 역할을 담당하는 Job을 단일하게 개발할 수 있어 재사용성을 높일 수 있다.<ul><li>데이터 입출만 잘 맞춰 놓으면 단일 역할을 하는 Job을 여러개 만들고, 블록처럼 쌓을 수 있다.</li></ul></li></ul></li></ul></li><li><p>argo의 단점</p><ul><li>Pod를 생성하고 삭제하는 비용이 크다.<ul><li>간단한 작업이라면 프로세스 또는 스레드 레벨에서 처리하는 게 효율적일 때가 있다</li></ul></li><li>각 스텝마다 개별적인 컨테이너를 실행해서 Job간에 데이터를 빠르게 공유하는 것이 힘들다.<ul><li>Pod 내부 컨테이너 간에만 volume공유가 가능하다</li></ul></li></ul></li><li><p>데이터 파이프라인 및 기계학습 모델 훈련에 활용 가능</p><ul><li>데이터 타입에 따라 추출하는 소스에 따라 상이한 실행환경<ul><li>데이터 추출 - Apache sqoop(Java runtime, hadoop 라이브러리 필요)<ul><li>S3 - aws cli, boto3…</li></ul></li><li>ML<ul><li>python</li><li>R</li></ul></li></ul></li><li>고립성이 굉장히 좋기 때문에 ML워크플로우 툴로도 활용이 가능함</li></ul><p><br></br></p><h2 id="GitOps-실습"><a href="#GitOps-실습" class="headerlink" title="GitOps 실습"></a>GitOps 실습</h2></li><li><p>목표하는 CI/CD 파이프라인은 다음과 같습니다.</p><img src="/images/eks-workshop/Untitled 8.png" alt="CI/CD" style="zoom:50%;" /></li><li><p>개발에서 작업한 내용은 Github Action을 통해 ECR에 업로드 되고, 이것을 클러스터 Ops쪽에서 Pull을 받아 k8s에 argo CD를 사용해 배포하는 방식입니다.</p></li><li><p>필요한 레포는 두 가지 입니다. application용과 k8s 메니페스트 관리용 레포입니다.</p><ul><li><strong><em>front-app-repo</em></strong>: Frontend 소스가 위치한 레파지토리</li><li><strong><em>k8s-manifest-repo</em></strong>: K8S 관련 메니페스트가 위치한 레파지토리</li></ul></li><li><p>git remote를 위해서 cloud9에 있는 amazon-eks-frontend 디렉토리 git을 초기화 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>rm -rf .git</li><li>소스 파일들을 푸시합니다.<ul><li>cd ~/environment/amazon-eks-frontend<br>git init<br>git add .<br>git commit -m “first commit”<br>git branch -M main<br>git remote add origin <a href="https://github.com/jinseo-jang/front-app-repo.git">https://github.com/jinseo-jang/front-app-repo.git</a><br>git push -u origin main</li></ul></li></ul></li><li><p>CI/CD 파이프라인을 위해서는 권한이 필요합니다.</p><ul><li><p>front app을 빌드하고 docker image로 만들어지면, 이것을 ECR로 푸시해야 합니다.</p></li><li><p>이 과정은 github Action으로 이루어지기 때문에 최소한의 권한을 넣어줍니다.</p><ul><li><p>IAM  user 생성</p><ul><li>aws iam create-user –user-name github-action</li></ul></li><li><p>ECR policy 생성</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environmentcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">ecr-policy.json&#123;</span>    <span class="string">"Version"</span><span class="string">:</span> <span class="string">"2012-10-17"</span><span class="string">,</span>    <span class="string">"Statement"</span><span class="string">:</span> <span class="string">[</span>        <span class="string">&#123;</span>            <span class="string">"Sid"</span><span class="string">:</span> <span class="string">"AllowPush"</span><span class="string">,</span>            <span class="string">"Effect"</span><span class="string">:</span> <span class="string">"Allow"</span><span class="string">,</span>            <span class="string">"Action"</span><span class="string">:</span> <span class="string">[</span>                <span class="string">"ecr:GetDownloadUrlForLayer"</span><span class="string">,</span>                <span class="string">"ecr:BatchGetImage"</span><span class="string">,</span>                <span class="string">"ecr:BatchCheckLayerAvailability"</span><span class="string">,</span>                <span class="string">"ecr:PutImage"</span><span class="string">,</span>                <span class="string">"ecr:InitiateLayerUpload"</span><span class="string">,</span>                <span class="string">"ecr:UploadLayerPart"</span><span class="string">,</span>                <span class="string">"ecr:CompleteLayerUpload"</span>            <span class="string">],</span>            <span class="string">"Resource"</span><span class="string">:</span> <span class="string">"arn:aws:ecr:ap-northeast-2:$&#123;ACCOUNT_ID&#125;:repository/demo-frontend"</span>        <span class="string">&#125;,</span>        <span class="string">&#123;</span>            <span class="string">"Sid"</span><span class="string">:</span> <span class="string">"GetAuthorizationToken"</span><span class="string">,</span>            <span class="string">"Effect"</span><span class="string">:</span> <span class="string">"Allow"</span><span class="string">,</span>            <span class="string">"Action"</span><span class="string">:</span> <span class="string">[</span>                <span class="string">"ecr:GetAuthorizationToken"</span>            <span class="string">],</span>            <span class="string">"Resource"</span><span class="string">:</span> <span class="string">"*"</span>        <span class="string">&#125;</span>    <span class="string">]&#125;EOF</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>이 파일을 사용해 IAM 정책을 생성합니다. 이름은 <code>ecr-policy</code> 입니다.</p><ul><li>aws iam create-policy –policy-name ecr-policy –policy-document file://ecr-policy.json</li></ul></li><li><p>만든 정책을 IAM 유저에게 넣어줍니다.</p><ul><li>aws iam attach-user-policy –user-name github-action –policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/ecr-policy</li></ul></li></ul></li><li><p>Github Secret 생성</p><ul><li>이제 github action에서 사용할 AWS credential, github token을 설정해줍니다.</li><li>AWS credential 생성<ul><li>이 또한 최소한의 권한을 갖는 유저를 만들어 줍니다. 이름은 <code>github-action</code>입니다.</li><li>aws iam create-access-key –user-name github-action</li><li>생성된 accesskey와 secretkey를 저장해놓습니다.</li></ul></li><li>Github으로 들어가서 PAT를 생성해주고 저장해놓습니다.</li><li>front-app-repo로 들어가 Settings에 Secret을 선택합니다.<ul><li>이 레포의 secret을 넣어줍니다.</li><li>Name은 <code>ACTION_TOKEN</code></li><li>Value는 PAT를 넣어줍니다.</li><li>마찬가지로 <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> 도 넣어줍니다.</li></ul></li></ul></li><li><p>Github Action을 위한 workflow 스크립트를 생성합니다.</p><ul><li><p>github action을 생성하면 .github/workflows에 만들어지기 때문에 같은 디렉토리를 먼저 만들어줍니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>mkdir -p ./.github/workflows</li></ul></li><li><p>실제 build 코드를 작성합니다. <code>build.yml</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/amazon-eks-frontend/.github/workflowscat</span> <span class="string">&gt; build.yaml &lt;&lt;EOFname: Build Fronton:  push:    branches: [ main ]jobs:  build:    runs-on: ubuntu-latest    steps:      - name: Checkout source code        uses: actions/checkout@v2      - name: Check Node v        run: node -v      - name: Build front        run: |          npm install          npm run build      - name: Configure AWS credentials        uses: aws-actions/configure-aws-credentials@v1        with:          aws-access-key-id: \$<span class="template-variable">&#123;&#123; secrets.AWS_ACCESS_KEY_ID &#125;&#125;</span>          aws-secret-access-key: \$<span class="template-variable">&#123;&#123; secrets.AWS_SECRET_ACCESS_KEY &#125;&#125;</span>          aws-region: $AWS_REGION      - name: Login to Amazon ECR        id: login-ecr        uses: aws-actions/amazon-ecr-login@v1      - name: Get image tag(verion)        id: image        run: |          VERSION=\$(echo \$<span class="template-variable">&#123;&#123; github.sha &#125;&#125;</span> | cut -c1-8)          echo VERSION=\$VERSION          echo "::set-output name=version::\$VERSION"      - name: Build, tag, and push image to Amazon ECR        id: image-info        env:          ECR_REGISTRY: \$<span class="template-variable">&#123;&#123; steps.login-ecr.outputs.registry &#125;&#125;</span>          ECR_REPOSITORY: demo-frontend          IMAGE_TAG: \$<span class="template-variable">&#123;&#123; steps.image.outputs.version &#125;&#125;</span>        run: |          echo "::set-output name=ecr_repository::\$ECR_REPOSITORY"          echo "::set-output name=image_tag::\$IMAGE_TAG"          docker build -t \$ECR_REGISTRY/\$ECR_REPOSITORY:\$IMAGE_TAG .          docker push \$ECR_REGISTRY/\$ECR_REPOSITORY:\$IMAGE_TAGEOF</span></span><br></pre></td></tr></table></figure><ul><li>IMAGE_TAG값은 랜덤으로 생성되어 ECR로 Push됩니다.<ul><li>$</li></ul></li></ul></li><li><p>github action이 잘 동작하는지 테스트 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add github action build script”<br>git push origin main</li><li></li></ul></li></ul></li><li><p>Kustomize 사용을 위한 k8s manifest 구조화</p><ul><li><p>앞서 설명한 대로, kustomize는 manifest를 base와 overlays로 나뉘어 관리합니다.</p></li><li><p>이 구조를 만들어줍니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environmentmkdir</span> <span class="bullet">-p</span> <span class="string">./k8s-manifest-repo/basemkdir</span> <span class="bullet">-p</span> <span class="string">./k8s-manifest-repo/overlays/devcd</span> <span class="string">~/environment/manifestscp</span> <span class="string">*.yaml</span> <span class="string">../k8s-manifest-repo/basecd</span> <span class="string">../k8s-manifest-repo/basels</span> <span class="bullet">-rlt</span></span><br></pre></td></tr></table></figure></li><li><p><em><code>base</code></em> : kubernetes manifest 원본이 위치한 디렉토리 입니다. 이 안에 위치한 manifest 들은 <em><code>overlays</code></em> 아래에 위치한 <strong>kustomize.yaml</strong> 파일에 담긴 <strong>사용자 지정 설정</strong> 내용에 따라 변경됩니다.</p></li><li><p><em><code>overlays</code></em> : <strong>사용자 입맛에 맞는</strong> 설정 값이 위치한 디렉토리 입니다. 이 설정은 <strong>kustomize.yaml</strong> 에 담습니다. 이 하위에 있는 <em><code>dev</code></em> 디렉토리는 실습을 위해 만든 것으로, 개발 환경에 적용할 설정 파일을 모아 두기 위함 입니다.</p></li></ul></li><li><p>Kustomize manifest 생성</p><ul><li><p>frontend app에 대한 배포 구성을 할 것이기 때문에 frontend부분만 작업 하겠습니다.</p><ul><li>frontend-deployment.yaml 과 frontend-service.yaml 파일을 kustomize 를 통해 배포 시점에 의도한 값(e.g. Image Tag)을 반영 할겁니다</li><li>반영될 값<ul><li><strong><code>metadata.labels</code>:</strong> <code>&quot;env: dev&quot;</code>을 frontend-deployment.yaml, frontend-service.yaml 에 일괄 반영 합니다.</li><li><strong><code>spec.selector</code></strong> : <code>&quot;select.app: frontend-fargate&quot;</code> 를 frontend-deployment.yaml, frontend-service.yaml 에 일괄 반영 합니다.</li><li><strong><code>spec.template.spec.containers.image</code></strong> : <code>&quot;image: &quot;</code> 값을 새롭게 변경된 Image Tag 정보로 업데이트 합니다.</li></ul></li></ul></li><li><p>kustomize.yaml 파일을 만들어서 관리 변경할 manifest대상을 정의합니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/base</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">kustomization.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Kustomization</span></span><br><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">frontend-deployment.yaml</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">frontend-service.yaml</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>overlays/dev 부분에 바꿀 부분을 정의합니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/dev</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">front-deployment-patch.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">demo-frontend</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    env:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">frontend-fargate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">frontend-fargate</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/devcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">front-service-patch.yamlapiVersion:</span> <span class="attr">v1kind:</span> <span class="attr">Servicemetadata:</span>  <span class="attr">name:</span> <span class="string">demo-frontend</span>  <span class="attr">annotations:</span>    <span class="string">alb.ingress.kubernetes.io/healthcheck-path:</span> <span class="string">"/"</span>  <span class="attr">labels:</span>    <span class="attr">env:</span> <span class="attr">devspec:</span>  <span class="attr">selector:</span>    <span class="attr">app:</span> <span class="string">frontend-fargateEOF</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>마지막으로 위에서 설정 한 파일들(값)을 사용하고 frontend app 빌드에 따라 만들어진 새로운 <strong>Image Tag</strong> 를 사용 하겠다고 정의 하겠습니다. 구체적으로는, <code>name</code> 에 지정된 image는 <code>newName</code>의 image와 <code>newTag</code>의 값으로 사용 하겠다는 의미 입니다.</p></li><li><p>이를 활용해 <code>newTag</code> 값을 변경해 새로운 배포가 이루어질 때 마다 이를 kubernetes 클러스터까지 변경 할 수 있습니다.</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/overlays/devcat</span> <span class="string">&lt;&lt;EOF&gt;</span> <span class="string">kustomization.yamlapiVersion:</span> <span class="string">kustomize.config.k8s.io/v1beta1kind:</span> <span class="attr">Kustomizationimages:-</span> <span class="attr">name:</span> <span class="string">$&#123;ACCOUNT_ID&#125;.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend</span>  <span class="attr">newName:</span> <span class="string">$&#123;ACCOUNT_ID&#125;.dkr.ecr.ap-northeast-2.amazonaws.com/demo-frontend</span>  <span class="attr">newTag:</span> <span class="attr">abcdefgresources:-</span> <span class="string">../../basepatchesStrategicMerge:-</span> <span class="string">front-deployment-patch.yaml-</span> <span class="string">front-service-patch.yamlEOF</span></span><br></pre></td></tr></table></figure><ul><li>이상 -patch.yaml 파일에 정의한 내용들은 배포 과정에서 kustomize 에 의해 자동으로 kubernetes manifest 에 반영 됩니다.</li><li>이미지의 태그명이 abcdefg로 나오면 성공입니다.</li></ul></li><li><p>kubernetes manifest 용 github repo를 만들어줍니다.</p><ul><li>k8s-manifefst-repo <strong>**</strong>생성</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/k8s-manifest-repo/git</span> <span class="string">initgit</span> <span class="string">add</span> <span class="string">.git</span> <span class="string">commit</span> <span class="bullet">-m</span> <span class="string">"first commit"</span><span class="string">git</span> <span class="string">branch</span> <span class="bullet">-M</span> <span class="string">maingit</span> <span class="string">remote</span> <span class="string">add</span> <span class="string">origin</span> <span class="attr">https://github.com/jinseo-jang/k8s-manifest-repo.gitgit</span> <span class="string">push</span> <span class="bullet">-u</span> <span class="string">origin</span> <span class="string">main</span></span><br></pre></td></tr></table></figure></li><li><p>Argo CD 설치</p><ul><li>kubectl create namespace argocd<br>kubectl apply -n argocd -f <a href="https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml">https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml</a></li></ul></li><li><p>Argo CD cli 설치</p><ul><li>cd ~/environment<br>VERSION=$(curl –silent “<a href="https://api.github.com/repos/argoproj/argo-cd/releases/latest">https://api.github.com/repos/argoproj/argo-cd/releases/latest</a>“ | grep ‘“tag_name”‘ | sed -E ‘s/.*”([^”]+)”.*/\1/‘)</li><li>sudo curl –silent –location -o /usr/local/bin/argocd <a href="https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64">https://github.com/argoproj/argo-cd/releases/download/$VERSION/argocd-linux-amd64</a></li><li>sudo chmod +x /usr/local/bin/argocd</li></ul></li><li><p>Argo CD는 퍼블릭하게 노출되지 않지만, ELB를 통해 접속 가능하도록 만들겠습니다.</p><ul><li>kubectl patch svc argocd-server -n argocd -p ‘{“spec”: {“type”: “LoadBalancer”}}’</li></ul></li><li><p>접속할 uri를 얻습니다.</p><ul><li>export ARGOCD_SERVER=<code>kubectl get svc argocd-server -n argocd -o json | jq --raw-output .status.loadBalancer.ingress[0].hostname</code><br>echo $ARGOCD_SERVER</li></ul></li><li><p>기본 username은 admin입니다. password는 다음과 같습니다.</p><ul><li>ARGO_PWD=<code>kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;{.data.password}&quot; | base64 -d</code><br>echo $ARGO_PWD</li></ul></li><li><p>argo login</p><img src="/images/eks-workshop/Untitled 9.png" alt="login화면" style="zoom:60%;" /></li><li><p>로그인 후 좌 상단 애플리케이션 설정 메뉴를 클릭하고 새 application을 만들어줍니다.</p></li><li><p><strong>Application Name</strong> 은 <code>eksworkshop-cd-pipeline</code>, <strong>Project</strong>는 <code>default</code>를 입력 합니다.</p></li><li><p><strong>SOURCE</strong> 섹션의 <strong>Repository URL</strong> 에는 앞서 생성한 <strong><code>k8s-manifest-repo</code>의 git 주소</strong>, <strong>Revision</strong> 에는 <code>main</code>, <strong>Path</strong> 에는 <code>overlays/dev</code>를 입력 합니다.</p></li><li><p><strong>DESTINATION</strong> 섹션의 <strong>Cluster URL</strong>에는 <code>https://kubernetes.default.svc</code>, <strong>Namespace</strong> 에는 <code>default</code>를 입력 하고 상단의 <strong>Create</strong> 를 클릭 합니다.</p></li><li><p>eksworkshop-cd-pipeline이 만들어졌습니다.</p></li><li><p>Kustomize 빌드 단계 추가</p><ul><li>github action에서 kustomize를 이용하여 image tag를 업데이트 한 후 k8s-manifest-repo에 커밋 푸시하는 단계를 추가 해줘야 합니다.</li><li>이 단계가 동작하면, argo CD가 k8s-manifest-repo를 센싱 하다가 새로운 변경사항이 감지되면 Kustomize build 작업을 수행해 새로운 kubernetes manifest를 eks클러스터에 배포합니다.</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cd</span> <span class="string">~/environment/amazon-eks-frontend/.github/workflows</span></span><br><span class="line"><span class="string">cat</span> <span class="string">&lt;&lt;EOF&gt;&gt;</span> <span class="string">build.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Setup</span> <span class="string">Kustomize</span></span><br><span class="line"><span class="attr">        uses:</span> <span class="string">imranismail/setup-kustomize@v1</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Checkout</span> <span class="string">kustomize</span> <span class="string">repository</span></span><br><span class="line"><span class="attr">        uses:</span> <span class="string">actions/checkout@v2</span></span><br><span class="line"><span class="attr">        with:</span></span><br><span class="line"><span class="attr">          repository:</span> <span class="string">jinseo-jang/k8s-manifest-repo</span></span><br><span class="line"><span class="attr">          ref:</span> <span class="string">main</span></span><br><span class="line"><span class="attr">          token:</span> <span class="string">\$&#123;&#123;</span> <span class="string">secrets.ACTION_TOKEN</span> <span class="string">&#125;&#125;</span></span><br><span class="line"><span class="attr">          path:</span> <span class="string">k8s-manifest-repo</span></span><br><span class="line"></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">Update</span> <span class="string">Kubernetes</span> <span class="string">resources</span></span><br><span class="line"><span class="attr">        run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          echo \$<span class="template-variable">&#123;&#123; steps.login-ecr.outputs.registry &#125;&#125;</span></span></span><br><span class="line"><span class="string">          echo \$<span class="template-variable">&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;</span></span></span><br><span class="line"><span class="string">          echo \$<span class="template-variable">&#123;&#123; steps.image-info.outputs.image_tag &#125;&#125;</span></span></span><br><span class="line"><span class="string">          cd k8s-manifest-repo/overlays/dev/</span></span><br><span class="line"><span class="string">          kustomize edit set image \$<span class="template-variable">&#123;&#123; steps.login-ecr.outputs.registry&#125;&#125;</span>/\$<span class="template-variable">&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;</span>=\$<span class="template-variable">&#123;&#123; steps.login-ecr.outputs.registry&#125;&#125;</span>/\$<span class="template-variable">&#123;&#123; steps.image-info.outputs.ecr_repository &#125;&#125;</span>:\$<span class="template-variable">&#123;&#123; steps.image-info.outputs.image_tag &#125;&#125;</span></span></span><br><span class="line"><span class="string">          cat kustomization.yaml</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span><span class="attr">      - name:</span> <span class="string">Commit</span> <span class="string">files</span></span><br><span class="line"><span class="attr">        run:</span> <span class="string">|</span></span><br><span class="line"><span class="string">          cd k8s-manifest-repo</span></span><br><span class="line"><span class="string">          git config --global user.email "github-actions@github.com"</span></span><br><span class="line"><span class="string">          git config --global user.name "github-actions"</span></span><br><span class="line"><span class="string">          git commit -am "Update image tag"</span></span><br><span class="line"><span class="string">          git push -u origin main</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li><li><p>소스를 만들었으면, 커밋 푸시 해줍니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add kustomize image edit”<br>git push -u origin main</li></ul></li><li><p>github action이 잘 동작하는지를 확인하고, k8s-manifest-repo에 새 manifest가 커밋 되는지 확인합니다.</p></li><li><p>이렇게 새 manifest가 배포되면, argo CD에서 감지하여, Sync Status가 업데이트 됩니다.</p><ul><li>아무 설정을 하지 않았다면, <strong>CURRENT SYNC STATUS</strong>의 값이 <strong>Out of Synced</strong> 입니다.</li><li>git repository 가 변경되면 자동으로 sync 작업이 수행 하도록 하려면 <strong>Auto-Sync</strong> 를 활성화 해야 합니다. 이를 위해 <strong>APP DETAILS</strong> 로 이동 하여 <strong>ENABLE AUTO-SYNC</strong> 버튼을 눌러 활성화 합니다.</li><li>활성화 되었다면, ArgoCD에 의해 k8s-manifest-repo의 커밋 내용이 ArgoCD에 의해 eks클러스터에 반영됩니다.</li><li>마지막으로 정상적으로 새 manifest가 배포되었는지를 확인하기 위해 k8s-manifest-repo의 커밋 히스토리를 통해 image tag를 살펴봅니다.</li><li>abcdefg가 아니라 새로운 태그값으로 들어갔다면 성공입니다.</li></ul></li><li><p>frontend application에 코드를 변경해서, GitOps 파이프라인이 정상 동작하는지를 최종 점검합니다.</p></li><li><p><strong>amazon-eks-frontend/src/</strong> 로 이동하여 <strong><code>App.js</code></strong> 더블 클릭하여 파일을 오픈 합니다.</p><ul><li><p><strong>line 67</strong>의 값을 <strong><code>EKS DEMO Blog version Hyuby</code></strong> 로 변경 하고 저장 합니다. 저장은 <strong>ctrl+s</strong> 를 누릅니다.</p></li><li><p>변경된 소스를 커밋 푸시 합니다.</p><ul><li>cd ~/environment/amazon-eks-frontend<br>git add .<br>git commit -m “Add new blog version”<br>git push -u origin main</li></ul></li><li><p>Sync작업이 모두 끝나면, url로 접속하여 변경사항을 확인합니다.</p><ul><li>echo <a href="http://%24/">http://$</a>(kubectl get ingress/backend-ingress -o jsonpath=’{.status.loadBalancer.ingress[*].hostname}’)</li><li>짠!</li></ul><p><img src="/images/eks-workshop/demo_hyuby.png" alt="정상동작 확인!"></p></li></ul><p><br></br></p></li></ul><hr><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://daddyprogrammer.org/post/14102/argocd-kubernetes-cluster-deploy/">https://daddyprogrammer.org/post/14102/argocd-kubernetes-cluster-deploy/</a></li><li><a href="https://junghyeonsu.tistory.com/65">https://junghyeonsu.tistory.com/65</a></li><li><a href="https://wookiist.dev/159">https://wookiist.dev/159</a></li><li><a href="https://cwal.tistory.com/23">https://cwal.tistory.com/23</a></li><li>[<a href="https://coffeewhale.com/kubernetes/workflow/argo/2020/02/14/argo-wf/]">https://coffeewhale.com/kubernetes/workflow/argo/2020/02/14/argo-wf/]</a>(</li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/11/03/EKS-workshop/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Docker hub pull limit이 발생했다면?</title>
      <link>http://tkdguq05.github.io/2021/10/07/docker-hub-limit/</link>
      <guid>http://tkdguq05.github.io/2021/10/07/docker-hub-limit/</guid>
      <pubDate>Thu, 07 Oct 2021 13:10:16 GMT</pubDate>
      <description>
      
        &lt;p&gt;Docker Hub 에서 신나게 pull 받다가 limit 때문에 문제가 발생한 경우와, 이를 회피하는 방법에 대해서 작성해봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Docker Hub 에서 신나게 pull 받다가 limit 때문에 문제가 발생한 경우와, 이를 회피하는 방법에 대해서 작성해봤습니다.</p><a id="more"></a><h2 id="Docker-Hub-Pull-Limit"><a href="#Docker-Hub-Pull-Limit" class="headerlink" title="Docker Hub Pull Limit"></a>Docker Hub Pull Limit</h2><p>Docker를 사용하는 여러 이유가 있겠지만, 무엇보다도 Docker Hub에 있는 이미지들을 자유롭게 받아서 사용할 수 있다는 점이 큰 매력 중에 하나라고 생각합니다. 그래서 저도 public 이미지들을 자유롭게 받아서 사용하고 이를 운영하는 서버에도 적용해서 배포를 하고 있었습니다. 그러던 와중에 청천벽력같은 소식이 전해집니다. </p><blockquote><p>Hello:</p><p>You are receiving this email because of a policy change to Docker products and services you use. On Monday, November 2, 2020 at 9am Pacific Standard Time, Docker will begin enforcing rate limits on container pulls for Anonymous and Free users. Anonymous (unauthenticated) users will be limited to 100 container image pulls every six hours, and Free (authenticated) users will be limited to 200 container image pulls every six hours, when enforcement is fully implemented. Docker Pro and Team subscribers can pull container images from Docker Hub without restriction, as long as the quantities are not excessive or abusive.</p><p>In addition, we are pausing enforcement of the changes to our image-retention policies until mid-2021, when we anticipate incorporating them into usage-based pricing. Two months ago, we announced an update to Docker image-retention policies. As originally stated, this change, which was set to take effect on November 1, 2020, would result in the deletion of images for free Docker account users after six months of inactivity. Today’s announcement means Docker will not enforce image expiration on November 1, 2020.</p></blockquote><p>이메일을 통해서 받았었는데, 당시에는 사실 바빠서 그냥 읽지도 않고 넘겼었습니다. 그때는 도커도 자주 사용하지 않았기 때문에 별로 신경을 쓰지 않았던 것입니다. Docker Hub Pull Limit에 대해서는 <a href="https://www.docker.com/increase-rate-limits">여기서 자세히 알아볼 수 있습니다.</a></p><p>요약하자면, <code>정책이 변경되었고, 맘껏 쓰고 싶으면 돈내고 써라</code> 입니다.</p><p>그리고 시간이 지나, 이 이메일을 메일함에 방치하고 있던 와중에, ECS와 Code Build를 통해서 배포를 하던 와중에 일이 발생하고야 말았습니다.</p><h2 id="배포의-실패"><a href="#배포의-실패" class="headerlink" title="배포의 실패"></a>배포의 실패</h2><p>저희 팀에서는 Airflow를 통해 특정 추천 서비스의 전처리와 학습을 진행하고, 이 내용을 바탕으로 개발서버에 먼저 배포를 한 뒤에 결과를 일부 확인하고 운영서버에 배포하는 작업을 수행하고 있습니다. 어느때와 같이 새로운 고객사에 대해 추천 서비스 DAG를 생성하고 DAG를 켜줬습니다. 별 다른 이상이 없어서 시계를 확인하고 마침 퇴근 시간이 되었길래 칼퇴!를 했습니다. 지하철을 타는 순간에 핸드폰에서 불안한 알람이 온 것을 느꼈고, 그 내용은 DAG가 실패했다는 것이었습니다.</p><p>그런데 이 DAG의 태스크는 왠만해서는 실패가 되지 않는 부분이었습니다. 학습된 내용을 도커 이미지에 작성하고 작성된 이미지를 그냥 빌드하는 것이었기 때문입니다. 찜찜해서 가는 내내 고민하다가, 다른 동료가 보내준 코드 빌드에서 에러 로그를 보자마자 스치듯 지나갔던 그 메일이 생각났습니다.</p><p>에러의 내용은 다음과 같았습니다. <code>toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit</code> </p><p>돈을 내라는 것이었습니다. </p><p>하지만 저는 돈을 내기는 싫었고, 6시간을 기다려서 초기화 될때까지 기다려서 배포를 해볼까 했습니다. 하지만 고객사 추가가 잦아지거나 다른 이미지들을 pull해올 때 또 다시 문제가 발생할 수 있기에, 이를 한번 회피해야 겠다는 생각을 했습니다. 회피하는 방법은 생각보다 간단했습니다.</p><h2 id="🤫-pull-limit-제한-없이-쓰기"><a href="#🤫-pull-limit-제한-없이-쓰기" class="headerlink" title="🤫 pull limit 제한 없이 쓰기"></a>🤫 pull limit 제한 없이 쓰기</h2><p>많은 회사들에서는 클라우드 기반에서 서비스를 운영하실 것이라고 생각합니다. 온프레미스에서 운영하실 수도 있으나, 저희는 클라우드 기반에서 운영하기에, 또 AWS를 사용하기에 이 환경에 국한한 문제 해결 방법을 소개해 드리겠습니다.</p><p>AWS에는 이미지를 관리할 수 있는 저장소가 따로 있습니다. <code>ECR</code> 이라고 불립니다. 이 ECR에 다양한 이미지들을 빌드해서 올려놓고, ECS에 있는 TASK들의 컨테이너가 띄워질 때, 이 이미지를 사용해서 올라오게 됩니다. ECS말고도 인스턴스에서 이미지들을 갖고와서 사용할 수도 있고, 또 다르게 다양하게 사용할 수 있을 것입니다. </p><p>그래서 다양하게 한 번 사용을 해봤습니다. 필요한 이미지를 받아서 빌드하고 이 이미지를 ECR에 올려놓고, 이 이미지를 받아서 한번 사용해보는 겁니다. 그렇게 되면 이 이미지 자체는 ECR에 존재하게 되어서, ECS에서 태스크를 띄우든, 다른 곳에서 사용하든 무제한으로 이 이미지를 사용할 수 있게 될 것입니다.</p><p>저희 쪽에서 특히 자주 사용하는 이미지는 바로, python 이미지였습니다. <code>python:3.7-slim-buster</code> 를 특히 자주 썼었던 것 같은데, 이 이미지를 구글에서 검색해서 원본 이미지를 pull 해왔습니다. 이미지는 <a href="https://github.com/docker-library/repo-info/blob/master/repos/python/remote/3.7-slim-buster.md">이 곳</a>에서 찾았습니다.</p><p>나와있는 대로</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull python@sha256:88e9ed044abd15c15069477046e8c65fb7e97cebd7cf140e901ae35440bfc073</span><br></pre></td></tr></table></figure><p>docker 명령어를 통해 필요한 이미지를 받아줍니다. 그리고 <code>docker image ls</code>를 하면?</p><blockquote><p>amd64/python                                                             3.7                 7cb3330faa6c        8 days ago          903MB</p></blockquote><p>이렇게 잘 있는 것을 확인했습니다.</p><p>그 다음 이미지 이름에 태그를 달아주고 어디로 보내줄 것인지를 정의해줬습니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker tag amd64&#x2F;python:3.7 xxxxxxxxxx.dkr.ecr.ap-northeast-2.amazonaws.com&#x2F;python37_slim_buster</span><br></pre></td></tr></table></figure><p>그런데 이때 중요한 것은, ECR의 레포지토리를 미리 만들어줘야 한다는 것입니다. 미리 만들어두지 않으면, 알아서 레포지토리를 만들지 못하기 때문에 에러가 나서 실망하게 됩니다. 한 번에 딲! 끝내고 싶다면 미리 만들어 두십시오.</p><p>태그를 해 준 다음에는 push를 해줍니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push amd64&#x2F;python:3.7 xxxxxxxxxx.dkr.ecr.ap-northeast-2.amazonaws.com&#x2F;python37_slim_buster</span><br></pre></td></tr></table></figure><p>이렇게 Push까지 하게되면? 원하는 이미지를 ECR 레포지토리에서 확인할 수 있습니다.</p><p>이제 이렇게 만들어진 이미지의 URI를 복사해서 Dockerfile의 FROM에 집어넣어 주면? 이제 제한없이 배포를 마음껏 할 수 있게 됩니다!</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://www.docker.com/increase-rate-limits">https://www.docker.com/increase-rate-limits</a></li><li><a href="https://github.com/docker-library/repo-info/blob/master/repos/python/remote/3.7-slim-buster.md">https://github.com/docker-library/repo-info/blob/master/repos/python/remote/3.7-slim-buster.md</a></li><li><a href="https://subicura.com/k8s/2021/01/02/docker-hub-pull-limit/">https://subicura.com/k8s/2021/01/02/docker-hub-pull-limit/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/10/07/docker-hub-limit/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Airflow와 야근하기</title>
      <link>http://tkdguq05.github.io/2021/09/12/work-overtime-with-airflow/</link>
      <guid>http://tkdguq05.github.io/2021/09/12/work-overtime-with-airflow/</guid>
      <pubDate>Sun, 12 Sep 2021 07:00:11 GMT</pubDate>
      <description>
      
        &lt;p&gt;지난 주에 쓸데없이 야근한 일이 있어 이를 기록해두고자 합니다.&lt;/p&gt;
&lt;p&gt;재발 방지를 위하여.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>지난 주에 쓸데없이 야근한 일이 있어 이를 기록해두고자 합니다.</p><p>재발 방지를 위하여.</p><a id="more"></a><h2 id="평화로운-9월-7일-오후"><a href="#평화로운-9월-7일-오후" class="headerlink" title="평화로운 9월 7일 오후"></a>평화로운 9월 7일 오후</h2><p>9월 5일에 스파르톤을 마치고 컨디션이 화요일까지 영 회복이 되지 않았습니다. 월요일에도 왜인지 잠을 제대로 이루지 못해서 화요일 컨디션이 좋지 않았습니다. 빠르게 할 일 하고 칼퇴해야겠다는 결심을 하고, 출근을 했습니다. 당일에 계획된 작업은 ECS TASK로 변경한 airflow의 워커 상태와 main server, metaDB를 점검하고,  airflow의 스케쥴러를 안정화 하기 위해서 웹서버와 스케쥴러를 데몬 서비스로 등록하는 것이었습니다. 그동안 서비스로 등록하는 것에 대해서 망설였었는데, 공식 문서에도 서비스 등록에 대한 내용이 있기도 했고, 파이썬 코드를 통해서 상태를 점검하는 것보다 더 안정적일 것이라는 생각에 서비스로 등록하기로 했습니다. 서비스로 등록하는 것에 대해서는 <a href="https://github.com/apache/airflow/tree/main/scripts/systemd">airflow 공식 github</a>에 있는 파일을 사용하면 됩니다. </p><p>서비스로 등록해서 사용하는 글은 생각보다 굉장히 많았습니다. 쿠버네티스를 활용해서 스케쥴러를 돌보는게 요즘 트렌드긴 하지만, 쿠버네티스를 사용할 정도가 아닌 조직같은 경우에는 서비스로 등록해서 많이 사용하는 것 같았습니다. 어쨌든 이 파일을 이용해서 서비스 파일을 만들었고 서비스에 필요한 요소들, 예를 들어 Unit, Service, Install에 대해서 다시 한번 살펴봤습니다. 각 항목에 대한 내용들을 작성하고 환경에 맞게 튜닝을 조금 했습니다. 그리고 여러번 적용하고 다시 등록하고… 결국 서비스가 등록되고 start된 것이 확인 되었습니다. 하지만, 조금 이상했던 것은 main 서버의 CPU이용률이 이전보다 증가했다는 것입니다. CPU이용률이 벌써부터 올라가면 별로 좋지 않을 것 같아서 CPU이용률을 줄여보기 위해 airflow.cfg를 조금 수정해주기로 했습니다. 예전에 살펴봤던 글에 <a href="https://burning-dba.tistory.com/111">CPU이용률을 낮추는 설정</a>이 기억나서 이를 적용해보기로 했습니다. <code>scheduler_heartbeat_sec = 60</code>, <code>min_file_process_interval =60</code>, <code>max_threads = 1</code> 로 설정을 변경하는 것이 주 내용이었습니다. scheduler_heartbeat_sec은 스케줄러가 새로운 작업을 실행해야 하는 빈도 (초)를 정의하는 설정입니다. 이 시간을 적게 설정한다면, 더 자주 스케쥴러를 실행시키기 때문에 CPU의 이용률이 높아질 수 밖에 없습니다. min_file_process_interval은 DAG 업데이트 시간을 조정하는 값입니다. 스케쥴러와는 큰 관계가 없지만 웹서버의 부하를 일으킬 수 있기때문에 CPU와 연관이 있습니다. 이 두 값을 조정하고 적용했습니다. CPU가 조금 내려갔습니다. 서비스도 문제가 없었습니다.</p><h2 id="문제의-발생"><a href="#문제의-발생" class="headerlink" title="문제의 발생"></a>문제의 발생</h2><p>서비스가 성공적으로 등록되어서, 서비스 등록에 대해서 팀원들에게 공유하는 시간을 가졌습니다. 여러저러 설명을 하고 airflow의 스케쥴러의 상태를 확인을 했습니다. airflow주소:8080/health 면 간단하게 메타DB와 스케쥴러의 상태를 확인할 수 있습니다. 그런데 여기에 <code>unhealthy</code> 라고 나오기 시작했습니다. 더 이상한 것은, unhealthy하다면 last_heartbeat_time은 죽은 이후부터 업데이트가 되면 안되는데, 계속해서 업데이트가 되고 있었습니다. 심신이 매우 피곤했던 상황이었기 때문에 멘탈이 깨지기 시작했습니다. 가뜩이나 ECS워커로 변경한 뒤에 문제가 없나 노심초사하고 있었는데, ECS워커의 설정이 뭐가 잘못된 것인지, 서비스에 이상이 있는 건지 감이 잘 잡히지 않았습니다. 정신없던 와중에 퇴근시간이 지났습니다. </p><h2 id="야근의-시작"><a href="#야근의-시작" class="headerlink" title="야근의 시작"></a>야근의 시작</h2><p>본격적으로 초과근무를 하기 시작했습니다. 금방 끝날 것이라고 생각해서 서비스를 중단하고 웹서버와 스케쥴러를 다시 시작하면 해결이 될 것이라고 생각했습니다. 몇 번을 죽이고 살리고를 반복했지만 별 효과가 없었습니다. airflow가 사람을 놀리기라도 하는듯이 다시 시작하고 처음 몇 초간은 <code>healthy</code> 라고 나오다가 ‘휴 끝났네… 한번만 더 확인해볼까?’ 하는 순간 unhealthy로 나왔습니다. 정신이 혼미해져가는 와중에 ‘새로 airflow서버를 띄우면 고생하긴 하겠지만 되겠지’란 생각에 메인 서버 하나를 종료하고 미리 저장된 AMI를 이용해서 새 서버를 올렸습니다. </p><p>새로 서버를 올렸더니, 아주 잘 되기 시작합니다. </p><p>‘휴 해치웠나…?’ </p><p>클리셰는 역시 클리셰였습니다. airflow를 사용하는 것이 두 영역인데, 한 영역은 잘 되었지만, 다른 영역은 비웃기라도 하듯 잘 되지 않는 것이 아니겠습니까. 무엇이 문제일까 다시 고민하기 시작했습니다. 하지만 크게 문제될 부분도 없었고 ECS워커부분의 로그도 너무 조용했습니다. 그러다가 로그에 작업이 실행되는 것이 발견되어서, ‘스케쥴러가 죽어있는데 작업이 어떻게 돌지?’ 란 생각에 한 DAG를 수동으로 돌렸습니다. 놀랍게도 느리긴 했지만 작업이 실행되었습니다. 작업은 DAG를 파싱하고, 스케쥴링이 된 상태에서, 큐를 내주고, 워커에서 해당 큐를 가져가서 코드에 적힌대로 수행하게 되어있습니다. 그렇다면 스케쥴러는 unhealthy하긴 하지만, 실제로는 죽어있는 것이 아닌 것이 됩니다. </p><p>여러 커뮤니티에 도움을 청해놔서 그 분들의 의견을 확인해 봤습니다. “스케쥴러가 때로는 실제로 돌아가고 있는데 webserver에서 이를 못 잡을 수도 있으니 다시 완벽하게 kill하고 다시 실행해 봐라” 라는 의견의 주였습니다. 또 다시 스케쥴러를 내리고 올리고를 반복했습니다. 하지만 해결이 되지 않았습니다. 할 수 있는 건 정말 다 해봤는데, 그래서 희대의 뻘짓인 <code>airflow reset db</code> 까지 하게 됩니다. 이 명령어를 metaDB에 있는 모든 데이터를 날리는 일입니다. 제가 설정해둔 variables와 connections의 값을 다 날리는 것입니다. 물론 값을 백업해두고 한 일이라 복구하는데 어렵지는 않았지만, 이렇게 해결해서는 안된다고… 생각합니다. </p><p>또 다시 airflow를 종료하고 새롭게 airflow를 올렸는데, 이번에는 ECS에서 이상한 로그를 하나가 발견되기 시작합니다. <code>Cannot connect to .... redis....</code> 레디스에 연결이 안된다는 것입니다. 그래서 main서버에 접속해서 redis-server의 프로세스를 확인해 봤더니, 프로세스가 정말 없었습니다. 왜 레디스가 죽었지? 하다가 데몬 서비스로 등록한 파일이 생각났습니다. 그 파일의 Unit부분에 After와 Wants에는 redis.service 부분이 있었는데, 서비스가 실행될때 레디스를 건드린 것 같았습니다. 레디스를 복구하고 다시 작업을 확인해보니, 이제는 작업은 실행되지만, 여전히 unhealthy한 스케쥴러가 남아있는 것이 확인되었습니다. 그러다가 오전에 CPU 이용률 때문에 설정한 scheduler_heartbeat_sec가 생각났습니다.</p><p>오전에 설정할 때 60초는 너무 긴 것 같아 40초로 설정을 했습니다. 그리고 설정 값을 쭉 보다가 scheduler_health_check_threshold라는 것도 한번 봤습니다. 기본 값이 30초였는데, 관련 없는 값이라고 생각되어 그냥 넘어갔었습니다. 하지만, 이게 바로 이 야근을 한 원인이었다는 것을 12시가 다되어서야 발견을 하게 되었습니다. </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">If the last scheduler heartbeat happened more than scheduler_health_check_threshold</span><br><span class="line">ago (in seconds), scheduler is considered unhealthy.</span><br><span class="line">This is used by the health check in the &quot;&#x2F;health&quot; endpoint</span><br></pre></td></tr></table></figure><p>설정에는 이렇게 설명되어 있습니다. last scheduler heartbeat가 scheduler_health_check_threshold보다 더 자주 발생하면, unhealthy라고 나올 것이라고, 그리고 이 정보는  /health 에 사용된다고…</p><p>그러니까 정리하자면, 오전에 CPU이용률을 줄이려고 scheduler_heartbeat_sec을 40초로 설정을 했는데, 이 값보다 scheduler_health_check_threshold가 더 적은 값인 30이게 되어서 health 체크를 하는 시간 보다 sheduler_hearbeat_sec가 더 자주 발생하게 되어서 unhealthy하다고 /health에서 표시가 된 것이었습니다. 스케쥴러는 사실상 프로세스가 떠 있고 작업을 하고 있으니 문제는 없었던 것이 맞았습니다. </p><p><img src="https://www.pngkey.com/png/detail/14-142665_crying-pepe-png-pepe-cry-png.png" alt=""></p><p>그러니까… 쓸데없이 airflow를 처음부터 다시 띄우고… metaDB를 rest할 필요도 없었고, 야근할 필요도 없었던 거시었슴미다.</p><p>피곤이 몰려오기 시작했습니다.</p><h2 id="Outro"><a href="#Outro" class="headerlink" title="Outro"></a>Outro</h2><p>9월 8일 00:20분 경에는 비가 왔습니다. 택시를 콜하니 대어를 물기 위해 5초만에 기사님이 잡혔습니다. 풀이 죽은 야간 추가 근무자와는 반대로 택시기사님은 격양되어 있는 것 같았습니다. 조용히 이어폰을 꽂았고 재밌어보이는 유튜브 영상을 보면서, 무슨 내용인지도 기억이 안나지만, 그렇게 집으로 왔습니다. </p><p>29,600원짜리 교훈이었습니다. </p><p>설정값을 제대로 확인을 하지 않고, 수면 부족의 콜라보로 이루어낸 결과였습니다. airflow가 또 터지는 악몽을 꿨고, 다음날 겨우 지각을 면하면서 아슬아슬하게 출근을 했습니다.</p><p>잠은 평소에 잘 자야하는 것 같습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/09/12/work-overtime-with-airflow/#disqus_thread</comments>
    </item>
    
    <item>
      <title>sparton</title>
      <link>http://tkdguq05.github.io/2021/09/04/sparton/</link>
      <guid>http://tkdguq05.github.io/2021/09/04/sparton/</guid>
      <pubDate>Sat, 04 Sep 2021 11:44:04 GMT</pubDate>
      <description>
      
        &lt;p&gt;9pm to 6am 스파르톤 생존일지&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>9pm to 6am 스파르톤 생존일지</p><a id="more"></a><h2 id="스파르톤"><a href="#스파르톤" class="headerlink" title="스파르톤"></a>스파르톤</h2><p>코딩에 미쳐보고 싶던 찰나에 스파르톤이 열렸다. 저녁 9시부터 다음날 오전 6시까지… 무사히 목표한 바를 이루고 싶다.</p><h2 id="타임-테이블"><a href="#타임-테이블" class="headerlink" title="타임 테이블"></a>타임 테이블</h2><p><code>**9:00pm**</code> 팀별 아이스 브레이킹<br><code>**9:10pm**</code> 웰컴 인사 및 행사 안내<br><code>**11:30pm**</code> 이벤트 퀴즈쇼 ‘도전 스파르타 골든벨’<br><code>**01:00am**</code> 인스타 피드 챌린지 1 (#스파르타코딩클럽 #스파르톤 #힙한취미 #코딩 @spartacodingclub) &amp; 찐관<br><code>**03:00am**</code> 전체 마라톤<br><code>**04:00am**</code> 인스타 피드 챌린지 2 (#스파르타코딩클럽 #스파르톤 #힙한취미 #코딩 @spartacodingclub) &amp; 찐관<br><code>**05:30am**</code> 행사 마무리 준비<br><code>**06:00am**</code> 행사 마감, 완주 축하하기</p><h2 id="9-26"><a href="#9-26" class="headerlink" title="9:26"></a>9:26</h2><p>스파르톤이 시작되기 직전, 레드불을 하나 까서 마시기 시작했다. 조금 졸린데 걱정이 되기 시작한다.</p><h2 id="9-40"><a href="#9-40" class="headerlink" title="9:40"></a>9:40</h2><p>선서가 끝나고 본격적으로 학습에 들어갔다. 3주차에 못들었던 강의부터 시작했다.</p><p>해쉬 -2 강의를 눌러서 시청했다.</p><h2 id="10-30"><a href="#10-30" class="headerlink" title="10:30"></a>10:30</h2><p>3주차 강의를 모두 수강했고, 부여된 문제들을 풀기 시작했다. 노트북 배터리 충전이 잘 되지 않아 리부팅 했다.</p><p>게더타운때문인지 뭔지 리소스를 너무 많이 잡아먹는 프로세스가 있어 타자가 버벅거린다.</p><h2 id="11-00"><a href="#11-00" class="headerlink" title="11:00"></a>11:00</h2><p>과제 첫번째 문제를 빠르게 해결했다. </p><p>사실 해결하는데 정렬이 갑자기 공부가 안나서 다시 공부를 했다. </p><p>할인률이 가장 높게 상품 구매를 하는 문제였는데, 정렬을 한뒤 뒤에서부터 하나씩 가져와서 곱한뒤 쿠폰이 없다면 나머지는 다 더해주는 방식으로 코딩을 했다.</p><p>두 번째 문제에 도전하고 있다. 괄호가 올바른지 체크를 해야하는 문제다. 예전에 풀었었던 것 같은데 뭘로 풀어야 될지 감이 잘 안잡히는 중이다.</p><h2 id="1-00AM"><a href="#1-00AM" class="headerlink" title="1:00AM"></a>1:00AM</h2><p>중간에 골든벨 이벤트가 끝났다. 게더타운과 슬랙이 폭발하는 바람에 내 CPU도 막막 올라가서 결국 먹통이 됐다.</p><p>결국 재시작을 몇번 하고, 슬랙과 게더타운을 종료하고 기다렸다.</p><p>이벤트가 끝나니 CPU도 같이 내려갔다. 3주차 과제를 풀어봤고 해설강의를 들었다. 내가 생각한 아이디어와 비슷한 점도 보였고, 새롭게 푼 문제도 있었다. 괄호문제는 스택으로 푸는게 맞을까 했는데 맞았다. 올바른 괄호에 대한 체크 로직이 스택과 동일했다. 빠르게 생각하지 못한게 아쉬웠다. 이제 4주차 강의로 넘어간다.</p><h2 id="4-37AM"><a href="#4-37AM" class="headerlink" title="4:37AM"></a>4:37AM</h2><p>4주차 강의로 넘어가서 DFS 구현이 잘 안되서 시간이 좀 걸렸다. 와중에 세시에 마라톤 이벤트를 한대서 참여해봤는데</p><p>게더타운의 코스에서 두 바퀴 도는거였다. 물론 돌다가 터졌고, 내 노트북도 터질뻔했다.</p><p>무사히 완주하고 다시 자리를 잡았고 조금 힘들어서 밖에 나가서 걸었다. 편의점 앞까지 가서 따뜻한걸 하나 사서 먹을까 했다가 돌아왔다. 먹으면 졸릴 것 같았다.</p><p>사람이 없어서 시원하게 가스를 배출하며 오려던 찰나에 어떤 커플을 마주쳤다. 어색하게 자리를 피하고 집으로 돌아왔다.</p><p>DFS와 BFS가 끝났다. DFS는 스택과 관련이 있었고, BFS는 큐와 관련이 있었다.</p><p>이제 DP로 들어간다.</p><h2 id="5-35"><a href="#5-35" class="headerlink" title="5:35"></a>5:35</h2><p>4주차 마지막 까지 다 들었다. 과제 설명까지 봤는데, 과제가 생각보다 어려웠다. DFS BFS가 익숙하지 않아 이 개념을 문제에 활용하지 못하는 느낌이다. 개념을 한번 복습하고 다시 한번 문제를 어떻게 풀어야 할까 생각해봐야겠다.</p><p>06시가 되기 25분 전이다. 코딩을 마무리하고 생존일지를 쓰러 왔다. 전날 9시부터 오늘 6시까지 9시간이었는데, 생각보다 길지않았고 힘들지 않은 것 같다. </p><p>시험기간에도 밤을 새본 적 없고, 프로젝트 때 한번 밤을 새봤는데 그 마저도 3시까지 작업하다가 집에가서 자고 좀 쉬다가 다시 왔었다. 이렇게 풀로 깨어있으면서 코딩한적은 처음인 것 같다. </p><p>알고리즘을 제대로 공부하고 코딩테스트를 정복하고 싶다는 마음이 들어서 이 강의를 시작했고, 알고리즘에 시간을 못 쏟아부은 것 같아서, 또 그렇게 마음먹기가 힘들어서 스파르톤에 참여했다. 내 목표를 위해서 밤새 몰입했었고, 스파르타 코딩클럽의 지원과 같이 참여한 다른 <code>용사님</code> 들 덕분에 지치지 않고 마무리 할 수 있게 된 것 같았다.</p><p>새벽에 잠시 나와 걸으면서, 그리고 이 글을 쓰면서 느낀 것 중에 하나는,  밤새 코딩한 기억은 잊혀지지 않을 것 같다는 것이다.</p><p> 앞으로도 계속 무엇인가에 열정을 갖고 싶다는 마음이 들었고, 새벽이 어떻게 가는지도 모른채 어떤 일에 몰입하고 싶어졌다. 동시에 그런 일을 할 수 있는 곳에 가고 싶어졌고, 그렇게 환경을 만들어 주는 곳에 가고 싶어졌다.</p><p>강의와 스파르톤을 통해서 코딩테스트 실력을 길러보고 테스트해봤는데, 일에 대한 기본적인 마인드와 내가 정말 원하는 일, 그리고 직장에 대해서 다시 한번 생각해 보게 되는 계기가 되었던 것 같다.</p><p>스파르톤은 기회가 된다면 또 다시 참여해보고 싶다. 일에 지쳐있었는데 다시 뜨거워져 볼 수 있는 기회가 됐었다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/09/04/sparton/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Airflow Workers on ECS Fargate</title>
      <link>http://tkdguq05.github.io/2021/08/28/airflow-ecs/</link>
      <guid>http://tkdguq05.github.io/2021/08/28/airflow-ecs/</guid>
      <pubDate>Sat, 28 Aug 2021 02:27:55 GMT</pubDate>
      <description>
      
        &lt;p&gt;Airflow 워커를 ECS에 띄워서 연결해보자, MWAA흉내내기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Airflow 워커를 ECS에 띄워서 연결해보자, MWAA흉내내기</p><a id="more"></a><h2 id="기존-구조"><a href="#기존-구조" class="headerlink" title="기존 구조"></a>기존 구조</h2><p>기존에 사용하던 구조는 굉장히 구식이었다고 볼 수 있었습니다. 전임자가 만들어 놓고 나간 구조인데, Airflow의 main서버와 metadb를 AWS에 각각 인스턴스로 올리고, Auto Scaling 그룹으로 워커들을 띄워놓고 연결한 구조였습니다. 버전도 1.10.9 버전이라 잔 버그 같은 이슈도 있어보였고, 워커들을 좀 더 심플하게 관리하고 싶었습니다. 아무래도 인스턴스 비용이 나가다보니 부담스러운 점도 있었구요.</p><img src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/04/17/sagemaker-airflow-2.gif" alt="기존의 구조" style="zoom:80%;" /><p>마침 ECS관련해서 작업을 하고 마무리 짓던 중이었고, 워커들을 ECS Fargate로 변경해볼까란 생각이 들었습니다. 워커들은 이미 도커 이미지로 띄워져서 실행되고 있는 상태였기 때문에, 이미지만 Task Definition으로 넣어서 서비스를 만들어주면 간단하게 끝나지 않을까?? 생각했습니다. <del>잔 이슈 때문에 한 1-2주정도 걸리게 되었습니다.</del></p><h2 id="ECS란"><a href="#ECS란" class="headerlink" title="ECS란"></a>ECS란</h2><p>AWS에서 제공하는 ECS는 쿠버네티스와 유사한 부분이 많습니다. 쿠버네티스를 조금이라고 공부해 보신 분이라면, 간단한 버전의 ECS라고도 느끼실 것 같습니다. </p><p>먼저 ECS는 Elastic Container Service의 약자로 AWS에서 제공 하고 있는 컨테이너 오케스트레이션 서비스입니다. 컨테이너 오케스트레이션 서비스에는 대표적으로 쿠버네티스, Docker Swarm 등이 있습니다. 물론 AWS에도 쿠버네티스 제품, EKS가 나와 있지만 ECS는 좀 더 간소화되어 있고, 심플한 버전의 컨테이너 오케스트레이션 서비스라고 생각되네요. </p><p>ECS를 사용하게 되면 클러스터를 관리하기 위한 별도의 인스턴스를 구성, 관리하지 않아도 되고, 클러스터 관리에 대한 비용도 없습니다. 그래서 저희 다른 팀원 분이 인스턴스에 올린 API서버를 ECS로 변경했을때, 비용 감소 효과가 발생한 것 같습니다. 서버관리를 해주니까, 관리포인트도 적어지고 오토스케일링까지 지원이 됩니다. 또한 쿠버네티스에 비해 학습 시간이 매우 적기 때문에 금방 서치해서 원하는 서비스를 빠르게 올릴 수 있는 장점이 있습니다.</p><h2 id="ECS의-기본-개념"><a href="#ECS의-기본-개념" class="headerlink" title="ECS의 기본 개념"></a>ECS의 기본 개념</h2><p>ECS의 기본 개념으로는 클러스터, 작업정의(Task Definition), Task, Service가 있습니다. 하나씩 자세히 알아가도록 하겠습니다.</p><p><img src="https://kb.novaordis.com/images/4/43/AmazonECSConcepts.png" alt="ECS의 구조"></p><h3 id="클러스터-Cluster"><a href="#클러스터-Cluster" class="headerlink" title="클러스터(Cluster)"></a>클러스터(Cluster)</h3><p>클러스터는 도커 컨테이너를 실행할 수 있는 논리적인 공간입니다. 사용하려는 도커 이미지는 컨테이너에서 실행되는데, 이 컨테이너가 실행되는 인스턴스들을 묶어놓은 것이 클러스터입니다. 그냥 논리적인 공간이기 때문에 빈 클러스터도 생성이 가능합니다. airflow를 ECS에 올릴 때도 클러스터는 비어있는 상태로 올렸습니다. 만약에 빈 클러스터가 아니라 몇 몇개의 인스턴스를 만들어서 클러스터를 생성한 경우, ECS Agent에 의해서 클러스터와 인스턴스는 연결됩니다. </p><h3 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h3><p>태스크 데피니션, 작업 정의는 실제 컨테이너를 구성하는 것이라고 볼 수 있습니다. ECS는 기본적으로 ECR 레포지토리와 연동이 되어 있는데, 이 레포지토리에서 어떤 이미지를 사용할 것인지, 포트는 몇번을 열 것인지, 환경 변수는 어떤 것을 줄 것인지 등등을 설정할 수 있습니다. </p><p>docker run에서 사용했던 명령어들이 있다면 여기서 설정하면 됩니다. 처음에 이걸 잘 몰라서 고생을 꽤나 했는데, 작업 정의를 통해 설정하면 끝입니다. 작업 정의는 버전 관리가 가능하다는 장점이 있습니다. 만약에 새로 작업정의를 만들어서 배포를 했는데 원하는 결과가 제대로 나오지 않는다면, 이전 버전으로 롤백하면 됩니다. 이 구성이 정말 잘 되어 있어서, Airflow worker들을 ASG로 만들었을 때보다 훨씬 안정적으로 서비스를 운영할 수 있는 것 같네요.</p><h3 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h3><p>태스크는 작업 정의에 의해 만들어진 컨테이너의 셋들이며, ECS에서 컨테이너를 실행하는 최소 단위입니다. 한 태스크에는 1개 이상의 컨테이너를 구성할 수 있고, 해당 Task 내의 컨테이너는 모두 같은 ECS 클러스터 인스턴스 또는 Fargate 내에 실행되도록 보장 받습니다. 하지만 저는 한 태스크에 한 컨테이너만 실행되도록 했습니다. 여러 컨테이너를 docker-compose같이 사용하는 것도 가능하다고 합니다. </p><blockquote><ul><li>태스크에선 익숙한 EC2와 함께 Fargate가 등장합니다.</li></ul><p>💡<strong>Fargate로 설정하면</strong>! ECS 클러스터내에 인스턴스가 없어도, Task에 정의한 CPU, 메모리 설정에 따라 관리하는 EC2 인스턴스 없이 Serverless 하게 서비스를 실행할 수 있습니다. </p><p>Bespin Global 설명 : <a href="https://aws.amazon.com/fargate/">AWS </a><a href="https://aws.amazon.com/fargate/">Fargate</a> 는 AWS에 컨테이너를 배포하는 쉬운 방법입니다. 간단히 말하면 Fargate는 EC2와 비슷하지만 가상 시스템을 제공하는 대신 컨테이너를 얻습니다. 기본 인스턴스를 관리 할 필요없이 컨테이너를 기본 계산 프리미티브로 사용할 수있게 해주는 기술입니다. 컨테이너 이미지 작성, CPU 및 메모리 요구 사항 지정, 네트워킹 및 IAM 정책 정의 및 실행 만하면됩니다. Fargate를 사용하면 응용 프로그램 요구 사항과 밀접하게 일치하는 유연한 구성 옵션을 사용할 수 있으며 초 단위로 세분화됩니다.</p></blockquote><h3 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h3><p>서비스는 태스크의 라이프 사이클을 관리해주는 역할을 합니다. 태스크의 상태를 지속적으로 감시, 관리해주는 것이라고 볼 수 있습니다. 또한 태스크를 클러스터에 몇 개 배포할 것인지를 결정하고, 서비스를 만들때 어떤 작업정의로 태스크를 생성할지 정할 수 있습니다. </p><p>만약 태스크가 어떤 문제로 중지되거나 다운되면 이것을 감지해서 새로운 Task를 클러스터에 배포하게 하는 역할도 수행합니다. 실제로 서비스에 동작하고 있는 Task를 강제로 중지시킬 경우에 Task는 잠시동안 중단(종료)되었다가 다시 프로비저닝 받아서 생성되는 것을 확인할 수 있습니다. 따라서 테스트를 위해서 ECS를 켜두고 만들어 놓은게 아까워서 중지시켜버리면 다음날에 큰 혼란이 생길 수 있습니다. 저는 어차피 작업정의를 사용해서 서비스를 만들 수 있으니, 깔끔하게 종료하고 다시 만들었습니다. 한 번 크게 데일뻔한 경험이 있었습니다.</p><h2 id="Airflow-Workers-On-ECS"><a href="#Airflow-Workers-On-ECS" class="headerlink" title="Airflow Workers On ECS"></a>Airflow Workers On ECS</h2><p>이제 본격적으로 에어플로우의 워커들을 ECS에 올려보도록 하겠습니다. 먼저 클러스터를 만들어주겠습니다. </p><h3 id="클러스터-생성"><a href="#클러스터-생성" class="headerlink" title="클러스터 생성"></a>클러스터 생성</h3><p>저는 빈 깡통의 클러스터가 필요했습니다. ASG로 되어 있는 인스턴스를 Fargate로 변경하고 인스턴스 비용을 감소하려는 목적이 있기 때문에 빈 클러스터인 네트워킹 전용 클러스터를 만들어줍니다. </p><img src="/images/airflow-ecs/ecs-cluster01.png" alt="ecs-cluster01" style="zoom:80%;" /><p>그 다음은 빈 깡통답게 이름만 적어주고 VPC를 만들어줄 것인지, 태그를 넣을 것인지만 설정하면 됩니다. 전 이름만 설정하고 다른 건 건드리지 않고 그냥 생성했습니다.</p><h3 id="Task-Definition-생성"><a href="#Task-Definition-생성" class="headerlink" title="Task Definition 생성"></a>Task Definition 생성</h3><p>이제 작업정의를 만들어볼 시간입니다. 작업정의 생성을 누르면 <code>시작 유형 호환성 선택</code>이 나옵니다. 저는 Fargate로 만들어보겠습니다. </p><p><img src="/images/airflow-ecs/task_definition_01.png" alt="task_definition_01"></p><p>그 다음 단계를 누르면 본격적으로 작업 정의를 구성할 수 있습니다. 작업 역할에는 <code>ecsTaskExecutionRole,</code> <code>ecsS3FullAccessTaskRole</code>, <code>ecsTaskInstanceRole</code> 이 있습니다. 원하는 목적에 맞게 설정하시면 됩니다. 저는 ecsTaskExecutionRole를 선택하겠습니다. 작업 실행 IAM도 같이 세팅해주시고 다음으로 넘어갑니다.</p><p><img src="/images/airflow-ecs/task_definition_02.png" alt="task_definition_02"></p><p>이제 작업 크기가 어떻게 되는지, 작업 CPU가 얼마나 필요한지 설정해줘야 합니다. Fargate를 사용하기 때문에 사용량을 결정해야 하는 것입니다. 만약에 인스턴스를 선택한다면, 필요한 인스턴스의 크기를 골라주면 되겠습니다. 저는 기존에 사용하던 인스턴스 크기를 알기 때문에 해당 인스턴스의 스펙을 넣어주었습니다. 이제 가장 중요한 컨테이너 정의를 해보겠습니다.</p><p><img src="/images/airflow-ecs/task_definition_03.png" alt="task_definition_03"></p><p>컨테이너 추가를 누르면 다음과 같은 화면이 등장합니다. 컨테이너 이름에 원하는 이름을 넣어주시고, 이미지에는 ECR에 올려둔 이미지의 주소와 이미지명, 태그를 예시와 같이 넣어주시면 됩니다. 메모리 제한은 사용하는 메모리의 제한 정도를 뜻하는데 저는 위에서 넣어준 작업 메모리의 80%정도를 넣어놨습니다.  그리고 포트 매핑에는 열어줄 포트를 기입하면 됩니다. 만약에 <code>docker run -p 8793:8793</code>이런 식으로 docker run을 한다면 8793을 넣어주면 됩니다. 굉장히 편리하죠??</p><p><img src="/images/airflow-ecs/container_01.png" alt="container_01"></p><p>그 밑에 고급 컨테이너 구성으로 넘어가면 상태검사 할 부분을 넣어주면 되는데 워커에는 딱히 상태검사할 게 없기 때문에 패스하겠습니다. </p><p><img src="/images/airflow-ecs/container_02.png" alt="container_02"></p><p>환경 부분으로 넘어가면 사용할 CPU단위와 GPU가 나옵니다. GPU는 사용하지 않기 때문에 넘어갔고, CPU는 위에서 작성한 CPU코어 정도를 넣어놨습니다. 그리고 제가 가장 아쉬워 하는 부분인 진입점과 명령, 작업 디렉토리 부분입니다. 쌩 번역체를 사용한 것으로 보이는 이 부분은 바로 도커의 ENTRYPOINT와 CMD, WORKDIR를 나타내는 말이였습니다. 처음에 이 부분을 몰라서 헤맸는데, 헤매지 마시고 사용하는 명령어가 따로 있으시다면 넣어주시면 됩니다. 그리고 환경변수에는 key와 value로 나와있는데 docker run을 할때 <code>-e</code>로 넣어주는 파라미터가 있다면 여기에 넣어주시면 됩니다. 저는 <code>AIRFLOW_HOME</code>, <code>queue</code> 정도를 넣어놨습니다. </p><p>이제 맨 하단에 도커 레이블을 작성해주면 끝입니다. </p><p>다시 작업 정의쪽으로 나와서 맨 밑으로 내려가면 Tag가 있는데 저는 이름으로 구분해야 될 게 있었기 때문에 태그에 Name을 넣어주고 원하는 이름을 넣어줬습니다. 그리고 생성하면 작업 정의:1 버전이 나옵니다!</p><h3 id="Service-생성"><a href="#Service-생성" class="headerlink" title="Service 생성"></a>Service 생성</h3><p><img src="/images/airflow-ecs/service_01.png" alt="service_01"></p><p>시작 유형에서 우리가 원하는 Fargate를 선택하고 작업정의에 아까 만든 것을 선택해줍니다. 개정은 revision으로 수정된 버전을 말합니다. 기본값은 latest로 최근에 만든 것을 반영합니다. 클러스터도 만든 것을 넣어주고 서비스 이름을 넣어줍니다. </p><p>Task 개수는 원하는 만큼 넣어주면 됩니다. 최소 정상 상태 백분율은, 기본적으로 띄워져 있는 Task의 상태를 말하는데, 문서에는  <code>배포 과정에서 실행 중인 작업의 개수에 하한선을 제공하여 추가 클러스터 용량을 사용하지 않고도 배포할 수 있게 해줍니다.</code>라고 나와있습니다. 그러니까 기본 값인 100이라고 설정하게 되면 위에서 설명한대로, 태스크를 중지해도 계속 올라오게 되는 것입니다. </p><p><img src="/images/airflow-ecs/segvice_02.png" alt="service_02"></p><p>다음으로 넘어가면 보안그룹과 VPC를 설정할 수 있습니다. 사용하는 VPC를 넣어주고 보안그룹에는 Airflow worker를 위한 보안 그룹을 넣어주면 됩니다. Airflow worker의 보안그룹은 메인서버와 메타디비를 연결해주는 부분만 열어주면 되겠습니다. Airflow는 기본적으로 flask를 통해 8793포트로 열리고 열린 8793포트를 통해서 작업을 할당받으며, 작업로그를 메타디비(mysql)에 3306포트를 통해 남기므로 메인서버와 8793을 매핑해주고 메타디비와 3306(mysql)로 매핑해주겠습니다. </p><p>로드 밸런싱은 필요하다면 미리 LB를 만들어놓고 넣어주면 되지만, Airflow에는 LB가 따로 필요하지 않습니다. 메시지 브로커인 Redis를 통해서 작업을 워커에서 받아가기 때문입니다. 그냥 airlfow.cfg에 메인서버 아이피와 metaDB 아이피를 적어주면 알아서 가져가는 것을 보실 수 있습니다. </p><p>오토스케일링을 패스하고 만들어주면? 서비스가 만들어졌습니다.</p><p>이제 클러스터화면에서 작업 탭을 누르면 Task와 상태를 확인하실 수 있습니다. </p><p><img src="/images/airflow-ecs/ecs-service-complete.png" alt="ecs-service-complete"></p><p>여기서 원하는 작업을 이름으로 선택하고 log로 들어가면, 컨테이너에서 발생하고 있는 이력들을 확인하실 수 있습니다. 이 곳에서 airflow가 잘 설치가 됐는지 정상적으로 동작이 되고있는지, 포트는 제대로 열려서 작업을 받고있는지 등등을 체크할 수 있습니다.  이 로그는 CloudWatch에 로그그룹에서 더 자세하게 확인할 수 있습니다.</p><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><p>이제 제가 겪은 자잘한 에러들에 대한 해결에 대해서 설명드리겠습니다. 알고보면 간단한데 해결을 못해서 쩔쩔매느라 아까운 시간을 다 보냈습니다… 저 같이 시간을 버리시지 말기를 바라며…</p><h3 id="Airflow-initdb시-SqlAlchemy-ModelSchema-Not-found-error"><a href="#Airflow-initdb시-SqlAlchemy-ModelSchema-Not-found-error" class="headerlink" title="Airflow initdb시 SqlAlchemy ModelSchema Not found error"></a>Airflow initdb시 SqlAlchemy ModelSchema Not found error</h3><p>작업 로그를 보던 중에 airflow initdb를 할 때마다 ModelSchema 관련 에러가 발생하는 것을 확인했습니다. SqlAlchemy부분에서 에러가 나길래 열심히 구글링을 해보니, 제가 사용하고 있는  <code>airflow 1.10.9</code>버전에서 사용하는 sql_alchemy, marshmallow 등의 버전이 맞지 않아 발생하는 문제였습니다. 버전이 바뀐 이후부터 ModelSchema를 불러오는 메서드의 디렉토리 구조가 변경되었는데, 이 때문에 메서드를 제대로 불러오지 못해 발생한 것이었습니다. </p><p>requirements.txt에 다음과 같이 설정하면 해결이 가능합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#requirements.txt</span></span><br><span class="line">marshmallow==2.21.0</span><br><span class="line">marshmallow-enum==1.5.1</span><br><span class="line">marshmallow-sqlalchemy==0.22.3</span><br></pre></td></tr></table></figure><h3 id="hostname-resolver-py-문제"><a href="#hostname-resolver-py-문제" class="headerlink" title="hostname_resolver.py 문제"></a>hostname_resolver.py 문제</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests.exceptions.ConnectionError: HTTPConnectionPool(host&#x3D;&#39;169.254.169.254&#39;, port&#x3D;80): Max retries exceeded with url: &#x2F;latest&#x2F;meta-data&#x2F;local-ipv4 (Caused by NewConnectionError(&#39;&lt;urllib3.connection.HTTPConnection object at 0x7f3e4d9966d8&gt;: Failed to establish a new connection: [Errno 22] Invalid argument&#39;,))</span><br></pre></td></tr></table></figure><p>작업 로그에서 또 다시 에러가 발생해서 확인해보니 위의 에러가 발생하고 있었습니다. 169.254.169.254는 AWS의 meta데이터가 담겨있는 곳인데 이 곳에 접근하지 못해서 발생하는 에러였습니다. 기존 ASG워커에서는 metadata를 통해 EC2의 privateIP를 갖고와야 해당 IP에 작업내용을 전달할 수 있었습니다. 그렇지 않으면 docker 컨테이너의 호스트명만 갖고오게 되어 작업이 제대로 실행되지 않습니다. 하지만 ECS에서 Fargate는 docker의 컨테이너 호스트명을 부여하는 대신 privateIP를 부여합니다. <a href="https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/userguide/fargate-task-networking.html">AWS 문서</a> <code>기본적으로 모든Amazon ECS작업Fargate에는 기본 프라이빗 IP 주소가 포함된 elastic network interface (ENI) 가 제공됩니다.</code> </p><p>물론 ECS에서 metadata를 활성화 하는 것도 한 방법이겠으나, Private IP가 제공되니, 이 IP를 갖고오면 작업을 바로 할당받을 수 있으므로, 굳이 metadata를 이용할 필요가 없었습니다. 따라서 hostname_resolver.py의 파일안의 resolve함수의 내용을 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resolve</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">return</span> socket.gethostbyname(socket.gethostname())</span><br></pre></td></tr></table></figure><p>이런식으로 사용해서 privateIP를 전달했습니다. </p><h3 id="airflow에서-emr에-작업-실행시-권한-문제"><a href="#airflow에서-emr에-작업-실행시-권한-문제" class="headerlink" title="airflow에서 emr에 작업 실행시 권한 문제"></a>airflow에서 emr에 작업 실행시 권한 문제</h3><p>이제 정말 다 되었다고 생각하고 airflow에 EMR에 실행하는 작업을 내렸습니다. 하지만 또 다시</p><p><code>ERROR - An error occurred (AccessDeniedException) when calling the RunJobFlow operation: User: arn:aws:sts::xxxxx:assumed-role/ecsTaskExecutionRole/xxxxxxxx is not authorized to perform: elasticmapreduce:RunJobFlow on resource: arn:aws:elasticmapreduce:ap-northeast-2:xxxxxx:cluster</code></p><p>권한이 없다는 얘기가 나왔습니다. 다행히 로그에 권한 문제라고 써있어서, 쉽게 해결할 수 있었습니다. IAM에서 <code>ecsTaskExecutionRole</code> 를 찾았고, 여기에 기존에 사용하던 권한을 넣어주니, EMR에서 작업이 실행되고 EMR 클러스터도 컨트롤 하는 것을 확인할 수 있었습니다. </p><hr><p>이외에도 자잘한 이슈들이 많았지만, 너무 자잘해서 글로 옮기는게 민망해 올리지는 않았습니다. 이렇게 1-2주간의 삽질이 글 한편으로 정리가 되었습니다. 어려움도 많았지만 이를 통해서 ECS를 자세하게 배워보고 활용할 수 있었던 기회였던 것 같아 보람있었네요. ECS로 바꾸게 되면서, 여러 기능적 문제 때문에 점점 Kubernetes로 바꾸어야 겠다는 생각이 많이 들기 시작했습니다. 한 달 이내에는 Airflow를 Kubernetes에 올리는 글을 쓰게 되지 않을까 싶습니다. 읽어주셔서 감사합니다. 부족한 부분은 댓글로 남겨주세요!</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><ul><li><a href="https://boostbrothers.github.io/technology/2020/01/29/AWS-ECS-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0.html">https://boostbrothers.github.io/technology/2020/01/29/AWS-ECS-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0.html</a></li><li><a href="https://wooono.tistory.com/133">https://wooono.tistory.com/133</a></li><li><a href="https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/userguide/fargate-task-networking.html">https://docs.aws.amazon.com/ko_kr/AmazonECS/latest/userguide/fargate-task-networking.html</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/08/28/airflow-ecs/#disqus_thread</comments>
    </item>
    
    <item>
      <title>yml 파일을 잘 다뤄보자</title>
      <link>http://tkdguq05.github.io/2021/08/11/yaml-break/</link>
      <guid>http://tkdguq05.github.io/2021/08/11/yaml-break/</guid>
      <pubDate>Wed, 11 Aug 2021 08:45:07 GMT</pubDate>
      <description>
      
        &lt;p&gt;yml파일을 갖고 놀아보자&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>yml파일을 갖고 놀아보자</p><a id="more"></a><h2 id="Yaml"><a href="#Yaml" class="headerlink" title="Yaml"></a>Yaml</h2><blockquote><p><strong>YAML</strong>은 <a href="https://ko.wikipedia.org/wiki/XML">XML</a>, <a href="https://ko.wikipedia.org/wiki/C_(프로그래밍_언어)">C</a>, <a href="https://ko.wikipedia.org/wiki/파이썬">파이썬</a>, <a href="https://ko.wikipedia.org/wiki/펄">펄</a>, <a href="https://ko.wikipedia.org/w/index.php?title=RFC2822&action=edit&redlink=1">RFC2822</a>에서 정의된 e-mail 양식에서 개념을 얻어 만들어진 ‘사람이 쉽게 읽을 수 있는’ 데이터 직렬화 양식이다. 2001년에 <a href="https://ko.wikipedia.org/w/index.php?title=클라크_에반스&action=edit&redlink=1">클라크 에반스</a>가 고안했고, Ingy dot Net 및 Oren Ben-Kiki와 함께 디자인했다. <a href="https://ko.wikipedia.org/wiki/YAML">위키백과</a></p></blockquote><p>YAML의 이름은 YAML Ain’t Markup Language 라는 이름에서 유래되었고 원래의 뜻은 Yet Another Markup Language로 또 다른 마크업 언어였으나, 핵심은 문서 마크업이 아닌 데이터 중심에 있습니다. XML과 Json이 데이터 직렬화에 주로 사용되고 있는데, 많은 사람들이 YAML을 가벼운 마크업 언어로 사용하려고 하고 있습니다.</p><p><code>yaml이 있는데 yml은 뭐지?</code></p><p>yaml은 공식 확장자이며 그 외의 확장자로 yml도 사용됩니다. 옛날에는 파일의 확장자 길이가 3자로 제한되었기 때문에 3글자 확장자 스타일을 고수하는 사람들 때문에 yml파일 확장자가 보이는 것 같습니다.</p><h2 id="왜-YAML을-사용할까"><a href="#왜-YAML을-사용할까" class="headerlink" title="왜 YAML을 사용할까?"></a>왜 YAML을 사용할까?</h2><p>yaml을 사용하는 이유는 간단합니다. 한 파일에서 모든 configuration을 관리하기 위해서 입니다. 혹시 yaml을 잘 접해보지 못하신 분들이 있다면, 블로그 파일구조를 살펴보면 좋을 것 같습니다. 저의 경우는 hexo를 사용하고 있는데, icarus theme를 들어가 보면 _config.yml파일로 이 테마의 구성요소를 관리할 수 있는 파일이 있습니다. 이 파일을 통해 글또 아이콘을 등록한다던가 category페이지를 오른쪽이나 왼쪽으로 옮기는 등의 작업을 간단하게 처리할 수 있습니다.</p><p>이것을 실무에도 적용하고 있는데, 저희 회사의 경우에는 Airflow의 추천 DAG와 Segmentation DAG들을 한 파일로 관리하기 위해 yaml파일을 만들어서 처리하고 있습니다. 고객사에 대한 정보와 고객사에 필요한 서버의 스펙이나 작업 메모리들을 yaml에  넣어서 굉장히 편리하게 작업 및 세부 리소스들을 관리할 수 있게 되었습니다. yaml과 github action 그리고 쉘 스크립트 등을 이용해서 작업을 하고 있는데, 기회가 되면 더 자세하게 소개해보도록 하겠습니다. 다른 회사의 경우에는 일반적으로 등장하는 것이 아마 kubernetes일 것입니다. kubernetes에서는 대부분의 정보를 yaml파일로 만들어서 kubectl에 제공합니다. 저희가 실무에서 사용하는 것과 비슷하게 kubernetes에서도 편리하게 컨테이너 이미지나 기타 설정을 편리하게 할 수 있겠습니다.</p><h2 id="YAML-문법"><a href="#YAML-문법" class="headerlink" title="YAML 문법"></a>YAML 문법</h2><p>YAML은 기본적으로 사람 눈으로 보기가 편합니다. 간단한 문법을 익히고 난다면, python 코드나 다른 코드들에 비해서 가독성이 훨씬 좋아 눈으로 어떤 작업을 할지 알아보기가 더 좋을 것이라고 생각됩니다. 그러면 yaml 문법들을 살펴보겠습니다!</p><h3 id="주석과-기본-문법"><a href="#주석과-기본-문법" class="headerlink" title="주석과 기본 문법"></a>주석과 기본 문법</h3><p>yaml은 python과 마찬가지로 <code>#</code>을 주석을 다는 문자로 사용합니다. 코드 앞에 #표시를 해주면 그 라인은 읽지 않습니다. 또한 python과 비슷하게 indentation에 민감합니다. 파이썬에서는 tab(space 4번)을 주로 사용하지만 yaml에서는 기본이 스페이스바 두 번 입니다. 기본적인 문법은 <a href="https://subicura.com/k8s/prepare/yaml.html#%E1%84%80%E1%85%B5%E1%84%87%E1%85%A9%E1%86%AB%E1%84%86%E1%85%AE%E1%86%AB%E1%84%87%E1%85%A5%E1%86%B8">쿠버네티스 안내서</a> 에 너무 잘 정리가 되어있어서, 링크로 대체하겠습니다. 한 번 읽고 스크롤을 내려주시면 감사하겠습니다. </p><h3 id="리스트"><a href="#리스트" class="headerlink" title="리스트"></a>리스트</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="comment"># block format</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">apple</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">banana</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">kiwi</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="comment">#inline format</span></span><br><span class="line"><span class="string">[apple,</span> <span class="string">banana,</span> <span class="string">kiwi]</span></span><br></pre></td></tr></table></figure><p>리스트의 형식은 두 가지로 나뉘어 집니다. block format과 inline format입니다. 저는 사실 이 포맷을 굉장히 오래 전 부터 봐왔습니다. hexo로 포스팅을 하다보면, 맨 위에 사이드 바나 태그, 카테고리를 정하는 부분이 있는데, 이 부분이 바로 yaml의 리스트 형식을 사용하고 있습니다.</p> <img src="/images/yaml/hexo-yaml.png" alt="hexo-yaml" style="zoom:50%;" /><p>예전에는 뭔지도 모르고 사용했었는데, 다시 살펴보니 구조가 눈에 들어오기 시작합니다.</p><h3 id="해시"><a href="#해시" class="headerlink" title="해시"></a>해시</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="comment"># Block</span></span><br><span class="line"><span class="attr"> name:</span> <span class="string">sanghyub</span></span><br><span class="line"><span class="attr"> age:</span> <span class="number">29</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">--</span> <span class="comment"># Inline</span></span><br><span class="line"><span class="string">&#123;name:</span> <span class="string">sanghyub,</span> <span class="attr">age:</span> <span class="number">29</span><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p>해시는 키-밸류 구조라고 보시면 됩니다. 이것도 역시 Block, Inline방식이 있습니다. 저는 개인적으로 Block방식을 선호해서 주로 사용하고 있는데, 편한 걸 사용하시면 됩니다.</p><h4 id="리스트와-해시"><a href="#리스트와-해시" class="headerlink" title="리스트와 해시"></a>리스트와 해시</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">men:</span> <span class="string">[sanghyub,</span> <span class="string">lee]</span></span><br><span class="line"><span class="attr">women:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">sanghyub</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">lee</span></span><br></pre></td></tr></table></figure><p>만약 키에 여러 값들을 넣고 싶다면 위와 같은 방식으로 넣을 수 있습니다.</p><h4 id="해시의-리스트"><a href="#해시의-리스트" class="headerlink" title="해시의 리스트"></a>해시의 리스트</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">-</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">messi</span></span><br><span class="line"><span class="attr">  assists:</span>  <span class="number">61</span></span><br><span class="line"><span class="attr">  goals:</span>  <span class="number">99</span></span><br><span class="line"><span class="bullet">-</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">Son</span></span><br><span class="line"><span class="attr">  assists:</span>  <span class="number">77</span></span><br><span class="line"><span class="attr">  goals:</span><span class="number">100</span></span><br></pre></td></tr></table></figure><p>반대로 하고 싶다면 이런 방식을 사용하면 됩니다. 이해하기 어렵다면, 직접 파일을 만들어서 확인해보겠습니다.</p><p><img src="/images/yaml/yaml-code.png" alt="python으로 값을 확인해보자"></p><p>python으로 pyyaml을 불러와서 확인해보면 값이 위와 같이 들어가 있는 것을 확인해 볼 수 있습니다.</p><h3 id="상속"><a href="#상속" class="headerlink" title="상속"></a>상속</h3><p>yaml에서 재밌는 부분은 상속에 대한 부분입니다. 사실 이번 포스팅에서 주로 다루고 싶었던 것이기도 합니다. 위에서 말한 프로젝트를 진행했을 때에도, 이 상속을 처음 접하고 적용했습니다. 물론 처음엔 좀 헷갈렸지만 응용하면 코드를 적을 양이 효과적으로 줄어드는 것을 체감하실 수 있을 것입니다. 상속에서 중요한 것은 상속에 대한 문자와 Alias입니다. 미리 정한 양식에 별명을 붙이고 그 별명 붙인 것을 불러오면 내용을 그대로 사용할 수 있습니다. 굳이 똑같이 따라 칠 필요가 없어지는 것입니다.</p><h4 id="Alias"><a href="#Alias" class="headerlink" title="Alias"></a>Alias</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">b-anchor:</span> <span class="meta">&amp;name</span> <span class="string">value</span></span><br><span class="line"></span><br><span class="line"><span class="attr">b-alias:</span> <span class="meta">*name</span></span><br></pre></td></tr></table></figure><p>별명은 굉장히 간단합니다. &amp;로 별명을 붙여주면 끝입니다! 불러올 때는 *를 이용해서 값을 불러올 수 있습니다.</p><h4 id="상속-lt-lt"><a href="#상속-lt-lt" class="headerlink" title="상속(&lt;&lt;)"></a>상속(&lt;&lt;)</h4><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">maltese:</span> <span class="meta">&amp;dog</span></span><br><span class="line"><span class="attr">  cute:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">  bark:</span> <span class="string">"bow-wow"</span></span><br><span class="line"><span class="attr">jindol:</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*dog</span></span><br><span class="line"><span class="attr">  nationality:</span> <span class="string">"korea"</span></span><br><span class="line"><span class="attr">pug:</span></span><br><span class="line">  <span class="string">&lt;&lt;:</span> <span class="meta">*dog</span></span><br><span class="line"><span class="attr">  ugly:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>위의 예시를 사용해서 상속의 종합적인 설명을 해보겠습니다. 말티즈라는 키에 여러 값들을 담아놨습니다. 귀여움과 짖음을 넣어 놨습니다. 말티즈는 개라는 특성을 담아둘만 한 것 같아서 &amp;dog라고 별명을 붙여놨습니다. jindol이에 똑같은 개의 특성을 넣고 싶은데 타자 많이 치는 것은 싫어서 별명 붙인 것을 가져왔습니다. 그리고 추가적인 특성이 필요해서 값을 넣었습니다. pug도 마찬가지입니다. 기본적인 개의 특성은 별명을 불러와서 붙여넣었고 추가적인 특성을 붙여넣었습니다.</p><p>그리고 한 가지 더 알아야 할 점은, 이렇게 붙인 별명은 같은 디렉토리에서 공유가 가능하다는 것입니다. 즉, 한 yml파일에서 붙여놓은 별명을 다른 yml파일에서 *로 불러와서 사용이 가능합니다. </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#config.yml</span></span><br><span class="line"><span class="attr">EXECUTOR_SPEC:</span> </span><br><span class="line"><span class="number">10</span><span class="string">G-10G-15</span> <span class="string">:</span> <span class="string">&amp;10G-10G-15</span></span><br><span class="line"><span class="attr">  executor_memory:</span> <span class="number">10</span><span class="string">G</span></span><br><span class="line"><span class="attr">  driver_memory:</span> <span class="number">10</span><span class="string">G</span></span><br><span class="line"><span class="attr">  num_executors:</span> <span class="number">15</span></span><br><span class="line"><span class="number">20</span><span class="string">G-20G-30</span> <span class="string">:</span> <span class="string">&amp;20G-20G-30</span></span><br><span class="line"><span class="attr">  executor_memory:</span> <span class="number">20</span><span class="string">G</span></span><br><span class="line"><span class="attr">  driver_memory:</span> <span class="number">20</span><span class="string">G</span></span><br><span class="line"><span class="attr">  num_executors:</span> <span class="number">30</span></span><br><span class="line"><span class="number">20</span><span class="string">G-20G-50</span> <span class="string">:</span> <span class="string">&amp;20G-20G-50</span></span><br><span class="line"><span class="attr">  executor_memory:</span> <span class="number">20</span><span class="string">G</span></span><br><span class="line"><span class="attr">  driver_memory:</span> <span class="number">20</span><span class="string">G</span></span><br><span class="line"><span class="attr">  num_executors:</span> <span class="number">50</span></span><br><span class="line"><span class="number">50</span><span class="string">G-50G-50</span> <span class="string">:</span> <span class="string">&amp;50G-50G-50</span></span><br><span class="line"><span class="attr">  executor_memory:</span> <span class="number">50</span><span class="string">G</span></span><br><span class="line"><span class="attr">  driver_memory:</span> <span class="number">50</span><span class="string">G</span></span><br><span class="line"><span class="attr">  num_executors:</span> <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="attr">feather:</span> <span class="meta">&amp;feather</span></span><br><span class="line"><span class="attr">  exspec:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="string">*10G-10G-15</span></span><br><span class="line"><span class="attr">middle:</span> <span class="meta">&amp;middle</span></span><br><span class="line"><span class="attr">  exspec:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="string">*20G-20G-30</span></span><br><span class="line"><span class="attr">lightheavy:</span> <span class="meta">&amp;lightheavy</span></span><br><span class="line"><span class="attr">  exspec:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="string">*20G-20G-50</span></span><br><span class="line"><span class="attr">heavy:</span> <span class="meta">&amp;heavy</span></span><br><span class="line"><span class="attr">  exspec:</span></span><br><span class="line">    <span class="string">&lt;&lt;:</span> <span class="string">*50G-50G-50</span></span><br></pre></td></tr></table></figure><p>Airflow의 각 DAG에서 EMR 스펙을 다르게 사용하고 Airflow의 DAG를 종합하는 yml파일이 있다고 가정해 봅시다. 만약에 위와 같이 yml이 작성되어 있지 않았다면, 종합하는 yml파일의 고객사 DAG마다 executor, driver 메모리, 익스큐터 수를 일일이 지정해줘야 되었을 것입니다. 고객사가 100개라면? 100개씩 적는 노가다를 하다가 정신이 혼미해져서 휴먼에러가 발생할 가능성이 매우 높아질 수 있습니다. 그렇다면, Airflow를 통해 실행한 DAG들에 심각한 에러가 발생하고, 원인 파악하기가 매우 힘들어질 것입니다.(실제로 그래 봤습니다.) </p><p>좀 더 들어가면, 이 clients.yml과 종합하는 yml을 연결하기 위한 python파일들이 존재합니다. 이 파이썬 파일들을 이용해서 clients.yml에 있는 값들을 종합하는 yml에 옮겨주고 합쳐진 yml을 통해 완전해진 값들을 가지고 고객사들의 DAG들을 생성할 수 있게 되는 것입니다. </p><hr><p>고객사들의 DAG를 자동으로 생성해주는 방법에 대해서 궁금해지셨을 것 같습니다만, yaml글을 통해서 yaml을 한번 갖고 놀아보시고?! 다음 글을 기대해주시면 좋을 것 같습니다. 다음 글에서는 위에서 만든 yml파일과 종합하는 yml파일, 그리고 github action을 사용해서 Airflow의 DAG를 자동으로 만들어주고 배포하는 과정에 대해서 알아보겠습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/08/11/yaml-break/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글을 왜 써야 할까? - 고민과 정리의 시간</title>
      <link>http://tkdguq05.github.io/2021/08/01/geultto6/</link>
      <guid>http://tkdguq05.github.io/2021/08/01/geultto6/</guid>
      <pubDate>Sun, 01 Aug 2021 01:06:23 GMT</pubDate>
      <description>
      
        &lt;p&gt;글또 6기를 시작하며, 고민한걸 정리하기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>글또 6기를 시작하며, 고민한걸 정리하기</p><a id="more"></a><h2 id="글또"><a href="#글또" class="headerlink" title="글또"></a>글또</h2><p>2019년 7월에 입사를 하고 설레는 맘으로 글또 3기에 지원했을때가 생각난다. 페이스 북에 글또를 모집한다는 글을 보고 열심히 자기소개를 준비해서 구글 폼을 입력하고 결국 합격했다는 메일을 받았다. 당시에는 코로나가 없었을 때라, 잠실 쪽에 배민 작은 집을 빌려서(참가하신 다른 글또분의 도움으로) OT를 진행했다. 하필 맨 앞에 앉았어서 가장 처음으로 자기소개를 했는데, 첫 회사에 입사했다고 하니 다른 분들이 축하한다고 박수를 쳐주셨다. 이것이 글또에 대한 강렬한 첫인상이었다. </p><p>그 후 열심히 글또에 참여했다. 다른 사람에게 도움이 될 수 있도록, 많이 공부하고 정리해서 글을 작성했다. 다른 분들의 피드백과 글을 통해서 더 빠르게 성장할 수 있었던 것 같다. 그리고 2년이 지났다. 아직 나는 첫 회사에 그대로 남아있다. 내일 채움 공제 덕분이었다. 말그대로 존버 했더니, 어느새 오지 않을 것 같았던 마지막 납입을 하게 되었다. 기뻤다. 그런데 내가 지금 일을 통해 행복한게 아니라 내채공을 겨우 채워넣어서 행복한 느낌만 든 것 같았다. 처음 일을 시작했을 때는 일 자체가 너무 재밌었는데, 나는 지금 매너리즘에 빠진게 아닌가 하는 생각이 들었다.</p><br></br><h2 id="매너리즘"><a href="#매너리즘" class="headerlink" title="매너리즘"></a>매너리즘</h2><blockquote><p>매너리즘 : 예술 창작에 있어서, 늘 같은 수법(手法)을 되풀이하여 신선미(新鮮味)나 독창성을 잃는 일. 순화어는 `타성’.</p></blockquote><p>출근 하고 나서의 일과는 거의 반복된다. 몇몇 이슈 때문에 변주가 일어나긴 하지만, 대개는 거의 비슷한 일들이다. 비슷한 일들. 같은 일을 반복하기 때문에 매너리즘에 빠지게 된 것일까? 나는 언제부터 새로운 것을 시도해 보지 못하게 되었을까? 왜 도전하고 있지 못하는가? 왜 나는 흥미를 잃어가고 있는가? 등 여러 질문들이 머리에 스쳤고, 그 이유가 궁금해졌다. </p><p>먼저 다른 사람들도 2년차 쯤에 이런 고민을 많이 할까 궁금해졌다. <a href="http://www.recruittimes.co.kr/news/articleView.html?idxno=85481">한 기사</a>에서 직장생활 권태기에 대해 조사해 놓은게 있어서 봤는데, 흥미로웠다. 기사 내용을 보니 3년차 쯤에 권태기가 온다는 비율이 23.1% 였다. 나랑 비슷한 사람들은 18.9%였다. 이 사람들의 이유는 무엇일까. ‘반복되는 업무에 대한 지루함’(58.2%, 복수응답)이 가장 많았다. 나 역시 그런 것 같았다. 이어 과도한 업무량, 회사 비전의 불투명함 등이 공감되는 내용이었다. 권태기의 증상으로는 퇴사 충동이 가장 많았다. 스트레스 때문에 쉬고 싶다는 생각을 했었다. 회사를 그만둘까 까지는 가지 않았었는데. 육체적 질병도 권태기의 한 증상이었다. 사실 일을 시작하고 나서 목과 어깨가 너무 안 좋아졌고 그 외에 전반적인 몸 상태와 스트레스 관리가 제대로 되지 않고 있다. 코로나 이후에 운동을 제대로 하지 못해서 더 그런 것 같다. 육체적인 증상과 정신적인 스트레스가 겹치면서 업무에 대한 집중도도 많이 낮아진 것 같다. 하지만 가끔 재밌는 일이 생기면 힘든지도 모르고 계속 일을 붙잡고 있는 걸 보면, 정말 중요한 건 업무에 대한 흥미인 것 같았다. </p><h2 id="이유"><a href="#이유" class="headerlink" title="이유"></a>이유</h2><p>나의 진짜 이유를 생각해보려고 ‘이유’를 적어놓고 한참을 고민했다. 그러다가 알게 된 것은, 내 업무 환경이 너무 산만하다는 것이었다. 집중이 제대로 되지 않을 수 밖에 없는 환경이었다. 가만히 내가 일하는 것을 떠올려보니, 잡다한 이슈들이 많았다. 일을 조금 해보려고 하면, ‘지금 추천 아이템 잘 쌓여있는거 맞지?’, ‘API 제대로 동작하고 있나요?’, ‘신규 고객사 세팅 좀 부탁해’, ‘~건 관련해서 의견 부탁드려요’ 나 기타 회의 등등 바로바로 처리해야 할 것들이 자주 발생하고 있었다. 그 외에 신입으로 오신 분의 업무를 체크하고 잘 하고 계시는 지에 대해 파악을 해야해서 은근히 신경쓰이는 일도 존재했다. 기타 등등, 2년차 실무자가 이거까지 해야하나 싶기도 하지만 일단 내 일이라고 생각해서 하는 중이다. 이렇다 보니 진짜 내 업무를 하는 중에도 뭔가 불안한 마음이 들기 시작한 것 같았다. 업무 환경이 정리되고 내 일을 하면 되는데, 왠지 모를 불안함과 산만함 때문에 집중이 잘 되지 않았다. 집중이 잘 되지 않으니까, 새로운 프로젝트를 하기 힘들었고 하던 일만 계속하게 되는 악순환이 반복되고 있었다.</p><p><strong>또 다른 이유</strong>는 다른 문화를 가진 사람들이다. 현재 데이터 사이언스 팀에서 하는 일들을 다른 팀들에서는 잘 이해하지 못하거나 공감하지 못하는 일이 굉장히 자주 발생하고 있다. A/B테스팅 플랫폼이라던가 추천 모델에서 발생하고 있는 문제점들을을 공유해도 참 설득하는 게 어렵다. 오랜만에 페이스북을 보니, 한 아티클이 눈에 들어왔다. ‘사람들은 생각하기 귀찮아하고 한 번 믿으면 바꾸려하지 않는다.’, ‘사람들의 생각과 믿음을 바꾸는 데는 굉장한 에너지가 들고, 사람들은 자신의 생각이 틀렸다는 것을 알게 되는 것을 힘들어한다.’ 꼬젯님의 글이었다. 이 분도 글의 마지막에서 지치고있고 일 할 기력도 부족했다고 남기셨다. </p><p>나를 많이 도와주셨던 교수님이 하신 말씀이 생각난다. ‘앞으로 시간관리하고 일을 쪼개는 능력이 인생에서 참 중요해질 거야, 젊었을때는 그냥 밤을 새거나 해도 큰 무리가 없는데, 나이를 먹어갈수록 그게 잘 안되고 리소스는 한정되어 있다는 걸 느끼게 돼. 시간을 잘 사용하고 일의 우선순위를 나누는 걸 많이 훈련해놔야 할거야.’</p><p>지금과 같은 힘든 일도 거쳐가야할 성장통이겠지. 일의 우선순위를 잘 나눠서 하나하나 해결해가는 걸 연습해보는 시간이라고 생각해야 겠다.</p><br></br><h2 id="그럼에도-글을-쓰는-이유"><a href="#그럼에도-글을-쓰는-이유" class="headerlink" title="그럼에도 글을 쓰는 이유"></a>그럼에도 글을 쓰는 이유</h2><p>힘든 상황이고 글을 쓸 여유가 많이 없는 상황이다. 이런 와중에 슬랙에 글을 작성했다는 알람이 와서 다른 분들의(대부분은 다짐글) 글을 봤다. 앞으로의 작성할 글의 계획과 글또를 지원한 계기 등이 눈에 띄었다. 밝은 느낌의 글들이 참 많아서 읽으면서 나도 힘을내게 되었다. 그런데 나의 글을 다시보니 밝은 색감보다는 회색 빛에 가까운, 일에 쩌든 듯한 느낌이다. 첫 글부터 회색 빛이라 읽으시는 분들께 조금 미안한 마음이 들긴 하지만, 2년차나 또는 직장생활 중에 한 번은 마주할 수 있는 감정이라고 생각한다. 비슷한 감정을 가진 다른사람들이 이 블로그를 들락날락하다가 <strong>‘’이런 사람도 역시 있구나”</strong>하며 글을 통해 공감하고, 위로가 되었으면 한다. 같은 사람이기에 누군가의 감정 정리 방법이 다른 누군가에게도 적용이 되지 않을까 하는 생각이다.</p><p>개발 글 쓰기 모임이다. 개발과 관련된 글감을 찾아야 하기에, 공부해야 하고 정리하는 게 습관화 되어야 한다. 아마 이 생각마저 없었더라면 더 정체되었을 것 같다. 글또 6기에 참여할 생각은 진작부터 있었기에 노션에 개인 공부용으로 정리된 내용들이 보관되어 있어서 어느정도 보완하고 정리해 글을 만들어서 올릴 생각이다. Hadoop에 대한 기본 개념과 Spark와의 연결, 개념. 예전에는 이해하지 못했던 에러 로그들에 대해서 다뤄보려고 한다. Kubernetes에 대한 관심이 꺼지지 않고 있다. 사용 중인 Airflow를 Kubernetes로 옮겨보면서 개념을 익히고 여기에 대한 내용을 정리해보면 어떨까 생각하고 있다. MLOps에 대해서 꾸준히 다뤄 보고 싶어서 BentoML이나 KubeFlow 등을 살펴보고 있다. 그리고 개발하면서 마주하는 자잘한 이슈들과 성장한 내용들을 성실하게 작성해보려고 한다. </p><p><strong>다른 사람들에게 도움이 많이 되는 글을 작성하는 것</strong>이 이번 기수의 목표였다. 뿐만 아니라, <strong>글을 작성하는 행위가 매너리즘에 빠지지 않도록</strong>, 다른 분들 글을 관심있게 보면서 피드백을 드리고 나의 글에 반영해볼 생각이다. <strong>처음</strong>으로 다시 돌아가야겠다. 머리가 조금 커졌다고 어설프게 살면 안되겠다는 생각이 강하게 들었다. 열의를 가지신 글또의 다른 분들을 보면서 많이 배워나가고 성장해야겠다. 지쳐있어서 그 동안의 주말에 누워만 있었는데, 오늘부터는 뭔가 힘이 나기 시작한다. 오늘 해야할 일을 하나씩 해 나가면서 주말을 마무리하고, 한 주를 시작할 준비를 해야겠다. 글은 힘이된다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/08/01/geultto6/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또 5기를 끝내고, 회고하기</title>
      <link>http://tkdguq05.github.io/2021/05/02/geultto5-end/</link>
      <guid>http://tkdguq05.github.io/2021/05/02/geultto5-end/</guid>
      <pubDate>Sun, 02 May 2021 06:51:54 GMT</pubDate>
      <description>
      
        &lt;p&gt;글또 5기를 마치면서, 나는 어떻게 일하고 있는가…라고 하지만 넋두리 및 내 작은 목표에 대해서&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>글또 5기를 마치면서, 나는 어떻게 일하고 있는가…라고 하지만 넋두리 및 내 작은 목표에 대해서</p><a id="more"></a><h1 id="글또-5기가-끝났다"><a href="#글또-5기가-끝났다" class="headerlink" title="글또 5기가 끝났다"></a>글또 5기가 끝났다</h1><p>11월 부터 시작되었던 글또 5기가 끝이 났다. 어떻게 하다보니 해가 바뀌었고 벌써 5월이 되었고, 글또 5기도 끝이 나버렸다. 시간이 정말 빠르게 흘러가고 있다. 이렇게 속절없이 흘러가는 시간 속에서 글또 활동을 어떻게 해왔는지, 그리고 글또 외에 내가 어떻게 살아왔는지 회고해보는 시간을 가져보려고 한다.</p><p><br></br></p><h2 id="무엇을-썼을까"><a href="#무엇을-썼을까" class="headerlink" title="무엇을 썼을까"></a>무엇을 썼을까</h2><p>이번 기수에서는 Airflow와 관련된 글을 정말 많이 썼다. 글또에서 같은 채널의 온라인 회고시간을 가졌을 때도, Airflow 글 쓰는 사람으로 다 알고 계시는 것 같았다. 다른 글들도 많이 작성하고 싶었는데 업무 상… 다른 걸 공부할 시간이 없었다고 말하고 싶지만, 핑계처럼 보이기도 했다.</p><p><strong>아카이브를 보니 10개의 글을 작성했다.</strong></p><p><img src="/images/geultto5end/2020to2021.png" alt="5기에 작성한 글"></p><p>(글또5기 다짐하기 글이 짤렸음) 생각보다 많이 썼다. 글 쓴 것만 봐도 Airflow쳐돌이라는 것을 잘 알 수 있다. Airflow 글들과 그 배경지식을 위한 네트워크 글, 추천시스템 스터디를 하면서 쓴 FP-Growth, 추천 파이프라인 구성 글로 채워져 있다. 채널에 있는 분들에게는 매우 생소할 수도 있고, 그래서 오히려 재밌게 느껴질 수 있겠다는 것은 내 생각을 뿐이겠고, 실제로 피드백 들어오는 걸 보니까 무슨 말인지 몰라서 피드백 하기 어려워 하시는게 많이 느껴졌다. 그 분들에게는 핵노잼 글(ㅠㅠㅜ) 이었을 것 같아 가슴이 아프다. 엔지니어링 관련 글이 피드백 당첨되고 피드백을 해야 할 때면 참 막막했으리라… 물론 나름 쉽게 적는다고 쉽게 적고, 여러 짤을 사용해서 접근하기 쉽게 만들긴 했는데, 효과는 미미했던 것 같다. 하지만 알아주시는 분이 있어서 매우 감사하게 생각하고 그로 인해 힘이 많이 되었다. </p><p><img src="/images/geultto5end/thanks.png" alt="고마운 분들"></p><p><br></br></p><h3 id="이전-기수에서는"><a href="#이전-기수에서는" class="headerlink" title="이전 기수에서는"></a>이전 기수에서는</h3><p><img src="/images/geultto5end/geultto4was.png" alt="geultto4was"></p><p>이전 기수에서도 물론 엔지니어링 관련된 기본적인 내용을 많이 정리하고 작성했다. 하지만 확실히 5기에 비해서 논문 정리 내용이  몇 개 더 있는 게 확인된다(사진 길이 때문에 CRAFT 논문 요약이 짤림…). 4기 때는 딥러닝이나 모델링 관련해서도 서치를 많이 했고 엔지니어링 쪽에도 신경을 쓴 것 같다. 사진 찍으려고 아카이브를 다시 보다가 5기에 들어서면서 엔지니어링에 완전 집중한 게 확실히 느껴졌다.</p><p><br></br></p><h2 id="달라진-업무"><a href="#달라진-업무" class="headerlink" title="달라진 업무"></a>달라진 업무</h2><h3 id="일"><a href="#일" class="headerlink" title="일"></a>일</h3><p>이렇게 글이 달라진 이유는 뭐니뭐니해도 일 때문이다. 아무래도 일하면서 얻은 정보나 지식을 정리하게 되기 때문인 것 같다. 21년 2월 부터 직무가 데이터 사이언티스트에서 데이터 엔지니어로 옮겨졌다. 직무상 분류해 놓긴한 건데, 이로 인해서 데이터 엔지니어링 업무에 더 집중하게 된 것 같다. 책임감이 더 생겨서 그런 것일까? </p><p>아무튼 데이터 엔지니어로 변경된 후 부터 기존에 서비스 되고 있던 구조를 더 효율적인 구조로 변경하고, 확장하는 등의 프로젝트를 많이 진행하게 되었다. 이를 통해서 네트워크의 중요성을 느끼게 되었고 구조를 업그레이드를 하려면, 그 툴을 제대로 알아야 하기 때문에 사용하는 툴에 대해서 더 파고들어서 공부하게 되었다. 이렇게 탄생한게 바로 Airflow 시리즈 글…! <a href="https://tkdguq05.github.io/tags/airflow/">🌟Airflow 시리즈 글🌟</a></p><p><br></br></p><h3 id="일하는-방식"><a href="#일하는-방식" class="headerlink" title="일하는 방식"></a>일하는 방식</h3><p>데이터 엔지니어링에 집중하게 된 것은 조직의 변화가 있었기 때문이다. 5기때에는 성장통이라고 해야 할까, 빠른 속도로 변화하는 조직과 그 방향을 정하는 데에서 여러 시행착오를 겪고 있었다. <a href="https://tkdguq05.github.io/2020/12/27/20201227/">2020년 회고하기</a> 글에도 나와있긴 하지만 갑자기 팀원이 몇 명 나가게 되면서 분위기가 굉장히 어수선 했었다. 이럴때일수록 프로세스를 만들고 지키는 게 중요하다고 생각해서 팀원들과 앞으로의 방향, 그리고 일하는 방식, 어떻게 체크하고 공유할 것인지 이야기를 많이 나눴다. 그래서 탄생한 것이 데이터 사이언스팀의 칸반 보드였고, 자체 오전 스크럼 시간이 부활하게 되었다(칸반 만드는 것도 참 우여곡절이 많았지만 생략…). 금요일마다 칸반 보드에 작성한 업무들의 진행상황을 확인했고 늦춰지고 있으면 왜 늦춰지고 있는지 이야기를 나눴고, 완료된 일은 코드리뷰를 통해서 확인한 후 Task Complete에 넣어놨다. </p><p>그리고 이런 프로세스를 통해서 각자의 업무가 더 명확해졌다. 내 직무를 변경했기 때문일지도 모르겠지만, 각자의 일이 구체화 되었고 이를 통해서 어떤 업무의 담당자를 정할 수 있었다. 이런 프로세스가 만들어진 데에는 좋은 동료들과 끊임없이 소통을 했기 때문이기도 했지만, 개인적으로는 팀장님의 도움이 컸다. 기획 팀장님께서 데이터 사이언스 팀을 겸직해서 맡게 되셨는데, 기획팀 팀장으로서 기획팀과 개발팀의 프로세스를 잘 구축했었던 경험이 있었던 분이었다. 팀장님께서 주도적으로 프로세스를 만들어주신 것은 아니지만, 기획-개발의 프로세스를 만드는 모습과 설명을 들으면서 많은 걸 배울 수 있었고, 이를 잘 적용할 수 있었다. ‘어른이란 이런 거구나’, ‘직장 선배란 이런 것이구나’를 느끼게 해주신 분이기도 하다.</p><p><img src="/images/geultto5end/kanban.png" alt="업무 프로세스"></p><p><br></br></p><h3 id="Data-Enginnering"><a href="#Data-Enginnering" class="headerlink" title="Data Enginnering"></a>Data Enginnering</h3><p>회고를 하다보니 앞으로의 쓸 글들도 데이터 엔지니어링 글이 될 것 같아서 이에 대한 생각을 잠시 작성해보려고 한다. </p><p>각종 커뮤니티를 보면, 직무 명 데이터 엔지니어는 하는 일이 어느정도 정해진 것 같은데 내 생각은 조금 다르다. 아직도 많은 회사에서 데이터 엔지니어를 구하고, 보유하고 있지만 각 데이터 엔지니어가 하는 일이 회사마다 다 다른것 같다는 생각이다. 회사가 작은지 큰지, 직종이 IT인지 제조업인지, B2C인지 B2B인지에 따라 하는 일이 참 많이 달라지는 것 같다는 걸 느끼고 있다. 지금 있는 이 회사에는 데이터 엔지니어가 없었다. 물론 데이터 사이언티스트도 없었다. 내가 들어오면서 데이터 팀이 구성되기 시작했고, 데이터의 중요성에 대해서 자각하기 시작했다(수 없이 여기에 대해서 자료를 공유하고 설득했다). <del>여담인데 생각해보니 이 회사의 1호 데이터 사이언티스트이지 데이터 엔지니어가 되었다. 허허 참…</del></p><p>다시 본론으로 돌아와서, 회사에서 데이터에 대한 중요성을 느끼기 시작해서 데이터 인프라를 제대로 구성하기 시작했다. 그래서 내가 요즘 하는 일은 데이터 인프라를 설계하고 구성하는 일과 여러 API를 만들고 FastAPI라는 새로운 프레임 워크로 갈아끼는 일, 그리고 의사 결정권자 및 실무자들을 설득하는 일이다. 아무 걱정없이 코드만 만지고 인프라를 구성하고 싶은데 그게 참 어렵다. 물론 예전에는 아무 걱정없이 모델링하고 데이터를 분석하고 싶었었다. 데이터 사이언티스트였으니깐! 하지만 계속 안된다고 하는 벽에 막혔었다. 필요한 데이터가 없거나 할 수 없는 구조이거나, 기타 등등의 이유로. 그래서 답답해서 직접하게 되다 보니 이렇게 되었다. 벽이 하나씩 뚫리고 있는 느낌이라 재밌긴 하지만 동시에 새로운 벽이 나타나는 것 같다는 힘든 점이 있다. </p><p><br></br></p><p>그래서 요즘 하는 일을 좀 더 자세히 적어보자면, 내 궁극적 목표는 <code>데이터가 흐르는 조직 만들기</code> 이다. 첫 단추로 회사에 GCP의 BigQuery를 도입하고자 한다. 여러 데이터 서비스를 하기 위해서는, 그리고 고객들에게 가치가 높은 서비스를 제공하려면 속도가 생명이라고 생각한다. 그런데 데이터 전처리하는데, 그리고 데이터를 끌어오는데 시간이 너무 오래 걸리는 문제가 지속적으로 발생하고 있다. 그래서 큰 데이터를 빠르게 쿼리할 수 있고 OLAP성으로 활용할 수 있는 BigQuery를 사용해보려고 한다. 이를 위해서 기존 DB에서 BigQuery에 데이터를 적재할 때 Embulk라는 새로운 오픈소스를 사용해야할 것 같다. </p><p>또한 BigQuery에 넣고 꺼내 쓸 때도 불필요한 작업을 하고 싶지 않기 때문에 AWS의 Glue처럼 ETL처리를 해서 전처리 된 데이터를 바로바로 사용할 수 있게 만들 것이다. 이렇게 BigQuery에 데이터를 담아놓게 되면, 여러 다른 제품들과도 결합해 사용할 수 있기 때문에 확장성 또한 확보할 수 있을 것이라고 생각한다. BigQuery에서 사용하는 주제 별로 주기적으로 전처리해 테이블을 만들어 데이터 마트를 구성할 수도 있겠고, 이를 통해 다른 부서에서 자유롭게 데이터 분석이 가능할 것이다. 또한 Data Studio나 Redash를 이용해서 자유롭게 데이터를 시각화 할 수 있게 만들 것이다. </p><p>무엇보다도 BigQuery에 데이터가 쌓이기 시작하면, 조직 내 SQL교육을 실시할 것이다. 데이터를 다루는 기본적인 언어인 SQL을 교육함으로써 모두가 데이터에 접근해 원하는 데이터를 확인하고 이 결과를 통해 의사결정을 내릴 수 있게 된다면… </p><p>그렇게 된다면 회사가 많이 바뀌지 않을까. 개인적으로 그래서 기대를 많이 하고 있다. </p><p><br></br></p><hr><h3 id="글을-마치며…"><a href="#글을-마치며…" class="headerlink" title="글을 마치며…"></a>글을 마치며…</h3><p>이렇게 글또 5기 작성한 글들을 보고 회고 및 넋두리 하는 시간이 끝났다. 누구에게 보여지려고 하는 회고가 어디있으랴, 회고는 본질적으로 넋두리가 기본이 아닐까 하는 생각으로 애써 정리를 하려고 했지만 정리가 잘 안된 글을 쓴 나를 위로하면서, 그리고 넋두리를 통해 글또 6기에 작성할 글에 대해서 잠시 고민해보면서 글을 마무리한다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/05/02/geultto5-end/#disqus_thread</comments>
    </item>
    
    <item>
      <title>AWS_Immersion_DAY, 추천 파이프라인</title>
      <link>http://tkdguq05.github.io/2021/04/18/AWS-Immersion-DAY/</link>
      <guid>http://tkdguq05.github.io/2021/04/18/AWS-Immersion-DAY/</guid>
      <pubDate>Sun, 18 Apr 2021 06:21:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;AWS Immersion Day를 참석했다… 추천 파이프라인 다시 생각해보기&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>AWS Immersion Day를 참석했다… 추천 파이프라인 다시 생각해보기</p><a id="more"></a><h2 id="AWS-Immersion-Day"><a href="#AWS-Immersion-Day" class="headerlink" title="AWS Immersion Day"></a>AWS Immersion Day</h2><p>(AWS 광고아님 주의, 회사 지원 세션)</p><p>평화로운 오후를 보내던 어느날 Analytics for E-Commerce Immersion Day 행사 안내 메일이 왔습니다. 분석 시스템에 대해 AWS에 문의한 적이 있었는데, 관련된 웨비나 행사 메일을 척척 보내주시니 아주 감사했습니다. 더군다나 Kinesis나 Kafka에 관심이 있었던지라 학습을 어떻게 해볼까 고민했었는데, 이런 행사가 잡히니 고민이 해결될 수 있겠다는 기대감이 있었습니다. </p><p><img src="/images/aws_immersion/immersion_day.png" alt="봄 향기 가득한 안내문"></p><p>행사는 다음과 같이 진행된다고 했습니다. 저는 다 필요없고 Hands On Lab이 아주 기대가 되었습니다.</p><p><br></br></p><h2 id="행사의-시작"><a href="#행사의-시작" class="headerlink" title="행사의 시작!"></a>행사의 시작!</h2><p>15일 13시 부터 시작이 되었습니다. 물론 점심시간을 최대로 활용하는 바람에 늦어버려서 여유롭게 커피한잔과 함께 13시 20분에 접속을 했습니다. AWS의 분석 파트에 있는 제품들을 소개시켜 주셨는데 흥미로운 내용이었지만 제품의 소개같아서 13시부터 16시까지 진행했던 내용을 요약해서 작성을 할 것 입니다. 오늘의 주제는 Hands On Lab이고 이 실습을 하면서 얻은 교훈(?)을 정리할 것이거든요.</p><p><br></br></p><h3 id="AWS-Analytics"><a href="#AWS-Analytics" class="headerlink" title="AWS Analytics"></a>AWS Analytics</h3><p>데이터 분석에 대한 전반적인 내용이 주로 이뤘습니다. 데이터 분석의 목적에 대해서 간략하게 설명해주시면서, 분석을 통해 의사결정을 도와주거나, 어떤 서비스로 이루어져야 한다는 내용이었습니다. 그 중에 눈에 들어왔던 것은 데이터 분석의 속도에 대한 것과 추천 시스템이었습니다. 데이터 분석이 빠르게 이루어질 수록 그 가치가 커진다는 것인데, 이는 회사생활을 하면서 어느정도 느끼고 있던 부분이었습니다. </p><p><img src="/images/aws_immersion/fast_anal.png" alt="빠른 데이터 분석의 가치"></p><p>빠른 데이터 분석을 위해서는 당연하게도 파이프라인이 잘 구축되어 있어야 하겠습니다. 내용이 이어지면서 Kinesis제품을 소개해주셨습니다. Kinesis는 세 종류의 제품이 있는데 Kinesis Data Firehose, Kinesis Data Streams, Kinesis Data Analytics 입니다. Firehose는 데이터를 DW나 S3등의 데이터 스토어로 쉽게 넣을 수 있는 시스템이고, Streams는 실시간 데이터 스트리밍 서비스로 이를 통해 이상탐지에 적용하거나 실시간 대시보드로 활용할 수 있습니다. 마지막으로 Data Analytics는 말 그대로 스트리밍 데이터를 변환 및 분석해주는 시스템입니다. </p><p><img src="/images/aws_immersion/streaming.png" alt="스트리밍은 어려워"></p><p>이커머스 영역에서는 속도가 매우 중요하기 때문에 스트리밍 데이터가 필수적이라고 할 수 있습니다. 하지만 스트리밍 데이터 처리를 위한 시스템 구축은 생각보다 힘이 많이 듭니다. 그래서 이 세션에서는 극복해야할 과제로 설명해주셨습니다. Kafka를 예로 들어 설명을 많이 해주셨는데, Kafka는 쉽다고는 하지만 설치가 어렵고 유지보수도 어렵고 신경쓸게 참 많습니다. 그래서! 이 포인트에서 Kinesis를 적극 권장하고 있었습니다. 사용하면 물론 좋겠지만 Kinesis는 비싼 편이라 고려를 좀 해봐야겠습니다. </p><p><br></br></p><h2 id="Hands-On-Lab"><a href="#Hands-On-Lab" class="headerlink" title="Hands On Lab"></a>Hands On Lab</h2><p>드디어 기다렸던 Hands On Lab시간입니다. 아까 말씀드렸다시피, 데이터 분석을 이용해서 어떤 서비스로 이루어질수 있고 그 중 대표적인 것은 추천시스템입니다. 핸즈온 시간에는 Analytics에서 설명한 제품들을 갖고 추천시스템 파이프라인을 구성하고 캠페인까지 만들어 보는 시간을 가졌습니다. </p><p>가상 시나리오를 주고 실습하는 부분이 아주 맘에 쏙 들었습니다.</p><blockquote><p>여러분들은 반려동물 용품을 판매하는 가상의 E-commerce 회사인 ‘몽스토어’ 회사를 운영하고 있습니다. 지금까지 비즈니스는 꾸준히 성장해왔지만 반년전부터 매출이 크게 성장하지 못하고 멈춰 있는 상태입니다. 이에 따라 데이터 분석에 대한 니즈가 발생하였으며, 데이터 분석을 통해 개인별 추천서비스를 도입하여 매출의 성장을 도모할 때라는 결론에 이르렀습니다.</p></blockquote><p>투잡 뛰는 느낌이랄까? 아주 설레는 마음으로 실습을 진행했습니다. </p><p>실습내용의 전체 아키텍쳐는 다음과 같습니다.</p><p><img src="/images/aws_immersion/architect.png" alt="전체 구성"></p><p>AWS 시스템 내에서 AWS 제품을 갖고 AWS를 이용한 추천 제품인 AWS Personalize를 사용해서, 추천 캠페인을 진행해보자는 것입니다. 주제가 뭐라구요? AWS냐구요? 맞긴한데 거기에 ‘추천 파이프라인을 만들어봅시다!’ 까지가 주제입니다.</p><p><br></br></p><h3 id="본격-실습"><a href="#본격-실습" class="headerlink" title="본격 실습"></a>본격 실습</h3><p>실습 내용은 최대한 간결하게 요약해서 정리할 것입니다. 이걸 일일이 쓰는 것보다 들어가서 확인하면 되거든요. <a href="http://public-aws-workshop.s3-website.us-east-1.amazonaws.com/analyticsworkshop/">워크샵 링크</a></p><p>정말 궁금하신 분들은 링크로 들어가서 쭉 따라가보시면 됩니다. 물론 실습 엔진은 제공되지 않고 Cloud Formation도 제공되지 않습니다. </p><p>실습 순서는 다음과 같았습니다.</p><ol><li>Cloud Formation으로 환경(스택) 구성하기</li><li>S3 생성하기</li><li>Glue를 이용한 RDS DataBase 크롤링</li><li>크롤링한 데이터 S3로 ETL 전송하기</li><li>Web 로그 데이터 Kinesis Firehose활용하여 수집</li><li>수집 데이터 S3로 ETL</li><li>S3 데이터 Glue활용하여 ETL<ul><li>Athena를 사용해서 데이터 살펴보기</li></ul></li><li>Personalize를 사용해 추천 데이터 생성하기</li></ol><p><br></br></p><p>Cloud Formation을 활용해서 위 그림과 같은 환경을 만들어 주게 됩니다. 일일이 세팅해서 환경 구성을 하려면 시간이 너무 들게 되니까 빠르게 환경을 구축해줍니다. </p><p>Cloud Formation을 활용해 구성한 항목은 위 그림에서 S3기준 왼쪽 부분이라고 할 수 있겠습니다. 망 설정과 webserver, 그리고 RDS입니다. </p><p>S3를 만들어 줄 것인데 S3는 여기서 Data Lake로 활용됩니다. Data Lake는 대규모 데이터를 기본 형식으로 저장하고 있는 Storage로 그럴싸해보이지만 제가 보기에는 데이터를 다 때려넣는 곳이라는 생각이 듭니다. 다른 의견이 있으신 분은 댓글 남겨주세요. 아무튼 S3를 만들어 주고 Glue를 사용해보겠습니다. </p><p><br></br></p><h3 id="Glue"><a href="#Glue" class="headerlink" title="Glue"></a>Glue</h3><p>Glue는 데이터 처리에 사용됩니다. 데이터는 여기서 두 종류로 나뉘는데, RDS에 갖고 있는 구매이력 데이터와, 수집되고 있는 행동 데이터, 즉, 로그데이터 입니다. </p><h4 id="구매이력-데이터-처리"><a href="#구매이력-데이터-처리" class="headerlink" title="구매이력 데이터 처리"></a>구매이력 데이터 처리</h4><ul><li>Glue<ul><li>RDS에서 크롤링, 데이터 베이스의 데이터 구조, 스키마를 바로 알 수 없음<ul><li>스키마, 파티션 구조 추론 뒤 데이터 카탈로그 생성</li></ul></li><li>크롤링 → RDS에 있는 데이터 테이블 확인 가능</li><li>2차 크롤링<ul><li>데이터 카탈로그를 생성한 뒤 데이터를 S3로 보내기(ETL)</li><li>Transform할때는 스크립트를 입력, pyspark 코드를 활용함</li><li>Transform한 뒤 S3에 적재[Load]</li></ul></li></ul></li></ul><h4 id="행동-데이터-로그-데이터-처리"><a href="#행동-데이터-로그-데이터-처리" class="headerlink" title="행동 데이터, 로그 데이터 처리"></a>행동 데이터, 로그 데이터 처리</h4><ul><li>Webserver에서 생성되는 데이터를 Kinesis Data Firehose를 사용해 수집</li><li>수집한 데이터는 S3에 적재</li></ul><p>자 이제 Glue를 통해서 S3에 사용할 데이터를 모두 적재해 놓았습니다. 이제 추천데이터를 생성해보겠습니다. AWS Personalize를 사용해 볼 것인데, Personalize는 사용할 데이터 형식이 따로 존재합니다. 그렇기 때문에 원하는 형식에 맞게 전처리를 해주어야 합니다. 또 다시 Glue를 사용해서 전처리하고 Personalize에 전달해보도록 하겠습니다. </p><p><br></br></p><h3 id="추천-데이터-생성"><a href="#추천-데이터-생성" class="headerlink" title="추천 데이터 생성"></a>추천 데이터 생성</h3><ul><li><p>전처리, Glue</p><ul><li><p>Glue를 통해 ETL처리를 해줌, 과정은 위와 유사함 [구매이력]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> awsglue.transforms <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> awsglue.utils <span class="keyword">import</span> getResolvedOptions</span><br><span class="line"><span class="keyword">from</span> pyspark.context <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> awsglue.context <span class="keyword">import</span> GlueContext</span><br><span class="line"><span class="keyword">from</span> awsglue.job <span class="keyword">import</span> Job</span><br><span class="line"></span><br><span class="line"><span class="comment">## @params: [JOB_NAME]</span></span><br><span class="line">args = getResolvedOptions(sys.argv, [<span class="string">'JOB_NAME'</span>])</span><br><span class="line"></span><br><span class="line">sc = SparkContext()</span><br><span class="line">glueContext = GlueContext(sc)</span><br><span class="line">spark = glueContext.spark_session</span><br><span class="line">job = Job(glueContext)</span><br><span class="line">job.init(args[<span class="string">'JOB_NAME'</span>], args)</span><br><span class="line"></span><br><span class="line"><span class="comment">##create dynamic frame</span></span><br><span class="line">digital_df = glueContext.create_dynamic_frame.from_catalog(database=<span class="string">'demogo-mongstore-database'</span>, table_name=<span class="string">'product'</span>).toDF()</span><br><span class="line">digital_df.createGlobalTempView(<span class="string">"productview"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##sql query</span></span><br><span class="line">client_df = spark.sql(<span class="string">"SELECT productcode as ITEM_ID, productname as PRODUCTNAME, category1||'|'||category2||'|'||category3 as CATEGORY FROM global_temp.productview"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##write output to S3</span></span><br><span class="line">client_df.repartition(<span class="number">1</span>).write.format(<span class="string">'csv'</span>).option(<span class="string">'header'</span>, <span class="string">'true'</span>).save(<span class="string">'s3://demogo-mongstore-[사용자이름]/personalize-items'</span>)</span><br><span class="line"></span><br><span class="line">job.commit()</span><br></pre></td></tr></table></figure></li><li><p>spark sql을 사용해서 Transformation 뒤 S3에 적재하는 구조</p><ul><li><p>AS-IS, 이 데이터를</p><p><img src="/images/aws_immersion/asis_purchase.png" alt="AS-IS"></p></li><li><p>TO-BE, 이렇게 바꿀 것</p><p><img src="/images/aws_immersion/tobe_purchase.png" alt="TO-BE"></p></li></ul></li></ul></li></ul><ul><li><p>로그데이터 처리</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> awsglue.transforms <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> awsglue.utils <span class="keyword">import</span> getResolvedOptions</span><br><span class="line"><span class="keyword">from</span> pyspark.context <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> awsglue.context <span class="keyword">import</span> GlueContext</span><br><span class="line"><span class="keyword">from</span> awsglue.job <span class="keyword">import</span> Job</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment">## @params: [JOB_NAME]</span></span><br><span class="line">args = getResolvedOptions(sys.argv, [<span class="string">'JOB_NAME'</span>])</span><br><span class="line"></span><br><span class="line">sc = SparkContext()</span><br><span class="line">glueContext = GlueContext(sc)</span><br><span class="line">spark = glueContext.spark_session</span><br><span class="line">job = Job(glueContext)</span><br><span class="line">job.init(args[<span class="string">'JOB_NAME'</span>], args)</span><br><span class="line"></span><br><span class="line"><span class="comment">##create dynamic frame</span></span><br><span class="line">digital_df = glueContext.create_dynamic_frame.from_catalog(database=<span class="string">'demogo-mongstore-database'</span>, table_name=<span class="string">'purchase'</span>).toDF()</span><br><span class="line">digital_df.createGlobalTempView(<span class="string">"purchaseview"</span>)</span><br><span class="line">digital_df = glueContext.create_dynamic_frame.from_catalog(database=<span class="string">'demogo-mongstore-database'</span>, table_name=<span class="string">'accesslog2021'</span>).toDF()</span><br><span class="line">digital_df.createGlobalTempView(<span class="string">"accesslog2021view"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##sql query</span></span><br><span class="line">client_df = spark.sql(<span class="string">"SELECT userid as USER_ID, REGEXP_REPLACE(pageurl, '[^0-9]+','') as ITEM_ID, to_unix_timestamp(CAST(time AS timestamp)) as TIMESTAMP, 'view' as EVENT_TYPE FROM global_temp.accesslog2021view UNION ALL SELECT userid as USER_ID, productcode as ITEM_ID, to_unix_timestamp(ordertime) as TIMESTAMP, 'order' as EVENT_TYPE FROM global_temp.purchaseview"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">##write output to S3</span></span><br><span class="line">client_df.repartition(<span class="number">1</span>).write.format(<span class="string">'csv'</span>).option(<span class="string">'header'</span>, <span class="string">'true'</span>).option(<span class="string">'header'</span>, <span class="string">'true'</span>).save(<span class="string">'s3://demogo-mongstore-[사용자이름]/personalize-interactions'</span>)</span><br><span class="line"></span><br><span class="line">job.commit()</span><br></pre></td></tr></table></figure><ul><li><p>AS-IS, 이 데이터를</p><p><img src="/images/aws_immersion/asis_log.png" alt="AS-IS"></p></li><li><p>TO-BE, 이렇게 처리</p><p><img src="/images/aws_immersion/tobe_log.png" alt="TO-BE"></p></li></ul></li></ul><p>추천 데이터 생성은 데이터 스키마 매핑 후 데이터 임포트를 해주면 됩니다.  이제 솔루션 생성을 해주면 되는데 여기에는  1시간 이상이 소요되네요. 캠페인을 생성해서 데이터를 보면. 유사도 별 랭킹이 매겨진 추천 데이터를 확인할 수 있습니다. </p><p><img src="/images/aws_immersion/personalize_recomendation.png" alt="생성된 추천 데이터"></p><p><br></br></p><h2 id="실습-종료-무엇을-얻었나"><a href="#실습-종료-무엇을-얻었나" class="headerlink" title="실습 종료, 무엇을 얻었나"></a>실습 종료, 무엇을 얻었나</h2><p>이 실습을 통해 현재 서비스하고 있는 추천 시스템의 구조에 대해서 다시 생각해보게 되었습니다. 현재 서비스하고 있는 추천 시스템은 자세하게 밝히지는 못하지만, 간략하게 구조를 말씀드리면, 로그 데이터를 NoSQL DB로 받아 넣고 이것을 추천용 DB에 일부 전처리 하여 넣어주는 형태입니다. 이 추천용 DB에 데이터를 끌어와서 한 고객사의 추천 데이터를 AWS EMR을 이용해서 배치 스케쥴마다 생성하고 EMR을 종료하고 있습니다. 실습에서 제시된 구조와는 많은 차이가 존재합니다. 물론 실습은 B2C 서비스로 제시되었고 현재 회사는 B2B 서비스기 때문에 차이는 있습니다만, 차이점을 정리하자면 다음과 같습니다.</p><ol><li>Data Lake</li><li>ETL 파이프라인의 부재</li><li>Transform한 데이터의 적재</li></ol><h4 id="먼저-Data-Lake가-없습니다"><a href="#먼저-Data-Lake가-없습니다" class="headerlink" title="먼저 Data Lake가 없습니다."></a>먼저 Data Lake가 없습니다.</h4><p>Data Lake가 꼭 필요하다고는 할 수 없고, 목적에 따라 구성항목에 넣기도 합니다. 물론 회사에서도 전체 데이터가 다 들어있는 DB는 있습니다만, S3제품만큼 고가용성이 보장된다고는 할 수 없을 것 같습니다. S3에도 connection pool size가 있습니다, 그렇지만 현재 사용하고 있는 DB보다야 훨씬 size가 크고 관리하기도 편하다고 생각합니다. 그리고 이렇게 고가용성이 보장되는 Data Lake가 있으면 새로운 서비스를 구상하더라도 connection pool이나 메모리 때문에 장애나는 상황이 거의 없기 때문에 안정적으로 새 서비스를 생각해보고 토이 프로젝트를 해볼 수 있습니다. 현재 운영과 개발DB가 있긴 하지만 개발DB에 문제가 좀…</p><h4 id="ETL-파이프라인이-제대로-구성되지-않은-것-같습니다"><a href="#ETL-파이프라인이-제대로-구성되지-않은-것-같습니다" class="headerlink" title="ETL 파이프라인이 제대로 구성되지 않은 것 같습니다."></a>ETL 파이프라인이 제대로 구성되지 않은 것 같습니다.</h4><p>이 실습에서는 E, T, L이 명확하게 나뉘어져 있다는 것이 느껴지는데, 이 실습을 하고 회사 서비스를 돌아보니 어디부터 어디까지가 ETL인지 구분이 잘 되지 않았습니다. Transform을 하긴 하는데 Load를 안하는 것 같고… Transform이 제대로 되고 있는 건지… 뭐 이런 생각을 하게 되었습니다. 하지만 Glue를 사용한다고 생각하고 구성을 생각해봤을 때, 크게 어려울 것 같지 않았습니다. Glue를 간단히 살펴보긴 했지만, ETL프로세스이고 Transform은 거의 Pyspark로 돌아가고 있었습니다. Pyspark로 이미 추천 데이터를 생성하고 있기에 파이프라인을 정리해주면 금방 적용할 수 있지 않을까 생각해봤습니다. 어디서 끌어오고, 적재할지를 잘 정하는 게 중요할 것입니다. 그렇다면 DB를 잘 알아야 하는데, 이렇게 공부 포인트가 늘어났습니다! 하하</p><h4 id="Transform한-데이터는-어디"><a href="#Transform한-데이터는-어디" class="headerlink" title="Transform한 데이터는 어디?"></a>Transform한 데이터는 어디?</h4><p>큰 문제점 중에 하나라고 생각하는데, 현재 비용 효율적인 아키텍쳐를 지향하고 있기 때문에 EMR을 상시 구동하고 있지 않습니다.      생성한 추천 데이터는 DB에 넣고 있지만, 전처리한 데이터는 여지없이 삭제되고 맙니다. 실습을 진행하고 생각해보니 이 데이터가 너무 아깝다는 생각을 하게 되었습니다. 만약 전처리한 데이터가 남아있다면 다른 서비스에 적용을 해볼 수 있지 않을까 생각이 들었습니다. 추천에 사용되는 데이터가 어떻게 보면 구매나, 클릭, View에 대한 패턴 분석된 데이터인데, 이것을 고객사 레포팅이나 기타 분석 시스템에 활용할 여지가 많을 것 같았습니다. </p><p><br></br></p><hr><h2 id="Kaizen"><a href="#Kaizen" class="headerlink" title="Kaizen!"></a>Kaizen!</h2><p>그러면 어떻게 개선할 수 있을까요? TA님이나 팀장님과 의논을 같이 하면서 구체화해야겠지만 우선 생각나는 개선점은 다음과 같습니다. </p><ol><li>Extract하는 데이터 포인트를 변경</li><li>Data Warehouse</li><li>상시 구동 EMR서버 구성</li></ol><p>이게 명확한 답이 될지 모르겠습니다. 하지만 우선 이렇게 글로 만들어놓고 다른 사람들의 의견을 받아서 두들겨 맞으며 고쳐나가는 게 맞다고 생각합니다. 일단 저질러야 변화가 생기니까요.</p><p>데이터 포인트를 변경한다는 것은 Extract하는 데이터 베이스가 혹사당하고 있기 때문입니다. 너무 자주 데이터를 끌어오고 나가고 있는 상황이기 때문에, 사용하는 데이터를 따로 저장하는 DB를 만들면 어떨까 생각합니다. 그리고 이는 2번과 이어지는데, 이것을 Data Warehouse로 사용하는 것입니다. 이 DW는 Big Query를 검토하고 있습니다. Big Query에 일단 적재를 하고 전처리 스케쥴을 걸어서 추천에 사용할 데이터를 아주 예쁘게 구성해 놓을 예정입니다. 마치 Personalize에 Glue를 사용해서 데이터를 전처리하고 넣는 것 처럼요. 동시에 EMR 서버를 상시 구성해놔서 추천 생성할 때 발생하는 일부 데이터를 DW에 저장해놓으려고 합니다. 물론 중간에 데이터를 확인해보고 이게 사용할만한 가치가 있을지 분석가분들과 고민해봐야겠지만, 구상은 이렇게 해놓고 있습니다. 어떻게 보면 DW에 스케쥴을 걸어서 작업을 돌려놓으면 중간에 EMR로 작업하면서 나오는 데이터는 굳이 필요가 없을 수도 있겠습니다. </p><p>헛된 구상일 수 있겠지만, 발전할 수 있는 포인트를 어느정도 찾은 것 같아서 얻은게 있는 세션이었다고 생각합니다. 이제 해야할 것은 TA님과 팀장님과의 미팅, 그리고 부족한 부분을 채워넣는 학습 시간이겠습니다. </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/04/18/AWS-Immersion-DAY/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Airflow Clusterization</title>
      <link>http://tkdguq05.github.io/2021/04/04/airflow-clusterization/</link>
      <guid>http://tkdguq05.github.io/2021/04/04/airflow-clusterization/</guid>
      <pubDate>Sun, 04 Apr 2021 06:23:20 GMT</pubDate>
      <description>
      
        &lt;p&gt;Airflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Airflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인</p><a id="more"></a><h1 id="Airflow-Clusterization"><a href="#Airflow-Clusterization" class="headerlink" title="Airflow Clusterization"></a>Airflow Clusterization</h1><h2 id="Airflow를-나눠봤습니다-그런데-이제-Autoscaling을-곁들인"><a href="#Airflow를-나눠봤습니다-그런데-이제-Autoscaling을-곁들인" class="headerlink" title="Airflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인"></a>Airflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인</h2><p>Airflow의 구성요소들을 다 쪼개볼 겁니다. 어떻게 나눌 것이냐면, 크게 Airflow Main, Airflow DB, Airflow Worker입니다. Airflow Main에는 웹 서버와 스케쥴러를 돌아가게 만들 것이고, DB에는 MySQL을 띄워 놓을 것입니다. Worker는 오토스케일링을 걸어둘 것이구요. 귀찮게 <strong>왜 이렇게 하냐구요?</strong></p><p>기존에 Airflow는 하나의 서버에서 잘 돌아가고 있었습니다. 그래서 <a href="https://tkdguq05.github.io/2021/02/21/airflow-basic2/#more">Airflow Basic</a> 글을 통해서 정리해 놓기도 했습니다. 문제는 처리할 DAG와 Task가 너무 많아진 것이었습니다. Airflow의 구성요소 각각이 어떤 역할을 하는지 궁금하시거나 잘 기억이 안나시는 분은 <a href="https://tkdguq05.github.io/2021/02/21/airflow-basic2/#more">이 글</a>을 읽어보시면 좋습니다.</p><p>처리할 작업이 많아지면 Worker 하나로는 처리하기 부담스러워집니다. 스케쥴러가 내려준 많은 작업을 처리하다가 다른 작업을 못하게 현상이 자주 발생하게 됩니다. 그래서! 워커를 늘려줘야겠다는 생각을 한 것입니다. 워커에 오토스케일링을 걸어서, 작업 부하가 걸리면 워커를 늘려서 처리하고, 부하가 줄어들면 워커를 줄이려는 것입니다.<br><img src="https://miro.medium.com/max/2800/1*W0hyXlN4H0x0BNK4NwYPTw.gif" alt="힘들때 도와줘!"></p><hr><p>작업 계획은 다음과 같습니다.</p><ol><li>Airflow Main 세팅</li><li>Airflow DB 세팅</li><li>Airflow Worker 기본 이미지 세팅</li><li>Airflow Worker 세팅</li><li>Assemble!</li></ol><hr><p><br></br></p><h2 id="Airflow-Main"><a href="#Airflow-Main" class="headerlink" title="Airflow Main"></a>Airflow Main</h2><p>Airflow Main에는 Webserver와 Scheduler를 구성해 놓을 것입니다. Airflow Main의 기본적인 설정은 <a href="https://tkdguq05.github.io/2020/12/13/airflow-on-ec2/#more">이 글</a>에서의 설정과 같습니다. 여기서 MySQL을 따로 가져갈 것이니 MySQL설정만 빼놓고 따라하시면 좋을 것 같습니다. 다만 저는 Airflow 1.10.14 버전을 사용하도록 하겠습니다. 최신 2.0이상 버전은 DAG가 조금 달라져서 수정을 해줘야 하거든요.<br><br></br></p><h3 id="Airflow-Home"><a href="#Airflow-Home" class="headerlink" title="Airflow Home"></a>Airflow Home</h3><p>먼저 저는 AWS환경을 이용해서 구축을 할 것이기 때문에 EC2를 하나 만들어주고, 새로 생긴 인스턴스에 Airflow home을 잡아주도록 하겠습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> AIRFLOW_HOME=/home/ec2-user/airflow <span class="comment">#기본주소</span></span><br></pre></td></tr></table></figure><p><strong>이 HOME주소는 꽤나 중요합니다</strong>. Airflow가 시작되는 곳이기도 하고, <strong>설정 파일을 불러오는 곳</strong>이기도 하기 때문입니다.  원하는 주소로 작성하시되, Worker에도 동일한 주소를 입력해주셔야 합니다. 그렇지 않으면 <code>execute_command encountered a CalledProcessError</code> ,<code>Celery command Failed</code> 를 만나시게 될 것입니다. <sup><a href="#footnote_1">1</a></sup></p><p>Airflow의 Dag를 실행하려면 <strong>airflow를 구성하고 있는 모든 서버에 동일한 경로와 이름으로 DAG파일이 존재</strong>해야 한다는 것을 기억해주세요.<br><br></br></p><h3 id="Redis-설정"><a href="#Redis-설정" class="headerlink" title="Redis 설정"></a>Redis 설정</h3><p>레디스 설정은 다행히 저번 글에 나와 있는 것과 똑같습니다. 그대로 따라하시면 됩니다.<br><br></br></p><h3 id="Airflow-설치"><a href="#Airflow-설치" class="headerlink" title="Airflow 설치"></a>Airflow 설치</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sudo yum update -y</span><br><span class="line"></span><br><span class="line">sudo yum install group <span class="string">"Development tools"</span> -y</span><br><span class="line"></span><br><span class="line">sudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel python3-devel.x86_64 cyrus-sasl-devel.x86_64 -y</span><br><span class="line"></span><br><span class="line">sudo yum install libevent-devel -y</span><br><span class="line"></span><br><span class="line">sudo pip3 install apache-airflow==1.10.14</span><br><span class="line"></span><br><span class="line">airflow version <span class="comment">#버전이 뜬다면 성공!</span></span><br></pre></td></tr></table></figure><p>설치가 되었으면 <code>airflow initdb</code> ,  <code>airflow webserver</code>를 사용해서 airflow가 뜨는지 확인해 봅니다.</p><p>에러가 발생한다구요? 그렇다면 아래 Trouble Shooting 부분을 참고해보세요. (sqlalchemy version 조정) <sup><a href="#footnote_2">2</a></sup>  </p><hr><p><br></br></p><h2 id="MySQL-설정"><a href="#MySQL-설정" class="headerlink" title="MySQL 설정"></a>MySQL 설정</h2><p>Airflow Main 세팅이 끝났다면, DB로 사용할 새 인스턴스를 만들고 MySQL을 설치해줍니다.</p><p>이전 글과 달라지는 부분이 DB 세팅에서 발생합니다. mysql 5.7버전으로 설치하는 것 까지는 동일합니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm</span><br><span class="line">sudo yum localinstall mysql57-community-release-el7-11.noarch.rpm </span><br><span class="line">sudo yum install mysql-community-server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 설치된 이후</span></span><br><span class="line">sudo service mysqld start</span><br></pre></td></tr></table></figure><p>airflow 사용자를 추가해봅시다. 먼저 root로 접속을 합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p</span><br><span class="line"><span class="comment"># 패스워드를 입력</span></span><br><span class="line">Enter Password : <span class="comment">#/var/log/mysqld.log에 있는 임시비밀번호 사용</span></span><br></pre></td></tr></table></figure><p><strong>달라지는 부분이 발생하는 곳이 여기입니다.</strong> airflow 사용자를 생성할 때 모든 접속 <code>%</code> 에 대해서 허용해주어야 합니다. 기존에는 내부 주소, <code>localhost</code>에 대해서 허용했습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 사용자 생성</span></span><br><span class="line">create user <span class="string">'airflow'</span>@<span class="string">'%'</span> identified by <span class="string">'비밀번호'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DB 권한 부여</span></span><br><span class="line">grant all privileges on *.* to <span class="string">'airflow'</span>@<span class="string">'%'</span>;</span><br><span class="line"></span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>DB가 설정되었으면 Airflow Main과 연결을 해주어야 합니다. 이 DB 서버가 사용하는 Private IP가 있습니다. 이 주소를 AWS에서 확인하고 Airflow Main의 airflow.cfg에 넣어줘서 어떤 DB를 바라볼지 Airflow Main에게 알려줘야 합니다.</p><p>기본적인 airflow main의 airflow.cfg 설정은 이렇게 해줍니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 사용할 dag 폴더 지정</span></span><br><span class="line"><span class="comment"># subfolder in a code repository. This path must be absolute. 꼭 절대경로!</span></span><br><span class="line">dags_folder = /home/ec2-user/airflow/dags <span class="comment">#원하는 위치와 디렉토리로 지정</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># executor = SequentialExecutor</span></span><br><span class="line">executor = CeleryExecutor <span class="comment">#원하는 Executor 선택</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sql_alchemy_conn = sqlite:////home/airflow/airflow/airflow.db</span></span><br><span class="line">sql_alchemy_conn =  mysql+pymysql://airflow:@[mysql서버PrivateIP]:3306/airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># catchup_by_default = True</span></span><br><span class="line">catchup_by_default = False</span><br><span class="line"></span><br><span class="line"><span class="comment"># broker_url = sqla+mysql://airflow:airflow@127.0.0.1:3306/airflow</span></span><br><span class="line">broker_url = redis://airflow@[main서버PrivateIP]:6379/0</span><br><span class="line"></span><br><span class="line"><span class="comment"># result_backend = db+mysql://airflow:airflow@localhost:3306/airflow</span></span><br><span class="line">result_backend = db+mysql://airflow:비밀번호@[mysql서버PrivateIP]:3306/airflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># load_examples = True</span></span><br><span class="line">load_examples = False <span class="comment">#예시를 보고 싶다면 True</span></span><br></pre></td></tr></table></figure><p><code>broker_url</code> , <code>cluster_address</code>에는 Main의 Private IP주소를 넣고, <code>sql_alchemy_conn</code>, <code>result_backend</code> 에는 DB의 Private IP주소를 넣습니다.</p><p>이렇게 설정해주고 다시 한번 <code>airflow initdb</code> 를 해주면 새로 작성해준 주소로 DB경로가 입력이 됩니다.</p><p>추가로, <strong>MySQL서버에도 Redis를 설치해주어야 합니다</strong>. broker인 Redis를 통해 scheduler와 executor를 연결해주어야 하기 때문입니다. </p><p><img src="https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2019/04/17/sagemaker-airflow-2.gif" alt="AWS Managed Airflow 사진 참조"></p><p>위 그림에서 보듯이 main은 webserver와 scheduler를 담당하고 MySQL 서버는 DB와 executor쪽을 담당하게 됩니다. executor에 보낼 매개가 필요하기 때문에 브로커인 Redis를 설정해줍니다.<br><br></br></p><h3 id="RBAC"><a href="#RBAC" class="headerlink" title="RBAC"></a>RBAC</h3><p>webserver를 띄우고 잘 되는지 확인해봅시다. 잘 나오면 다행입니다. 그런데 1.10.10 이상 버전으로 오면서 UI에 변화된 부분이 있습니다. 아주 좋은 기능이라고 생각하는 것 중 하나인데, 그건 시간대를 드디어 설정해서 볼 수 있다는 것입니다. 기존에는 UTC가 고정이어서 +9해서 계산하는게 영 별로였거든요. </p><p><img src="/images/airflow_cluster/airflow_ui.png" alt="ㅇ"></p><p>사진에서 보다시피 <strong>KST로 설정할 수 있습니다</strong>. 그리고 유저도 설정해서 볼 수 있죠. 이걸 적용하기 위해서는 config에서 하나를 더 수정해주어야 합니다.</p><p>다시 airflow.cfg로 들어가서</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use FAB-based webserver with RBAC feature</span></span><br><span class="line">rbac=True</span><br></pre></td></tr></table></figure><p>이렇게 설정해줍니다. <strong>RBAC는 Role-Based Access Control</strong>의 약자로써, 아까 보셨듯이 유저별로 접근을 통제하는 시스템입니다. 그렇다면 유저도 만들어주어야겠죠.</p><p>Airflow Main의 쉘로 들어가서</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">airflow create_user -r Admin -u admin -e email주소 -f admin -l user -p 비밀번호</span><br></pre></td></tr></table></figure><p>이렇게 입력을 해줍시다. 각 플래그의 의미는 다음과 같습니다.</p><ul><li>-r : Role, 역할. Admin, Op, User, Viewer, Public 이 정해져 있고, 커스텀 롤 생성 가능. (자세한 내용은 <a href="https://oboki.net/workspace/data-engineering/airflow/rbac/">여기</a>를 참고하세요)</li><li>-u : User명</li><li>-e : Email 주소</li><li>-f : First Name</li><li>-l : Last Name</li><li>-p : Password, 비밀번호</li></ul><p>이제 webserver를 열어주고 들어가면</p><p><img src="/images/airflow_cluster/lotsofcircles.png" alt="wtf"></p><p>아름다운 원 운동을 바라보면서 멘탈이 흔들릴 수 있습니다. 하지만 이렇게 나오게 된 것은 한 가지를 빼먹었기 때문입니다. 앞서서 우리가 중요한 config를 바꿨을 때는 DB에 알려준 것을 기억하실 것입니다. RBAC는 보안에 관련된 것이니까 중요하다고 볼 수 있겠습니다. 그렇다면 DB에도 뭐가 바뀌었는지 알려주어야 합니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">airflow upgradedb</span><br></pre></td></tr></table></figure><p>이 명령어는 DB를 초기화 시키지 않고 설정값을 DB에 업데이트 시켜주는 역할을 합니다. 이 명령어를 입력해주면?</p><p><img src="/images/airflow_cluster/upgradedb.png" alt="해-결!"></p><p><img src="/images/airflow_cluster/airflow_ui.png" alt="UI 등장"></p><p>짠! 1.10.14의 UI가 등장했습니다. 오른쪽 상단의 시간 설정이 가능하다면 성공하신 것입니다.  </p><p><br></br></p><h3 id="Fernet-Key"><a href="#Fernet-Key" class="headerlink" title="Fernet Key"></a>Fernet Key</h3><p>RBAC를 통해 보안이 강화되었습니다. 이를 통해 활성화 된 것이 하나 더 있다면, <code>fernet_key</code> 를 통한 encryption입니다. Variable이나 Connection을 이용하는 분이라면, DAG를 실행시켰을 때 <code>JSONDecodeError: Expecting value: line 1 column 1</code> 이런 에러를 마주할 가능성이 높습니다. 실제 에러가 난 부분을 보면 Variable의 Value를 가져오는 부분에서 문제가 발생하고 있습니다. 실제로 어떤 Value를 가져오는지 확인해보면, “g8dgasv90s8fd09x9adxfcx” 같이 알 수 없는 암호문으로 되어 있는 것을 목격할 수 있습니다. </p><p>‘아 그렇다면 encrypt 옵션을 False로 바꿔야지!’라는 생각으로 mysql로 들어가서 variables를 찾은 뒤에 <code>is_encrypt</code> 를 다 0으로 변경해봤습니다. 하지만 이렇게 해도 이미 Variable에 등록할 때에 암호화된 코드로 DB에 들어가기 때문에 key값으로 value를 조회해도 나오는 값은 위에 있는 알 수 없는 암호문입니다. </p><p>결국에는 이것을 해독해주어야 합니다. 이 암호를 해독해주는 열쇠가 바로 <code>fernet_key</code> 입니다. fernet_key는 한번 생성해주어야 합니다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> cryptography.fernet <span class="keyword">import</span> Fernet</span><br><span class="line">fernet_key= Fernet.generate_key()</span><br><span class="line">print(fernet_key.decode())</span><br></pre></td></tr></table></figure><p>여기서 나온 키 값을 복사해서 airflow.cfg의 fernet_key 부분에 넣어줍니다. 그리고 이 키 값은 저장해놨다가 airflow worker에 있는 airflow.cfg에도 동일하게 적용을 해줍니다.  </p><hr><p><br></br></p><h2 id="Airflow-Worker-기본-이미지-세팅"><a href="#Airflow-Worker-기본-이미지-세팅" class="headerlink" title="Airflow Worker 기본 이미지 세팅"></a>Airflow Worker 기본 이미지 세팅</h2><p>Airflow Worker를 세팅해보겠습니다. 그 전에 기본 이미지를 설정해 줄 것입니다. 물론 Worker를 쫙 설치하고 세팅해준다음에 AMI를 만들어서 오토스케일링을 진행해도 되지만, 만약 설정값을 바꿀일이 생긴다면? 그때마다 AMI를 새로 만들어야 할 것입니다. 당연히 AWS관리자나 TA분과의 관계가 좋지 않아질 것입니다. </p><p>제가 선택한 방법은 Docker입니다. Docker를 이용해서 위에서 진행한 세팅을 한번에 잡아줄 것이고, 이 이미지를 시작 템플릿으로 설정할 것입니다. 설정 값을 바꿀 일이 있다면 이미지가 저장된 곳에 들어가서 변경한 후 다시 Push해주면 됩니다. </p><p>기본 이미지는 Docker와 기타 사용할 명령어에 대한 라이브러리가 설치된 정도면 됩니다. 저는 Docker정도만 설치했고 이것을 AMI로 만들어 줬습니다. 이 AMI를 시작템플릿에 넣을 것이고 시작템플릿에 있는 고급 설정을 통해 서버가 시작되면서 Docker Image를 Run 해 줄 수 있는 명령어를 넣어줄 것입니다.</p><p><img src="/images/airflow_cluster/docker_ami.png" alt="Airflow%20Clusterization%202f196f07c5a7412285ad666bc5db2e46/docker_ami.png">  </p><p><br></br></p><h3 id="Docker-Airflow-Worker-이미지-구성"><a href="#Docker-Airflow-Worker-이미지-구성" class="headerlink" title="Docker Airflow Worker 이미지 구성"></a>Docker Airflow Worker 이미지 구성</h3><p><strong>본격적으로 Worker 이미지를 만들어보겠습니다.</strong></p><p>구성요소는 다음과 같습니다.</p><ul><li>Dockerfile</li><li>files<ul><li>airflow.cfg</li><li>config<ul><li>log_config.py</li></ul></li></ul></li><li>sources<ul><li>airflow.sh</li><li>cron</li><li>hostname_resolver.py</li><li>requirements.txt</li></ul></li></ul><p>디렉토리명은 임의로 정해놓은 것이기 때문에 다르게 설정하셔도 됩니다. files부터 설명드리면, airflow.cfg는 위에서 보셨던 그 설정파일입니다. airflow main에서 설정했던 파일을 복사해서 넣어주시면 좋습니다. config 폴더에는 airflow의 log설정과 관련된 파일이 있습니다.</p><p>sources에는 airflow.sh로 쉘 스크립트를 통해 실행하는 명령이 담겨 있고, cron작업을 위한 cron, 도커 내부에서 동작하기 때문에 도커의 호스트를 알려주는 hostname_resolver.py, 그리고 필요한 python 라이브러리 설치를 위한 requirements.txt가 있습니다.  </p><p><br></br></p><h4 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu:18.04</span><br><span class="line">RUN apt-get update &amp;&amp; apt-get install -y \</span><br><span class="line">  git \</span><br><span class="line">  cron \</span><br><span class="line">  vim \</span><br><span class="line">  python3-pip \</span><br><span class="line">  python3-dev \</span><br><span class="line">  build-essential \</span><br><span class="line">  libmysqlclient-dev \</span><br><span class="line">  libssl-dev \</span><br><span class="line">  libkrb5-dev \</span><br><span class="line">  libsasl2-dev \</span><br><span class="line">&amp;&amp; rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line">ADD ./<span class="built_in">source</span>/* /app/</span><br><span class="line">ADD ./files/airflow.cfg /app/</span><br><span class="line">ADD ./files/config/ /data/airflow/config/</span><br><span class="line">ADD ./<span class="built_in">source</span>/hostname_resolver.py /usr/<span class="built_in">local</span>/lib/python3.6/dist-packages/airflow/</span><br><span class="line">RUN chmod 0744 /app/airflow.sh</span><br><span class="line">RUN <span class="built_in">export</span> AIRFLOW__CORE__HOSTNAME_CALLABLE=airflow.hostname_resolver:resolve</span><br><span class="line">RUN <span class="built_in">export</span> AIRFLOW__CORE__FERNET_KEY=***</span><br><span class="line">ADD ./<span class="built_in">source</span>/cron /etc/cron.d/</span><br><span class="line">RUN chmod 0744 /etc/cron.d/cron</span><br><span class="line">RUN crontab /etc/cron.d/cron</span><br><span class="line">RUN touch /var/<span class="built_in">log</span>/cron.log</span><br><span class="line"></span><br><span class="line">WORKDIR /app</span><br><span class="line">RUN mkdir -p /data/airflow/DAGS <span class="comment">#제가 만든 DAG 폴더</span></span><br><span class="line">RUN git <span class="built_in">clone</span> DAGS <span class="comment">#제 github에 있는 DAGs</span></span><br><span class="line"></span><br><span class="line">RUN pip3 install -r requirements.txt</span><br><span class="line">RUN python3 -m pip install sqlalchemy==1.3.15 <span class="comment">#에러 방지</span></span><br><span class="line"><span class="comment">#CMD ["tail","-f","/var/log/cron.log"]</span></span><br><span class="line">CMD ./airflow.sh <span class="variable">$NAME</span></span><br></pre></td></tr></table></figure><p>기본 세팅으로 들어가는 라이브러리가 좀 있습니다. git이나 cron, vim등은 자주 사용하기 때문입니다. 그 외에 ADD 부분을 보시면 /source라든가 /files가 있는데 이것은 제가 빌드하기 전에 만들어 놓은 디렉토리 입니다. 이 디렉토리 안에 필요한 파일들을 넣어놨고, 빌드 후에 원하는 위치로 옮기고 실행해줄 것입니다.</p><p>export로 환경 설정 해주는 부분이 있는데 여기에 원래는 AIRFLOW_HOME 설정도 해줬습니다. 하지만 이것은 docker run할 때 넣어주기 때문에 빼놨습니다. 그리고 fernet key부분은 아래에서 설명하겠습니다.  </p><p><br></br></p><h4 id="airflow-sh"><a href="#airflow-sh" class="headerlink" title="airflow.sh"></a>airflow.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="built_in">export</span> C_FORCE_ROOT=<span class="string">"true"</span></span><br><span class="line">crontab /etc/cron.d/cron</span><br><span class="line">/usr/<span class="built_in">local</span>/bin/airflow initdb</span><br><span class="line">mv /app/airflow.cfg /data/airflow/</span><br><span class="line">/usr/<span class="built_in">local</span>/bin/airflow worker -q <span class="variable">$1</span> &amp;</span><br><span class="line">cron &amp;&amp; tail -f /var/<span class="built_in">log</span>/cron.log</span><br></pre></td></tr></table></figure><p>쉘 스크립트에는 Celery 작업을 위한 <code>C_FORCE_ROOT</code>가 있습니다. 이것은 root 권한으로 셀러리를 돌리겠다는 표시이구요. 그 외에는 cron 등록, db initialize 등이 있는데 worker -q에 $1이 있는 이유는 docker를 run할 때 어떤 큐 값으로 실행시킬 건지 정하기 위해서 입니다. 워커를 여러 대 둘 수 있는데 큐를 따로 관리할 수 있으면 좋으니까요.  </p><p><br></br></p><h4 id="cron"><a href="#cron" class="headerlink" title="cron"></a>cron</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">* * * * * <span class="built_in">cd</span> /data/airflow/DAGs &amp;&amp; git fetch --all &amp;&amp; git reset --hard origin/master</span><br><span class="line">* * * * * find /data/airflow/logs/* -ctime +7 -<span class="built_in">exec</span> rm -f &#123;&#125; \;</span><br></pre></td></tr></table></figure><p>cron 작업에는 사용할 DAG들이 있는 git repo를 받는 부분과, log관리하는 코드가 같이 들어있습니다.  </p><p><br></br></p><h4 id="hostname-resolver-py"><a href="#hostname-resolver-py" class="headerlink" title="hostname_resolver.py"></a>hostname_resolver.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resolve</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> requests.get(<span class="string">'http://169.254.169.254/latest/meta-data/local-ipv4'</span>).text</span><br></pre></td></tr></table></figure><p>hostname_resolver.py에는 aws의 메타 데이터를 이용해서 docker의 host명을 가져오는 코드가 들어있습니다. 혹시 웹 서버에서 DAG 작업로그를 보려고 하는데 나오지 않는다면, 이 부분을 의심해 보세요.  </p><p><br></br></p><h4 id="requirements-txt"><a href="#requirements-txt" class="headerlink" title="requirements.txt"></a>requirements.txt</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">boto3==<span class="number">1.12</span><span class="number">.0</span></span><br><span class="line">pymongo==<span class="number">3.10</span><span class="number">.1</span></span><br><span class="line">celery==<span class="number">4.4</span><span class="number">.0</span></span><br><span class="line">mysqlclient==<span class="number">1.4</span><span class="number">.6</span></span><br><span class="line">pymysql==<span class="number">0.9</span><span class="number">.3</span></span><br><span class="line">redis==<span class="number">3.4</span><span class="number">.1</span></span><br><span class="line">apache-airflow==<span class="number">1.10</span><span class="number">.14</span></span><br></pre></td></tr></table></figure><p>requirements에는 필요한 라이브러리들이 담겨있습니다.</p><p>이렇게 워커 이미지를 구성했으면, 빌드를 시작해봅시다.   </p><p><br></br></p><h3 id="Docker-Build-and-Run"><a href="#Docker-Build-and-Run" class="headerlink" title="Docker Build and Run"></a>Docker Build and Run</h3><p>만든 이미지를 AWS ECR에 올려볼 것입니다. 이를 위해 다음과 같은 순서의 작업을 수행하여야 합니다.</p><ol><li><p>인증 토큰을 검색하고 레지스트리에 대해 Docker 클라이언트를 인증합니다. AWS CLI 사용:</p><p><code>aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin *******</code></p><p>참고: AWS CLI를 사용하는 중 오류가 발생하면 최신 버전의 AWS CLI 및 Docker가 설치되어 있는지 확인하십시오.</p></li><li><p>다음 명령을 사용하여 도커 이미지를 빌드합니다. 도커 파일을 처음부터 새로 빌드하는 방법에 대한 자세한 내용은 <a href="http://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html">여기</a>  지침을 참조하십시오. 이미지를 이미 빌드한 경우에는 이 단계를 건너뛸 수 있습니다.</p><p><code>docker build -t 이미지명 .</code></p></li><li><p>빌드가 완료되면 이미지에 태그를 지정하여 이 리포지토리에 푸시할 수 있습니다.</p><p><code>docker tag 이미지명:tag 리포지토리주소/이미지명:tag</code></p></li><li><p>다음 명령을 실행하여 이 이미지를 새로 생성한 AWS 리포지토리로 푸시합니다.<br><code>docker push 리포지토리주소/이미지명:tag</code></p></li></ol><p>위에 있는 것들은 ECR의 푸시 명령에 있는 것들이기 때문에 해당 페이지에 있는 코드를 복사해서 넣으시면 됩니다.</p><p>빌드가 되었다면, 이미지 구성은 끝입니다. 이제 필요한 것은 시작 템플릿을 방금까지 만든 이미지를 이용해서 구성하고, 이것을 오토스케일링 그룹에 넣어서 워커를 쫙 만들어 주면 됩니다.  </p><p><br></br></p><h3 id="Start-Template"><a href="#Start-Template" class="headerlink" title="Start Template"></a>Start Template</h3><p>시작 템플릿 구성</p><p><img src="/images/airflow_cluster/start_template.png" alt="도커 기본이미지를 만들고 여기에 넣어주기"></p><p>시작 템플릿 이름은 잘 넣어주시면 되고, AMI에는 아까 구성한 Docker AMI를 넣어주도록 합니다. 이 도커 AMI를 기본으로 설치하고 그 위에 도커 이미지를 Run 해 줄 것입니다.</p><p>이제 맨 밑에 고급 세부 정보로 들어와서 사용자 데이터를 넣어줍니다.</p><p><img src="/images/airflow_cluster/advanced_template.png" alt="여기에 docker 명령어를 넣어주자"></p><p>사용자 데이터는 이 시작 템플릿이 시작될 때 실행할 수 있는 명령어입니다. 여기에 docker run 명령어를 넣을 것입니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">su - ubuntu -c <span class="string">"aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin *******;docker run -p 8793:8793 -e AIRFLOW_HOME=/data/airflow -e NAME=main -d ***********/작성한이미지명:태그"</span></span><br></pre></td></tr></table></figure><p><strong>shebang을 꼭 넣어주셔야 합니다.</strong></p><p>shebang을 넣지 않으면 이 명령어를 인식할 수 없어서 에러가 발생합니다. 한 번 더 강조합니다. <strong>shebang 꼭 넣어야 한다고!</strong>  빼먹어서 저 처럼 삽질 많이 하지 마십시오. 그 뒷 부분에는 su ubuntu가 있습니다. 이후에 실행할 명령어는 ubuntu유저로 실행해야하기 때문입니다. 그래서 switch user를 해주시고 다음의 명령어를 입력해줍니다.</p><p>하나씩 자세히 볼까요?</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin *******</span><br></pre></td></tr></table></figure><p>이것은 아까 사용했던 명령어로, 인증 토큰을 검색하고 레지스트리에 대해 Docker 클라이언트를 인증하는 것입니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -p 8793:8793 -e AIRFLOW_HOME=/data/airflow -e NAME=main -d ***********/작성한이미지명:태그</span><br></pre></td></tr></table></figure><p>도커 이미지를 run하는 명령어입니다. 8793포트를 열어줘야 워커에 대해서 통신이 가능합니다. <code>-p</code>를 이용해 포트를 적어주세요. <code>-e</code> 명령어는 도커에서 환경설정 관련된 부분입니다. airflow home을 airflow main과 같은 위치로 잡아주시면 됩니다. <code>NAME</code>은 worker -q 다음에 들어갈 큐의 topic 명입니다. 제가 작성한 DAG들은 main 토픽을 사용하므로 main으로 했습니다. 다른 topic을 사용하시는 분은 다른 걸로 넣어주세요. <code>-d</code>는 백그라운드로 실행시킨다는 의미이며, 그 다음에는 ECR 이미지의 주소를 넣어줍니다.</p><p>시작템플릿 작성이 끝났습니다.   </p><hr><p><br></br></p><h3 id="Assemble"><a href="#Assemble" class="headerlink" title="Assemble!"></a>Assemble!</h3><p>이제 구성이 모두 되었습니다. airflow main에 들어가서 airflow webserver와 airflow scheduler를 올려줍니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup airflow webserver &amp;</span><br><span class="line">nohup airflow scheduler &amp;</span><br></pre></td></tr></table></figure><p>오토스케일링 그룹에도 워커를 늘려주고 도커 이미지가 올라올 때까지 조금 기다려줍니다. 다 올라오고 나면 dag를 잘 돌리는지 test 해 봅니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">airflow list_tasks [TASK명]</span><br><span class="line">airflow [TASK명] [DAG id] [Task id] [date]</span><br></pre></td></tr></table></figure><p>잘 돌았다면 성공입니다!</p><p><img src="https://media.makeameme.org/created/yas-yaas.jpg" alt=""></p><hr><p>  <br></br></p><h2 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h2><h3 id="1-DAG가-무한-생성"><a href="#1-DAG가-무한-생성" class="headerlink" title="1. DAG가 무한 생성?"></a>1. DAG가 무한 생성?</h3><p><img src="/images/airflow_cluster/catchup.png" alt="그만돌아!"></p><p>갓 만든 따끈따끈한 DAG를 On했습니다. 그런데 DAG하나가 끝나기도 전에 새로운 DAG들이 계속해서 실행되는 현상을 목격할 수 있습니다. 이런 경우는 <code>catch_up</code> 옵션이 활성화 되어있기 때문입니다. catupup은 <code>start_date</code>부터 현재 시간까지 실행하지 못한 DAG들을 실행하겠다는 의미입니다. 임시로 실행하는 DAG야 신경 안써도 큰 문제는 되지 않겠지만, API가 엮어있거나 대용량 데이터를 끌어서 사용하는 DAG라면 문제가 커질 수 있습니다. 이는 airflow.cfg에서 해결할 수 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">catchup_by_default = False</span><br></pre></td></tr></table></figure><p>airflow.cfg에서 <code>catchup_by_default</code> 옵션을 찾아서 False로 변경해주시면 됩니다. 기본값은 True입니다. </p><p>혹시 이렇게 했는데도 문제가 계속 발생한다면, <strong>웹 서버와 스케쥴러를 모두 종료하신 후에 다시 실행시켜 수정한 옵션 값을 적용해주면 됩니다</strong>.<br><br></br></p><h3 id="2-pip3-permission-denied"><a href="#2-pip3-permission-denied" class="headerlink" title="2. pip3 permission denied"></a>2. pip3 permission denied</h3><p>열심히 글을 따라 치는데 뜬금없이 <code>pip3 permission denied</code> 이런 에러가 발생할 수 있습니다. 이런 경우에는 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install [library]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위 명령어가 안된다면</span></span><br><span class="line">python3 -m pip install --user [library]</span><br></pre></td></tr></table></figure><p>위와 같은 명령어로 해결할 수 있습니다.  </p><p><br></br></p><h3 id="3-Command-python-setup-py-egg-info-failed-with-error-code-1"><a href="#3-Command-python-setup-py-egg-info-failed-with-error-code-1" class="headerlink" title="3. Command python setup.py egg_info failed with error code 1"></a>3. Command python <a href="http://setup.py/">setup.py</a> egg_info failed with error code 1</h3><p>역시나 ec2-user에서 열심히 설치를 하는 중에 이 에러를 마주칠 수 있습니다. 이 에러는 pip 업데이트가 되지 않아 발생한 에러로 볼 수 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo -H pip install --upgrade --ignore-installed pip setuptools</span><br><span class="line"></span><br><span class="line"><span class="comment"># 또는</span></span><br><span class="line">python3 -m pip install -U pip</span><br></pre></td></tr></table></figure><p>  <br></br></p><h3 id="4-ModuleNotFoundError-No-module-named-‘sqlalchemy-ext-declarative-clsregistry’"><a href="#4-ModuleNotFoundError-No-module-named-‘sqlalchemy-ext-declarative-clsregistry’" class="headerlink" title="4. ModuleNotFoundError: No module named ‘sqlalchemy.ext.declarative.clsregistry’"></a>4. ModuleNotFoundError: No module named ‘sqlalchemy.ext.declarative.clsregistry’</h3><p> <a name="footnote_2">2</a><br>위의 에러는 sqlalchemy 버전이 맞지 않아서 발생하는 문제입니다. 그렇다면 버전을 낮춰주면 됩니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#sqlalchemy 버전 낮추면서 해결</span></span><br><span class="line">python3 -m pip install sqlalchemy==1.3.15</span><br></pre></td></tr></table></figure><p><br></br></p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a name="footnote_1">1</a> 출처 : <a href="https://louisdev.tistory.com/26">https://louisdev.tistory.com/26</a></li><li><a href="https://yahwang.github.io/posts/87">https://yahwang.github.io/posts/87</a></li><li><a href="https://potensj.tistory.com/73">https://potensj.tistory.com/73</a></li><li><a href="https://oboki.net/workspace/data-engineering/airflow/rbac/">https://oboki.net/workspace/data-engineering/airflow/rbac/</a></li><li><a href="https://github.com/puckel/docker-airflow/issues/387">https://github.com/puckel/docker-airflow/issues/387</a></li><li><a href="https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html#rotating-encryption-keys">https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html#rotating-encryption-keys</a></li><li><a href="https://devlog.jwgo.kr/2019/07/05/celery-daemonization/">https://devlog.jwgo.kr/2019/07/05/celery-daemonization/</a></li><li><a href="https://aws.amazon.com/ko/blogs/korea/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/">https://aws.amazon.com/ko/blogs/korea/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/</a></li></ul>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2021/04/04/airflow-clusterization/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
