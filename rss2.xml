<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Unreasonable Effectiveness</title>
    <link>http://tkdguq05.github.io/</link>
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Sat, 18 Jul 2020 04:08:41 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>2020년 상반기를 보내면서, 회고하기</title>
      <link>http://tkdguq05.github.io/2020/07/18/retrospect2020/</link>
      <guid>http://tkdguq05.github.io/2020/07/18/retrospect2020/</guid>
      <pubDate>Sat, 18 Jul 2020 01:28:40 GMT</pubDate>
      <description>
      
        &lt;p&gt;2020년 상반기 회고.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>2020년 상반기 회고.</p><a id="more"></a><p>(썸네일 사진은 대구의 앞산. 불꽃놀이도 운 좋게 구경하고 좋은 기억이었다.)</p><p>2020년 새해가 밝은 이후로 벌써 7월이 다 지나가고 있다. 아차 싶을 사이에 시간은 쏜살 같이 흘렀고 동시에 회사에 들어온지도 1년이 되었다. 작년의 나는 많은 것들을 다짐했다. 특히 글또를 하면서, 글에 대해서 고민을 많이 했고 완성도 있는 글을 쓰고자 노력했다. 2주에 한 번 글을 쓰는게 글또의 룰이지만, 스스로의 의지로 더 많은 글을 쓰고 싶었고 공부를 꾸준히 하면서 정리하고 성장하고 싶었다. 나는 많이 성장했을까? 내가 지키고자 한 약속들을 지키고 있는걸까?</p><p>글또를 시작하면서의 다짐 <a href="https://tkdguq05.github.io/2020/02/24/geultto4/#more">https://tkdguq05.github.io/2020/02/24/geultto4/#more</a><br>2019년도 회고, 2020년을 맞이하며 <a href="https://tkdguq05.github.io/2019/12/22/adios-2019/#more">https://tkdguq05.github.io/2019/12/22/adios-2019/#more</a></p><hr><h2 id="“2020을-시작하며”를-다시-보면서"><a href="#“2020을-시작하며”를-다시-보면서" class="headerlink" title="“2020을 시작하며”를 다시 보면서"></a>“2020을 시작하며”를 다시 보면서</h2><h4 id="2020년에-나는"><a href="#2020년에-나는" class="headerlink" title="2020년에 나는"></a>2020년에 나는</h4><ul><li>추천 시스템이나 다른 서비스를 위한 데이터 파이프라인 구성을 성공적으로 한다.</li><li>퇴근 후 개인 공부시간을 매일 30분 이상 갖는다.</li><li>캐글 대회에 도전한다.</li><li>캐글 대회에서 동메달 이상의 성과를 낸다.</li><li>일주일에 3일 이상 운동한다.</li><li>3대 운동 300에 도전한다.</li><li>돈을 모아서 피렌체에 간다.</li><li>꾸준히 글을 쓴다.</li><li>책을 꾸준히 읽자.</li><li>일을 즐겁게 한다.</li><li>소중한 사람들을 잘 챙기자.</li></ul><p>나는 2020년을 시작하면서 위와 같은 목표를 세웠었다. 보자마자 헛웃음이 나왔다. 11개의 목표 들을 하나하나 훑어보자.   </p><h4 id="먼저-파이프라인에-대해서는-잘-구성한-것-같다"><a href="#먼저-파이프라인에-대해서는-잘-구성한-것-같다" class="headerlink" title="먼저 파이프라인에 대해서는 잘 구성한 것 같다."></a>먼저 파이프라인에 대해서는 잘 구성한 것 같다.</h4><p>오픈소스와 나름 최신 스택의 기술을 사용해서(Airflow, Kubeflow, Spark …) 파이프라인을 운영하고 있고 아직까지는 잘 돌아가고 있다. 파이프라인 구성을 잘 한 것도 기분이 좋았는데 새로운 영역을 공부하는 재미를 알게 된 것 같다. 아마 데이터 엔지니어링이라고 불리우는 이 영역에 대해서 항상 마음만 갖고 제대로 공부해 본 적이 없었고, 어떤 걸 먼저 공부해야 하는지 몰랐었다. 특히 기본 지식이 부족한 게 큰 것 같았다. 가상화와 리눅스에 대해서 알아야 이해가 빨랐었을 것 같은데 예전에는 전혀 몰랐다. 다행히 같이 일하시는 매니져님이 오픈소스도 많이 다뤄보시고 가상화와 리눅스에 대해서 잘 알려주셔서 다른 개념들을 이해하는데 큰 도움이 되었다. 공부한 내용을, 특히, 가상화와 도커에 대해서 최근에 정리를 했다. 사실 마음에 들지는 않는다. 더 필요한 내용들이 많다고 생각되었는데, Kubernetes나 Kubeflow를 빠르게 다뤄보고 싶은 사람들이 타겟이었기 때문에 가볍게 작성했다. 추후에 시간이 되면 스핀오프 글로 도커에 대한 내용을 더 자세하게 다루고 해당 개념이 Kubernetes나 Kubeflow에서 어떻게 적용되는지 자세하게 다뤄보고 싶다. 이미 스포가 된 것 같은데 앞으로의 글은 Kubeflow와 Kubernetes에 대한 글이 될 것이다. 처음 다뤄 보는 내용이지만 기존 시스템을 혁신적으로 바꿀 수 있을 만하기 때문에 흥미롭게 보고 있다.   </p><h4 id="퇴근-후에-공부시간을-갖고-있지-못하고-있다"><a href="#퇴근-후에-공부시간을-갖고-있지-못하고-있다" class="headerlink" title="퇴근 후에 공부시간을 갖고 있지 못하고 있다."></a>퇴근 후에 공부시간을 갖고 있지 못하고 있다.</h4><p>공부도 공부인데 목하고 어깨가 너무 안좋아졌다. 조금만 무리하면 목디스크처럼 통증이 있어서 왠만하면 퇴근하고는 운동을 하거나 스트레칭을 하고 있다. 이렇게라도 하지 않으면 회사 업무에 신경쓰기 힘들어져서 일단 몸을 먼저 챙겨야 겠다는 생각이다. 주말에는 가끔 도수치료나 물리치료를 받고 있고 여유롭게 지내고 있다. 하지만 최근 들어서 공부하고싶은 내용도 생겼고, 좀 더 달려봐야겠다는 마음이라 앞으로는 공부시간을 갖게 되지 않을까 한다.  </p><h4 id="캐글-대회에는-도전하지-못했다"><a href="#캐글-대회에는-도전하지-못했다" class="headerlink" title="캐글 대회에는 도전하지 못했다."></a>캐글 대회에는 도전하지 못했다.</h4><p>물론 동메달 이상의 성과도 내지 못했다. 하지만 회사 동료들과 챌린지에 나가서 나름 대회에 참가해보기는 했다. 회사에 들어오면서 내가 세운 다른 목표중에 하나였는데, 결과는 좋지 않았지만 훌륭한 출발이었던 것 같다. 추후에 캐글에 도메인 성격과 맞는 대회가 나오면 같이 참가해보기로 했다. 챌린지 결과 때문인지 다들 눈에 독기가 생긴 것 같다. </p><h4 id="일주일에-3일-이상-운동을-하고-있다"><a href="#일주일에-3일-이상-운동을-하고-있다" class="headerlink" title="일주일에 3일 이상 운동을 하고 있다."></a>일주일에 3일 이상 운동을 하고 있다.</h4><p>3대 운동 300이라는 목표를 달성하려고 했는데 무리하게 하다보니까 몸이 너무 피곤해서 업무에 지장이 가는 것 같았다. 단백질을 많이 챙겨먹어야 하는데 그렇지도 못하고 있고… 일단 운동을 통해서 기본 체력을 올리고 스트렝스 훈련을 강화해야겠다. 무게가 잘 늘지 않는데 일단 유지하고 심페 능력부터 길러야겠다. 나이를 먹으면서 느끼는 것 중 하나는 내 상태를 유지하는 것도 힘든 일이라는 점이다.   </p><h4 id="피렌체에-가려고-했다"><a href="#피렌체에-가려고-했다" class="headerlink" title="피렌체에 가려고 했다."></a>피렌체에 가려고 했다.</h4><p>4월에 가는 티켓을 구매했고 모든 걸 준비해 놨었다. 하지만 코로나19로 인해……<br>나이 서른이 되기 전에 유럽을 다시한번 가고 싶었는데 이건 뭐 어쩔 수 없으니깐^^ 하…<br>결국 누나도 이탈리아에서 돌아왔고 앞으로 피렌체에 가려면 돈이 더 들거 같다. 언젠가 한번쯤은 꼭 가고 싶기 때문에 돈을 모아서 피렌체와 로마 밀란을 가봐야겠다.  </p><h4 id="글은-글또를-하면서-꾸준히-쓰고-있다"><a href="#글은-글또를-하면서-꾸준히-쓰고-있다" class="headerlink" title="글은 글또를 하면서 꾸준히 쓰고 있다."></a>글은 글또를 하면서 꾸준히 쓰고 있다.</h4><p>돈의 강제성이란 정말 무서운 것이다. 아 정말 쓰기 싫다라는 생각이 들어도 보증금을 생각하면 맘이 바뀐다. 애초에 목표를 ‘10만원 보증금 유지하기’로 잡고 있어서 더 그런 것도 있지만, 직접 돈을 벌다 보니 푼돈 몇 푼 나가는 게 모여서 10만원이 되고 카드값이 되었다. 티끌모아 태산이란 말은 내 통장의 잔고에는 해당이 되는 말이 아니었고 카드 값에 대한 말인 것 같았다.<br>2주에 글 하나 쓰는 건 글또를 하면서 조금 아깝다는 생각이 들었다. 공부할 시간이 더 많아지면 정리할 내용도 많아지고 글 쓸 거리도 많아지겠지! 시리즈 물도 다시 한번 써보고 싶다. 확실히 시리즈를 써야 내 관련 글도 많이 보고 글을 쓰는 내 태도도 좋아 지는 것 같다.  </p><h4 id="책은-기술관련-서적-말고는-거의-못-보고-있다"><a href="#책은-기술관련-서적-말고는-거의-못-보고-있다" class="headerlink" title="책은 기술관련 서적 말고는 거의 못 보고 있다."></a>책은 기술관련 서적 말고는 거의 못 보고 있다.</h4><p>‘인간의 무늬’나 ‘정확한 사랑의 실험’을 꼭 마저 보고 싶은데, 아… 책 읽는 거 힘들다. 아이패드를 항상 갖고 다니면서 버스나 지하철에서 봐야겠다. 유튜브 프리미엄을 끊었더니 유튜브가 너무 너무 좋아졌는데, 책 몇 페이지라도 보고 유튜브 보고 이래야겠다. 하루에 몇 페이지 꾸준하게 정해놓고 읽으면 모여서 책 한 권이되고 다섯 권이 되지 않을까? 책 많이 보고 싶어서 yes24 많이 들어가긴 하는데, 기술 서적이 더 눈에 띄고 그러면서 다른 책은 언제 보나 또 고민하고 있고… 꾸준히… 꾸준하게…!</p><h4 id="일에-대해서-고민이-커지고-있다"><a href="#일에-대해서-고민이-커지고-있다" class="headerlink" title="일에 대해서 고민이 커지고 있다."></a>일에 대해서 고민이 커지고 있다.</h4><p>사실 데이터 팀에서의 일을 너무 즐겁고, 이 일에 대해서 고민하는 것도 행복하다. 하지만 어떤 프로젝트가 늦어지고 그것 때문에 우리가 만들어 놓은 모델을 붙이지 못해서 하염없이 기다리고 있을 때 좀 힘들었다. 큰 불만이 없었는데 조금씩 불만이 쌓여가고 있는 것 같다. 아예 신경을 쓰지 말고 내 일만 해야할까? 고민한다고 상황이 나아지기는 하는 걸까? 잘 모르겠다. 일단 내 생각은 우리 일에 집중하자는 것이다. 고민해봤자 나아지는 건 없는 것 같았다.<br>이런 고민 말고 다른 고민은 데이터 엔지니어와 모델러에서 어떤 커리어 패스를 잡아야 할까 하는 고민이다. 엔지니어 쪽 공부가 재밌긴 하지만, ‘내가 다른 사람보다 잘 할 수 있을까?’, ‘경력이 너무 모자른건 아닐까?’라는 생각이 많다. 물론 그렇다고 모델러의 역할을 잘 하는 것 같지도 않다ㅎㅎ. 팀원들이랑 얘기도 많이 해보고, 추후에 글또 모임을 또 갖게 된다면, 다양한 분들과 얘기를 하고 싶다. 글을 쓰다보니 마음이 정리되는 것 같은데, 이야기를 나누다 보면 고민도 어느정도 풀리겠지.</p><h4 id="소중한-사람들을-잘-챙기려고-노력하고-있다"><a href="#소중한-사람들을-잘-챙기려고-노력하고-있다" class="headerlink" title="소중한 사람들을 잘 챙기려고 노력하고 있다."></a>소중한 사람들을 잘 챙기려고 노력하고 있다.</h4><p>소중한 사람들을 챙길 때 마음이 좋아지고 행복해질 때가 있는 반면에, 썩 기분 좋지 않은 일도 있었다. 보통 어떤 사람을 챙겨줄 때 내가 기대한 만큼 고마워 한다거나 그 마음이 느껴지면 행복하고 기분이 좋아졌는데, 그냥 항상 있는 일인듯 양, 반응을 보이면 기분이 좋지 않았다. 실망했다고 해야하나? 난 소중하게 생각했는데, 상대방은 그렇지 않은 것 같다고 느낄 때 마음이 좋지 않았다. <del>INFJ형 인간은 이렇때 과감하게 싹을 잘라내 버리는 편</del> 이긴 하지만, 왠만하면 내가 더 잘해야지 라는 마음가짐으로, 차카게 살려고 노력하고 있다.<br><img src="https://lh3.googleusercontent.com/proxy/iprrbOIjA2Qg8N4Zwp5yR8WhFsA5GhhtRtOgBpjslArKS8nRIYV3BsuzRNfuF73aD0XktO_ig1ZAb6jwnLNfE8obxzn8cHefAgW9hc3J6jMkG4IxMph3rwSl33WSfNuF8MQc-2H-alrC9lhnX3pzzAzKZtWDHAmt9GwDaucni3vQ08EbrH82c_M" alt="짜증을 내어서 무얼하나요">  </p><hr><h3 id="회고를-하면서"><a href="#회고를-하면서" class="headerlink" title="회고를 하면서."></a>회고를 하면서.</h3><p> 내가 어떻게 살아왔는지에 대해서 고민하면서 생각을 많이 정리할 수 있어서 좋았다. 한 동안 이런 시간이 없었는데, 강제로라도 회고글을 쓰게 해주는 글또에 큰 감사함을 느낀다. 오랜 고민을 하고 생각을 정리한 끝에 내가 뭘 해야 하고 어떤 행동을 할때 행복한지 기분이 좋은 지에 대해서 덤으로 알게 되는 것 같다. 한 번쯤 뒤돌아 보는 삶은 괜찮은 것 같다. 더 바쁘게 살아야지 하면서 앞만 보고 달리다보면 지치게 된다고 생각한다. 가끔은 뒤돌아보고 내가 걸어온 길이 크게 돌아온 것은 아닌지, 오면서 놓친 것이나 잃어버린 건 없는지, 확인해보고 다음 걸음을 준비하는 게 어떨까 하고 글을 마무리해본다. </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/07/18/retrospect2020/#disqus_thread</comments>
    </item>
    
    <item>
      <title>인프라의 기초, Docker에 대해서 알아보자</title>
      <link>http://tkdguq05.github.io/2020/07/03/docker/</link>
      <guid>http://tkdguq05.github.io/2020/07/03/docker/</guid>
      <pubDate>Fri, 03 Jul 2020 00:18:28 GMT</pubDate>
      <description>
      
        &lt;p&gt;여러 서비스에서 다양하게 사용되고 있는 Docker에 대해서 알아보자.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>여러 서비스에서 다양하게 사용되고 있는 Docker에 대해서 알아보자.</p><a id="more"></a><p>main 출처 : 완벽한 it 인프라 구축을 위한 docker.<br>완벽한 it 인프라 구축을 위한 docker을 읽고 정리한글임을 밝힙니다.  </p><p>sub 출처 : <a href="https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html">https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html</a></p><h1 id="Docker"><a href="#Docker" class="headerlink" title="Docker?"></a>Docker?</h1><p><img src="https://i.pinimg.com/originals/4e/73/01/4e7301538895cdc19b0eb5f2a3b60730.png" alt="귀여운 도커"><br>도커는 애플리케이션의 실행에 필요한 환경을 하나의 이미지로 모아두고, 이미지를 사용해 다양한 환경에서 앱 실행 환경을 구축하고 운영하기 위한 오픈소스 플랫폼입니다. 도커는 내부에서 컨테이너를 사용하는데, 일반적으로 생각하는 물류시스템에서의 컨테이너를 생각해도 좋습니다. 컨테이너로 실어서 다른 곳에 나르는 것처럼, 다양한 개발환경을 컨테이너로 추상화하기 때문에 동일한 환경을 누구에게나 제공할 수 있습니다. 이렇게 동일한 개발환경을 제공하게 되기 때문에 프로그램의 배포 및 관리를 쉽게 할 수 있게 됩니다. 하루종일 환경세팅만 하다가 하루를 날린 경험이 있다면 도커로 환경세팅하는 것이 얼마나 감사한 일인지를 잘 느낄 수 있을 것 입니다. 도커를 이용해서 개발을 하게 되면 폭포형 개발에서 벗어나서 지속적 딜리버리가 가능한 구조의 개발 스타일이 가능해집니다. 도커를 이용한 블루 그린 디플로이먼트 방법이 그 예시 중 하나인데. 블루 그린 디플로이 먼트는 글 하단에서 자세하게 다뤄보도록 하겠습니다.</p><p>*참고로 도커 컨테이너를 가장 잘 사용하고 있는 회사는 구글이고, 모든 서비스들이 컨테이너로 동작하며 매주 20억 개의 컨테이너를 구동 한다고 합니다.</p><h1 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h1><p>도커는 컨테이너 기술을 활용합니다. 컨테이너 기술은 도커가 시작되면서 만들어진 기술은 아니고, 기존에 존재하던 기술이었습니다. 컨테이너란 호스트 OS상에 논리적인 구획, 즉, 컨테이너를 만들고, 어플리케이션을 작동하기 위해 필요한 라이브러리나 애플리케이션 등을 하나로 모아 마치 별도의 서버인 것처럼 사용할 수 있게 만든 것입니다. 호스트 OS의 리소스를 논리적으로 분리시키고 여러개의 컨테이너가 이것을 공유해 사용합니다. 컨테이너는 가볍고 속으로 작동합니다. 컨테이너는 가상화 기술을 사용해서 다양한 기능을 제공하게 되는데, 가상화에 대해서는 이 글을 참고하시면 좋습니다. <a href="https://tkdguq05.github.io/2020/06/07/virtualization/">가상화</a>. </p><p>컨테이너 기술의 장점을 잠깐 소개하자면, 기존의 컨테이너를 이용하지 않는 시스템에서는 하나의 OS 상에서 움직이는 여러 애플리케이션들에 대한 관리를 해주어야 합니다. 다양한 디렉토리와 IP주소를 공유하게 됩니다. 이런 개발 환경에서는 각 어플리케이션이 서로 영향을 받을 가능성이 높습니다. 반면에 컨테이너를 활용하면 OS나 디렉토리, IP 주소 등을 각 어플리케이션이 독립적으로 갖고 있는 것처럼 보이게 할 수 있습니다. 이런 개발환경에서는 마이크로 서비스가 구현될 가능성이 높습니다.</p><h1 id="Image"><a href="#Image" class="headerlink" title="Image"></a>Image</h1><p>개발자는 도커를 이용해서 자신이 개발한 프로그램에 필요한 모든 것이 포함되어 있는 도커 이미지를 작성합니다. 이 이미지는 도커의 가장 큰 특징 중 하나입니다. 개발자가 개발한 환경을 도커 이미지로 만들면 이 이미지를 기반으로 해서 컨테이너가 동작하게 됩니다. 이렇게 만들어진 이미지는 기본적으로 어디서든 동작합니다. ‘테스트 환경에서는 됐는데, 제품 환경에서는 안돈다’라는 리스크를 줄일 수 있게 됩니다. 이를 통해 지속적 딜리버리가 가능하게 되고 변화에 강한 시스템을 구축할 수 있습니다.  </p><p>데이터 사이언스 분야에서는 대량의 컴퓨터 리소스를 사용하게 되고 다양한 라이브러리들을 사용하게 되는데, 환경 세팅에 너무 힘을 쓰다보면 모델 개발에 집중할 수 없게 됩니다.(공감하시는 분들 많으실 겁니다( xgboost설치 할 때를 생각해보십시오). 이런 경우 환경을 도커 이미지로 모아두면 어디에서나 다른 환경에서도 작동하는 실행환경을 만들 수 있게 됩니다. </p><h1 id="Docker의-기능"><a href="#Docker의-기능" class="headerlink" title="Docker의 기능"></a>Docker의 기능</h1><p>도커에는 크게 세 가지 기능이 있습니다.</p><ul><li>Docker Build, 이미지 만들기</li><li>Docker Ship, 이미지 공유</li><li>Docker Run, 컨테이너 실행</li></ul><h2 id="Docker-Build"><a href="#Docker-Build" class="headerlink" title="Docker Build"></a>Docker Build</h2><p>도커는 앞서 소개했듯이, 프로그램 실행에 필요한 프로그램 본체, 라이브러리, 미들웨어, OS, 네트워크 등을 하나로 모아서 Image로 만듭니다. 그리고 이 이미지는 컨테이너의 바탕이 됩니다. 보통 Docker에서 빌드시에 권장하는 내용은 하나의 이미지에 하나의 어플리케이션만 넣어 두고, 여러개의 컨테이너를 조합해 서비스를 구축하는 것입니다.</p><p>도커 이미지를 만드는 방법은 Docker 명령어를 사용해 수동으로 만들 수도 있고, Dockerfile이라는 설정 파일을 만들어서 작성한 내용을 바탕으로 자동으로 이미지를 만들 수 있습니다. Dockefile을 사용하여 관리하는 것이 지속적 integration과 지속적 딜리버리 관점에서 바람직해 보입니다.</p><p>또한 도커 이미지는 겹쳐서 사용할 수 있다는 것이 중요한 특징입니다. 도커에서는 변경이 있었던 부분을 이미지 레이어로 관리합니다.<br><img src="https://adeeshfulay.files.wordpress.com/2017/09/docker-layers1.png?w=529" alt=""></p><h2 id="Docker-Ship"><a href="#Docker-Ship" class="headerlink" title="Docker Ship"></a>Docker Ship</h2><p>만들어진 이미지는 공유가 가능합니다. Docker Hub에 자유롭게 공개를 할 수 있습니다. 많은 이미지가 있으므로 원하는 내용이 있다면 받아서 사용이 가능합니다. 물론 가입은 해야합니다. 또한 도커는 github이나 bitbucket과 연계가 가능하기 때문에 github같은 곳에서 Dockerfile을 관리하고 거기에서 이미지를 자동으로 생성해서 Docker Hub에 공개 할 수도 있습니다. </p><h2 id="Docker-Run"><a href="#Docker-Run" class="headerlink" title="Docker Run"></a>Docker Run</h2><p>Docker는 리눅스 상에서 컨테이너 단위로 서버 기능을 작동시킵니다.  이 때 사용되는 것이 도커 이미지입니다. 이미지만 있다면 여러대의 컨테이너를 기동시키는 것도 가능합니다. 도커는 다른 가상화 기술과는 다르게 떠 있는 OS 상에서 프로세스를 실행시키는 것과 같은 속도로 빠르게 실행을 할 수 있습니다.<br><img src="https://subicura.com/assets/article_images/2017-01-19-docker-guide-for-beginners-1/vm-vs-docker.png" alt=""></p><p>도커는 하나의 리눅스 커널을 여러 개의 컨테이너가 공유하는 구조입니다. 각 그룹별로 프로세스나 파일에 대한 엑세스는 독립적으로 가져가게 됩니다. 이렇게 독립적으로 사용하게 위해 리눅스의 namespace나 cgroup등의 개념이 사용됩니다. </p><p>한 대의 호스트 머신에서 모든 도커 컨테이너를 작동시키고 운용하는 것은 힘들기 때문에 분산환경을 구축하고, 컨테이너 관리를 위해서 오케스트레이션 툴을 사용합니다. 컨테이너 오케스트레이션 툴 중 가장 핫하고 유명한 것은 Kubernetes입니다.</p><h2 id="Docker-Image-사용해보기"><a href="#Docker-Image-사용해보기" class="headerlink" title="Docker Image 사용해보기"></a>Docker Image 사용해보기</h2><h3 id="Docker-Install"><a href="#Docker-Install" class="headerlink" title="Docker Install"></a>Docker Install</h3><p>먼저 Docker를 사용하기 위해서는, 도커를 먼저 설치해야 합니다. Mac이나 Windows를 사용하시는 분들은 <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a> 에서 다운받아 간단하게 설치하실 수 있습니다. 설치 후에</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker version</span><br></pre></td></tr></table></figure><p>명령어를 쳤을 때, 이상 없이 나오면 성공입니다. </p><h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image pull [이미지명]</span><br></pre></td></tr></table></figure><p>이미지를 받는 방법은 간단합니다. <code>pull</code> 명령어를 통해서 원하는 이미지를 받으면 됩니다.<br>받고 싶은 이미지를 search를 통해서 검색하고 Pull해서 받아봅니다. 받은 이미지들의 목록은 <code>ls</code>를 이용해면 됩니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image ls</span><br></pre></td></tr></table></figure><p><img src="/images/image_ls.png" alt="이미지 목록"></p><p>생각보다 이미지가 너무 많습니다. 원하지 않는 이미지를 지우고 싶을 때는 <code>rm</code>을 사용하면 됩니다. 리눅스 명령어와 비슷합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image rm [이미지id]</span><br></pre></td></tr></table></figure><p>이미지 id를 적어주고 명령어를 실행시키면, 해당 이미지가 없어진 것을 볼 수 있습니다. 사용하지 않는 이미지를 제거하고 싶다면 <code>prune</code>을 사용하면 됩니다.</p><p>이번에는 컨테이너로부터 이미지를 직접 작성해 봅시다. 작성자에 ‘hyub’이라는 정보를 설정하고 webserver라는 컨테이너를 hyuby/webfront라는 이름으로 태그명을 지정해서 새 이미지를 작성해 보겠습니다. 명령어는 다음과 같습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container commit -a <span class="string">'hyub'</span> webserver tkdguq05/webfront:1.0</span><br></pre></td></tr></table></figure><p>이렇게 만들어진 이미지의 작성자 정보는 docker image inspect로 확인할 수 있습니다.</p><p>만든 hyuby/webfront 이미지를 그러면 업로드 해보겠습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image push [이미지명:태그명]</span><br></pre></td></tr></table></figure><p>위와 같은 명령어가 기본값입니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image push tkdguq05/webserver:1.0</span><br></pre></td></tr></table></figure><p>혹시 <code>denied: requested access to the resource is denied</code>이런 에러가 나온다면, Docke Hub에 로그인이 되어있지 않았기 때문일 가능성이 높습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login</span><br></pre></td></tr></table></figure><p>이 명령어를 통해 로그인 하도록 합시다. 그래도 에러가 난다면, Docker Hub와 이미지에 있는 작성자 정보가 다르기 때문입니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image tag tkdguq05/webserver:1.0 [change user]/webserver:1.0</span><br></pre></td></tr></table></figure><p>이런 식으로 작성자명을 변경한 후에 업로드 해보도록 합니다. 업로드가 완료되면! Docker Hub에 올린 이미지가 공개됩니다.</p><p><img src="/images/docker_hub.png" alt="업로드된 이미지"></p><h3 id="Docker-export"><a href="#Docker-export" class="headerlink" title="Docker export"></a>Docker export</h3><p>컨테이너나 이미지나 모두 export하고 다시 import 할 수 있습니다. export를 하게되면 tar파일로 떨어지게 됩니다. 이 파일을 import하면 이미지나 컨테이너를 그대로 사용할 수 있습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#docker image save [옵션] &lt;파일명&gt; [이미지명]</span></span><br><span class="line">docker image save -o export.tar tensorflow</span><br></pre></td></tr></table></figure><p>읽는 것도 간단합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image load -i export.tar</span><br></pre></td></tr></table></figure><p>다만 컨테이너는 명령이 조금 다릅니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#docker container export &lt;컨테이너 시별자&gt; &gt; 저장 파일명</span></span><br><span class="line">docker container <span class="built_in">export</span> webserver &gt; latest.tar</span><br></pre></td></tr></table></figure><p>이렇게 만들어 놓은 tar파일을 이용해서 이미지를 작성할 수 있습니다. 위에 있는 import 명령어를 활용하면 됩니다. </p><p>여기서 한 가지 이상한 점이 있습니다. export/import, save/load가 왜 따로 구분되어 있을까? 하는 점입니다. 둘의 차이는 무엇일까요?</p><h3 id="export-import-save-load의-차이"><a href="#export-import-save-load의-차이" class="headerlink" title="export/import, save/load의 차이"></a>export/import, save/load의 차이</h3><p>컨테이너를 export하면 컨테이너를 실행하는데 필요한 파일을 모두 압축된 아카이브로 모을 수 있습니다. 이 tar파일을 풀면 컨테이너의 root파일 시스템을 그대로 추출할 수 있습니다. 반면 save는 이미지의 레이어 구조도 포함된 형태로 tar로 모을 수 있습니다.</p><p>바탕이 되는 이미지는 같아도 export를 사용할 때와 save 명령을 사용할 때는 내부적인 디렉토리와 파일 구조가 다릅니다.<br>따라서 docker container export 명령에는 docker image import 명령을, docker image save명령에는 docker image load를 사용하는 게 좋습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/07/03/docker/#disqus_thread</comments>
    </item>
    
    <item>
      <title>클라우드 서비스의 기초. Virtualization, 가상화에 대해서</title>
      <link>http://tkdguq05.github.io/2020/06/07/virtualization/</link>
      <guid>http://tkdguq05.github.io/2020/06/07/virtualization/</guid>
      <pubDate>Sun, 07 Jun 2020 08:18:14 GMT</pubDate>
      <description>
      
        &lt;p&gt;IT infrastructure, 클라우드 서비스의 기초인 가상화에 대해서 알아보자!&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>IT infrastructure, 클라우드 서비스의 기초인 가상화에 대해서 알아보자!</p><a id="more"></a><h1 id="1-가상화에-대해-알기-전에-IT-Infrastructure"><a href="#1-가상화에-대해-알기-전에-IT-Infrastructure" class="headerlink" title="1.가상화에 대해 알기 전에 IT Infrastructure."></a>1.가상화에 대해 알기 전에 IT Infrastructure.</h1><p>가상화에 대해서 알기전에 IT infra는 어떤 종류가 있는지 알아볼 필요가 흔히 세미나를 돌다보면 흔하게 들리는 ‘온프레미스’라던가 ‘클라우드 서비스’라는 개념이 도대체 정확히 뭘 말하는 건지 알아야 실제로 사용하는 AWS가 어떻게 돌아가는지 이해해볼 수 있을 것이다.</p><h2 id="1-1-IT-Infrastructure"><a href="#1-1-IT-Infrastructure" class="headerlink" title="1.1. IT Infrastructure"></a>1.1. IT Infrastructure</h2><p>IT 인프라는 크게 4가지로 나눠볼 수 있을 것이다. 온프레미스, 퍼블릭 클라우드, 프라이빗 클라우드, 하이브리드 클라우드 가 그 종류이다.</p><ol><li>온프레미스(On-premise) : 쉽게 말해서 물리서버를 구축한다는 이야기이다. 회사에서 서버실을 따로 두고 있다면, 온프레미스 방식의 구조를 사용하고 있다는 걸 말한다. 온프레미스 방식은 직접 서버를 놓고 사용하기 때문에 관리가 용이하고 기밀성이 굉장히 높다는 장점을 갖고 있지만 고가의 장비를 사용해야 하고 사용량 예측이 불가하며, 재해 등에 취약하다는 단점이 있다.</li><li>퍼블릭 클라우드(Public Cloud) : 클라우드 제공자가 물리서버(데이터 센터, 인프라 등)를 구축하고 가상화 기술을 이용해 불특정 다수에게 제공하는 시스템을 말한다. AWS, GCP, Azure등이 퍼블릭 클라우드 프로바이더이며, 서비스 이용자들이 퍼블릭 클라우드를 이용하는 것이라고 볼 수 있다. </li><li>프라이빗 클라우드(Private Cloud) : 회사에서 자체적으로 물리서버를 구축하고 가상화하여 클라우드 기술을 사용하는 것을 말한다. 높은 기밀성이 필요한 회사에서 주로 사용한다. 고객 정보에 대한 데이터 등의 기밀성이 높은 데이터를 AWS나 기타 퍼블릭 클라우드에 올릴 수 없을 때 자체적으로 프라이빗 클라우드를 구축하곤 한다.(금융회사 또는 삼성)</li><li>하이브리드 클라우드(Hybrid Cloud) : 온프레미스와 클라우드를 동시에 사용하는 방식이다.</li></ol><p>위에서 소개한 것처럼 클라우드에 대해서 다룰 때 가상화란 개념이 등장한다. 클라우드 서비스 유형에 따라 Iaas, Paas, Saas 등이 나뉘지만 이 글에서는 설명을 생략한다.</p><h1 id="2-가상화"><a href="#2-가상화" class="headerlink" title="2. 가상화"></a>2. 가상화</h1><p>가상화는 간단하게 설명하자면 하나의 물리적인 하드웨어 위에 여러 대의 OS를 올려 운영하는 것이다. 물리적인 CPU/서버를 이용할 때 하나의 운영체제를 이용하여 구동하는 경우에는 CPU 자원을 최대한으로 활용하기가 어려울 것이다. 하지만 하나의 물리적 CPU에서 여러 대의 가상 OS를 운영한다면 CPU 자원을 최대한 활용하는 효과를 가져올 수 있다. 그리고 자원이 많이 남는다면 AWS나 GCP처럼 퍼블릭 클라우드 서비스를 제공하는 것도 가능할 것이다. </p><h2 id="2-1-가상화의-구조-Hypervisor"><a href="#2-1-가상화의-구조-Hypervisor" class="headerlink" title="2.1 가상화의 구조 - Hypervisor"></a>2.1 가상화의 구조 - Hypervisor</h2><p>한 컴퓨터를 가상화해서 여러 개의 가상 머신을 만들었다고 가정해보자.<br><img src="/images/hypervisor.png" alt="세대의 가상머신을 만들어보자"><br>그림에서 보면 가상 머신은 Hypervisor위에서 돌아가고 Hypervisor는 OS 위에 위치하고 있는 것을 알 수 있다. 각 가상 머신은 각각의 OS(Linux, MacOS, WindowsOS…)를 갖고 돌아갈 것이며 Hypervisor 밑에 있는 OS는 물리서버의 OS이다. 이런 구조라면 Hypervisor의 기능은 여러 OS를 관리하는 것이 아닐까? 라고 눈치 빠르게 유추해 볼 수 있을 텐데, 정답이다. OS는 Kernel을 통해서 자원관리나 명령을 해석하고 컨트롤하는데 문제는 OS마다 Kernel의 규칙이 다르다는 점에 있다. 그러니까 가상머신에는 다양한 OS가 돌아가게 되고 이것들을 중재해 주는 것이 Hypervisor이다. Hypervisor 자체도 가상화 커널이기 때문에 Hypervisor는 운영체제를 스케줄 하는 가상화 커널이라고 볼 수 있겠다.</p><h2 id="2-2-가상화-유형"><a href="#2-2-가상화-유형" class="headerlink" title="2.2 가상화 유형"></a>2.2 가상화 유형</h2><p>가상화 유형은 크게 두 가지로 볼 수 있다. Type-1과 Type-2형이다. OS를 사용하는가 안하는가에 따라 나뉜다.<br>출처: <a href="https://en.wikipedia.org/wiki/Hypervisor">Wikipedia - Virtualization</a> </p><p><img src="https://upload.wikimedia.org/wikipedia/commons/e/e1/Hyperviseur.png" alt="Virtualization"></p><ol><li>Type-1(Bare-Metal) : 호스트 OS가 존재 하지 않는다. 베어메탈은 “운영체제가 없는 컴퓨터 하드웨어”를 의미한다. “깡통 컴퓨터”라고 표현하면 적당할 것 같다. 하이퍼바이저가 하드웨어를  제어하고 그 위에 게스트 운영체제(Guest OS)를 올리는 방식이다. 완전히 제어하는 방식을 베어 메탈 하이퍼바이저라고 부른다. </li><li>Type-2 :호스트 OS가 존재한다. 전통적인 OS에 하이퍼바이저를 실행하고, 이 하이퍼바이저 위에서 게스트 운영체제를 실행하는 방식이다. 하이퍼바이저를 실행하는 운영체제를 HOST 운영체제라고 부른다. 기존에 사용하던 운영체제 위에, 애플리케이션을 실행 하듯이 새로운 운영체제를 올릴 수 있다. 기존 운영체제에 익숙한 일반 사용자들이 주로 접하는 하이퍼바이저다. </li></ol><p>가상화는 다시 전가상화 반가상화로 나누어 볼 수 있다. 하드웨어를 완전히 가상화 하고 DOM0를 통해 모든 접근을 처리하는 것이 전가상화이고 완전 가상화 하지 않고 HyperCall이라는 Interface를 통해 접근을 처리하는 것이 반 가상화이다. 전가상화는 DOM0를 통해 모든 처리를 하므로 GuestOS를 수정할 필요가 없지만 Hypervisor가 모든 명령을 중재하므로 성능이 느린 편이다. 반면에 반가상화는 OS커널을 수정해 다른 OS에서 내리는 명령을 Hypercall에서 번역해서 처리하기 때문에 성능이 빠른 편이다. OS커널을 수정해야 하기 때문에 오픈소스 OS만 사용가능하다.</p><h2 id="2-3-Hypervisor-기능"><a href="#2-3-Hypervisor-기능" class="headerlink" title="2,3 Hypervisor 기능"></a>2,3 Hypervisor 기능</h2><p>본격적으로 Hypervisor에서 사용가능한 기능들에 대해서 알아보자. AWS콘솔을 다루다 보면 자주나오는 용어들이 보일 것이다. 대표적으로 Migration, Snapshot, Templet이다.</p><h3 id="그-전에-용어-정리"><a href="#그-전에-용어-정리" class="headerlink" title="그 전에 용어 정리"></a>그 전에 용어 정리</h3><p>기능을 다루기 전에 용어 정리를 하고 넘어가야 이해가 쉬울 것 같다. 가상화의 구조에는 Host, Cluster, Datacenter가 있다. </p><h4 id="Host"><a href="#Host" class="headerlink" title="Host"></a>Host</h4><p>Host는 Storage 에 붙어있는 한 물리서버를 말한다. Host의 기준이 헷갈릴 수 있는데, Storage 기준이 아니라 물리서버라는 점을 명심하자.<br><img src="/images/host.png" alt="Host 머신"> </p><h4 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h4><p>Cluster는 여러대의 Host머신을 한 Storage로 묶어 놓은 것을 말한다. 클러스터는 한 Storage로 묶일 수 있다.<br><img src="/images/cluster.png" alt="Cluster"></p><h4 id="Datacenter"><a href="#Datacenter" class="headerlink" title="Datacenter"></a>Datacenter</h4><p>Datacenter는 여러대의 Cluster를 하나로 묶어 놓은 것을 말한다.<br><img src="/images/datacenter.png" alt="Datacenter"><br>정리하자면 데이터 센터 아래에 여러대의 클러스터가 있고 각 클러스터에는 여러대의 호스트 머신이 띄워져 있으며, 호스트 머신에는 가상머신들이 떠 있다. </p><h4 id="Provisioning"><a href="#Provisioning" class="headerlink" title="Provisioning"></a>Provisioning</h4><p>쉽게 말해서 제공하는 것. 클라우드 서비스 업체에서 어떤 걸 제공받느냐에 따라서 Server Resource Provisioning : CPU, Memory, IO 등과 같은 실제 서버의 자원을 할당해주고 운영할 수 있게 제공해주는 것을 말한다.</p><p>출처 : <a href="https://jins-dev.tistory.com/entry/%ED%94%84%EB%A1%9C%EB%B9%84%EC%A0%80%EB%8B%9DProvisioning-%EC%9D%B4%EB%9E%80">프로비저닝이란</a></p><ul><li>OS Provisioning : OS를 서버에 설치하고 구성작업을 해서 사용할 수 있도록 제공하는 것을 말한다.</li><li>Software Provisioning : WAS, DBMS 등의 소프트웨어를 설치하고 세팅하여 실행할 수 있도록 제공하는 것을 말한다.</li><li>Account Provisioning : 접근 권한을 가진 계정을 제공해주는 것을 말한다. 클라우드 인프라 쪽에서는 해당 업무를 담당하던 관리자가 변경된 경우 권한의 인계를 Account Provisioning 을 통해 하는 경우가 많다.</li><li>Storage Provisioning : 데이터를 저장하고 관리할 수 있는 Storage 를 제공할 수 있다. 특히 클라우드에서는 제공하는 Storage 의 종류와 용도에 따라 다양한 방식의 제공이 이루어진다.</li></ul><h3 id="2-3-1-Migration"><a href="#2-3-1-Migration" class="headerlink" title="2.3.1 Migration"></a>2.3.1 Migration</h3><p>Migration은 한 클러스터 내에서 어떤 호스트에 붙어있는 가상머신을 다른 호스트로 옮기는 것을 말한다. Storage가 같기 때문에 가상머신을 다른 호스트로 옮기는 것이 가능하고 데이터도 그대로 사용가능하다.<br><img src="/images/migration1.png" alt="migration"></p><h3 id="2-3-2-Storage-Migration"><a href="#2-3-2-Storage-Migration" class="headerlink" title="2.3.2 Storage Migration"></a>2.3.2 Storage Migration</h3><p>Storage Migration이란 Storage를 갈아타는 Migration이라고 생각하면 된다. 한 클러스터에 있는 가상머신을 다른 클러스터로 옮기는 것이다.<br><img src="/images/migration2.png" alt="Storage migration"></p><blockquote><p><code>중요한 점</code>: 구조를 살펴보면 Storage가 가장 핵심인 것을 알 수 있다. Storage위의 가상머신 등은 얼마든지 죽어도 다시 살릴 수도 있고 다른 호스트로 옮길 수도 있지만 Storage는 죽으면 참…답이 없다. 이에 대해서 여러가지 처리를 해놓았지만 이 글에서는 다루지 않겠다.</p></blockquote><h3 id="2-3-2-Snapshot"><a href="#2-3-2-Snapshot" class="headerlink" title="2.3.2 Snapshot"></a>2.3.2 Snapshot</h3><p>스냅샷은 특정 시간대의 가상머신 데이터와 설정 정보를 백업하는 기술이다. Snapshot으로 저장된 백업 데이터를 이용하면 VM에 장애가 발생하더라도 빠르게 복구 가능하다.</p><h3 id="2-3-3-Templet"><a href="#2-3-3-Templet" class="headerlink" title="2.3.3 Templet"></a>2.3.3 Templet</h3><p>서버 템플릿이란 서버 다수를 사용하는데 사용할 수 있도록 공통 요소들을 프로비저닝해 둔 서버 이미지를 말한다.</p><hr><p>기능들에서 살펴보니 AWS에서 사용되는 여러가지 용어들이 익숙하게 보이는 것을 확인할 수 있다. 이런 지식들을 가지고 AWS나 기타 클라우드에서 활용할 수 있고 후에 Docker나 Kubernetes 등에도 활용할 수 있을 것이다.  </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/06/07/virtualization/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Google Cloud Platform - Task</title>
      <link>http://tkdguq05.github.io/2020/05/19/google-task/</link>
      <guid>http://tkdguq05.github.io/2020/05/19/google-task/</guid>
      <pubDate>Tue, 19 May 2020 00:13:46 GMT</pubDate>
      <description>
      
        &lt;p&gt;Google Task를 통해 비동기처리에 대한 Toy Project를 진행해보자.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Google Task를 통해 비동기처리에 대한 Toy Project를 진행해보자.</p><a id="more"></a><h1 id="1-Google-Cloud-Task"><a href="#1-Google-Cloud-Task" class="headerlink" title="1. Google Cloud Task"></a>1. Google Cloud Task</h1><p>먼저 Google Cloud Task가 무엇인지 살펴보자. Google Cloud Tasks는 대규모 분산형 태스크의 실행, 디스패치, 전송을 관리할 수 있는 완전 관리형 서비스이다. Cloud Tasks를 사용하게 되면 사용자 또는 서비스 간 요청 이외의 작업을 <strong>비동기적으로</strong> 수행할 수 있게 된다. 여기서 비동기적으로 작업을 수행한다는 말이 있다. 비동기적으로 작업을 수행한다는 것은 어떤 의미이고 어떤 상황에서 사용해야 할까?</p><h1 id="2-비동기-처리"><a href="#2-비동기-처리" class="headerlink" title="2. 비동기 처리"></a>2. 비동기 처리</h1><p>먼저 동기식 처리가 뭔지 알아보자. 동기식 처리 모델(Synchronous processing model)은 직렬적으로 태스크(task)를 수행한다는 의미이다. 즉, 태스크는 순차적으로 실행되며 어떤 작업이 수행 중이면 다음 작업은 대기하게 된다. 반면, 비동기식 처리는 (Asynchronous processing model 또는 Non-Blocking processing model)은 병렬적으로 태스크를 수행한다. 즉, 태스크가 종료되지 않은 상태라 하더라도 대기하지 않고 다음 태스크를 실행한다. 예를 들어 서버에서 데이터를 가져와서 화면에 표시하는 태스크를 수행할 때, 서버에 데이터를 요청한 이후 서버로부터 데이터가 응답될 때까지 대기하지 않고(Non-Blocking) 즉시 다음 태스크를 수행한다. 이후 서버로부터 데이터가 응답되면 이벤트가 발생하고 이벤트 핸들러가 데이터를 가지고 수행할 태스크를 계속해 수행한다. (출처 : <a href="https://poiemaweb.com/js-async">https://poiemaweb.com/js-async</a>)  </p><p>다른 상황을 가정해보자. 어떤 API가 두 개 있다. 여기서 정보를 가져올 건데, get_user_list라는 API에서는 이름만을 가져올 수 있는 API이고, get_user_record에는 특정 유저의 구체적인 정보가 담겨 있다. get_user_record는 제한 조건이 있어 한번에 많은 데이터를 가져올 수 없다. 예를 들어 get_user_list에서 모든 user list를 받는다고 하더라도 bulk로 get_user_record에 쿼리를 보내 데이터를 갖고 오는 것은 불가능하다는 것이다. 이런 조건에서 빠르게 데이터를 확보하려면 어떻게 해야할까? for loop을 돌려 user name 하나씩 정보를 얻어 온다면 많은 시간이 걸릴 것이다. </p><p>비동기 처리를 활용해 보자. 워커를 여러 대 만들고 워커에 수행할 작업(get_user_record에서 name별 정보를 갖고 오는 일)을 넣어주면 훨씬 빠르게 데이터를 모을 수 있을 것이다. 그렇다면 파이프 라인은 다음과 같다.</p><ol><li>get_user_record에 쿼리를 날리고 실시간성 DB에 데이터를 담아두기</li><li>실시간성 DB에 있는 데이터를 배치성 DB에 넣기<br>여기서 1번 작업은 Google Cloud Function에 작업 내용을 넣어주고 Google Cloud Task를 이용해서 비동기 작업을 통해 빠르게 수행하도록 할 것이다. </li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">https://us-central1-contxtsio<span class="number">-267105.</span>cloudfunctions.net/get_user_list</span><br><span class="line">param: &#123;&#125;</span><br><span class="line"></span><br><span class="line">https://us-central1-contxtsio<span class="number">-267105.</span>cloudfunctions.net/get_user_by_name</span><br><span class="line">param: &#123;</span><br><span class="line"><span class="string">"name"</span>: <span class="string">"Braund, Mr. Owen Harris"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>다음과 같이 API가 있다. 첫 번째 API는 user의 list를 받아올 수 있는 API이고, 두 번째는 user의 name별로 정보를 가져올 수 있는 API이다. 먼저 Local에서 정보를 제대로 받아오는지 테스트를 해보자.</p><h1 id="Toy-Project"><a href="#Toy-Project" class="headerlink" title="Toy Project"></a>Toy Project</h1><h2 id="Toy1-get-user-list-get-user-by-name-확인"><a href="#Toy1-get-user-list-get-user-by-name-확인" class="headerlink" title="Toy1. get_user_list, get_user_by_name 확인"></a>Toy1. get_user_list, get_user_by_name 확인</h2><p>API에서 정보를 받기 위해서 자주 사용하는 라이브러리인 requests를 이용한다. <a href="https://www.geeksforgeeks.org/get-post-requests-using-python/">requests 쉽게 사용하기</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_list'</span></span><br><span class="line">response = requests.post(url)</span><br><span class="line">user_list = response.json()</span><br><span class="line">user_list = user_list[<span class="string">'data'</span>]</span><br></pre></td></tr></table></figure><p>user_list는 아주 잘 나온다. 이제 get_user_by_name을 확인해보자</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">url2 = <span class="string">'https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_by_name'</span></span><br><span class="line">params = &#123;<span class="string">'name'</span> : user_list[<span class="number">0</span>]&#125;</span><br><span class="line">response = requests.post(url2, json=params)</span><br><span class="line">response.json()</span><br></pre></td></tr></table></figure><p>post로 parameter를 보내면 역시 값이 잘 나온다. </p><h2 id="Toy2-mongoDB-설정"><a href="#Toy2-mongoDB-설정" class="headerlink" title="Toy2. mongoDB 설정"></a>Toy2. mongoDB 설정</h2><p>작업을 수행하고 나온 데이터를 임시로 넣을 DB는 mongoDB로 해볼 것이다. 무료로 사용할 수 있는 mongoDB가 있는데 <a href="https://mlab.com/">https://mlab.com/</a> 여기를 이용하면 된다. 아니면 mongoDB Atlas를 사용해도 된다. mongoDB 구축하는 건 간단하니 Pass한다.</p><h2 id="Toy3-gcloud-설정"><a href="#Toy3-gcloud-설정" class="headerlink" title="Toy3. gcloud 설정"></a>Toy3. gcloud 설정</h2><p>google cloud의 다양한 서비스를 이용하려면 gcloud를 설정해야 한다. google cloud Task를 사용하기 위한 퀵 가이드 문서는 잘 되어 있는 편이다. <a href="https://cloud.google.com/tasks/docs/quickstart-appengine">Cloud Task Quick Guide</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gcloud</span><br></pre></td></tr></table></figure><p>gcloud를 먼저 설치하고 따라하면 된다. SDK를 만들고 난 후에 Cloud Tasks API, API 라이브러리 페이지에 갔을 때 사용가능 하다는 초록 표시가 나오면 퀵 가이드 문서의 샘플 설정 및 하단의 내용을 수행하면 된다.<br><img src="/images/available.png" alt="사용 가능!"></p><p>퀵 가이드 문서 내용을 다 따라하고 task페이지로 오게 되면 my-queue가 생성된 것을 확인할 수 있다. 이렇게 되면 성공이다.<br><img src="/images/my_queue.png" alt="task 작업 준비 완료"></p><h2 id="Toy4-Functions-설정"><a href="#Toy4-Functions-설정" class="headerlink" title="Toy4. Functions 설정"></a>Toy4. Functions 설정</h2><p>Cloud Functions로 이동해보자. Cloud Functions는 AWS Lambda와 비슷하다(서버리스 아키텍쳐) .<br>Functions를 세팅할 때 주의할 점이 있다. 위치나 메모리는 아무렇게나 설정해도 되지만 인증 부분을 꼭 체크하고 코드를 작성할 런타임을 python 3.7로 바꿔주도록 한다.<br><img src="/images/unauth.png" alt="인증되지 않은 호출 허용 체크"></p><p>조금 기다리면 funtion이 만들어진다. funtion에 들어가서 수정 버튼을 누르면 작업을 수행할 소스코드가 등장한다. main.py에 작업할 코드를 넣어주고 requirements.txt에 필요한 라이브러리들을 작성해 import할 수 있도록 한다.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#requirements.txt</span></span><br><span class="line">pymongo</span><br><span class="line">requests</span><br><span class="line">dnspython</span><br></pre></td></tr></table></figure><p>requirements에는 mongoDB 연결과 저장을 위한 pymongo, API를 위한 requests 그리고 기타 dnspython을 넣어준다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#MAIN.py</span></span><br><span class="line"><span class="keyword">from</span> pymongo <span class="keyword">import</span> MongoClient</span><br><span class="line"><span class="keyword">import</span> ast</span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">mongo_uri = <span class="string">"mongodb://****:****@ds263018.mlab.com:63018/****?retryWrites=false"</span></span><br><span class="line">client = pymongo.MongoClient(mongo_uri)</span><br><span class="line"></span><br><span class="line">db = client.****</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello_world</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="string">"""Responds to any HTTP request.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        request (flask.Request): HTTP request object.</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        The response text or any set of values that can be turned into a</span></span><br><span class="line"><span class="string">        Response object using</span></span><br><span class="line"><span class="string">        `make_response &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response&gt;`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    payload = request.get_data(as_text=<span class="keyword">True</span>)</span><br><span class="line">    request_json = ast.literal_eval(payload)</span><br><span class="line"></span><br><span class="line">    r = requests.post(url = <span class="string">"https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_by_name"</span>, json=request_json) </span><br><span class="line"></span><br><span class="line">    db.temp.insert_one(r.json())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"Success"</span></span><br></pre></td></tr></table></figure><p>main 소스에는 local에서 테스트한 내용을 넣어준다. mongo_url에서 retryWrite=false는 mongoDB에서 <code>MongoError: This MongoDB deployment does not support retryable writes. Please add retryWrites=false to your connection string.</code>이런 에러가 나온다면 사용해보자. </p><p>소스를 넣은 후에 잘 세팅 되었는지를 확인해보기 위해 테스트를 이용해 볼 수 있다. 테스트를 누르면 트리거 이벤트라고 나오고 <code>{}</code>이렇게 나와 있다. 여기에 테스트 할 파라미터 값을 넣어보면 된다. 우리의 파라미터는 name이므로 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"name"</span>: <span class="string">"Braund, Mr. Owen Harris"</span>&#125;</span><br></pre></td></tr></table></figure><p>이렇게 파라미터를 적어주고 테스트한다. success가 나오면 성공이다. </p><h2 id="Toy5-Task"><a href="#Toy5-Task" class="headerlink" title="Toy5. Task"></a>Toy5. Task</h2><p>이제 로컬로 돌아와서 task와 function을 붙여볼 차례다. 먼저 코드부터 소개한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> storage</span><br><span class="line"><span class="keyword">from</span> google.cloud <span class="keyword">import</span> tasks_v2</span><br><span class="line">task_client = tasks_v2.CloudTasksClient() <span class="comment"># Credential</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dispatch_task</span><span class="params">(name)</span>:</span></span><br><span class="line"> <span class="comment">#json 같은 string</span></span><br><span class="line">    payload = str(&#123;</span><br><span class="line">        <span class="string">"name"</span>: name,</span><br><span class="line">    &#125;)</span><br><span class="line">    </span><br><span class="line">    resp = create_task(project=<span class="string">'affable-**********'</span>, queue=<span class="string">'my-queue'</span>, location=<span class="string">'asia-northeast3'</span>, payload=payload)</span><br></pre></td></tr></table></figure><p>dispatch_task를 통해서 수행할 작업을 어떤 프로젝트에 연결하고 어떤 큐에 보낼 것인지 선택할 수 있다. 이렇게 프로젝트와 Task에 연결하고 난 뒤에 create_task를 수행하게 된다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_task</span><span class="params">(project, queue, location, payload=None, in_seconds=None)</span>:</span></span><br><span class="line">    parent = task_client.queue_path(project, location, queue)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Construct the request body.</span></span><br><span class="line">    task = &#123;</span><br><span class="line">            <span class="string">'http_request'</span>: &#123;  <span class="comment"># Specify the type of request.</span></span><br><span class="line">                <span class="string">'http_method'</span>: <span class="string">'POST'</span>,</span><br><span class="line">                <span class="string">'url'</span>: <span class="string">'https://us-central1-affable-audio-277311.cloudfunctions.net/function-2'</span> <span class="comment">#function url</span></span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> payload <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># The API expects a payload of type bytes.</span></span><br><span class="line">        converted_payload = payload.encode() <span class="comment">#여기서 인코딩</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add the payload to the request.</span></span><br><span class="line">        task[<span class="string">'http_request'</span>][<span class="string">'body'</span>] = converted_payload</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> in_seconds <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># Convert "seconds from now" into an rfc3339 datetime string.</span></span><br><span class="line">        d = datetime.datetime.utcnow() + datetime.timedelta(seconds=in_seconds)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create Timestamp protobuf.</span></span><br><span class="line">        timestamp = timestamp_pb2.Timestamp()</span><br><span class="line">        timestamp.FromDatetime(d)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add the timestamp to the tasks.</span></span><br><span class="line">        task[<span class="string">'schedule_time'</span>] = timestamp</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use the client to build and send the task.</span></span><br><span class="line">    response = task_client.create_task(parent, task)</span><br><span class="line">    print(response)</span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure><p>create_task에서 본격적으로 funtions와 연결이 되어 작업들이 돌아가게 된다. task에 보면 url 파라미터가 보인다. 이 url이 작업을 수행할 funtions의 위치를 나타내는 것으로, functions의 트리거 부분에 있는 url을 넣어주면 된다.   </p><p>이렇게 넣어주고 나면 payload를 받아서 인코딩을 해주고 task에서 request를 날려주게 된다. </p><p>함수를 다 지정하고 나서 user_list있는 name들을  dispatch_task에 넣어주게 되면 task가 돌아간다!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dispatch_task('Braund, Mr. Owen Harris') #하나짜리로 먼저 테스트</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> user_list:</span><br><span class="line">    dispatch_task(user)</span><br></pre></td></tr></table></figure><hr><h1 id="Trouble-Shooting"><a href="#Trouble-Shooting" class="headerlink" title="Trouble Shooting"></a>Trouble Shooting</h1><h2 id="Trouble-Credential"><a href="#Trouble-Credential" class="headerlink" title="Trouble, Credential"></a>Trouble, Credential</h2><p>바로 성공이 되는 경우가 있는 반면, 에러가 터져나와 제대로 돌아가지 않는 경우가 발생하곤 한다. 보통은 credential 문제가 대부분이다. dispatch_task의 윗부분에 #credential이라고 작성한 부분에서 보통 에러가 나는데</p><blockquote><p>DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see <a href="https://cloud.google.com/docs/authentication/getting-started">https://cloud.google.com/docs/authentication/getting-started</a></p></blockquote><p>이러한 에러가 발생한다면 Credential관련한 에러이며, 이런 에러가 나오지 않고 아래 for loop을 돌 때,</p><blockquote><p>AttributeError: ‘str’ object has no attribute ‘create_task’</p></blockquote><p>이런 에러가 나와도 task_client에서 credential이 제대로 되지 않았다는 뜻이다. </p><h2 id="Solution-1"><a href="#Solution-1" class="headerlink" title="Solution.1"></a>Solution.1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">task_client = tasks_v2.CloudTasksClient(&lt;json&gt;)</span><br></pre></td></tr></table></figure><p>다음과 같이 task_client에 cloud 프로젝트에 있는 json key를 받아서 절대경로를 <json>에 넣어준다.<br>json key는 다음과 같이 얻을 수 있다. 내 프로젝트로 들어가서 작업을 누르고 키를 받는다.<br><img src="/images/cloud_json_key.png" alt="json 키"></p><h2 id="Solution-2"><a href="#Solution-2" class="headerlink" title="Solution.2"></a>Solution.2</h2><p>이렇게 해결이 되는 경우도 있지만 사용하는 컴퓨터의 종류나 gcloud설정에 따라 제대로 되지 않을 수 있다. 두 번째 방법은 구글 문서에 있는 방법이다. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">explicit</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># Explicitly use service account credentials by specifying the private key</span></span><br><span class="line">    <span class="comment"># file.</span></span><br><span class="line">    storage_client = storage.Client.from_service_account_json(</span><br><span class="line">    &lt;json&gt;)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Make an authenticated API request</span></span><br><span class="line">    buckets = list(storage_client.list_buckets())</span><br><span class="line">    print(buckets)</span><br></pre></td></tr></table></figure><p>이 역시 아까 받은 json 키를 사용하는 방법이다. 이렇게 함수를 만든 뒤 explicit함수를 실행시키다.  그리고 다시 for loop을 돌려보자.</p><h2 id="Solution-3"><a href="#Solution-3" class="headerlink" title="Solution.3"></a>Solution.3</h2><p>마지막 방법이다. os 라이브러리를 이용해 google credential에 사용하는 json 키 값을 직접 지정해 주는 방식이다. 본인 생각으로는 가장 확실한 방법이라고 생각한다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'GOOGLE_APPLICATION_CREDENTIALS'</span>] = <span class="string">'&lt;json&gt;'</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/05/19/google-task/#disqus_thread</comments>
    </item>
    
    <item>
      <title>pyspark trouble shooting, schema</title>
      <link>http://tkdguq05.github.io/2020/05/10/trouble-shooting_pyspark/</link>
      <guid>http://tkdguq05.github.io/2020/05/10/trouble-shooting_pyspark/</guid>
      <pubDate>Sun, 10 May 2020 08:32:26 GMT</pubDate>
      <description>
      
        &lt;p&gt;pyspark로 작업 하던 중 파일이 읽히지 않는다면?????&lt;br&gt;cannot cast DOCUMENT into a ArrayType&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>pyspark로 작업 하던 중 파일이 읽히지 않는다면?????<br>cannot cast DOCUMENT into a ArrayType</p><a id="more"></a><h1 id="Pyspark-Trouble-Shooting"><a href="#Pyspark-Trouble-Shooting" class="headerlink" title="Pyspark Trouble Shooting"></a>Pyspark Trouble Shooting</h1><p>간만에 pyspark로 작업할 일이 생겼다. 거의 한 달 만에 쓰는 거라 조금 어색했다. purchase관련 데이터와 view데이터를 갖고 작업을 해야 했었다. Purchase 데이터로 분석을 끝내고, view 데이터를 열어봤다. 파일이 커서 돌려놓고 dataframe을 만든 후 show()를 통해서 잘 불러왔는지 확인하려고 했었다. </p><p><img src="/images/pyspark/pyspark_error.png" alt="에러..."></p><blockquote><p>cannot cast DOCUMENT into a ArrayType</p></blockquote><p>자주 보지만 정이 안드는 친구가 등장했다. 도대체 이해가 되지 않았다. purchase데이터를 읽어올 때와 똑같은 방식으로 schema를 지정해줬고 변수명도 다 바꿔서 문제가 없을 줄 알았는데 에러가 발생한 것이다. DOCUMENT를 ArrayType으로 바꿀 수 없다는 내용인데… 왜 아까는 됐고 지금은 안되는지 참… 답답했다.</p><p>에러메세지를 복붙해서 구글을 뒤져봤지만, 문제를 해결할 만한 소스는 없었다. 결국 혼자 답을 찾아보기로 했다. </p><h2 id="데이터의-구조"><a href="#데이터의-구조" class="headerlink" title="데이터의 구조"></a>데이터의 구조</h2><p>그 전에 사전지식으로 알아야 할 점은, 데이터가 어떻게 구성되어 있느냐이다. 글을 읽는 사람들의 이해를 돕기위해 간단하게 설명을 해보자면, purchase데이터에는 사용자의 구매내역이 들어있고 이는 purchase라는 칼럼에 잘 담겨있다. 구매내역이란 구매한 상품 내용, 상품의 갯수, 상품의 가격, 상품의 이미지 등등이 들어있다. 이 데이터는 한 칼럼에 담겨있으므로 묶어줄 수 있는 자료구조가 필요하다. 여기서 사용되는 자료구조는 list이며, 리스트 안에는 dictionary형태로 담겨 있다.  </p><blockquote><p> [{‘purchaseGoods’ : ‘값싸고질좋은 상품’}, …]<br>예를 들면 이런식으로 담겨 있는 것이다. pyspark로 데이터를 불러올 때는 schema를 지정해서 가져온다. list로 묶여있는 경우에는 ArrayType(list가 pyspark에서는 ArrayType으로 나타난다)으로 지정하고 이걸 purchase 데이터에 적용했을 때에는 너무나 잘 불러와졌었다. printSchema를 쳐봐도 잘 나왔다.  </p></blockquote><h2 id="데이터의-차이점"><a href="#데이터의-차이점" class="headerlink" title="데이터의 차이점"></a>데이터의 차이점</h2><p>view데이터를 불러오는 코드를 다시 들여보고 printSchema를 하니 역시 잘나왔다. spark의 신기한 점 중에 하나인데, spark는 직접 작업을 수행하는, collect나 show 등을 수행하기 전까지는 작업 스케쥴링만 해놓고 실제로는 돌지 않는다. 그렇게 지나가려는 순간 이상한 점을 발견했다.</p><p><img src="/images/pyspark/goods_dict.png" alt="purchase의 schema"></p><p><img src="/images/pyspark/purchase_list.png" alt="view의 schema"></p><p>둘의 구조를 비교해보자. 뭔가 다른 점이 보인다. 위의 goods에서는 그냥 dictionary로 묶여있고, 밑에 있는 purchase에는 list로 묶여있다. 이 차이점 때문에 동일한 schema를 사용하게 되면 에러가 발생하게 되는 것이다. </p><p>list는 ArrayType으로 지정하면 된다면, dictionary는 무엇일까?<br>dictionary는 StructType으로 지정하면 된다. 스키마를 제대로 지정하고 나면 제대로 데이터가 나오게 된다.</p><h4 id="purchase-Schema"><a href="#purchase-Schema" class="headerlink" title="purchase Schema"></a>purchase Schema</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">purchaseSchema =  StructType([</span><br><span class="line">    StructField(<span class="string">"purchaseGoodsName"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"purchaseGoodsCode"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"purchaseGoodsAmount"</span>, IntegerType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"purchaseGoodsCount"</span>, IntegerType(),<span class="keyword">True</span>)</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">userschema = StructType([</span><br><span class="line">    StructField(<span class="string">"cookieId"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"currentTime"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"sessionSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"userSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"purchase"</span>, ArrayType(purchaseSchema),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"actionType"</span>, StringType(),<span class="keyword">True</span>)</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure><h4 id="view-Schema"><a href="#view-Schema" class="headerlink" title="view Schema"></a>view Schema</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">viewSchema =  StructType([</span><br><span class="line">    StructField(<span class="string">"goodsName"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"goodsCode"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"goodsAmount"</span>, IntegerType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"goodsCount"</span>, IntegerType(),<span class="keyword">True</span>)</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">userschema = StructType([</span><br><span class="line">    StructField(<span class="string">"cookieId"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"currentTime"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"sessionSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"userSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"view"</span>, StructType(viewSchema),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"actionType"</span>, StringType(),<span class="keyword">True</span>)</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure><p>추가로 이런 타입들을 지정할 때는 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> StructType, ArrayType</span><br></pre></td></tr></table></figure><p>이런식으로 불러와야 한다. import가 되지 않았다면, 에러가 발생한다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/05/10/trouble-shooting_pyspark/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Studio3T trouble shooting</title>
      <link>http://tkdguq05.github.io/2020/05/10/trouble-shooting_studio3t/</link>
      <guid>http://tkdguq05.github.io/2020/05/10/trouble-shooting_studio3t/</guid>
      <pubDate>Sun, 10 May 2020 08:05:18 GMT</pubDate>
      <description>
      
        &lt;p&gt;Studio3T에서 bson file을 import하려고 했다. 근데 왜 안될까??&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Studio3T에서 bson file을 import하려고 했다. 근데 왜 안될까??</p><a id="more"></a><h1 id="Studio3T에-bson-파일-import하기"><a href="#Studio3T에-bson-파일-import하기" class="headerlink" title="Studio3T에 bson 파일 import하기"></a>Studio3T에 bson 파일 import하기</h1><p>Pyspark를 쓰던 중, MongoDB에서 데이터를 끌어다 써야 하는 일이 있어서 bson 파일을 Studio3T에 import 하기로 했다.<br>그냥 끌어다가 놓으면 될 줄 알았는데, 그런 건 되지 않았다. </p><p>먼저 콜렉션을 선택하고 import 버튼이 보여서 눌러봤다.<br><img src="/images/studio3t/studio3t_import.png" alt="import누르면 나오는 화면"></p><p>내가 import 하려는 파일은 bson이고 파일 하나만 있으니까 mongodump archive를 눌렀다.<br>그럼 여러 옵션들이 나오는데 무시하고 경로만 설정해서 Execute시켜줬다.</p><p>하지만 진행이 되지 않았고</p><blockquote><p>Import mongodump folder groobee_<strong><strong><em>for_feature_engineering: error creating collection<br>groobee</em></strong></strong><em>for_feature_engineering: error running create command: The field ‘background’ is not valid for an _id<br>index specification. Specification: { v: 2, name: “_id</em>“, ns: “groobee_****_for_feature_engineering”, background: true,<br>key: { _id: 1 } }</p></blockquote><p>위와 같은 에러가 등장하게 되었다. 대충 내용을 보아하니 ‘background’라는 필드가 유효하지 않다는 내용이었고 json형식처럼 보이는 게 확인 되었다. 열심히 구글링을 해보았지만, 관련된 내용은 거의 없었고 좌절하고 있던 찰나에 bson파일을 받을 때 같이 등장하는 metadata.json 파일이 생각났다.</p><h2 id="metadata-json"><a href="#metadata-json" class="headerlink" title="metadata.json"></a>metadata.json</h2><p>바로 확인해 보니</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"options"</span>:&#123;&#125;,<span class="attr">"indexes"</span>:[&#123;<span class="attr">"v"</span>:<span class="number">1</span>,<span class="attr">"key"</span>:&#123;<span class="attr">"_id"</span>:<span class="number">1</span>&#125;,</span><br><span class="line"><span class="attr">"name"</span>:<span class="string">"_id_"</span>,<span class="attr">"ns"</span>:<span class="string">"groobee_****.userDataInfo.****"</span>&#125;,</span><br><span class="line">&#123;<span class="attr">"v"</span>:<span class="number">1</span>,<span class="attr">"key"</span>:&#123;<span class="attr">"cookieId"</span>:<span class="number">1</span>,<span class="attr">"sessionSeq"</span>:<span class="number">1</span>&#125;,</span><br><span class="line"><span class="attr">"name"</span>:<span class="string">"cookieId_1_sessionSeq_1"</span>,<span class="attr">"ns"</span>:<span class="string">"groobee_****.userDataInfo.****"</span>,</span><br><span class="line"><span class="attr">"background"</span>:<span class="literal">true</span>&#125;,&#123;<span class="attr">"v"</span>:<span class="number">1</span>,<span class="attr">"key"</span>: .....</span><br></pre></td></tr></table></figure><p>아까 에러에서 본 내용들이 여기에 있었다. 에러가 왜 났는지 보아하니, DB안에 중복된 파일명이 있을까봐 bson파일명을 바꿨는데, metadata 안에는 이전의 파일명과 DB, collection명이 적혀있어서 Studio3T가 제대로 인식을 못한 것이었다.</p><p>metadata에서 DB와 Collection을 일일이 수정해준 후, 다시 한번 시도 해봤다.</p><h2 id="mongodump-folder"><a href="#mongodump-folder" class="headerlink" title="mongodump folder"></a>mongodump folder</h2><p><img src="/images/studio3t/mongodump_folder.png" alt="mongodump folder로 시도"><br>이번에는 아까와는 다르게 mongodump folder로 해봤다. 왜냐면 옆에 계시는 매니져님이 folder로 할때 더 잘되더라는 얘기를 들었기 때문이다. </p><p>이 방식은 아까와는 조금 다른데, 아무이름의 폴더를 하나 만들어 주고, 그 안에 DB에 있는, 내가 import하려는 대상collection명으로 폴더를 만들어야 한다. 폴더를 만들고 경로를 지정해 주자.</p><p><img src="/images/studio3t/dump_location.png" alt="경로 지정하고 옵션은 건들지 않았다"></p><h3 id="Execute"><a href="#Execute" class="headerlink" title="Execute"></a>Execute</h3><p>지정을 해주고 Execute를 시켜주면! 진행이 되기 시작한다.<br>그렇게 시간이 지나서 100%가 될 때까지 기다리다 보면, Restoring은 100%로 끝났는데 완료가 되지 않는 걸 볼 수 있다.<br>‘아 뭐야 다시해야 되나’ 하고 중지를 누른다면 소중한 시간을 날리게 된다. 이 bson데이터의 경우 780만 row로 꽤 큰 데이터였는데, 30분 정도 넣고 이상해서 중지하고 다시 시작하고… 이런 작업을 3번정도 반복했다.  </p><p>소중한 시간을 낭비하지 말고 기다리자. Restoring이 끝났다면, Restoring index작업이 남아있다. index를 restore하는 작업은 생각보다 오래걸린다. 인내심을 갖고 화장실을 다녀오든, 커피를 한잔하든 여유롭게 기다리다 보면.</p><p>초록색 동그라미와 함께 <code>done</code>이라는 메세지를 볼 수 있을 것이다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/05/10/trouble-shooting_studio3t/#disqus_thread</comments>
    </item>
    
    <item>
      <title>first</title>
      <link>http://tkdguq05.github.io/2020/04/17/first/</link>
      <guid>http://tkdguq05.github.io/2020/04/17/first/</guid>
      <pubDate>Fri, 17 Apr 2020 06:33:54 GMT</pubDate>
      <description>
      
        
        
          &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;

&lt;p&gt;kdkdsjafljsdalkfjsdlkfjlaskj.jfa.sjfa.j&lt;/p&gt;

        
      
      </description>
      
      <content:encoded><![CDATA[<a id="more"></a><p>kdkdsjafljsdalkfjsdlkfjlaskj.jfa.sjfa.j</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/04/17/first/#disqus_thread</comments>
    </item>
    
    <item>
      <title>What is Transformer?</title>
      <link>http://tkdguq05.github.io/2020/04/12/transformer/</link>
      <guid>http://tkdguq05.github.io/2020/04/12/transformer/</guid>
      <pubDate>Sun, 12 Apr 2020 07:16:01 GMT</pubDate>
      <description>
      
        &lt;p&gt;보는 논문 마다 Transformer와 Attention가 빠지지 않고 등장하곤 합니다.&lt;br&gt;이에 대해서 공부하고 정리해 봤습니다.&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>보는 논문 마다 Transformer와 Attention가 빠지지 않고 등장하곤 합니다.<br>이에 대해서 공부하고 정리해 봤습니다.</p><a id="more"></a><h1 id="Transfomer"><a href="#Transfomer" class="headerlink" title="Transfomer"></a>Transfomer</h1><p>글 미리보기 : </p><ul><li>Transformer는 번역에서 RNN 셀을 이용하지 않고 순차적 계산도 하지 않는다.</li><li>이를 통해 속도를 크게 향상 시켰다</li><li>성능도 크게 오르게 되었다. </li><li>RNN을 사용 안하는데 단어의 위치와 순서 정보도 활용할 수 있다.</li><li>인코더 디코더 방식을 활용한다</li></ul><h2 id="Attention은-뭐지"><a href="#Attention은-뭐지" class="headerlink" title="Attention은 뭐지?"></a>Attention은 뭐지?</h2><h3 id="번역을-하는-상황을-가정해보자"><a href="#번역을-하는-상황을-가정해보자" class="headerlink" title="번역을 하는 상황을 가정해보자"></a>번역을 하는 상황을 가정해보자</h3><p><a href="https://arxiv.org/abs/1706.03762">논문참고* Attention is All you Need</a>   </p><p>RNN을 통해 번역을 하는 상황을 가정해 보자. 한 단어를 다른 언어로 번역하는 일을 하기 위해서는 word embedding작업이 필요하다. 일단 임베딩에 관한 설명은 간단하게만 하고 넘어가자면, 텍스트를 수치화 하는 개념이다. 워드 임베딩이 끝난 후에 단어는 정해놓은 차원의 공간으로 임베딩 된다.</p><p>RNN은 인풋 벡터와 히든 state의 벡터를 받아 아웃풋 벡터를 뱉어 낸다. 신경망 번역기의 구성은(Seq 2 Seq) Encoder와 Deoder로 이루어져 있다. 인풋이 인코더로 들어오면 이를 기반으로 hidden state를 만들어내고 업데이트를 하게 된다. 업데이트 된 hidden state는 차례로 인코더에 input값과 들어가 최종 hidden state를 만들어내고, 이것이 디코더로 들어가서 인풋에 대한 번역된 아웃풋을 출력하게 된다.  이 과정에서 업데이트 된 마지막 hidden state는 디코더에게 전달되는 context라고 할 수 있다. decoder 역시  hidden state를 갖고 있고 time step이 지나가면서 하나씩 이 hidden state를 다음으로 넘기는 과정이다.<br><img src="https://jalammar.github.io/images/context.png" alt="hidden state"></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention!"></a>Attention!</h3><p>je suis etudiant라는 문장을 i am a student로 바꿀 때, je와 i라는 단어를 연결시켜 해석 하는게 더 정확한 결과를 가져오는 게 당연할 것이다. 여기서 특정 문맥을 더욱 상세히 보게 해주는 것이 바로 Attention이다. 즉, 순차적으로 계산되는 각각의 RNN cell에서 나오는 state를 모두 활용해보자는 것이다.(보통 RNN번역에서는 최종 hidden state만을 context로 이용해서 번역한다.)<br>이 Attention은 보통의 Seq2Seq모델과는 두 가지 점에서 차이를 보인다. </p><h4 id="Seq2Seq-model-with-Attention"><a href="#Seq2Seq-model-with-Attention" class="headerlink" title="Seq2Seq model with Attention"></a>Seq2Seq model with Attention</h4><ol><li>인코더는 디코더에 더 많은 데이터를 보낸다. 마지막 hidden state를 디코더에 보내는 대신, 모든 hidden state를 디코더에 보낸다.</li><li>Attention 디코더는 아웃풋을 내기 전에 추가적인 작업을 거친다. 각 타임스텝에 있는 디코더에 해당하는 인풋의 부분들에 집중을 하기 위해서, 디코더는 다음과 같은 작업을 한다.<ol><li>각 인코더의 hidden state는 인풋 문장의 특정 단어와 관련된 부분을 가지고 있다.  (먼저 step 4에 해당한다고 하자, step 3까지 만들어진 h1,h2,h3 인코더 hidden state가 존재)</li><li>각 hidden state에 대해서 점수를 매긴다.(점수를 매기는 법에 대해서는 일단 무시하고 넘어감)</li><li>점수를 softmax화 해서 이 점수로 각 hidden state를 곱한다. 이를 통해서, 높은 점수를 가진 hidden state를 더 상세히 보고, 점수가 낮은 hidden state는 빼낸다</li><li>가중치가 적용된 hidden state 벡터들을 합한다. </li></ol></li></ol><p><img src="/images/seq2seq.gif" alt="Seq2Seq with Attention"></p><p>여기서 만들어진 context vector는 step4에 있는 디코더를 위한 것이다. 점수를 매기는 작업은 각 time step의 디코더에서 진행된다.<br>이제 정리를 하자면,  </p><ol><li>Attention 디코더 RNN은 임베딩된 <END>토큰을 받고 디코더의 시작 hidden state를 받는다.</li><li>RNN은 인풋을 처리하고 아웃풋과 새로운 hidden state 벡터(h4)를 만든다. output은 버려진다.</li><li>Attention 단계에서, 우리는 인코더의 hidden state와 h4 vector를 이용해 context vector(C4)를 만들어내고 이것은 다음 time step에 사용된다.</li><li>h4와 C4를 하나의 벡터로 합친다. (concatenate, 갖다 붙인다)</li><li>이 벡터를 Feed forward neural network에 넘긴다.</li><li>feed forward neural network의 아웃풋은 이 time step에 대한 결과물을 가리킨다.</li><li>다음 time step까지 반복한다.</li></ol><p><img src="/images/process.gif" alt="Process"><br><img src="/images/wholeprocess.gif" alt="Whole Process"></p><h2 id="Illustrated-Transformer"><a href="#Illustrated-Transformer" class="headerlink" title="Illustrated Transformer"></a>Illustrated Transformer</h2><p>transformer는 Attention is All You Need의 논문에서 제안 되었다. 먼저 high level에서 살펴보자.</p><h3 id="A-High-level-look"><a href="#A-High-level-look" class="headerlink" title="A High level look"></a>A High level look</h3><p>번역기 모델을 가정하고, transfomer을 사용한다고 하면, 인풋은 transformer를 통해 처리되고 output 이 나오게 된다. transformer의 구성을 보면, 인코딩과 디코딩 파트, 그리고 이를 연결해주는 부분으로 이루어져 있다. 인코딩 파트는 encoder를 stack시켜놓은 구성이다. 인풋 벡터와 아웃풋 벡터가 같기 때문에 쌓는 것이 가능한 게 transformer의 특징이다. 디코딩 부분 역시 decoder가 stack되어 있는 모습이다. 인코더들은 모두 같은 구조를 갖지만 weight를 공유하지는 않는다. 각각은 self-attention과 Feed Forword Neural Network의 sub layer로 구성되어 있다.<br><img src="https://jalammar.github.io/images/t/The_transformer_encoders_decoders.png" alt="Transformer"><br><img src="https://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png" alt="Detailed"></p><p>인풋은 첫번째로 self-attention layer로 들어간다. 이 layer는 인코더가 인풋 문장에 있는 다른 단어들을 볼 때, 특정 단어로 인코드 하는 것을 돕는다. 이 self-attention을 중점적으로 보도록 하자. self-attention의 아웃풋은 feed forward NN으로 들어간다. 똑같이 생긴 feed forward NN이 독립적으로 각 포지션에 들어가 있다.(인코더 또는 디코더에 다 들어가 있음)</p><p>디코더는 특이한 점이 있는데, self-attention과 FFNN 모두를 갖고 있지만, 이 사이에 Encoder-Decoder Attention이라는 layer를 추가적으로 갖고 있다는 것이다. 이것은 attention layer로써 인풋 문장에 대한 적절한 부분들에 대해 집중할 수 있도록 도와주는 역할을 한다. </p><h3 id="이-구조로-번역을-해보자"><a href="#이-구조로-번역을-해보자" class="headerlink" title="이 구조로 번역을 해보자"></a>이 구조로 번역을 해보자</h3><p>번역을 위해서, 위에서 했던 것과 같이 word embedding부터 실시한다. 임베딩은 맨 아래 인코더에서부터 시작한다. 모든 인코더에 공통되는 부분은, 512차원의 벡터를 받는다는 것이다. 벡터의 길이는 하이퍼 파리미터로서 우리가 설정할 수 있는 부분이다. 보통 이 값은 훈련 셋에 있는 가장 긴 문장을 기준으로 설정된다. 단어 임베딩이 끝나면, 각 단어들은 인코더의 두 레이어로 들어가게 된다.(self attention layer, FFNN layer)</p><p>여기서 Transformer의 큰 특징이 드러난다. 각 포지션에 있는 단어는 지정된 인코더의 path를 따라 간다는 것이다. self-attention에는 이런 의존성이 존재한다. FFNN에는 의존성이 없지만 그러므로 FFNN을 타면서 여러 path들이 병렬로 처리가 가능하게 된다.</p><h3 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h3><p>인풋 벡터를 받으면 인코더는 이 벡터들을 self-attention 층으로 보낸다. 그리고 FFNN을 통하고, 결과물을 만들어 다음 인코더로 보내게 된다(구조상 위로 보낸다). 문장 번역을 하는 예를 들어 보자.<br>”The animal didn’t cross the street because it was too tired”<br><code>it</code>은 무엇에 해당되는 것일까? street일까 아니면 animal일까? 사람들에게는 아주 쉬운 질문이지만, 알고리즘 상으로 답을 내기에는 어려운 질문이다.</p><p>it을 처리할때, self-attention은 it을 animal과 연결하는 것을 허용한다. 모델이 각 단어를 처리할 때, self-attention은 다른 위치에 있는 인풋 시퀀스를 보는 것을 허용해서 이 단서들을 이용해 단어를 잘 인코딩 하도록 돕는 역할을 한다.</p><p>결국 self-attention은 인코딩 파트에서 Transformer가 다른 연관된 단어를 갖고 우리가 지금 처리 중인 것에 대해서 잘 이해할 수 있도록 하는 방법이다. 다른 말로하면, self-attention은 self-attention 점수를 각 단어마다 매겨서 단어와 단어끼리의 매칭 점수를 이용해 연관정도를 파악하는 것 이라고도 할 수 있다. 더 자세히 알아보자.<br><img src="https://jalammar.github.io/images/t/encoder_with_tensors_2.png" alt="encoder layer"></p><h3 id="Self-attention-in-detail"><a href="#Self-attention-in-detail" class="headerlink" title="Self-attention in detail"></a>Self-attention in detail</h3><p>self-attention이 벡터들을 갖고 어떻게 계산하는지 보도록 하자. <strong>첫 번째로</strong> self-attention의 계산에서는, 각 인코더의 인풋 벡터들에서 세 개의 벡터들을 생성한다. 각 단어에서 이제 Query Vecotor를 만들어낸다. Key Vector, Value Vector로 이루어진다. 이 벡터들은 훈련단계에서 학습된 3개의 행렬들을 곱하여 생성된다.(Q, W, K)</p><p>주목할 점은 이 새 벡터들이 임베딩 벡터보다 차원이 작다는 것이다. 이것들의 차원은 64이고, 임베딩과 인풋/아웃풋 벡터들은 512차원이다. 이 행렬들이 작아질 필요는 없다. 이것은 단순히 multihead attention 상수를 계산하기 위한 것이기 때문에 선택의 문제다.</p><h4 id="query-key-value-벡터들"><a href="#query-key-value-벡터들" class="headerlink" title="query, key, value 벡터들?"></a>query, key, value 벡터들?</h4><p>이 세 벡터들은 attention에 대해 생각할 때 유용하게 계산되는 abstraction들이다. 하단에 attention이 어떻게 계산되는지를 진행해보면, 이 세 벡터들의 역할에 대해서 잘 이해할 수 있다.</p><p><strong>두 번째로</strong> self attention점수를 내는 것이다. Thinking이라는 단어에 대해서 self-attention 점수를 계산한다고 해보자. 우리는 문장에 있는 이 단어에 대한 각 단어들의 점수가 필요하다. 점수는 특정 위치에서 단어를 인코딩 할 때 입력 문장의 다른 부분에 집중할 정도를 결정한다. 이 점수는 각 단어에 점수를 매길 때 query vector와 key vector의 dot product로 계산된다. 그래서 만약 우리가 #1포지션에서 self-attention을 한다면 첫 점수는 q1과 k1의 dot product로 계산될 것이다. 두 번째 점수는 q1과 k2의 dot product로 계산된다.</p><p><strong>세 번째와 네 번째는</strong> 점수를 8로 나눠주는 것이다.(key value의 차원에 루트 씌운 값) 이렇게 하게되면, Key 벡터의 차원이 늘어날수록 dot product 계산시 값이 증대되는 문제를 막아주게 되어 안정적으로 gradient를 흐르게 만들어 줄 수 있다. 그리고 결과를 softmax 처리에 보낸다. softmax는 점수를 normalize해서 그 값들이 모두 1까지 갖는 양수로 만든다. softmax 스코어는 이 위치에서 각 단어가 얼마나 표현될지를 보여주는 점수인데, 분명히 그 위치의 단어는 가장 높은 점수를 갖겠지만 가끔은 현재 단어와 관련된 다른 단어를 위치시키는 것이 좋을 때도 있다. 예를 들어 <code>it</code>이 어떤 걸 의미 하는가에 대해서 궁금할 때.</p><p><strong>다섯 번째는</strong> 각 value 벡터를 softmax 점수로 곱하는 것이다. 여기서의 포인트는 우리가 집중하고 싶은 단어의 값을 유지하고 관련없는 단어의 값을 떨어트리는 것이다. <strong>여섯 번째는</strong> 가중치가 곱해진 벡터들을 더하는 것이다. 이 과정을 통해 이 위치의 self-attention 레이어 값을 얻게된다. 이것을 통해 self-attention 계산이 완료된다. 결과로 나온 vector는 FFNN에 보낼 수 있는 벡터이다. 이 계산은 matrix form으로 되어있다면 더 빠르게 가능하다.(Matrix Factorization으로 한번에 계산이 가능함) 이제 단어 수준에서 계산을 살펴 보자.</p><h2 id="Matrix-Calculation-of-self-attention"><a href="#Matrix-Calculation-of-self-attention" class="headerlink" title="Matrix Calculation of self-attention"></a>Matrix Calculation of self-attention</h2><p><img src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" alt="Q, K, V"><br>첫 번째는 Q, K, V 행렬을 계산하는 것이다. 임베딩을 matrix X로 만들 때 훈련된 가중치 행렬(WQ, WK, WV)를 곱해서 이 세가지 행렬을 이미 만들어 냈다. X와 WQ를 곱해서 Q가 나오고 WK를 곱해서 K, WV를 곱해서 V가 나오게 된다.</p><p>행렬들에 관련된 것이기 때문에, 이 2-6댠계를 하나의 공식으로 응축하여 self-attention의 아웃풋을 계산할 수 있다. Matrix Factorization을 통해서.</p><h2 id="multi-head-attention"><a href="#multi-head-attention" class="headerlink" title="multi head attention"></a>multi head attention</h2><p>multi head attention은 attention layer를 head의 수 만큼 병렬로 수행하는 것을 말하는데, 이를 통해서 모호한 문장을 해석하는데 연관된 정보를 다른 관점에서 바라보게 만들어서 퍼포먼스를 상승시키는 효과가 있다(it 구분하기 등). attention layer의 퍼포먼스는 다음 두 가지 방법을 통해 향상시킨다.  </p><ol><li>multi head attention은 단어의 위치를 잡는 능력을 여러 위치들로 잡는 것으로 확장한다. 이전에 실시해서 얻은 값에는 다른 인코딩 값이 있긴 하지만, 단어 그 자체의 값에 의해 dominate 될 수 있다. multi head attention을 이용하면 “The animal didn’t cross the street because it was too tired”라는 문장에서 우리는 it이 의미하는 것이 무엇인지 번역할 때 유용할 것이다.</li><li>multi head는 attention layer에 representation subspace들을 제공한다. multi head attention을 통해 여러개의 QKV 가중치 곱 행렬을 가질 수 있다. 각 셋들은 랜덤하게 값이 들어가고, 훈련이 끝나면, 각 셋들은 input 임베딩들을 다른 representation subspace에 투영하는데 사용된다.</li></ol><p>만약 input X를 8개의 다른 attention head에 넣고 계산하게 되면, 8개의 다른 Z 행렬들이 등장하게 된다. FFNN은 사실 8개의 행렬에 대해 예상하지 못한다. FFNN은 single 행렬을 기대하게 되는데 그래서 우리는 이 여덟개의 행렬을 응축해 하나의 matrix로 만들어야 한다. </p><ol><li>concatenate 한다</li><li>W0 행렬을 가중치 matrix와 곱해 훈련한 데이터를 만든다</li><li>결과는 정보를 가진 모든 atttention head의 값을 가진 Z matrix이다. 이걸 FFNN에 보낸다</li></ol><p>이 과정을 그림으로 요약하면 다음과 같다<br><img src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" alt="multi head attention"></p><p>모든 attention head를 더하면 굉장히 다양한 해석이 등장한다.<br><img src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_3.png" alt="add all heads"></p><h2 id="Positional-encoding을-통한-시퀀스-순서-나타내기"><a href="#Positional-encoding을-통한-시퀀스-순서-나타내기" class="headerlink" title="Positional encoding을 통한 시퀀스 순서 나타내기"></a>Positional encoding을 통한 시퀀스 순서 나타내기</h2><p>모델을 설명하면서 하나 놓친 부분은, 인풋 시퀀스의 순서를 어떻게 설명하느냐이다. RNN에서는 문장의 길이가 짧긴 하지만 각 단어의 sequence 정보를 잘 활용할 수 있었다. 하지만, Transformer는 속도가 느린 RNN을 사용하지 않고 Matrix Factorization을 활용하기 때문에 시퀀스 정보를 전달해 주는 과정이 필요하다.<br><img src="/images/positional_encoding.png" alt="Positional Encoding"></p><p>transfomer는 각 인풋 임베딩에 벡터를 더하고, 이 벡터들은 상대적인 위치 정보들을 갖게 되어 모델이 학습하는 패턴을 따르게 된다. 이것을 통해 각 단어의 위치를 결정하고 또는 시퀀스에 있는 다른 단어들과의 거리를 결정한다. 여기서 포인트는 이러한 벡터 값을 임베딩에 더하면 Q / K / V 벡터로 투영 된 후 내적시 임베딩 벡터간에 의미있는 거리를 제공할 수 있다는 것이다. positional encoding을 통해서 얻을 수 있는 또 하나의 장점은 훈련 셋보다 긴 문장이 들어왔을 때에도 scale이 가능하다는 것이다.</p><h3 id="Residuals"><a href="#Residuals" class="headerlink" title="Residuals"></a>Residuals</h3><p>인코딩 구조에서 한 가지 더 설명할 부분은 residual이다. positional encoding은 학습이 진행될 수록 역전파에 의해서 정보가 많이 손실된다. 각 인코더에 있는 각 sub-layer에서 residual connection을 취해주고 더해줌으로써 이 정보 손실을 막아준다. Residual connection 후에는 layer-normalization 단계를 통해서 학습의 효율을 증진시켜준다. </p><p>인코더는 이렇게 작업이 끝난다. 정리하자면, 인코더는 임베딩 - multi head attention - FFNN 그리고 Residual Connection으로 이루어진다.<br><img src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" alt="residual connection in Encoder"></p><h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>가장 위의 encoder의 아웃풋은 attention 벡터 셋 K, V로 변환된다. 이 두 벡터들은 각 디코더의 질문에 사용된다. K와 V는 encoder-decoder attention 층에서 디코더가 인풋 시퀀스에 적절한 자리에 집중하는 것을 돕는다.  </p><p>디코더는 masked Multi head attention - multi head attention - FFNN의 구조로 이루어져 있다.  디코더 입력값은 Query로 사용되고 Encoder의 최종 결과값을 Key, Value로 사용한다. 이것은 디코더의 현재값을 Query로 encoder에 질문하는 모습이 되겠고, 인코더 출력값에서 중요한 정보를 K, V로 획득해서 디코더의 가장 적합한 다음 단어를 출력하는 과정이라고 볼 수 있다. </p><p>이 과정이 Decoder layer 에서 쭉 이어지게 되고, 이후에는 linear layer와 softmax layer를 지나게 된다. 일반적으로 softmax를 이용해서 가장 높은 확률값을 전달해주는 과정이 여기서도 이루어 지게 된다. 가장 높은 확률값을 지닌 단어가 다음 단어로 오게 된다. 하지만 여기서도 Lable smoothing을 통해 한층 더 성능을 높여준다. one hot encoding으로 값을 확확 죽이는 것 보다, 정답은 1에 가깝게, 오답은 0애 가깝게 만들어 주는 과정이다.  Thanks가 고맙다와 감사하다로 label된 것을 예로들면, 고맘다와 감사하다는 둘 다 잘못 label된 것이 아니다. 하지만 one hot encoding을 시켜버리면, 이 둘은 완전히 다른 결과값을 갖게 될 것이다. 이렇게 되면 학습이 효율적으로 학습이 진행되지 않게 되는데, 이를 방지하는 것이 label smoothing인 것이다.<br><img src="https://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="encoder-decoder"></p><p>위의 과정은 <code>&lt;EOS&gt;</code>가 나올 때까지 반복된다. 각 스텝의 아웃풋이 가장 아래의 디코더에 주입되고, 디코더는 디코딩된 결과를 bubble up한다. 인코더가 그랬던 것 처럼 디코더의 인풋에 임베딩하고 임베딩 벡터에 positional 인코딩을 취해 각 단어의 위치를 가리킨다.<br><img src="https://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="repeated"></p><p>encoder-decoder attention 층은 multihead self-attention과 비슷하게 작동하지만, Query Matrix를 층 아래에 생성한다는 것과 K, V matrix만 인코더 스택의 아웃풋에서 취한다는 것에서 차이가 난다.  </p><h2 id="The-Loss-Function"><a href="#The-Loss-Function" class="headerlink" title="The Loss Function"></a>The Loss Function</h2><p>‘merci’를 “thanks’로 바꾸는 작업을 한다고 해보자. 이 작업은 아웃풋이 확률 분포에서 thanks 단어를 가리키는 걸 원한다는 것을 뜻한다. 하지만 이 모델은 훈련되지 않았고, 제대로 번역되지 않을 것이다. 어떻게 잘 학습된 분포와 학습이 안된 분포를 비교할까? 간단하게, 하나를 잡고 다른 걸 빼면 된다. cross entropy나 Kullback-Leibler divergence를 살펴보자.</p><p>하지만 이 예는 너무 단순화한 예이다. 보통 우리는 문장단위의 번역을 한다. “je suis étudiant”를 “i am a student”로 번역한다고 해보자. 각 확률분포는 vocal_size의 길이로 표현된다. 여기서는 <eos>까지 포함하여 6개이다. target output은 1과 0으로 나오게 되지만 학습에서는 값이 확률로써 등장한다. </p><p><img src="https://jalammar.github.io/images/t/output_target_probability_distributions.png" alt="target model outputs example"><br><img src="https://jalammar.github.io/images/t/output_trained_model_probability_distributions.png" alt="train model outputs"></p><hr><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Jay Alammar : <a href="https://jalammar.github.io/">https://jalammar.github.io/</a><br>허민석님 유튜브 : <a href="https://www.youtube.com/watch?v=mxGCEWOxfe8&amp;t=786s">https://www.youtube.com/watch?v=mxGCEWOxfe8&amp;t=786s</a>)<br>medium 글 : <a href="https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09">https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09</a></p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/04/12/transformer/#disqus_thread</comments>
    </item>
    
    <item>
      <title>CRAFT 요약 Character Region Awarenness for Text Detection)</title>
      <link>http://tkdguq05.github.io/2020/03/29/craft/</link>
      <guid>http://tkdguq05.github.io/2020/03/29/craft/</guid>
      <pubDate>Sun, 29 Mar 2020 11:20:22 GMT</pubDate>
      <description>
      
        &lt;p&gt;이미지 효과가 있는 글자를 인식하는 CRAFT에 대해서 파헤쳐보자&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>이미지 효과가 있는 글자를 인식하는 CRAFT에 대해서 파헤쳐보자</p><a id="more"></a><h1 id="CRAFT-Character-Region-Awarenness-for-Text-Detection"><a href="#CRAFT-Character-Region-Awarenness-for-Text-Detection" class="headerlink" title="CRAFT (Character Region Awarenness for Text Detection)"></a>CRAFT (Character Region Awarenness for Text Detection)</h1><p><strong>Clova AI Research, NAVER Corp.</strong></p><hr><p><a href="https://arxiv.org/pdf/1904.01941.pdf">CRAFT 논문</a></p><p>이미지에서 정보를 얻기 위한 방법을 찾기 위해 SOTA 논문을 뒤지던 중에 발견한 논문. Naver Clova AI Research 팀에서 작성했다. </p><p>이 논문의 요약은 다음과 같다. 어떤 텍스트가 논문에 있는 글처럼 바르게 작성되어 있지 않고, 그래픽 효과로 휘어져 있거나, 크기가 각각 다른 경우에 CRAFT를 사용하면, text detection이 잘 된다는 것이다. 실제 OCR 프로젝트를 해 본 경험이 있다면, 글자가 조금만 틀어져도 인식이 제대로 되지 않는 다는 것을 느낄 수 있다. 그러한 점에서 이 논문이 눈에 띄었고 집중해서 볼 수 밖에 없었다.</p><p>그럼 도대체 왜 잘되는 것 일까? 하나 하나 파헤쳐 보자.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>CRAFT는 기본적으로 CNN으로 디자인되어 있고, 여기서 $region$ $score$과 $affinity$ $score$이 나오게 된다. $region$ $score$은 <strong>이미지에 있는 각각의 글자들을 위치</strong> 시키는 데에 사용되고, $affinity$ $score$은 <strong>각 글자들을 한 인스턴스에 묶는데</strong>에 사용된다. 문자 수준의 annotation이 부족하기 때문에 CRAFT에서는 약한 정도의 supervised leargning 프레임 워크를 제안한다. 일반적으로 사용하는 텍스트 이미지  데이터 셋에 글자 수준의 groud-truth 데이터가 없기 때문이다. 데이터 셋으로는 ICDAR을 사용했고 그 외에 MSRATD500, CTW-1500 등을 통해 실험했다.</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>생략</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><p>CRAFT의 주 목표는 일반 이미지에 있는 각 글자들을 정확하게 포착하는 것이다. 이를 위해 딥러닝을 학습시켜서 글자의 위치와 글자들 간의 affinity(글자 옆에 글자가 있는 것 정도로 해석)를 예측한다. 데이터를 학습시킬 때, 이와 관련된 데이터 셋, 즉 글자 하나하나 학습시키는 데이터가 없으므로, 이 모델은 약한 지도학습 방법으로 학습된다. </p><h3 id="3-1-Architecture"><a href="#3-1-Architecture" class="headerlink" title="3.1 Architecture"></a>3.1 Architecture</h3><p>VGG-16에 Batch Normalization이 적용된 네트워크를 backbone 모델로 사용했다. 이 모델에는 디코딩 파트에 <code>skip-connection</code>이 있는데 이것은 U-Net과 비슷한 구조로 Low-level의 feature들을 취합한다. 최종 아웃풋에는 score map에 해당하는 두 개의 채널이 있고, 여기에서 위에서 말한 $region$ $score$과 $affinity$ $score$이 나오게 된다.</p><p><img src="https://media.arxiv-vanity.com/render-output/2142906/x2.png" alt="Schematic illustration of Network"></p><h3 id="3-2-Training"><a href="#3-2-Training" class="headerlink" title="3.2 Training"></a>3.2 Training</h3><h4 id="3-2-1-Ground-Truth-Label-Generation"><a href="#3-2-1-Ground-Truth-Label-Generation" class="headerlink" title="3.2.1 Ground Truth Label Generation"></a>3.2.1 Ground Truth Label Generation</h4><p>Training Image를 위해서 $region$ $score$과 $affinity$ $score$에 대한 ground thruth 라벨을 생성해야 한다. 이 라벨은 글자 단위의 박스를 이용해 만든다. $region$ $score$은 주어진 픽셀이 글자의 중앙에 있을 확률이고, $affinity$ $score$는 인접한 글자들의 공간의 가운데에 있을 확률이다. 이 가운데에 있을 확률을 가우시안 히트맵으로 인코딩한다. 왜냐하면 ground truth 지역이 엄격하게 경계쳐져 있지 않기 때문이다. 이 히트맵은 $region$ $score$과 $affinity$ $score$ 모두에 사용된다.</p><p><img src="https://media.arxiv-vanity.com/render-output/2142906/x3.png" alt="Ground Truth 생성 과정"></p><p>제안된 ground truth는 receptive field의 크기가 작은걸 사용함에도 불구하고, 모델이 크거나 또는 긴 텍스트를 찾아내는 것을 가능케 한다.</p><h4 id="3-2-2-Weakly-Supervised-Learning"><a href="#3-2-2-Weakly-Supervised-Learning" class="headerlink" title="3.2.2 Weakly-Supervised Learning"></a>3.2.2 Weakly-Supervised Learning</h4><p>학습에 사용되는 데이터는 단어 단위의 annotation을 가지고 있다. weakly-supervised learning을 사용해서 실제 이미지에 단어 단위의 annotation이 들어오면, 이미지에 글자 부분을 crop하고, 학습된 모델이 이미지에 있는 글자 지역을 예측해 글자 단위의 bounding-box를 만든다. </p><p><img src="https://media.arxiv-vanity.com/render-output/2142906/x4.png" alt="Weakly-Supervised Learning"></p><p>위의 사진에서 그 구조를 파악할 수 있다.  </p><ol><li>먼저 단어 단위의 이미지들이 crop된다.   </li><li>학습된 모델이 $region$ $score$를 예측한다.   </li><li>watershed 알고리즘이 글자 단위로 쪼갠다.   </li><li>crop단계에 사용했던 inverse transform을 사용해서 글자 박스의 좌표들이 원래의 이미지 좌표로 변형해 넣는다.</li></ol><p>만약 모델이 부정확한 region score로 학습되게 된다면, 결과의 글자들은 blurred 되어 나타나게 된다. 이것을 막기 위해서, psuedo-GT(제안된 Ground Truth)의 퀄러티를 측정한다. text annotation의 강력한 시그널인 word length를 이용해서 측정하게 되는데, 이것을 이용하면 psuedo-GT의 confidence를 계산할 수 있다.</p><p><img src="/images/formula_1.png" alt="confidnce formula 1"><br>먼저 첫 번째 식에 있는 변수들에 대해서 설명해보자. $s_{conf}(w)$는 우리가 구하려는 샘플 $w$에 대한 confidence 값이다. $R(w)$와 $l(w)$는 각각 샘플 $w$d에 대한 bounding box 영역과 단어의 길이이다. 글자단위로 쪼개는 과정에서 글자들의 추정된 bounding box들과 이에 상응하는 글자들의 길이를 알아낼 수 있다. 이 길이는 $l^c(w)$로 표현된다. 결국 이 식에서 confidence라 함은, 단어의 길이라는 정보를 이용해서 얼마나 단어의 길이를 잘 인식했는지를 나타내는 수치라고 할 수 있다.</p><p><img src="/images/formula_2.png" alt="confidnce formula 2"><br>이제 두 번째 식을 보자. $S_c(p)$는 pixel-wise의 confidence map이다. 이 픽셀이  $R(w)$에 속하면 아까 구한 $s_{conf}(w)$를 사용하고, 그렇지 않으면 confidence를 1로 준다.</p><p><img src="/images/formula_3.png" alt="confidnce formula 3"><br>마지막으로 L을 구하는 식을 보면, $S_{r}^\star(p)$와 $S_{a}^*(p)$는 각각 pseudo-GT의 region score과 affinity map을 의미한다. 그리고 $S_{r}(p)$와 $S_{a}(p)$는 각각 예측된 region score과 affinity map을 의미한다. 합성 데이터로 훈련시킬 때, 우리는 진짜 ground truth를 얻을 수 있으므로, $S_{c}(p)$는 1로 설정된다. 훈련이 수행되면, CRAFT모델은 각 글자들을 더 정확하게 예측할 수 있고, confidence 점수인 $s_{conf}(w)$ 는 점진적으로 증가하게 된다. 즉, 학습할 수록 confidence가 올라가게 되어 글자가 더 잘 인식된다는 말이다.</p><p>학습을 하면서, 만약 confidence score가 0.5보다 낮으면 추정된 글자의 바운딩 박스들은 모델을 학습하는데 악영향을 주기 때문에 무시하도록 설정된다.</p><h3 id="3-3-Inference"><a href="#3-3-Inference" class="headerlink" title="3.3 Inference"></a>3.3 Inference</h3><p>추론 단계에서, 결과물이 다양한 모양으로 전달된다. 예를들어, 단어 박스나 글자 박스들이나 다른 다각형 등으로 나오게 된다. 평가를 위해서 IoU(word-level intersection-over-union)을 사용한다. $QuadBox$를 만들어서 글자를 큰 박스로 인식하고 각 글자들을 polygon을 생성한다. 이 방식을 통해서 휘어져 있는 text  전체에 대해서 효과적으로 다룰 수 있게 된다. 이것은 OpenCV에 있는 $connectedComponents$와 $minAreaRect$를 사용해 만들 수 있다.</p><h3 id="4-Discussions"><a href="#4-Discussions" class="headerlink" title="4. Discussions"></a>4. Discussions</h3><p><strong>Robustness to Scale Variance</strong> 비록 데이터 셋에 있는 텍스트의 사이즈는 매우 달랐지만, 모든 데이터에 대해서 단일 스케일의 실험을 진행했다. 상대적으로 작은 receptive fiel는 큰 이미지에 있는 작은 글자를 잡는데 적합했고, CRAFT는 이를 통해서 다양한 글자들을 잡아내는데 robust하다는 결과이다.</p><p><strong>Multi language issue</strong> IC17에는 Bangla와 Arabic 글자들이 있지만, synthetic text 데이터 셋에는 Bangla와 Arabic 글자들이 포함되어 있지 않다. 게다가, 두 언어는 각각의 글자들을 개별적으로 세그먼트하기가 힘들다. 왜냐하면 모든 글자들이 필기체로 이루어져있기 때문이다. 그러므로 이 모델을 두 글자들을 분간해 내지 못하고, Latin이나 한글, 중국어, 일본어 등을 찾지 못한다.</p><p><strong>Generalization ability</strong> 3개의 다른 데이터 셋들로,  fine-tuning을 하지않고 실험 해 보았을 때, SOTA의 퍼포먼스를 보여주는 것을 확인했다. 이 모델이 한 데이터 셋에 오버피팅 되는 것 보다, 여러 글자들에 일반적인 성능을 내는 것을 증명하는 결과이다.</p><h3 id="5-결론"><a href="#5-결론" class="headerlink" title="5. 결론"></a>5. 결론</h3><p>글자 단위의 annotation이 주어지지 않았을 때 각 글자들을 detect하는 CRAFT 모델을 제시했다. $region$ $score$과 $affinity$ $score$로 다양한 모양의 텍스트 모양들을 커버할 수 있다. 여기에 weakly supervised learning을 사용해 pseudo-ground truth를 만들어 내었다. CRAFT는 SOTA에 해당하는 퍼포먼스를 보여줬으며 일반화 성능과 scale 변동에 덜 민감하지만, multi language를 다루지는 못한다는 점에서 한계를 보인다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/03/29/craft/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Airflow Basic</title>
      <link>http://tkdguq05.github.io/2020/03/14/airflow-start/</link>
      <guid>http://tkdguq05.github.io/2020/03/14/airflow-start/</guid>
      <pubDate>Sat, 14 Mar 2020 07:25:23 GMT</pubDate>
      <description>
      
        &lt;p&gt;Airflow의 기본적인 컨셉에 대해서 이해해보자&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Airflow의 기본적인 컨셉에 대해서 이해해보자</p><a id="more"></a><h1 id="Workflow-관리-Airflow-컨셉을-알아보자"><a href="#Workflow-관리-Airflow-컨셉을-알아보자" class="headerlink" title="Workflow 관리! Airflow 컨셉을 알아보자"></a>Workflow 관리! Airflow 컨셉을 알아보자</h1><p>요즘 왠만한 회사에서 Airflow를 안 쓰는 곳이 없습니다. 파이프라인 관련 세션을 들으면 심심치 않게 들을 수 있는 것이 Airflow를 이용한 워크플로우 관리일 것 입니다. 최근 저희 회사에서도 Airflow를 도입했습니다. 여러 세션에서 관련 내용을 기억해두고 정리해 두었다가, 잡 스케쥴을 관리할 필요성이 생겨서 Airflow 도입을 제안했습니다. 현재 Airflow를 이용해서 추천 쪽에 적용하고 있고, 대략 <code>전처리-모델링-Prediction</code>의 플로우를 돌리려고 합니다.  test를 계속해서 진행 중이고 시행착오법 끝에 DAG들이 잘 돌아가는 것을 확인하고 있습니다. </p><p>오늘 글은 간단하게 Airflow의 컨셉에 대해서 알아보는 내용입니다. 위에서도 등장한, Airflow의 핵심인 DAG에 대해서 알아보고, DAG안에 들어가는 요소들을 살펴보겠습니다.</p><h3 id="Workflow-Airflow"><a href="#Workflow-Airflow" class="headerlink" title="Workflow? Airflow?"></a>Workflow? Airflow?</h3><p>Airflow는 AirBnB에서 만든 Workflow 관리 툴입니다. workflow라고 하면, 대략적으로 ‘아 작업의 흐름’이라고 할 수 있겠습니다만, workflow를 조금 더 자세히 설명하자면 워크플로우는 작업 절차를 통한 정보 또는 업무의 이동을 의미하며, 더 자세히 말하면, 워크플로는 작업 절차의 운영적 측면이라고 할 수 있습니다.(출처 위키피디아) 여기서 업무라는 것이 등장합니다. Airflow에서 업무는 Task라고 합니다. 이 Task들이 연결 된 것이 Workflow고 이것을 관리하는 것이 Airflow입니다. 쉽게 말하자면, Airflow는 Task들을 잘 연결시키고 관리하는 툴이라고 볼 수 있겠습니다. Airflow를 활용하는 예는 다양합니다. 데이터 분야에서는 ETL 파이프라인에 사용해 데이터들을 관리할 수 있고, 모델의 학습주기를 관리할 수도 있습니다. 학습된 모델을 이용해 prediction해 배치성으로 결과들을 주기적으로 저장해 놓을 수도 있겠네요. 마케팅 도메인에서 활용한다면, 마케팅 자동화에 적용해서, 이메일을 자동으로 보내주는 일 등에도 활용할 수 있겠습니다. 어떤 작업이 성공했을때 다음 작업을 하게 한다거나(의존성, branching) 등의 작업도 가능하기 때문에 굉장히 다양하게 활용할 수 있습니다. </p><h3 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h3><p>방금 전까지 Airflow가 어떻게 흐르는지 알아봤습니다. 이 흐름은 어떻게 만들까요? Airflow에 대해서 찾아보면 다음과 같은 가지들을 볼 수 있습니다.<br><img src="/images/simple_dag.png" alt="깔끔한 DAG"></p><p>간단한 DAG라 보기 굉장히 편합니다. 하지만 DAG를 어떻게 짜느냐에 따라, 작업이 얼마나 복잡하냐에 따라 DAG는 복잡하게 변할 수 있습니다.<br><img src="/images/complex_dag.png" alt="흉악한 DAG"><br>그래도 이렇게 눈으로 보고 확인할 수 있으니 얼마나 편한지 모르겠습니다. 이걸 airflow없이 코드로 일일이 보고 작업 스케쥴 관리를 하려면 몸과 마음이 지쳐 월요일부터 글또 채널에 pass권을 사용할지도 모릅니다.</p><p>다행히도, 우리에겐 Airflow가 있고 Task들을 DAG를 통해서 이어주면 한 눈에 알아볼 수 있습니다. DAG는 Directed Acyclic Graph의 약자입니다. 번역하자면, ‘방향성 비순환 그래프’입니다. 노드와 노드가 단방향으로 연결되고 한번 노드로 향하면, 돌아오지 않는 특성을 가진다는 정도만 알면 될 것 같습니다. </p><p>DAG는 Python script로 작성되어 있습니다. 따라서 python에 익숙한 분들이라면 DAG작성은 별로 어렵지 않을 것입니다. DAG를 구성하는 요소들에 대한 개념만 알면 쉽게 쉽게 짤 수 있습니다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta, datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> utils.slack_alert <span class="keyword">import</span> SlackAlert</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> airflow <span class="keyword">import</span> DAG</span><br><span class="line"><span class="keyword">from</span> airflow.models <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> airflow.contrib.operators.bigquery_operator <span class="keyword">import</span> BigQueryOperator</span><br><span class="line"><span class="keyword">from</span> airflow.contrib.operators.bigquery_check_operator <span class="keyword">import</span> BigQueryCheckOperator</span><br><span class="line"></span><br><span class="line">alert = SlackAlert(<span class="string">'#test'</span>)</span><br><span class="line"><span class="comment"># Config variables</span></span><br><span class="line">dag_config = Variable.get(<span class="string">"bigquery_github_trends_variables"</span>, deserialize_json=<span class="keyword">True</span>)</span><br><span class="line">BQ_CONN_ID = dag_config[<span class="string">"bq_conn_id"</span>]</span><br><span class="line">BQ_PROJECT = dag_config[<span class="string">"bq_project"</span>]</span><br><span class="line">BQ_DATASET = dag_config[<span class="string">"bq_dataset"</span>]</span><br><span class="line"></span><br><span class="line">default_args = &#123;</span><br><span class="line">    <span class="string">'owner'</span>: <span class="string">'Jose Lee'</span>,</span><br><span class="line">    <span class="string">'depends_on_past'</span>: <span class="keyword">True</span>,    </span><br><span class="line">    <span class="string">'start_date'</span>: datetime(<span class="number">2018</span>, <span class="number">12</span>, <span class="number">1</span>),</span><br><span class="line">    <span class="string">'end_date'</span>: datetime(<span class="number">2018</span>, <span class="number">12</span>, <span class="number">5</span>),</span><br><span class="line">    <span class="string">'email_on_failure'</span>: <span class="keyword">True</span>,</span><br><span class="line">    <span class="string">'email_on_retry'</span>: <span class="keyword">False</span>,</span><br><span class="line">    <span class="string">'retries'</span>: <span class="number">2</span>,</span><br><span class="line">    <span class="string">'retry_delay'</span>: timedelta(minutes=<span class="number">5</span>),</span><br><span class="line">    <span class="string">'on_failure_callback'</span> : alert.slack_fail_alert</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set Schedule: Run pipeline once a day. </span></span><br><span class="line"><span class="comment"># Use cron to define exact time. Eg. 8:15am would be "15 08 * * *"</span></span><br><span class="line">schedule_interval = <span class="string">"00 21 * * *"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define DAG: Set ID and assign default args and schedule interval</span></span><br><span class="line">dag = DAG(</span><br><span class="line">    <span class="string">'bigquery_github_trends'</span>, </span><br><span class="line">    default_args=default_args, </span><br><span class="line">    schedule_interval=<span class="string">'@once'</span></span><br><span class="line">    <span class="comment"># schedule_interval</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Config variables</span></span><br><span class="line">BQ_CONN_ID = <span class="string">'my_gcp_conn'</span></span><br><span class="line"><span class="comment"># BQ_PROJECT = 'airflow-268501'</span></span><br><span class="line"><span class="comment"># BQ_DATASET = 'github_trends'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Task 1: check that the github archive data has a dated table created for that date</span></span><br><span class="line">t1 = BigQueryCheckOperator(</span><br><span class="line">        task_id=<span class="string">'bq_check_githubarchive_day'</span>,</span><br><span class="line">        sql=<span class="string">'''</span></span><br><span class="line"><span class="string">        #standardSQL</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">          table_id</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">          `githubarchive.day.__TABLES_SUMMARY__`</span></span><br><span class="line"><span class="string">        WHERE</span></span><br><span class="line"><span class="string">          table_id = "&#123;&#123; yesterday_ds_nodash &#125;&#125;"</span></span><br><span class="line"><span class="string">        '''</span>,</span><br><span class="line">        use_legacy_sql=<span class="keyword">False</span>,</span><br><span class="line">        bigquery_conn_id=BQ_CONN_ID,</span><br><span class="line">        dag=dag</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>제가 연습하느라 사용했던 DAG를 보면서 설명해보겠습니다. import를 뭘하는지 보면 방금까지 설명한 DAG, 그리고 Variable과 operator가 보입니다. </p><h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><p>Variable은 Airflow UI를 보면서 설명을 해야 할 것 같습니다.  UI에 Variable의 탭이 별도로 존재하기 때문입니다.<br><img src="/images/airflow_variable.png" alt="Airflow의 Variable탭"></p><p>위의 화면이 Variable을 보여주고 있습니다. Key와 Value로 이루어진 걸 알 수 있습니다. Key Value로 이루어진 데이터를 이용하고 싶다면, Variable에 등록해두면 편하게 dict로 값을 받듯이 <code>Variable.get</code>를 이용해서 값을 얻을 수 있습니다. 위의 예시에서는 <code>dag_config = Variable.get(&quot;bigquery_github_trends_variables&quot;, deserialize_json=True)</code><br><code>BQ_CONN_ID = dag_config[&quot;bq_conn_id&quot;]</code>이 부분이 되겠습니다. 저희 회사의 경우에는 각 고객사의 서비스키와 캠페인키를 Variable에 넣어두고 사용하고 있습니다. 신규 고객사가 발생하면 코드를 일일이 만들 필요없이 간단하게 Variable에 넣어두고 DAG를 태우면 끝입니다! </p><h3 id="default-args"><a href="#default-args" class="headerlink" title="default_args"></a>default_args</h3><p>default_args는 dictionary로 이루어져 있고 DAG에 들어가게 됩니다. 여기에 작성한 내용들이 operator들에 적용될 것입니다. 일일이 operator에 넣을 필요없이 default_args에 넣어서 수정하면 되니까 굉장히 간편하게 여러 오퍼레이터에 들어갈 파라미터들을 관리 할 수 있습니다.</p><h3 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h3><p>DAG가 워크 플로우를 어떻게 run할지를 설명한다면, Operator는 실제 Task가 수행하는 작업을 결정합니다. 실제 Task가 돌아가는 것을 Operator를 통해서 지정해준다고 보면 됩니다. operator의 종류는 굉장히 다양합니다. 대표적으로는</p><ul><li>BashOperator : bash 커맨드를 실행시킨다</li><li>PythonOperator : python 함수를 호출한다</li><li>EmailOperator : 이메일을 보낸다</li><li>SimpleHttpOperator : HTTP 요청을 보낸다<br>이런 것들이 있습니다. 위의 코드에서는 BigQueryCheckOperator를 사용했습니다. Bigquery에 접근해서 쿼리를 보내고 싶었기 때문입니다. Google Cloud Platform에 지원되는 다양한 Operator들이 있고, 저희 회사는 AWS를 주로 사용하기 때문에 AWS operator를 많이 사용하고 있습니다. 추가로 SlackAPIOperator 등도 있으니 아이디어만 있다면 왠만한 작업은 다 처리가 가능할 것 입니다.</li></ul><p>슬랙 이야기가 나와서 덧붙이자면,  default_args에 보면 <code>&#39;on_failure_callback&#39; : alert.slack_fail_alert</code>이 있습니다. 대략 유추가 가능하겠지만, 실패시에 슬랙에 알럿을 띄우려고 만들어 둔 파라미터 입니다. Airflow 작업을 하다가 실패가 나면 슬랙을 통해서 알럿을 보고 대응하기 위해서 작성해 두었습니다. 슬랙에 알람을 보내는 내용은 다음 글에서 다뤄보도록 하겠습니다.</p><p>이렇게 간단한 DAG가 만들어지고 airflow_home에 있는 dag폴더에 업로드가 되면 airflow UI에서 DAG가 떠있는 것을 확인할 수 있습니다.<br>(Airflow UI<a href="http://localhost:8080">http://localhost:8080</a>)<br><img src="/images/airflow_main.png" alt="내가 만든 DAG"><br><img src="/images/airflow_task.png" alt="DAG로 들어가면 Task들의 상태를 확인할 수 있다"></p><p>Task들의 상태는 다음과 같이 나눠집니다.<br><img src="/images/task_stages.png" alt="Task들의 상태들"></p><p>이제 이 상태들을 확인해 보면서 워크플로우를 관리해 나가면 됩니다. </p><hr><p>이번 글에서는 Airflow의 정말 기본적인 개념들에 대해서 다뤄봤습니다. 간단한 DAG를 직접 작성해보고 success를 한번 띄워보면 Airflow가 어떤 건지 대략적인 감을 잡을 수 있을 것 이라고 생각합니다. 다음 글에서는 슬랙을 통해서 메세지 알람을 보내는 걸 작성해 보려고 합니다. 저도 공부를 하는 입장이라 부족한 내용일 수 있지만, 보시면서 틀린 부분이나 수정할 부분 알려주시면 감사하겠습니다. </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/03/14/airflow-start/#disqus_thread</comments>
    </item>
    
    <item>
      <title>글또 4기를 시작하며 다짐하기</title>
      <link>http://tkdguq05.github.io/2020/02/24/geultto4/</link>
      <guid>http://tkdguq05.github.io/2020/02/24/geultto4/</guid>
      <pubDate>Mon, 24 Feb 2020 02:01:25 GMT</pubDate>
      <description>
      
        &lt;p&gt;&lt;img src=&quot;/images/geultto.png&quot; alt=&quot;이번에 생긴 글또 로고, 글또 1,2,3기를 하셨던 김나영님이 만들어주셨다&quot;&gt;&lt;/p&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p><img src="/images/geultto.png" alt="이번에 생긴 글또 로고, 글또 1,2,3기를 하셨던 김나영님이 만들어주셨다"></p><a id="more"></a><p><a href="https://www.notion.so/ac5b18a482fb4df497d4e8257ad4d516">글또 Notion 구경하기</a></p><h1 id="글또-3기가-끝나고-돌아보기"><a href="#글또-3기가-끝나고-돌아보기" class="headerlink" title="글또 3기가 끝나고 돌아보기"></a>글또 3기가 끝나고 돌아보기</h1><p>글또 3기를 성공적으로 마치고! 글또 4기를 시작하게 되었다. 작년 7월부터 연말까지 글을 쓰는 대장정이었던 3기에서 pass권 2번 쓴 걸 제외하면, 모든 주차에 글을 작성했었다. 피드백을 기간내에 까먹고 못해서 5000원이 차감되었지만, 원래 목표했었던 10만원 그대로 돌려 받기에는 성공했다(?). 그 동안 쓴 글들을 쭉 돌아보니, 글또하길 잘했다는 생각이든다. 블로그 자체의 콘텐츠도 많이 늘었을 뿐만 아니라, 항상 어떤 걸 배우면 기록하는 습관이 생겼다. 블로그에 적어야 겠다는 것이 머리 안에 자리잡힌 것 같고 일요일 저녁만 되면 ‘글 써야 되는데’ 하는 생각이 슬쩍 스쳐지나간다. 글쓰기 중독 초기 증세일까? 하루에 한번은 애드센스에 들어가서 예상 수입에 떠라 그 날 기분이 왔다 갔다 하니, 이쯤되면 중독이 맞는 것 같다.</p><h3 id="무엇을-썼을까"><a href="#무엇을-썼을까" class="headerlink" title="무엇을 썼을까?"></a>무엇을 썼을까?</h3><p>초기에는 강의를 들었던 내용을 쭉 정리해서 작성했고, 행사 다녀온 내용과 딥러닝 컴퓨터를 사서 서버로 만든 글, MAB 등 굉장히 다양한 글을 썼다. 좋게 말해서 다채로운 글을 쓴 것이지만, 포커싱 되지 않았다고 해야할까? 한 주제에 대해서 깊이 파헤치고 그 분야에 대해서 성장하는 느낌이 들지 않는 느낌이었다. 어떤 글을 쓰면서 부족한 건 나중에 채워넣어야 겠다고 생각한 글도 있었는데, 결국 채우지 못했다. <strong>완성도 있는 글을 써야하지 않았을까?</strong></p><p>최근에는 딥러닝 서버 구축기를 다시 읽을 일이 생겼다. 비슷한 문제 상황이 발생했고 내가 정리한 글을 보고 해결하기로 한 것이다. 그런데 그대로 따라해도 문제가 해결이 되지 않았다. 구축 환경도 비슷하다고 생각했고 똑같은 작업을 하는데 왜 안되는지, 당황스러웠다. 내가 인터넷에서 도움받은 만큼 트러블 슈팅한 내용을 작성해서 다른 사람들에게 도움을 주고 싶었는데, 내 글로 인해 괜히 망치는 건 아닐까?, 성급하게 글을 작성한 건 아닐까?, 좀 더 많이 찾아보고 깔끔하게 정리된 내용으로 글을 작성해야 하지 않았을까? 란 생각이 들었다. </p><h2 id="무엇을-쓸까"><a href="#무엇을-쓸까" class="headerlink" title="무엇을 쓸까?"></a>무엇을 쓸까?</h2><p>3기 때 목표는 명확했다. <code>예치금 깎이지 않기!</code> 하지만 이미 이룬 목표를 다시 사용하는 것은 성장하는 과정에서, 목표로서의 기능을 다 하지 못하는 것이라고 생각한다. 예치금을 다 받는 것은 기본 조건으로 하고, <strong>완성도 있는 글</strong>을 작성하기 위해서 노력해야겠다. 이를 위해서는 특별한 과정이 필요할 것 같다.</p><p>기존에 글을 쓸 때는 마감시간에 닥쳐서 쓴 글이 많았었다. 물론 글로 쓰려고 작업한 내용이 예상한 것보다 작업시간이 길어져서, 금요일부터 ubuntu 서버를 구축하기 시작해서 일요일 11시까지 질질 끄느라 그랬었던 적도 있긴 하지만, 어쨌든 마감을 맞추려고 급하게 쓴 글은 다시보면 티가 나는 것 같다.</p><p>결국 준비를 해야 좋은 글을 쓸 수 있는 것 같다. 어떻게 준비를 해야할까? </p><ul><li>1-2주 정도의 텀을 두고 쓸 글의 주제를 정해놓고 초고를 작성한다</li><li>초고를 적어도 3번이상 수정한다</li><li>잘 읽히는지 확인한다</li><li>시간이 지나서도 가치가 있는지 확인한다</li><li>업데이트 주기에 대해서도 생각한다</li></ul><p>일단 생각나는 것은 이 정도다. 8월까지 진행하게 되면서 매번 완성된 좋은 글을 작성할 수는 없겠지만, 최소한 한달에 한번은 공들여서 다른 사람에게 도움이 되는, 좋은 글을 작성해보는 게 목표다.</p><p><strong>세부적으로, 작성하고 싶은 글감들</strong></p><ul><li>Pyspark</li><li>Airflow</li><li>AWS(S3, EC2, Load Balancer)</li><li>궁극적인 목표에 대한 고민</li><li>최근에 일하면서 만나게 된 아이들에 대해서 정리해서 글을 써 보고 싶다. 그리고 최근에 학교 후배와 교수님과 만나서 얘기를 하던 중에 궁극적인 목표에 대한 주제가 나왔다. 그 후배는 목표가 확실했고, 목표에서 굉장한 동기부여를 받는 듯 했다. 취업한 이후에 동기부여가 되었다가 안되었다 하는 일이 자주 있는데 여기에 대해서 넋두리일지라도, 삶의 고민에 대한 흔적으로 남겨두고 싶다. 그 외에 데이터 엔지니어링 채널이나 백엔드 쪽 채널을 자주 기웃 거리면서 꼭 피드백을 해야하는 글이 아니더라도, 꾸준히 챙겨보고 관심있는 것들에 대해서 공부를 해봐야겠다.(사실 요즘 데이터 엔지니어링에 관심이 많고 잘하고 싶다는 마음이 마구마구 든다!)</li></ul><p>글또 4기가 3월 1일부터 본격적으로 시작된다. 6개월 동안 꾸준하게 블로그를 채워나가봐야겠다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/02/24/geultto4/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유</title>
      <link>http://tkdguq05.github.io/2020/01/16/spark-in-action/</link>
      <guid>http://tkdguq05.github.io/2020/01/16/spark-in-action/</guid>
      <pubDate>Thu, 16 Jan 2020 08:49:47 GMT</pubDate>
      <description>
      
        &lt;p&gt;Spark를 사용해서 데이터를 읽고 분석하자&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>Spark를 사용해서 데이터를 읽고 분석하자<br><a id="more"></a></p><h1 id="데이터-분석하기-전-데이터부터-읽자"><a href="#데이터-분석하기-전-데이터부터-읽자" class="headerlink" title="데이터 분석하기 전, 데이터부터 읽자"></a>데이터 분석하기 전, 데이터부터 읽자</h1><h3 id="Spark-Session-conf-설정"><a href="#Spark-Session-conf-설정" class="headerlink" title="Spark Session, conf 설정"></a>Spark Session, conf 설정</h3><p>기존 python의 pandas를 이용해서 데이터를 읽으려면 pd.DataFrame(‘…….’)를 통해 파일을 읽으면 간단히 해결 되었다. 하지만 spark에서 데이터를 읽기 위해서는 조금 더 손을 거쳐야 한다. 물론 Zeppelin을 이용한다면 바로 파일을 읽어들일 수 있겠지만, pycharm을 이용해서 pyspark application을 만드는 작업을 할 것이기 때문에 직접 spark세팅을 해주어야 한다. </p><p>pycharm에서는 Spark Session을 설정해줘야 spark를 사용할 수 있다. 이 Spark Session에 대한 설정값으로 Spark Conf를 설정해주어야 한다.<br>먼저 필요한 라이브러리를 불러들이고 Spark conf와 session을 설정한다.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.conf <span class="keyword">import</span> SparkConf</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">conf = SparkConf()</span><br><span class="line">conf.set(<span class="string">'spark.jars.packages'</span>, <span class="string">'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1'</span>)</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">"spark test"</span>).config(conf=conf).getOrCreate()</span><br></pre></td></tr></table></figure></p><p>conf.set에는 mongoDB와의 연결을 위한 spark connector를 넣어줬다. 이렇게 하면 mongoDB에 있는 데이터를 바로 읽을 수 있을까? 아직 할 작업이 조금 남았다. data를 불러오기 전에 스키마 지정을 해줘야 하기 때문이다.</p><h3 id="스키마-지정"><a href="#스키마-지정" class="headerlink" title="스키마 지정"></a>스키마 지정</h3><p>스키마란 간단하게 말해서 데이터 구조와 제약 조건에 대한 명세(Specification) 기술한 것을 의미한다.<br>여기서 설정할 스키마는 이 데이터의 칼럼이 어떤타입으로 들어갈 것인지(string, integer, double …)를 주로 뜻하게 될 것이다.<br>mongoDB에서 사용자들이 거래한 내용 중 카트에 어떤 상품을 담았는지 알기 위해서 다음과 같이 코드를 작성했다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cartSchema =  StructType([</span><br><span class="line">    StructField(<span class="string">"cartGoodsName"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"cartGoodsCode"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"cartGoodsAmount"</span>, IntegerType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"cartGoodsCount"</span>, IntegerType(),<span class="keyword">True</span>),</span><br><span class="line">  ])</span><br><span class="line"></span><br><span class="line">userSchema = StructType([</span><br><span class="line">    StructField(<span class="string">"cookieId"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"currentTime"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"sessionSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"userSeq"</span>, StringType(),<span class="keyword">True</span>),</span><br><span class="line">    StructField(<span class="string">"cart"</span>, ArrayType(cartSchema),<span class="keyword">True</span>)</span><br><span class="line">  ])</span><br></pre></td></tr></table></figure><p>이렇게 카트 데이터에 대한 스키마를 작성해서 유저스키마의 cart 부분에 넣어준 뒤 합쳐진 userSchema를 이용해 데이터를 읽었다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df = spark.read.schema(userSchema).format(<span class="string">"com.mongodb.spark.sql.DefaultSource"</span>) \</span><br><span class="line">    .option(<span class="string">"spark.mongodb.input.uri"</span>,</span><br><span class="line">            <span class="string">"mongodb://******/*****.userDataInfo.******"</span>) \</span><br><span class="line">    .option(<span class="string">"spark.mongodb.output.uri"</span>,</span><br><span class="line">            <span class="string">"mongodb://******/*****.userDataInfo.******"</span>) \</span><br><span class="line">    .load()</span><br></pre></td></tr></table></figure><p>읽은 결과는 따로 dataframe을 지정할 필요없이 바로 dataframe으로 떨어진다. 이제 바로 데이터에 대해서 작업을 수행할 수 있게 되었다.<br>카트에 담은 상품이 무엇인지 알고 싶어서 actionType이 viewCart인 부분을 가져왔다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">view_cart_df = df.filter(df.actionType ==<span class="string">'viewCart'</span>)</span><br></pre></td></tr></table></figure><p>가져오고 나서 전처리 작업을 하려고 했는데, 데이터프레임에 대한 이해가 적었었던 때라 어떻게 작업해야 할지 몰랐다. 그래서 먼저 RDD로 작업을 했고 뼈저리게 후회했다. 절대 발생하면 안되는 일이 일어났기 때문이다.</p><h1 id="RDD를-사용한-결과"><a href="#RDD를-사용한-결과" class="headerlink" title="RDD를 사용한 결과"></a>RDD를 사용한 결과</h1><p>RDD를 사용해서 전처리를 해보고 Cart에 담긴 Top N개의 상품을 가져와보기로 했다.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_info</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> i:</span><br><span class="line">            test = Row(code=k[<span class="number">1</span>],cart_count=k[<span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> test</span><br></pre></td></tr></table></figure></p><p>RDD를 이용해 전처리를 할 때 쓸 함수를 지정해 놓고 작업을 하기로 했다. 함수는 다음과 같이 작성했고 상품의 코드와 그 상품이 얼마나 담겼는지를 Row로 생성했다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = df.filter(df.cart.isNotNull()).withColumn(<span class="string">"currentTime"</span>, to_timestamp(<span class="string">"currentTime"</span>, <span class="string">"yyyy-MM-dd HH:mm:ss"</span>))</span><br><span class="line">view_cart_count = df.select(<span class="string">'cart'</span>).rdd.map(get_info).toDF()</span><br><span class="line"></span><br><span class="line">view_cart_count.groupBy(<span class="string">'code'</span>).count().show()</span><br></pre></td></tr></table></figure><p>이렇게 만든 함수를 df의 cart에서 rdd의 map을 이용해서 결과를 가져왔다. 그리고 상품코드 별로 그룹화 하고 sum을 해서 결과를 출력했다.</p><p><img src="/images/rdd_res.png" alt="RDD에 map한 결과"><br>그런데 뭔가 이상했다. sum을 했으면 결과값이 적어도 100은 넘어야 했는데, 100넘는 값이 너무 적었다. 그래서 특정 상품코드에 대해서 python으로 데이터 분석을 실시해서 결과를 매칭시켜 비교해보기로 했다.</p><p><img src="/images/python_res.png" alt="python을 통한 결과"><br>결과가 너무 차이가 났다. 이렇게 나온 결과로 아이템을 추천하게 되면 제대로 된 상품이 추천되지 않을  것이다. RDD에 함수를 map하는 것에 뭔가 문제가 있는 것이 분명했다. 방법을 찾다가 Dataframe으로 작업을 해보기로 했다.</p><h1 id="Dataframe을-사용한-결과"><a href="#Dataframe을-사용한-결과" class="headerlink" title="Dataframe을 사용한 결과"></a>Dataframe을 사용한 결과</h1><p>데이터 프레임으로 작업해야 결과값이 바뀌지 않는 다는 정보를 알게 되어 기존에 있던 df에 filter를 걸어 새 DF를 만들고 이걸 가지고 전처리 해보기로 했다.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cartDF = df.filter(df.cart.isNotNull()).withColumn(<span class="string">"currentTime"</span>,</span><br><span class="line">                  to_timestamp(<span class="string">"currentTime"</span>, <span class="string">"yyyy-MM-dd HH:mm:ss"</span>)).select(<span class="string">"cart"</span>) \</span><br><span class="line">    .withColumn(<span class="string">"cart"</span>, explode(<span class="string">"cart"</span>))</span><br><span class="line"></span><br><span class="line">cart_all = cartDF.withColumn(<span class="string">"goodsCode"</span>, cartDF[<span class="string">"cart"</span>].getItem(<span class="string">"cartGoodsCode"</span>))\</span><br><span class="line">    .withColumn(<span class="string">"goodsCount"</span>, cartDF[<span class="string">"cart"</span>].getItem(<span class="string">"cartGoodsCount"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">results_df = cart_all.groupby(<span class="string">'goodsCode'</span>).sum().orderBy(<span class="string">'sum(goodsCount)'</span>, ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></p><p>작업은 다음과 같이 실시했고 상품 갯수를 정렬하기 위해서 orderBy를 사용했다.<br>결과는 어떻게 나왔을까?</p><p><img src="/images/df_res.png" alt="Dataframe을 사용한 결과"></p><p>python을 사용한 결과와 똑같은 값이 등장했다. 성공했다!!!</p><h1 id="왜-값이-다를까"><a href="#왜-값이-다를까" class="headerlink" title="왜 값이 다를까?"></a>왜 값이 다를까?</h1><p>그렇다면 왜 RDD를 사용해서 함수를 적용할 때랑 Dataframe을 갖고 작업한 결과가 다른 것일까?<br>일단<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">view_cart_count = df.select(<span class="string">'cart'</span>).rdd.map(get_info)</span><br></pre></td></tr></table></figure></p><p>이 코드에서 rdd.map한 부분까지 가져와서 확인해보니 결과값이 많지 않았다. rdd에서 df로 바꿀때 데이터가 변하는 일은 없다는 것이다.<br>그렇다면 이 코드 전에 rdd.map(get_info)하는 부분에서 변형이 일어난 거라고 추측할 수 있다. 하지만 spark 이론에서 map을 적용할 때는<br>map 자체가 narrow transformation에 해당되기 때문에, shuffle이 일어나지 않는다고 나와있다. 결국 shuffle에 의한 데이터 변형의 가능성도 없다고 할 수 있는 것이다. 함수 자체에 이상이 있는 것일까? 그렇다고 보기엔 어렵다. 이 코드를 갖고 구매-할인율에 대한 것을 집계했을 때는 정확한 값이 나왔기 때문이다.</p><p>조금 더 공부해보고 왜 값이 다른지에 대해서는 추후에 계속 수정을 해 나가야겠다.</p><p>결국은 spark에서는 RDD를 사용할지 Dataframe을 사용할지, 그리고 Dataset을 사용할지 먼저 생각하고 작업하는 것이 중요하다.<br>이것에 관련해서는 Databricks에서 나온 <a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html">문서</a>가 있는데, 이것은 추후에 번역해서 업로드할 예정이다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/01/16/spark-in-action/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Zeppelin으로 Spark를 다뤄보자 01</title>
      <link>http://tkdguq05.github.io/2020/01/06/spark-zeppelin/</link>
      <guid>http://tkdguq05.github.io/2020/01/06/spark-zeppelin/</guid>
      <pubDate>Mon, 06 Jan 2020 12:44:53 GMT</pubDate>
      <description>
      
        &lt;p&gt;pyspark로 데이터를 읽으려면 어떻게 해야할까?&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>pyspark로 데이터를 읽으려면 어떻게 해야할까?<br><a id="more"></a></p><h1 id="Zeppelin-이용해서-pyspark로-데이터-읽기-01"><a href="#Zeppelin-이용해서-pyspark로-데이터-읽기-01" class="headerlink" title="Zeppelin 이용해서 pyspark로 데이터 읽기 01"></a>Zeppelin 이용해서 pyspark로 데이터 읽기 01</h1><p>Spark는 고속 범용 분산 컴퓨팅 플랫폼으로 정의되곤 합니다. 대용량 데이터를 가져와 빠르게 분석해 낼 수 있다는 점에서 많은 기업들에서 도입을 검토하고 있고 실제로도 많이 사용되고 있습니다. 오늘은 이 유명한 Spark를 다운받고 Zeppelin으로 띄워서 pyspark를 이용해 데이터를 읽어보는 작업까지 해 보겠습니다.</p><p>먼저 Spark를 다운받아 줍니다. <a href="https://spark.apache.org/downloads.html">Spark 설치</a></p><p><img src="/images/spark_down.png" alt="스파크 다운로드"><br>링크로 들어가면 다음과 같이 나오는데 다운받는 버전은 아무거나 받아도 상관 없지만 저는 AWS EMR로 Spark를 도입하기 전에 연습하는 용으로 사용하는 것이기 때문에 AWS EMR 버전과 같은 2.4.4버전을 다운받았습니다.</p><p>하둡 버전은 사진 그대로 2.7버전으로 진행 했습니다. </p><p>다운이 완료되면 폴더를 만들어서 그곳에 저장해주고 압축을 풀어줍니다.</p><p>tgz로 되어있는 파일은<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf [filename]</span><br></pre></td></tr></table></figure></p><p>이렇게 풀어줍니다.</p><p>다음은 Zeppelin입니다. <a href="https://zeppelin.apache.org/download.html">Zeppelin 설치</a></p><p><img src="/images/zeppelin_down.png" alt="제플린 다운로드"><br>제플린도 역시 두 가지 버전이 등장하는데, 저는 용량이 작은 버전으로 받았습니다. 큰 용량의 버전은 카산드라 등이 다 포함된 버전이기 때문에 굳이 받지 않았습니다.</p><p>제플린도 특정 폴더에 저장해주고 압축을 풀어줍니다.</p><h3 id="Spark-경로-지정"><a href="#Spark-경로-지정" class="headerlink" title="Spark 경로 지정"></a>Spark 경로 지정</h3><p>Spark의 경로를 잘 지정해줘야 Zeppelin이 실행되고 코드를 돌렸을 때 오류가 나지 않습니다.<br>먼저 쉘의 프로파일을 열어줍니다. 저는 zsh을 사용하기 때문에 zshrc를 열겠습니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.zshrc</span><br></pre></td></tr></table></figure></p><p>그 다음 설정해야 할 것은 java home 경로입니다. jdk가 없다면 jdk 1.8이상 버전을 다운받아 설치합니다.<br>java home 경로는<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="variable">$JAVA_HOME</span></span><br></pre></td></tr></table></figure></p><p>이 명령어로 알아낼 수 있습니다. java home의 경로를 알아냈다면 zshrc에 이 위치를 알려줘야합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=<span class="string">"/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home"</span></span><br></pre></td></tr></table></figure><p>저의 경우는 위치가 다음과 같아서 zsh의 아래쪽에 작성해 주었습니다.</p><p>그리고 설치된 Spark의 위치도 알려줘야 합니다. 아까 저장했던 폴더의 주소를 입력해 줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/Users/sanghyub/spark-2.4.4-bin-hadoop2.7</span><br></pre></td></tr></table></figure></p><p>저의 경우는 이렇게 되어있습니다. 절대경로로 작성해 주시면 됩니다.</p><p>Spark 세팅은 일단 여기까지 하고 Zeppelin으로 넘어가겠습니다.</p><h3 id="Zeppelin-환경-설정"><a href="#Zeppelin-환경-설정" class="headerlink" title="Zeppelin 환경 설정"></a>Zeppelin 환경 설정</h3><p>Zeppelin이 저장된 폴더로 들어가서 conf로 들어가줍니다. conf에는 <code>ls</code>를 입력해보면 여러 파일들이 있는 것을 볼 수 있습니다. </p><p><img src="/images/zeppelin_conf.png" alt="Zeppelin conf files"><br>이 파일들 중에서 zeppelin-env.sh와 zeppelin-site.xml을 사용해야 하는데, .template으로 된 파일들이 보일 것 입니다. template를 <code>cp</code>를 이용해서 바꿔줍니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp zeppelin-env.sh.template zeppelin-env.sh</span><br></pre></td></tr></table></figure><p><code>cp</code>는 복사하는 것도 있지만, 이렇게 이름을 바꿔주는데에도 사용됩니다.<br>zeppelin-env.sh와 zeppelin-site.xml을 얻었다면 vi를 이용해서 zeppelin-env.sh로 들어갑니다.<br>아까 작성한 자바 경로와 스파크 홈 경로를 그대로 갖고와서 작성해줍니다. zeppelin이 이 위치를 보고 Spark와 jdk를 이용할 수 있도록 적어두는 것 입니다.</p><p><img src="/images/zeppelin_sh.png" alt="zeppelin-env.sh 수정, 이렇게 적어주자"></p><p>zeppelin의 포트도 수정해 줍니다. 기본 포트는 8080포트인데 혹시 충돌될 수 있으니, 저는 안정적으로 9999포트로 변경하겠습니다.</p><p><img src="/images/zeppelin_xml.png" alt="zeppelin-site.xml 이렇게 작성!"></p><p>이제 기본적인 세팅은 끝났고 zeppelin을 실행시켜 봅니다.</p><h3 id="Zeppelin-실행"><a href="#Zeppelin-실행" class="headerlink" title="Zeppelin 실행"></a>Zeppelin 실행</h3><p>Zeppelin 실행은 jupyter notebook여는 것과는 조금 다릅니다. zeppelin을 입력해도 아무일도 일어나지 않습니다. Zeppelin을 열기 위해서는 zeppelin daemon을 실행시켜줘야 합니다.</p><p>zeppeliln daemon은 bin폴더에 있습니다. conf에서 빠져나와서 bin으로 들어가줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ../bin</span><br></pre></td></tr></table></figure></p><p><code>ls</code>를 입력하면 찾았던 daemon이 보일 것 입니다. 너무 반갑지만 쉘이 익숙하지 않다면 실행하는 방법을 모를 것입니다. 저도 그랬고 같이 공부했던 사람들도 눈치만 봤었습니다. 백날 눈치를 보고 째려봐도 실행은 되지 않습니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zeppelin-daemon.sh start</span><br></pre></td></tr></table></figure><p>이렇게 데몬을 실행시켜줍니다. 확인은 (<a href="https://localhost:9999)로">https://localhost:9999)로</a> 들어가서 해 보면 됩니다.</p><p><img src="/images/hello_zeppelin.png" alt="Hello Zeppelin"><br>짠! 제플린의 날개가 등장했습니다. 이제 노트를 만들고 데이터를 로드해보는 작업을 하겠습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/01/06/spark-zeppelin/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Failed to get convolution algorithm. This is probably because cuDNN failed to initialize 해결하기, feat. 케라스 창시자에게 배우는 딥러닝</title>
      <link>http://tkdguq05.github.io/2020/01/01/keras-trouble-shooting/</link>
      <guid>http://tkdguq05.github.io/2020/01/01/keras-trouble-shooting/</guid>
      <pubDate>Wed, 01 Jan 2020 07:13:44 GMT</pubDate>
      <description>
      
        &lt;p&gt;케라스 책을 따라하다 보면 나오는 에러에 대한 트러블 슈팅&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>케라스 책을 따라하다 보면 나오는 에러에 대한 트러블 슈팅<br><a id="more"></a></p><h1 id="Failed-to-get-convolution-algorithm-This-is-probably-because-cuDNN-failed-to-initialize"><a href="#Failed-to-get-convolution-algorithm-This-is-probably-because-cuDNN-failed-to-initialize" class="headerlink" title="Failed to get convolution algorithm. This is probably because cuDNN failed to initialize"></a>Failed to get convolution algorithm. This is probably because cuDNN failed to initialize</h1><p>케라스 창시자에게 배우는 딥러닝 책 진도를 쭉쭉 나가는 중이었다. 챕터5의 CNN코드를 돌리던 중 다음과 같은 에러가 등장했다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">0</span>) Unknown: Failed to get convolution algorithm. </span><br><span class="line">This <span class="keyword">is</span> probably because cuDNN failed to initialize, so <span class="keyword">try</span> looking to see <span class="keyword">if</span> a warning log message was printed above. </span><br><span class="line">[[&#123;&#123;node conv2d_1/convolution&#125;&#125;]] [[metrics/acc/Mean/_99]]</span><br></pre></td></tr></table></figure><p>황금같은 쉬는 날에 cudnn과 CUDA의 꼬임으로 인한 에러인 줄 알고 굉장히 낙담했다. 실제로 저 에러를 복사해다가 구글에 붙여넣어서 답을 찾아보니 다시 설치하라는 의견이 많았다. </p><p>하지만 정말 귀찮아서 내 설치 환경을 다시 살펴보고 이상이 있으면 수정하기로 했다.</p><p>일단 내가 설치한 CUDA와 CuDnn 환경은 저번 글들에 나와 있다.</p><p><a href="https://tkdguq05.github.io/2019/11/10/ubuntu/">ubuntu 딥러닝 서버 구축기 01</a><br><a href="https://tkdguq05.github.io/2019/11/22/ununtu2/">ubuntu 딥러닝 서버 구축기 02</a><br><a href="https://tkdguq05.github.io/2019/12/08/ubuntu3/">ubuntu 딥러닝 서버 구축기 03</a></p><p>CUDA와 CuDNN의 호환성은 문제가 없었다. 그렇다면 뭐가 문제였을까? 하던 중에 Python의 버젼을 살펴봤다.<br>콘다 가상환경에 올리고 쓰고 있었는데 python버전을 확인해보니 python 3.7버전이었다.</p><p>여러 글들을 보다보니 python 3.6버전이 keras와 궁합이 잘 맞는다는 것을 알게되었다. </p><p>그래서 바로</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n [환경 이름] python=3.6</span><br></pre></td></tr></table></figure><p>가상환경을 만들어주고 만든 가상환경을 jupyter kernelspec에 넣어주었다.<br><a href="https://medium.com/@5eo1ab/jupyter-notebook%EC%97%90-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD-kernel-%EC%B6%94%EA%B0%80%ED%95%98%EA%B8%B0-ed5261a7e0e6">jupyter kernelspec 추가하기</a></p><p>python 3.6으로 갈아끼워 준 뒤에<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">'relu'</span>))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">64</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"></span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line"><span class="comment"># train_images = train_images.reshape((60000, 28, 28, 1))</span></span><br><span class="line"></span><br><span class="line">train_images = train_images.reshape((<span class="number">60000</span>, <span class="number">28</span>,<span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">train_images = train_images.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">test_images = test_images.reshape((<span class="number">10000</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>))</span><br><span class="line">test_images = test_images.astype(<span class="string">'float32'</span>)/<span class="number">255</span></span><br><span class="line"></span><br><span class="line">train_labels = to_categorical(train_labels)</span><br><span class="line">test_labels = to_categorical(test_labels)</span><br><span class="line"></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>,</span><br><span class="line">             loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">             metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(train_images, train_labels,</span><br><span class="line">         epochs=<span class="number">5</span>, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure><p>코드를 돌렸더니 잘 돌아간다!!!</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2020/01/01/keras-trouble-shooting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2019년도 회고, 2020년을 맞이하며</title>
      <link>http://tkdguq05.github.io/2019/12/22/adios-2019/</link>
      <guid>http://tkdguq05.github.io/2019/12/22/adios-2019/</guid>
      <pubDate>Sun, 22 Dec 2019 11:10:22 GMT</pubDate>
      <description>
      
        &lt;p&gt;2019년을 돌아보는 글&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>2019년을 돌아보는 글<br><a id="more"></a></p><h2 id="2019-회고"><a href="#2019-회고" class="headerlink" title="2019 회고."></a>2019 회고.</h2><p>2019년은 감사할 일이 많았었던 해, 많은 변화가 있었고, 사람들을 제대로 챙기지 못했었던 해.</p><h3 id="상반기-예상치-못한-취준"><a href="#상반기-예상치-못한-취준" class="headerlink" title="상반기 - 예상치 못한 취준"></a>상반기 - 예상치 못한 취준</h3><p>상반기는 한 단어로 요약할 수 있을 것 같다. ‘취준. 1월부터 6월까지 패스트 캠퍼스 데이터 사이언스 익스텐션 2학기 조교생활을 하면서 동시에 취업준비를 했다. 사실 1월 2월에는 취업을 당장 준비하지 않을 계획이었다. 강의 들은 것과 프로젝트 진행했었던 내용들을 차분히 정리하고 하반기부터 본격적으로 취업에 도전하려고 했었다. 하지만 상반기 서류 지원이 시작되었고, 한번 넣어보자는 생각으로 데이터 분석 관련해서 인턴 몇개랑 정규직 원서를 제출했다. 인턴이라도 될 줄 알았었는데, 다 떨어져버렸다. 말그대로 광탈. 그래도 교수님께 첨삭받은 지원서들이었는데 어안이 벙벙했다. 그렇게 취업시장의 높은 벽을 느껴버리고는 바로 취업 준비를 시작했다. 패캠 강사분에게도 조언을 구하면서 6월까지 원서를 고치고 제출하고, 시험을 보고 면접만 봤었다. 몇 번을 쓴물을 삼키다가 마지막 남은 회사의 면접에서 ‘여기 안되면 다시 준비해서 내년에 도전해야겠다’라는 생각으로 마음 놓고 들어갔다. 지금까지 했었던 데이터 프로젝트를 차분히 설명하고, 2차 면접까지 진행한 후에 최종 합격 통보가 내게 전달되었고 7월 1일 첫 출근 하게 되었다.</p><h3 id="하반기-회사에서-혼자-살아남기"><a href="#하반기-회사에서-혼자-살아남기" class="headerlink" title="하반기 - 회사에서 혼자 살아남기"></a>하반기 - 회사에서 혼자 살아남기</h3><h4 id="입사"><a href="#입사" class="headerlink" title="입사"></a>입사</h4><p>7월부터 지금까지 정신없이 살아오고 있는 것 같다.(글또를 시작한 것도 7월!!!!) 입사한 회사는 이제 데이터 프로젝트를 진행하려는 회사였다. 데이터 사이언스 프로젝트를 위한 준비가 잘 되어있는 회사는 아니었기 때문에, 데이터를 분석하고 모델만 만드는 일은 할 수 없었다. 물론 처음에는 아무것도 몰랐다. OJT를 1주동안 받고 2주차부터 A/B 테스트 자동화 프로젝트를 맡게 되었다. 데이터 조직에 관련된 사람이 나 혼자 뿐이었기 때문에 혼자 리서치하고 코딩하고 결과를 분석했다. 리서치하는 일은 좋았다. 논문이나 글들을 정리해서 방향을 설정하는 것이었기 때문이었다.(물론 수식을 이해하는 부분은 어려웠다) 코딩을 하다 막히면 정말 도와줄 사람이 아무도 없었기 때문에 그 때는 정말 답답해서 자주 옥상으로 올라가 바람을 쐬었다. <del>처음으로 담배를 필까 고민을 했다.</del> </p><p>여차저차 모델을 만들고 정말 싫어하는 Flask를 이용해서 API를 만드는 작업을 했다. 백엔드 쪽은 공부를 얉게 했기 때문에 어떻게 데이터를 전달받고 결과를 출력해서 줘야하는지 이해하는 데 시간이 많이 들었지만, 개발팀의 도움으로 회사 서비스와 이어 붙이는 데 성공했다.(정말 이어 붙이는 데에만…) 이 작업을 진행하면서 어떻게 회사 제품이 서비스 되고 있는지 깊이 알 수 있게 되었고 백엔드와 데이터 엔지니어링에 흥미가 가기 시작했다. 이전에는 무슨 말인지 이해가 하나도 안되어서 해야되는 건 알았지만 하기가 싫었다…</p><h4 id="10월-이후"><a href="#10월-이후" class="headerlink" title="10월 이후"></a>10월 이후</h4><p>10월쯤에 되어서야 혼자있는 데이터팀에 대리님이 오시게 되었고 둘이서 열심히 A/B 테스트 베타 서비스한 결과를 가지고 수정에 수정을 거쳐 드디어 서비스할 수 있는 정도의 제품이 나오게 되었고 화면기획만 되면 내년 초에 오픈할 수 있게 되었다. 동시에 추천 시스템 개발을 10월부터 시작해서 12월 말까지 돌릴 수 있는 알고리즘을 선별해 놓았고 아키텍쳐를 짜는 중이다. A/B 테스트 자동화에 시간을 많이 써서 추천 쪽에 시간을 못 써서 아쉽긴 하지만 데이터 파이프라인을 붙이는 쪽에 신경을 많이 써볼 생각이다. 데이터 엔지니어링…데이터 엔지니어링…잘하고 싶다…</p><h3 id="여가-및-개인-공부"><a href="#여가-및-개인-공부" class="headerlink" title="여가 및 개인 공부"></a>여가 및 개인 공부</h3><p>취미생활은 따로 나가서 하는 건 없는 것 같다. 일주일에 한번 정도 하는 풋살? 그 외에는 집 앞 헬스장에서 살기 위해서 하는 운동과 축구를 잘하기 위해서 운동을 한다. 축구 잘하려고 운동은 정말 가끔했었는데, 입사하고 삼개월 정도 되었을 때 3키로 넘게 쪄버려서 꾸준히 운동을 해야겠다고 마음 먹었다. 운동하는 걸 제외하면 영화보고 음악듣는 게 내 취미생활의 전부인데 뭔가 새로운 걸 해보고 싶은 마음이 있다. 회사에서 마카롱 만드는 걸 배워서 직접 만든 걸 먹어본 적이 있었는데 굉장히 맛있어서 클래스를 한번 들어볼까? 생각해봤다. 생각만했다… 향수를 만들어 보거나 목공예를 하고 싶기도 한데 시간은 없고ㅠㅜㅜㅠㅠㅠ</p><p>입사한 첫 달을 제외하면 개인 공부 시간이 많이 줄었다. 처음에야 할 일도 많지 않고, 엄청난 열정을 갖고 시작했기 때문에 출근하는 버스에서도 관련 논문도 챙겨보고 했었지만, 5개월차에 벌써 피곤에 쩔어 버스만 타면 졸게 되어 버려서 기껏해야 동영상으로 설명해주는 영상 몇개 보는 정도에 그치고 있다. 그러다 딥러닝 공부를 제대로 해보고 싶기도 했고, 일해서 번 돈으로 딥러닝 서버를 집에 하나 두고 싶었던 작은 소망이 있어서 거금 200만원을 투자해서 딥러닝용 PC를 사들였다. <code>sanghyub machine</code>이라는 특징없는 이름을 갖고 있지만 아주 애착이 간다. 딥러닝 모델을 돌리기 위해서 세팅은 다 끝났고, 케라스 창시자에게 배우는 딥러닝을 읽으면서 모델을 한번 돌려보고 있다. 기존 노트북에서 하루종일 돌아가던게 쌩쌩 돌아간다. 역시 돈이 최고인 것 같다. </p><p>하지만, 진짜 하고 싶었던 건 12월까지 <code>sanghyub machine</code>과 함께 캐글 대회에 참가해보는 것이었다. 이것저것 세팅도 하고 회사일도 조금 바빴기에 미뤄두다가 연말이 되어버렸다. 내년에는 꼭 대회에 참가해서 좋은 결과를 만들어 내고 싶다.</p><h2 id="2020"><a href="#2020" class="headerlink" title="2020"></a>2020</h2><p>2020년에는 하고싶은? 목표라고 생각할 만한 것들이 몇개 고정되어 있다. 일적인 부분도 있고 내 개인 생활과 관련된 목표도 있다. 2019년은 닥치는 대로 살다보니 계획적으로 살지 못한 것 같아 아쉬운 것이 있어서, 할 걸 제대로 설정해서 똑바로 살아봐야겠다.(하지만 글또 3기를 한 건 2019년의 큰 계획 중에 하나였다. ‘글을 꾸준히 써보자’, 아주 성공적이진 않지만, 나름 한달에 글은 부끄럽지만 2개는 꾸준히 썼다!)</p><h4 id="2020년에-나는"><a href="#2020년에-나는" class="headerlink" title="2020년에 나는"></a>2020년에 나는</h4><ul><li>추천 시스템이나 다른 서비스를 위한 데이터 파이프라인 구성을 성공적으로 한다.</li><li>퇴근 후 개인 공부시간을 매일 30분 이상 갖는다.</li><li>캐글 대회에 도전한다.</li><li>캐글 대회에서 동메달 이상의 성과를 낸다.</li><li>일주일에 3일 이상 운동한다.</li><li>3대 운동 300에 도전한다.</li><li>돈을 모아서 피렌체에 간다.</li><li>꾸준히 글을 쓴다.</li><li>책을 꾸준히 읽자.</li><li>일을 즐겁게 한다.</li><li>소중한 사람들을 잘 챙기자.</li></ul><p>더 구체적인 목표를 더 쓰고 싶은데, 글로 적으려니 제대로 써지지가 않는다. 아직 2020년까지 8일 더 남았으니까 그 전까지 하고싶은 리스트를 더 늘리고, 더 구체적으로 작성해봐야겠다. 2019년 재밌게 살았다. 이번해와는 또 다르게 2020년을 계획하고 기록해서, 2020년 말에는 꽉 찬 느낌이 드는 해로 만들어 봐야겠다. 그리고 글또 3기가 끝나면 4기에 다시 도전해보고 싶다. 처음에 목표했었던 것은 10만원 deposit을 온전히 연말에 돌려받는 것이었는데, 리뷰하는 것을 까먹어서 10만원 돌려받기는 글러버렸다… 리뷰도 꼼꼼히 하고 지금 글쓰는 것과는 조금 다르게 글 구성에 신결을 써서 작성해보고 싶다. </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/12/22/adios-2019/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ubuntu를 이용한 딥러닝 Server 설치하기 3</title>
      <link>http://tkdguq05.github.io/2019/12/08/ubuntu3/</link>
      <guid>http://tkdguq05.github.io/2019/12/08/ubuntu3/</guid>
      <pubDate>Sun, 08 Dec 2019 06:57:40 GMT</pubDate>
      <description>
      
        &lt;p&gt;본격적으로 딥러닝을 하기 위한 준비과정&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>본격적으로 딥러닝을 하기 위한 준비과정<br><a id="more"></a></p><h1 id="Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-2"><a href="#Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-2" class="headerlink" title="Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2"></a>Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2</h1><h2 id="Ubuntu-with-mac-원격-접속-연결하기"><a href="#Ubuntu-with-mac-원격-접속-연결하기" class="headerlink" title="Ubuntu with mac, 원격 접속 연결하기"></a>Ubuntu with mac, 원격 접속 연결하기</h2><p>저번 글에서는 공유기를 이용한 고정ip설정과 포트포워딩, 원격 접속, 원격 파일 전송에 대해서 알아봤습니다. 이번 글에서는 본격적으로 딥러닝을 하기 위한 Cuda와 CuDnn, tensorflow, keras 설치에 이어 jupyter notebook 연결까지 해보겠습니다.</p><p>설치하는 순서는 다음과 같습니다.</p><ul><li>nvidia 그래픽 드라이버 설치</li><li>Cuda 설치</li><li>CuDnn 설치</li><li>tensorflow, keras 설치</li><li>jupyter notebook 연결</li></ul><h2 id="1-nvidia-그래픽-드라이버-설치"><a href="#1-nvidia-그래픽-드라이버-설치" class="headerlink" title="1. nvidia 그래픽 드라이버 설치"></a>1. nvidia 그래픽 드라이버 설치</h2><p>그래픽 드라이버 설치는 겜돌이들에게는 너무나 익숙한 일입니다. 예전엔가 카트를 하고 싶었는데 화면이 까맣게 나와서 네이버 지식인에 물어봤더니 드라이버 설치를 하라고 했던 기억이 나네요. 그래픽 드라이버 회사의 사이트로 들어가서 그래픽 드라이버를 받아서 설치하면 끝입니다. 같은 방식으로 Ubuntu 서버에도 그래픽 드라이버 종류를 알아본 뒤에, 이에 맞는 드라이버를 받아서 설치해주면 됩니다. 하지만 설치를 잘못하게 되면 검은화면에서 뚝뚝뚝 에러메서지만 흘러나오며 엔터키도 먹지 않는 악몽을 경험하게 될 것입니다. </p><p>자신의 그래픽 카드가 뭔지는 컴퓨터를 사신 분이 잘 아실 것입니다. 그 정보를 가지고 <a href="http://www.nvidia.com/Download/Find.aspx?lang=en-us">http://www.nvidia.com/Download/Find.aspx?lang=en-us</a> nvidia 홈페이지에 접속한 후에 그래픽 카드 정보를 입력하고 알맞은 그래픽 드라이버의 Version을 확인 합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apt-cache search nvidia</span><br></pre></td></tr></table></figure><p>서버로 돌아와 다음 명령어를 이용해 설치가능한 드라이버를 확인해 보고 본격적으로 설치해봅니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install nvidia-xxx</span><br></pre></td></tr></table></figure><p>이제 xxx에 들어갈 적절한 드라이버 Version을 입력하고 설치를 해주면 됩니다. 보통은 정상적으로 설치가 되는데, </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Errors were encountered while processing:</span><br><span class="line"> nvidia-xxx</span><br><span class="line"> libcuda1-xxx</span><br><span class="line"> nvidia-opencl-icd-xxx</span><br><span class="line">E: Sub-process /usr/bin/dpkg returned an error code (1)</span><br></pre></td></tr></table></figure><p>이런 식의 에러가 날 수 있습니다. 이런 경우에는<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir /usr/lib/nvidia</span><br><span class="line">$ sudo apt-get install nvidia-xxx</span><br></pre></td></tr></table></figure></p><p>를 이용해서 nvidia 폴더를 만들어주고 설치를 진행하면 됩니다.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dkms nvidia-modprobe</span><br></pre></td></tr></table></figure><p>그리고 드라이버 종류에 상관없이 위의 패키지를 설치해주고 그래픽 드라이버가 로드 될 수 있도록 reboot 시켜줍니다.</p><p><a href="https://hiseon.me/linux/ubuntu/install_nvidia_driver/">설치 가이드 출처</a></p><h3 id="Trouble-shooting"><a href="#Trouble-shooting" class="headerlink" title="Trouble shooting"></a>Trouble shooting</h3><p>설치 후에 뿌듯 뿌듯한 마음으로 reboot 명령어를 내리고 화면을 천천히 지켜보고 있다 보면, 심상치 않은 메세지가 뚝뚝뚝 나오고 엔터키도 듣지 않는 것처럼 보일 때가 있습니다. 저 또한 멘탈이 깨져버려서 10붕동안은 멍하니 쳐다보고만 있었습니다. </p><p>이유는 여러가지가 있겠습니다만, 저의 경우에는 설치된 드라이버 버전이 서버와 맞지 않아서 생기는 문제였습니다. 이럴 경우에는 설치된 드라이버를 삭제하고 다시 설치하는 방법을 생각해 볼 수 있겠습니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt purge nvidia*</span><br></pre></td></tr></table></figure><p>위의 명령어로 nvidia관련 드라이버를 싹 날려주고 reboot하여 맞는 드라이버를 설치해 줍니다. </p><h2 id="2-Cuda-CuDnn-설치"><a href="#2-Cuda-CuDnn-설치" class="headerlink" title="2.Cuda, CuDnn 설치"></a>2.Cuda, CuDnn 설치</h2><p>이제 CUDA 10.0과 cuDNN 7.5 설치를 진행해봅니다. Cuda는 다음의 명령어로 설치해 줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install cuda-10-0</span><br></pre></td></tr></table></figure></p><p>설치가 되는데 시간이 좀 걸릴 수 있습니다. 설치가 끝나면 제대로 설치가 되었는지를 확인하기 위해서</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc --version</span><br></pre></td></tr></table></figure><p>위의 명령어를 이용해 Cuda Compiler정보를 확인합니다. nvcc정보가 잘 나오면 설치가 잘 된 것입니다.</p><p>이제 CuDnn을 설치해야 하는데, CuDnn은 Nvidia에서 공개적으로 다운받을 수 있게 해놓지 않았습니다. Nvidia에 회원으로 등록을 해야 CuDnn을 다운 받을 수 있게 해줍니다. 따라서 <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a> 이 사이트로 들어가 회원가입을 해주고 우리는 Cuda 10.0을 설치해줬으니까 10.0에 맞는 Cudnn 7.5를 다운로드 해줍니다. 다운이 되었으면 저의 경우는 Mac을 이용해서 받았기 때문에, <a href="https://tkdguq05.github.io/2019/11/22/ununtu2/">이전 글</a>에서 다루었던 <code>sftp</code>를 이용해서 파일을 전송했습니다.</p><p>파일을 이동시킨 후에 tar로 말려져 있는 파일을 풀어줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf cudnn-10.0-linux-x64-v7.5.x.x.tgz</span><br></pre></td></tr></table></figure></p><p>7.5뒤의 버전은 다를 수 있으니 잘 확인하시고 수정해 넣으시면 됩니다. 이제 압축을 푼 파일을 cuda폴더에 복사해 넣어줍니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo cp -P cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda-10.0/lib64/</span><br><span class="line"></span><br><span class="line">sudo cp  cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda-10.0/include/</span><br></pre></td></tr></table></figure><p>마지막으로 이동시킨 파일들에 대한 권한을 부여해주면 됩니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod a+r /usr/<span class="built_in">local</span>/cuda-10.0/include/cudnn.h /usr/<span class="built_in">local</span>/cuda-10.0/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p><p>*추가<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#Nvidia에서 필요한 파일이라고 하네요</span><br><span class="line">sudo apt-get install libcupti-dev</span><br></pre></td></tr></table></figure></p><p>bashrc에 환경변수를 추가해주는 글을 많이 봤는데, 저의 경우에는 굳이 입력을 안해도 잘 인식하는 걸 봐서, 넣지 않았습니다. 제대로 불러오지 못한다면<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi ~/.bashrc</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line"></span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64\$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><p>를 추가해 주세요</p><h2 id="3-Tensorflow-Keras-설치"><a href="#3-Tensorflow-Keras-설치" class="headerlink" title="3. Tensorflow, Keras 설치"></a>3. Tensorflow, Keras 설치</h2><p>Keras 설치에 앞서, 백엔드로 사용하는 Tensorflow를 설치해 줍니다. 이제 설치는 정말 간단합니다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade tensorflow-gpu</span><br></pre></td></tr></table></figure><p>python3버전의 tensorflow를 설치해줍니다. cpu 버전을 설치하고 싶다면 -gpu를 빼주면 됩니다.</p><p>conda 가상환경을 만들었다면, 실행시킨 후에 필요 라이브러리를 설치하고 Keras를 최종적으로 설치하면 끝입니다!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda activate your_name</span><br><span class="line">(your_name) conda install h5py</span><br><span class="line">(your_name) conda install graphviz</span><br><span class="line">(your_name) conda install pydot</span><br><span class="line">(your_name) conda install keras</span><br></pre></td></tr></table></figure><h2 id="4-Jupyter-Notebook-설치"><a href="#4-Jupyter-Notebook-설치" class="headerlink" title="4. Jupyter Notebook 설치"></a>4. Jupyter Notebook 설치</h2><p>먼저 가상환경으로 들어가주고 pip를 이용해 jupyter notebook을 설치해 줍니다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate your_name</span><br><span class="line">pip install jupyter notebook</span><br></pre></td></tr></table></figure></p><p>설치가 되었지만, 외부에서 접속을 해서 jupyter notebook을 사용하려면 아직 몇가지 설정이 더 남았습니다.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure></p><p>를 이용해서 jupyter 설정파일을 만들어 준 후에</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ipython <span class="comment">#입력 후 In[1]이 나오면,</span></span><br><span class="line">from notebook.auth import passwd</span><br><span class="line">Enter password:  <span class="comment">#원하는 비밀번호 입력</span></span><br><span class="line">Verify password: </span><br><span class="line">Out[2]: <span class="string">'sha1:f24baff49ac5:863dd2ae747212ede58125302d227f0ca7b12bb3'</span></span><br></pre></td></tr></table></figure><p>이렇게 하고 나면 shai로 시작하는 암호문 같은 게 있습니다. 이걸 복사해서 잘 적어두시고, <code>jupyter_notebook_config.py</code>를 열어서<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># The IP address the notebook server will listen on. </span><br><span class="line">c.NotebookApp.ip = &apos;0.0.0.0&apos; </span><br><span class="line">c.NotebookApp.port_retries = 50 </span><br><span class="line">c.NotebookApp.port_retries = 8888</span><br><span class="line">c.NotebookApp.open_browser = False</span><br></pre></td></tr></table></figure></p><p>위와 같이 수정해 주고 <code>jupyter notebook --ip=0.0.0.0</code>으로 실행해주면 jupyter notebook이 짠 하고 등장할 것입니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/12/08/ubuntu3/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ubuntu를 이용한 딥러닝 Server 설치하기 2</title>
      <link>http://tkdguq05.github.io/2019/11/22/ununtu2/</link>
      <guid>http://tkdguq05.github.io/2019/11/22/ununtu2/</guid>
      <pubDate>Fri, 22 Nov 2019 08:18:37 GMT</pubDate>
      <description>
      
        &lt;p&gt;서버에 원격 접속하기와 원격 파일 전송하기&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>서버에 원격 접속하기와 원격 파일 전송하기<br><a id="more"></a></p><h1 id="Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-2"><a href="#Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-2" class="headerlink" title="Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2"></a>Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2</h1><h2 id="Ubuntu-with-mac-원격-접속-연결하기"><a href="#Ubuntu-with-mac-원격-접속-연결하기" class="headerlink" title="Ubuntu with mac, 원격 접속 연결하기"></a>Ubuntu with mac, 원격 접속 연결하기</h2><p>간밤에 평안하셨습니까? 지난번까지는 ubuntu설치를 열심히 했고, 원격 접속은 쉽게 해결할 것이라고 생각하며 가볍게 노트북을 열어서 신나게 원격접속 연결을 시도했습니다. 퇴근 후에 저녁을 먹고 여유롭게 9시부터 만지기 시작했는데, 어느새 11시가 넘어가고 새벽 1시가 넘어가곤 했습니다. 그렇게 며칠을 지내고서야 맥북으로 ubuntu서버에 접속을 할 수 있었습니다. </p><p>이번 글을 쓰기 까지 간밤에 한 일들은 다음과 같습니다.</p><ul><li>ubuntu 인터넷 연결 </li><li>KT 공유기 고정 아이피 설정</li><li>포트 포워딩</li><li>원격 접속 </li><li>원격 파일 전송 </li><li>nvidia 그래픽 드라이버 설치</li><li>Cuda 설치</li><li>CuDnn 설치</li><li>tensorflow, keras 설치</li><li>jupyter notebook 연결</li></ul><p>각 단계에서 다음 단계로 넘어가기 까지 우여곡절이 많았는데, 해결할때마다 은근 쾌감이 있어서 기분 좋았습니다. 리눅스, 그러니까 검은 화면만 보면 겁을 실컷 먹었었는데, 그 두려움을 극복해가는 스스로가 대견스럽다고 생각을 해봅니다. </p><p>이번 글은 ubuntu 인터넷 연결부터 원격 파일 전송까지가 되겠습니다. 가장 힘들었던 부분은 네트워크를 이해하지 못해서 삽질을 여러번해서 애먹었던 부분입니다. 이 부분은 KT 공유기 고정 아이피 설정에서 민낯을 샅샅이 밝힐 예정입니다.</p><h2 id="1-ubuntu-인터넷-연결"><a href="#1-ubuntu-인터넷-연결" class="headerlink" title="1. ubuntu 인터넷 연결"></a>1. ubuntu 인터넷 연결</h2><p>저는 공유기에 있는 랜선을 서버용 컴퓨터에 끼워주면 알아서 인터넷 연결이 될 줄 알았지만 그렇지 않았습니다. 그런 것은 검은 화면이 아닌 곳에서나 가능하다는 것을 왜 그때는 제대로 깨닫지 못했을까요? </p><p>ubuntu 서버 화면으로 넘어가서 vi 편집기를 이용해 다음 명령어를 날려줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/network/interfaces</span><br></pre></td></tr></table></figure></p><p>그러면 무언가 입력할 것이 있는 창이 하나 나와 있을 것 입니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line"></span><br><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto eth0</span><br></pre></td></tr></table></figure></p><p>저의 경우에는 이렇게 들어있는 내용이 얼마 없었던 걸로 기억합니다.(당시에 경황이 없어서 화면 사진을 하나도 찍지 못했네요ㅜㅜ)<br><code>ifconfig</code>명령어를 이용해 현재 이더넷의 주소를 알아내야합니다. 저의 경우에는 enpxxxx이었습니다. 이 값을 기억해두고 다시 interface로 들어가서</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line"></span><br><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto enpxxxx</span><br><span class="line">iface enpxxxx inet dhcp</span><br></pre></td></tr></table></figure><p>이런식으로 입력해 줍니다. 이 방식은 유동 ip일때 사용하는 방식이기는 하지만, 일단 인터넷 연결이 되어야 ubuntu 라이브러리를 받을 수 있기 때문에 설정해 줍니다.</p><p>설정이 다 되었다면<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service networking restart</span><br></pre></td></tr></table></figure></p><p>를 이용해서 network를 재시작해 반영시켜줍니다.</p><h2 id="2-KT-공유기-고정-ip-설정"><a href="#2-KT-공유기-고정-ip-설정" class="headerlink" title="2. KT 공유기 고정 ip 설정"></a>2. KT 공유기 고정 ip 설정</h2><p>이제 KT공유기의 ip를 고정시켜줄 차례입니다. 고정 아이피를 잡아주는 이유는 간단합니다. 외부 통신을 통해(예를 들어 맥북을 통해) 서버로 접속한다고 했을 때, 고정 ip가 잡혀있지 않다면, 동적 아이피로 인해 서버의 ip 주소는 계속 바뀔 것이고 결국 접속이 안될 것입니다. </p><p>정말 간단하게 설정할 수 있기 때문에 제가 보고 설정한 링크를 첨부해 놓겠습니다.<br><a href="https://extrememanual.net/12155#%EC%95%84%EC%9D%B4%ED%94%BC_%EC%88%98%EB%8F%99_%EC%84%A4%EC%A0%95">KT 공유기 고정IP 설정</a></p><p>이렇게 고정 ip를 잡아주고 난 뒤에 서버 컴퓨터로 와서 ifconfig 명령어를 쳐서 ip를 확인하고 설정한 ip대로 나온다면 성공입니다.</p><p>이제 아까 dhcp설정을 static으로 고쳐줄 차례입니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/network/interfaces</span><br></pre></td></tr></table></figure></p><p>명령어를 통해 수정할 부분으로 가서<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto enpxxxx</span><br><span class="line">iface enpxxxx inet static</span><br></pre></td></tr></table></figure></p><p>이렇게 고쳐줍니다.</p><h2 id="3-포트포워딩"><a href="#3-포트포워딩" class="headerlink" title="3. 포트포워딩"></a>3. 포트포워딩</h2><p>포트포워딩을 하는 이유는 외부에서 접속을 하게 될 때 공유기로 인해 내가 접속하고 싶은 컴퓨터를 제대로 연결을 못 해주게 됩니다. 공유기가 많은 컴퓨터에 대해서 ip를 뿌려주고 있기 때문입니다. 그래서 어떤 포트로 접속을 해줘야 내가 원하는 컴퓨터에 연결이 되는지를 알려줘야 제대로 연결을 할 수 있게 됩니다. 따라서 포트포워딩은 특정 컴퓨터에게 특정 포트로 연결해 주는 작업을 말하는 것입니다. 포트가 일종의 이정표 역할을 하게 되겠습니다. </p><p>다시 KT 홈허브 관리페이지로 가서 장치 설정에 트래픽 관리로 가면 바로 포트포워딩을 볼 수 있습니다. 포트 포워딩 설정에서 다른 건 건드리지 않고 외부/내부 포트, 내부 ip 주소만 작성해줍니다. 외부/내부 포트에는 내가 열고 싶은 포트 번호를 입력해주고, 내부 ip에서는 고정 ip를 넣어줍니다. 일단 8888포트를 열어줍니다.(jupyter notebook이 켜지는 포트가 주로 8888이니까 이렇게 설정을 했습니다)<br><img src="/images/portforwarding.jpg" alt="포트포워딩 화면"><br><a href="https://m.blog.naver.com/PostView.nhn?blogId=hiho33&amp;logNo=40206570339&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F">사진출처</a></p><h2 id="4-원격접속"><a href="#4-원격접속" class="headerlink" title="4. 원격접속"></a>4. 원격접속</h2><p>원격접속을 위해서 우리가 연결할 포트가 필요합니다. ssh접속은 22번 포트를 사용하고, 보통 처음 산 컴퓨터에는 이런 설정이 제대로 되어있지 않은 경우가 많습니다. 그렇기 때문에 직접 포트를 열어주도록 하겠습니다.</p><p>서버컴퓨터로 돌아와서<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure></p><p>명령어를 입력해 줍니다. 여기서 sshd_config가 없다면, </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install openssh-server</span><br></pre></td></tr></table></figure><p>를 통해 ssh를 설치해주면 됩니다.</p><p>sshd_config를 보면 Port라고 써있는 부분이 있습니다. 해당 라인이 #로 주석 처리 되어 있거나, 포트 넘버가 적혀있지 않은 경우에는, i를 눌러 insert모드로 바꾸고 port에 22를 넣어줍니다. 그리고 esc를 누르고 :wq(저장 후 종료)로 빠져나옵니다. 그러면 원격접속을 위한 준비는 끝났습니다.</p><p>이제 맥북으로 돌아와 터미널을 열어줍니다.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -p 22 [servername]@[ip address]</span><br></pre></td></tr></table></figure></p><p>이 명령어를 입력해주면 연결이 됩니다. ssh로 22번 포트에 접속해 준다는 것이고, 설정한 서버이름과 서버가 사용하는 ip주소를 입력해주고 설정한 비밀번호를 입력해주면<br><img src="/images/connect_server.png" alt="연결 성공!!"><br>연결에 성공하게 됩니다.<br>(*주의, 혹시 집에서 노트북도 같은 아이피를 쓰고 있는 상태에서 연결을 시도하게 되면 100% 연결 성공했다고 나오게 된다. 공유기를 통해 내부적으로 연결이 되어 있기 때문이다. 정확한 확인을 위해서 잠시 노트북의 wifi를 끄고 테더링을 걸어서 다시 확인해보자, 이렇게 해서 연결이 된다면 진짜 된 것이다.)</p><h2 id="5-원격-파일-전송"><a href="#5-원격-파일-전송" class="headerlink" title="5. 원격 파일 전송"></a>5. 원격 파일 전송</h2><p>원격 파일 전송은 로컬에 있는 파일을 서버에 보내고 싶을 때 혹은 서버에 있는 파일을 로컬로 가져오고 싶을 때 사용할 수 있습니다. 특히 Nvidia 그래픽 드라이버를 서버에 보내서 설치하고 싶을 때 아주 유용하게 사용할 수 있습니다.</p><p>scp를 사용해도 되고, sftp를 사용해도 된다.</p><p>개인적으로 sftp를 더 선호하기 때문에 sftp로 파일 전송하는 법을 간략히 설명해보면,<br>아까 원격접속을 했던 것과 마찬가지로<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sftp [servername]@[ip_address]</span><br></pre></td></tr></table></figure></p><p>앞에만 sftp로 바꿔주고 나머지는 똑같이 입력한다. 그러면 패스워드를 입력하라고 나오고 정상적으로 패스워드를 입력했다면 sftp&gt; 로 시작하는 간단한 화면이 등장한다. sftp는 쉘을 오픈한 폴더 위치에서 시작하게 되므로 파일이 있는 위치로 이동한 다음 쉘을 여는 것을 권장한다. </p><p>여기서 <code>put</code>명령어를 입력하고 보낼 파일이름을 끝까지 적으면, 서버에 파일이 전달되는 것을 확인할 수 있다.</p><hr><p>이제 진짜 딥러닝 PC를 만들기 위한 기본 세팅이 끝났다. 퇴근 하고 집에와서 잠도 적게 자면서 연결했는데 생각보다 금방 끝나서, 그리고 회사에서도 연결이 되어서 뿌듯했었다. 이제 간밤에 악몽을 꾸게 만든 Nvidia 그래픽 드라이버 설치, CUDA 설치 그리고 CuDNN 설치가 남았다. </p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/11/22/ununtu2/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ubuntu를 이용한 딥러닝 Server 설치하기 1</title>
      <link>http://tkdguq05.github.io/2019/11/10/ubuntu/</link>
      <guid>http://tkdguq05.github.io/2019/11/10/ubuntu/</guid>
      <pubDate>Sun, 10 Nov 2019 11:35:14 GMT</pubDate>
      <description>
      
        &lt;p&gt;ubuntu 설치와 트러블슈팅&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>ubuntu 설치와 트러블슈팅<br><a id="more"></a></p><h1 id="Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-1"><a href="#Installation-of-Ubuntu-18-04-LTS-for-Deep-Learning-Computer-1" class="headerlink" title="Installation of Ubuntu 18.04 LTS for Deep Learning Computer -1"></a>Installation of Ubuntu 18.04 LTS for Deep Learning Computer -1</h1><h2 id="Ubuntu-Installation-The-Trouble-Shooting"><a href="#Ubuntu-Installation-The-Trouble-Shooting" class="headerlink" title="Ubuntu Installation - The Trouble Shooting"></a>Ubuntu Installation - The Trouble Shooting</h2><p>평소에 캐글 대회에 관심이 있었고, 제대로 대회에 참여해봐야지 하는 생각이 있었습니다. 그러다가 IEEE 대회에 참여해 봤습니다. 결과는 노메달!, 동메달이라도 따보고 싶었는데 아쉽게 메달을 놓치게 되었습니다. 그때 정말로 캐글 대회에서 메달을 따보고 싶다는 생각이 들었고, 올해가 되기 전에 대회에 한번 더 참여하는 것과, 내년 초까지 Competition으로 Expert를 따보자는 계획을 하게 되었습니다.</p><p>계획을 위해서는 딥러닝용 사양 좋은 PC가 필수라는 생각이 들었고, 시원하게 2백만원 정도의 사양의 컴퓨터를 맞추게 되었습니다.</p><p><img src="/images/spec.jpeg" alt="PC의 사양"></p><p>딥러닝용 PC서버를 만들기 위해서 OS를 정해야 했는데, 캐글 고수분들께서 Ubuntu로 세팅을 많이 하시는 걸 보고 바로 usb에 부팅용 iso를 받아서 시동 디스크를 만들었습니다.</p><p>제가 주로 보고 따라한 블로그 주소는 다음과 같습니다. <a href="https://simonjisu.github.io/datascience/2018/06/02/gpuserver.html">장지수님 블로그, 개인 딥러닝용 서버 설치 과정기</a></p><p>이 과정을 보고 쭉 따라했고, usb를 넣고 부팅을 시켰더니,<br><img src="/images/error.jpeg" alt="에러에러에러!!!!"></p><h3 id="Ubuntu-18-04-USB-설치-error-Mac-OS-이용"><a href="#Ubuntu-18-04-USB-설치-error-Mac-OS-이용" class="headerlink" title="Ubuntu 18.04 USB 설치 error (Mac OS 이용)"></a>Ubuntu 18.04 USB 설치 error (Mac OS 이용)</h3><p>쉽게 될 줄 알았더니 똭!! 에러가 떠버렸습니다. Google신을 답을 알고 계시겠죠? 하지만 답을 쉽게 주지는 않으시는 것 같습니다. 3시간의 서칭 끝에 ‘<code>nouveau</code> unknown chipset’, 누보(새 시작 이라는 말이라고 합니다)에 대한 해결법을 찾았습니다. 부팅 디스크를 넣고, ubuntu Grub화면까지 간 다음(grub 화면이 안나올 경우 shift를 누르면 된다고 하네요), e를 눌러서 텍스트 편집기로 갑니다. 이후 나타나는 화면에서 <code>quiet splash</code> 혹은 <code>quiet ---</code>로 표시된 부분으로 갑니다. <code>quiet splash nomodeset</code> 또는 <code>quiet splash nomodeset</code>을 입력 후 F10을 눌러 저장하고 리부팅 합니다.</p><p>보통 이렇게 하면 문제없이 ubuntu화면으로 넘어가서 설치가 될 것입니다. 하지만 신은 저에게 편한 삶을 허락하지 않는 것 같습니다. 일요일에 좀 쉬고싶었는데 눈치없이 등장하는 에러들… <code>couldn&#39;t get size 0x80000000000e</code>와 <code>MODSIGN: Couldn&#39;t get UEFI db list</code> 그리고 <code>line 7: can&#39;t open /dev/sdc: No medium found</code>. 또 다시 Google 신에게 달려가서 2시간 동안 서칭하고 실패하고를 반복했습니다. 그러다가 사막의 오아시스 같은 솔루션을 발견했습니다.</p><p><a href="https://askubuntu.com/questions/47076/usb-boot-problems">line 7 no medium found 솔루션</a></p><p>3 보트업 받은 솔루션을 보면 약간의 야매 방법이 등장합니다. 아까 nouveau 메시지에 대한 해결을 하기 위해 <code>nomodeset</code>을 사용했는데 그것 대신 <code>break debug</code>를 입력하는 것 입니다. 이렇게 하면 debug 메시지가 쭈우욱 등장하고 엔터를 눌렀을 때 입력할 수 있는 터미널이 나오게 됩니다. 여기서</p><ol><li>부팅 디스크인 USB를 빼고</li><li>다시 USB를 넣은 후에</li><li>exit을 입력하고 엔터를 누릅니다.</li></ol><p>이렇게 되면 화면이 후루룩 쭉죽 지나가면서 된다 된다…라는 희망에 빠지게 됩니다.</p><p><img src="/images/success.jpeg" alt="성공이다ㅠㅠㅠㅠㅜㅠㅜㅜㅠ"><br>성공입니다. ubuntu 설치 화면으로 정상적으로 이동하였습니다. 여기서부터 세팅은 안내에 따라서 착착 맞춰서 하면 되고, 쭉 진행 하면 성공적으로 username과 서버명이 등록된 ubuntu 화면을 볼 수 있게 됩니다.</p><p>잠시 달콤한 성공을 맛봤으니 원격 부팅 세팅을 해보아야겠습니다. 블로그 설명에 따라</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi /etc/ssh/sshd_config</span><br></pre></td></tr></table></figure><p>를 쳐봅니다. </p><p>?????</p><p>그런건 없습니다. ssd_config를 오타냈나 싶어서 쳐보고 확인해보지만 블로그 설명과는 다른 값이 나옵니다. 찾아보니 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install openssh -server</span><br></pre></td></tr></table></figure><p>을 해야 만들어진다고 합니다. 자 그럼 sudo apt-get install openssh -server를 입력해봅니다. <code>Unable to fetch ~~</code>가 나올 것입니다. 이것은 현재 ubuntu 라이브러리를 받는 서버가 맛이 갔거나, 현재 인터넷 연결이 안되어 있다는 말입니다. 또 다시 문제에 봉착했습니다.</p><h3 id="인터넷-연결이-안되어있습니다-Ubuntu에서는-인터넷-연결을-어떻게-하는-것일까요"><a href="#인터넷-연결이-안되어있습니다-Ubuntu에서는-인터넷-연결을-어떻게-하는-것일까요" class="headerlink" title="인터넷 연결이 안되어있습니다. Ubuntu에서는 인터넷 연결을 어떻게 하는 것일까요?"></a>인터넷 연결이 안되어있습니다. Ubuntu에서는 인터넷 연결을 어떻게 하는 것일까요?</h3><p>또 다시 서칭을 시작했습니다. 인생은 깁니다. 주말은 짧습니다. 일단 떡볶이를 먹고 제정신을 차린 후 다시 시도해보았습니다. 여러 솔루션들이 나왔지만 고정 ip를 이용하는 글이 대부분이었고 제가 원하는 답은 없는 것 같았습니다. </p><p>먼저 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/network/interfaces</span><br></pre></td></tr></table></figure><p>명령어를 통해서 네트워크 설정이 어떻게 되어있는지 확인했습니다. 보통의 경우라면</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This file describes the network interfaces available on your system</span></span><br><span class="line"><span class="comment"># and how to activate them. For more information, see interfaces(5).</span></span><br><span class="line"><span class="comment"># The loopback network interface</span></span><br><span class="line">auto lo</span><br><span class="line">iface lo inet loopback</span><br><span class="line"><span class="comment"># The primary network interface</span></span><br><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br></pre></td></tr></table></figure><p>이렇게 나와있는 게 정상이고 default 값이라는 얘기가 많아서 똑같이 설정을 해주었습니다. 하지만 리부트해도 여전히 인터넷 연결은 되지 않았습니다. </p><p>하지만 스스로 답을 구하는 자에게는 복이 주어집니다. fresh ubuntu installation을 한 사람이 인터넷 연결이 안된다는 글을 올린 것을 보았고 솔루션 내용은 다음과 같았습니다. </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo ifconfig eth0 up   </span><br><span class="line">sudo dhclient eth0</span><br></pre></td></tr></table></figure><p>물론 바로 되지는 않습니다. 세상에는 쉬운일이 없습니다. 항상 자신의 상황에 맞는지 확인하고 문제에 쏙 맞게 다듬을 필요가 있습니다. 문제는 eth0 부분이었습니다. eth0에 해당하는, 자신의 값을 알아야 했습니다.</p><p><code>ifconfig -a</code>를 이용하면 logical name을 알 수 있습니다. 저의 경우에는 enp34s0였습니다. </p><p>이 값을 다시 넣어서 엔터를 눌러주고<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping www.google.com</span><br></pre></td></tr></table></figure></p><p>로 테스트를 해주면, 이제 구글에 ping을 날릴 수 있는 것을 확인할 수 있습니다.</p><p>일요일을 다 날렸지만, 이제 나름대로 세팅할 수 있는 환경을 만들어 놔서 뿌듯합니다. 월요일에 퇴근하고 이제 원격 부팅 접속을 만져봐야겠습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/11/10/ubuntu/#disqus_thread</comments>
    </item>
    
    <item>
      <title>2019 GDG DEVFEST SEOUL 행사를 다녀오다</title>
      <link>http://tkdguq05.github.io/2019/10/20/GDG-DEV-FEST/</link>
      <guid>http://tkdguq05.github.io/2019/10/20/GDG-DEV-FEST/</guid>
      <pubDate>Sun, 20 Oct 2019 08:30:50 GMT</pubDate>
      <description>
      
        &lt;p&gt;GDG Devfest 행사를 다녀왔다&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>GDG Devfest 행사를 다녀왔다<br><a id="more"></a></p><h1 id="2019-GDG-DEVFEST-SEOUL"><a href="#2019-GDG-DEVFEST-SEOUL" class="headerlink" title="2019 GDG DEVFEST SEOUL"></a>2019 GDG DEVFEST SEOUL</h1><p>2019 GDG DEVFEST SEOUL 행사가 세종대학교 광개토관 지하 2층 컨벤션홀에서 열렸습니다. 2주 전에 행사 소식을 접했는데 타임 테이블을 보니 ML/AI관련 세션이 많이 준비되어 있고 관심있는 내용이 있어서 바로 티켓을 구매했습니다. 특히 캐글 코리아에서 활동하시는 이유한 님의 BERT in Kaggle, 분자대회에서 BERT를 이용해서 어떻게 금메달을 획득했는지가 궁금해서 질러버렸습니다.(캐글 코리아 짱짱)</p><p><img src="/images/NAVERcloud/timetable.png" alt="GDG 시간표"><br>타임테이블은 다음과 같았습니다.</p><h2 id="쉽게-따라할-수-있는-한국어-임베딩-구축-by-이기창님"><a href="#쉽게-따라할-수-있는-한국어-임베딩-구축-by-이기창님" class="headerlink" title="쉽게 따라할 수 있는 한국어 임베딩 구축 by 이기창님"></a>쉽게 따라할 수 있는 한국어 임베딩 구축 by 이기창님</h2><p>맨 첫 번째로 Google Cloud Korea 양승도님께서 키노트 발표를 해주셨고 30분간의 쉬는시간을 가졌습니다. 그리고 제가 첫번째로 들으려고 했던 세션은 Naver 이기창님의 <strong>쉽게 따라할 수 있는 한국어 임베딩 구축</strong> 이었습니다.<br><img src="/images/NAVERcloud/Gichang.jpeg" alt="이기창님의 세션"> 쉬는시간에 밥을 빨리 먹으려고 하는데, 식사가 너무 늦게나오는 바람에 세션의 앞부분은 놓쳤습니다(ㅠㅠㅠㅠㅜㅠ). 아쉬운 부분이 있었지만, 핵심은 기존의 워드 기반의 임베딩보다 최근에는 문장 수준의 임베딩이 잘 된다는 것이었습니다. 문장 수준의 임베딩이 나온 이후로는 워드 기반의 임베딩에 관한 페이퍼가 거의 등장하지 않는다고 하셨습니다. 그 만큼 문장 수준의 임베딩의 결과가 훌륭하다는 것이겠습니다. 실제로 ELMo가 등장한 이후에 리서치 트렌드가 아예 문장 수준의 임베딩으로 바뀐것이 관찰되었습니다. 문장 수준의 임베딩에는 대표적으로 두 가지 알고리즘이 있는데 바로 앞서 말씀드린 ELMo와 그 유명한 BERT입니다. BERT는 transformer network이며 attention 기반으로 만들어진 것입니다. BERT는 기존의 일방향성의 문맥 흐름에서 양방향성의 문맥을 읽어냄으로써 동음이의어 분간을 가능하게 만들었습니다. BERT는 pretrain하고 fine tuning 하는 작업이 필요한데, pretrain하는 작업은 추천하지 않는다고 하였습니다. 그 이유는 이 모델을 훈련시키는데 GPU 8개로 2주의 시간이 필요하기 때문입니다. GPU리소스가 충분하다면야 가능하겠지만, 그 보다 공개된 BERT를 사용하면 좋겠습니다. 공개된 BERT 성능이 나쁘지 않기 때문입니다. 그리고 세션에서 마지막으로 강조한 부분은 임베딩 하고 그 위에 무엇을 올릴까에 관한 것이었습니다. 보통 Bi-Direction만 사용하곤 하는데, 그 위에 활용하는 부분은 자유롭게 바꿔보면서, 실험해보며 다양한 모델들을 만들어 볼 수 있다는 것으로 세션이 종료되었습니다.</p><h2 id="Clean-Code-for-ML-AI-by-한성민님"><a href="#Clean-Code-for-ML-AI-by-한성민님" class="headerlink" title="Clean Code for ML/AI by 한성민님"></a>Clean Code for ML/AI by 한성민님</h2><p>15분의 휴식시간이 끝나고 두 번째 세션에 들어갔습니다. 두 번째로 들은 것은 Naver 한성민님의 <strong>Clean Code for ML/AI</strong> 였습니다.<br><img src="/images/NAVERcloud/cleancode.jpeg" alt="한성민님의 클린코드">클린 코드에 대한 중요성을 말하기전에 설명해주신 개념은 깨진 유리창 이론에 관한 것이었습니다. 품질이 떨어지는 코드를 쌓아올리는 시작하는 순간 부터 다른 협업자의 코드 품질도 떨어지기 시작합니다. 그렇기 때문에 보이 스카웃 규칙대로 코딩을 하는 것이 중요합니다. 그 규칙은 간단합니다.</p><ul><li>떠날 때는 찾을 때보다 캠프장을 더욱 깨끗이 할 것</li><li>손을 거치게 된 코드는 원래 상태보다 더 낫게 만들고 떠나라</li></ul><p>훌륭한 도입부와 더불어 기존에 제가 짠 코드가 더럽다고 계속 생각을 했었고, Clean Code에 대해서 중요성을 항상 들어왔기 때문에 더 집중하게 되었습니다. ML/AI 관련 코드를 짤 때면 If else구문을 자주 활용했었고, indentation이 복잡해져 가독성이 많이 떨어지는 일이 빈번했습니다. 역시 한성민님이 지적한 부분도 같았습니다.<br><img src="/images/NAVERcloud/code_smell.jpeg" alt="코드 악취"><br>일명 <code>코드 악취</code>라고 불리는 더러운 코드들을 소개해주셨고, 깨끗하게 정리된 코드들을 보여주셨습니다.<br><img src="/images/NAVERcloud/cleaned_code.jpeg" alt="정리된 모습"><br>보면서 느끼는 것이 너무 많았고, 회사에 있는 코드를 빨리 정리하고 싶다는 생각이 들었습니다. 최근에 한 프로젝트의 코드들이 눈앞에 지나가면서 다시보기 싫었던 그 느낌이 생각났습니다. 이 외에도 다양한 코드 악취 Case들을 보여주시고 정리한 내용들을 보여주셨습니다. 대표적으로 너무 많은 분기는 좋지 않으므로, return이나 continue를 이용해 가드 클로져 구문을 사용하는 것을 추천해 주셨고, 주ㅜ석 남용도 코드 악취에 해당하기 때문에 함수로 정의하는 것이 좋다고 말씀해 주셨습니다. 한 함수에는 너무 많은 기능을 담지 않고, 50줄 안으로 쓰는 것이 적정하며, 동일한 작동을 너무 많이 반복하는 것을 지양해야 함을 알려주셨습니다. 그 외에 연구자와 개발자끼리 코드 컨벤션이 맞지 않는 문제, IDE로 사용하지 않는 변수들 제거, 수정이 빈번한 변수들은 전역 변수로 추출, 복잡성이 높은 로직에 대해서 함수 추출해 optimizer과 hyperparameter로 뽑아내기 등을 설명해 주셨고<br>google/gin-gonfig나 Lint와 Quality Gate와 같은 클린코드 툴을 알려주셨습니다. 개인적으로 느끼기에는 바로 써먹을 수 있고, 코드의 퀄러티가 높아질 수 있기에 너무 좋은 세션이었습니다. 클린 코드에 대해서 제대로 느끼게 만든 훌륭한 강의였습니다. </p><h2 id="BERT-in-Kaggle-by-이영수님-송원호님-이유한님"><a href="#BERT-in-Kaggle-by-이영수님-송원호님-이유한님" class="headerlink" title="BERT in Kaggle by 이영수님, 송원호님, 이유한님"></a>BERT in Kaggle by 이영수님, 송원호님, 이유한님</h2><p>세번째 세션은 듣고싶었던 캐글 세션이었습니다.<br><img src="/images/NAVERcloud/bert_in.jpeg" alt="Kaggle Korea의 갓갓갓"><br>먼저 이영수님께서 BERT의 개념에 대해서 간략하게 설명해 주셨습니다. 맨 처음에 들었던 세션인, 이기창님의 세션에서 이해가 잘 안되었던 부분이 이영수님의 세션을 통해 어느정도 해결이 되었습니다.<br><img src="/images/NAVERcloud/bert_transformer.jpeg" alt="이영수님이 설명해주시는 BERT"><br>설명이 어느정도 끝나고 다음 바통을 잡은 분은 송원호님이었습니다. 송원호님은 Toxic 대회를 진행하면서 BERT를 통해 스코어를 올렸었던 경험에 대해서 설명해 주셨습니다. Toxic대회는 2017년에 한번 진행했었던 대회인데, 어떤 코멘트에 대해서 이 코멘트가 정상인지 악성인지 분류하는 문제입니다. 2019년에 다시 열리기 된 것은 바로 Unintended Bias가 추가되었기 때문입니다. 특정 키워드가 들어가게 되면 악성으로 분류되어버리는 문제 때문에, 이것을 더 정밀하게 분류해 내는 모델이 필요했고, 더 정밀해진 Metric을 만족시켜야 하는 대회였습니다. 그렇기 때문에 Metric에 대한 설명이 쭉 진행되었고 Metric에 맞는, 기존 loss와 다른 loss function을 재 정의했어야 했던 점을 설명해 주셨습니다. 결구 새로 열린 이 대회에서 중요했던 것은 <code>문맥</code>이었습니다. 문맥을 잘 살려서 코멘트를 분류해야 했었기 때문에 BERT로 접근을 했고 그 결과가 Baseline을 사용할 때에 비해서 좋아질 수 있었습니다. Leaderboard 상으로는 10등으로 in-money권에 있었지만 결과는 26등이었습니다. 쉐이크업으로 인해 등수가 떨어지게 되었는데, 캐글의 discussion에서 그 이유가 등장했습니다. 원호님과 비슷하게 접근을 했지만 높은 등수를 기록했던 분과의 차이점은 모델에서 BERT large, small, medium을 다 사용했고 GPT 2 small도 활용하며, XLNet까지 사용했다는 점이었습니다. 크고 다양한 언어 모델을 사용했고 속도 문제에 있어서는 Sequence Bucketing을 통해 해결한 점이 큰 차이점이었습니다. 결론은 다음과 같았습니다. </p><ul><li>NLP 에서 좋은 머신은 꼭 필요하다.</li><li>작은모델 큰 모델 다 중요하다.</li><li>Evaluation에 맞는 loss를 잘 정의해야한다.</li><li>파이프라인을 일찍 구성하고 실험을 많이 해봐야 한다.</li><li>제한된 시간하에 높은 점수를 위해 속도 줄이기 위한 여러 기술이 필요하다.</li></ul><p>그 다음으로는 생일을 맞으신 이유한님이 발표해주셨습니다.<br><img src="/images/NAVERcloud/birthday.jpeg" alt="발표자이자 생일자 이유한님"><br>이유한님게서는 분자대회에서 BERT를 사용한 경험에 대해서 전달해주셨습니다. 먼저 과학에서 머신러닝이 어떻게 사용되는지에 대해 간략히 설명해 주셨고 과학과 머신러닝의 유사점에 대해서 정리해주셨습니다. 본격적으로 분자대회에 대해서 설명을 해주셨는데, 정확하게는 이해하지 못했고 아무튼 양자 계산으로 너무 힘든 분자에 관한 어떤 예측 문제를 딥러닝을 이용해서 풀어보는 대회였습니다. 보통 분자는 그래프 네트워크와 유사한 부분이 있습니다. 그래서 유한님도 처음에 GNN을 통해 접근을 했었다고 합니다. 하지만 분자를 sentence로 풀어서 BERT에 넣어봤더니 성적이 갑자기 뛰기 시작했습니다. 그래프 네트워크보다 오히려 더 좋은 성적이 나오기 시작한 것입니다. 분자의 정보를 문장화 시켰고, 각 정보를 임베딩하고 벡터화했습니다. 그리고 데이터에 기반해 자발적으로 학습된 임베딩을 뽑아냈습니다. 결국 임베딩도 학습된 것이고 임베딩된 결과도 학습되었습니다. end to end 모델로 구성한 것이고 결국 딥러닝 모델이 알아서 다 하게 되는 모델을 만들어 낸 것입니다. 여기에 Auxiliary targets을 이용해서 5개의 타겟으로 학습시켰고, 세밀한 정보가 도출되어 이것을 이용해서 점수를 끌어내었다고 설명해 주셨습니다. 여기까지 대회에 대해서 정리해 주셨고, 캐글 코리아 운영자답게 캐글 대회에서 높은 점수를 받는 방법에 대해서 알려주셨습니다. </p><h4 id="캐글에서-높은-점수를-얻으려면"><a href="#캐글에서-높은-점수를-얻으려면" class="headerlink" title="캐글에서 높은 점수를 얻으려면?"></a>캐글에서 높은 점수를 얻으려면?</h4><pre><code>- Diversity를 살려서 Ensemble하자- 다양한 모델의 사용- 다양한 시드, 아키텍쳐 같아도 시드가 다르면 다른 모델- Pseudo Labeling, 예측한 값을 타겟으로 한 테스트 셋으로 새로 학습을 진행한다.(대회니까 가능함)- 뭘 쓸까 항상 생각해야 한다. 학습 방식을 선택해야 하는데, EDA를 통해서 특성을 파악하고 모델을 선택해야 한다.- Loss functionn을 잘 찾아야 함,    - BCE, MAE, MSE, CORAL, DICE 등, 다 해보고 좋은 것을 선택한다- 학습이 잘 될 수 있는 학습 스케쥴링</code></pre><h2 id="구글-번역이-크라우드소스로-어떻게-언어를-배워요-by-Anh-Tuan-Nguyen"><a href="#구글-번역이-크라우드소스로-어떻게-언어를-배워요-by-Anh-Tuan-Nguyen" class="headerlink" title="구글 번역이 크라우드소스로 어떻게 언어를 배워요? by Anh Tuan Nguyen"></a>구글 번역이 크라우드소스로 어떻게 언어를 배워요? by Anh Tuan Nguyen</h2><p>마지막으로는 구글 브레인의 안 투앙님의 발표였습니다. 프랑스분이셨는데, 한국어로 발표해주신 점이 인상깊었습니다.<br><img src="/images/NAVERcloud/crowd.jpeg" alt="Crowd소스이다, Cloud가 아니다"><br>발표는 간단했습니다. 머신러닝에 대해서 설명하고, 학습과 추론에 대해서 짚었습니다. 지도학습에 대해서만 말씀해주셨고 이 모델을 활용하는 것에서는 Google의 photo에서 텍스트를 감지하는 것, 그리고 동물 감지, 예를 들어 강아지를 포토에서 검색하면 찍은 사진중에 강아지가 감지된 사진이 선택되는 것을 말씀해 주셨습니다. 그리고 번역에 대해서 말씀을 하기 시작하셨는데, 가끔 번역이 잘 안되는 내용이 있다고 하셨습니다.<br><img src="/images/NAVERcloud/gee.jpeg" alt="번역이 잘 안되는 한국말"><br><img src="/images/NAVERcloud/jmt.jpeg" alt="JMT"></p><p>위의 사진들이 그 예를 찍은 것입니다. 고치는 방법은 여러가지가 있겠습니다만, 많은 데이터를 통해서 해결해 나갈 수 있습니다. Google Crowdsource라는 앱을 통해서 가능하다는 것입니다. 일종의 게임을 만들어서 사용자가 해석에 대한 맞는 라벨을 골라주는 것입니다. 이 게임을 통해서 뱃지를 얻을 수 있고 레벨을 올릴 수 있다고 설명해 주셨습니다. 발표시간이 조금 남아서 질문 답변 시간을 길게 가졌고 이 마지막 세션을 끝으로 GDG DEVFEST행사가 종료되었습니다. </p><p>처음 가본 GDG행사였는데, 알고 싶었던 BERT모듈에 대해서 일부 알게 되었고, Kaggle에 대한 경험담과 피와 살이 될 것 같은 Clean code에 대해서 제대로 알 수 있게 되어 좋았습니다. 일찍 예매해서 만원정도에 티켓을 구매할 수 있었는데, 값이 아깝지 않았습니다.(물론 GDG에 참여한 회사들의 굿즈들을 많이 받고 간식도 많이 먹긴 했습니다.) 다음에 또 이런 행사가 있으면, 꼭 참여하고 싶다는 생각이 들었고 언젠가는 저기서 발표하고 싶다는 작은 목표를 기록해두면서 글을 마무리합니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/10/20/GDG-DEV-FEST/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Time Seires, 시계열 분석 세번째</title>
      <link>http://tkdguq05.github.io/2019/09/20/Time-Series-03/</link>
      <guid>http://tkdguq05.github.io/2019/09/20/Time-Series-03/</guid>
      <pubDate>Fri, 20 Sep 2019 02:08:36 GMT</pubDate>
      <description>
      
        &lt;p&gt;시계열 데이터 분석과 기계학습의 차이점, 회귀분석과의 차이점&lt;br&gt;
      
      </description>
      
      <content:encoded><![CDATA[<p>시계열 데이터 분석과 기계학습의 차이점, 회귀분석과의 차이점<br><a id="more"></a></p><h1 id="Dive-into-시계열-데이터-분석"><a href="#Dive-into-시계열-데이터-분석" class="headerlink" title="Dive into 시계열 데이터 분석"></a>Dive into 시계열 데이터 분석</h1><h2 id="시계열-데이터-분석에-대해서-공부해보자-03"><a href="#시계열-데이터-분석에-대해서-공부해보자-03" class="headerlink" title="시계열 데이터 분석에 대해서 공부해보자 03"></a>시계열 데이터 분석에 대해서 공부해보자 03</h2><p>시계열 데이터 분석과 기계학습의 차이점에 대해서 본격적으로 들어가보도록 하겠습니다. 시계열 알고리즘에는 기계학습과는 다른, 2가지 차별화 방향이 있습니다.</p><p>첫번째는 ‘과거데이터로 미래 데이터 뽑아낼 수 있는가’ 로 시간 축 기반의 예측이 가능하다는 것이고, 두번째는 시계열 알고리즘은 점추정이 아닌 구간추정 알고리즘으로 설명력 효과에 뿌리를 두었다는 것입니다. 대부분의 기계학습 모델은 통계분포에 기반하지 않끼 때문에 점추정 알고리즘이고, 시계열 알고리즘은 구간추정을 하기 때문에 점추정 보다 더 다양한 해석이 가능합니다.</p><h4 id="특별한-모델들"><a href="#특별한-모델들" class="headerlink" title="(특별한 모델들)"></a>(특별한 모델들)</h4><ul><li>Dynamic Modeling : y가 여러개인 모델입니다. y에 대한 영향을 비교하고 싶다면 y를 두 개로 둬서 모델을 두개 만들고 실험하는 것을 생각해 볼 수 있습니다.</li><li>XAI, LIME : 최근에 등장한 개념으로, 설명가능한 Aritificial Intelligence, 설명가능한 딥러닝에 대한 것입니다. Random Forest의 feature importance와 비슷하게 측정이 가능하다고 합니다.</li></ul><h3 id="시계열-데이터-분석-준비"><a href="#시계열-데이터-분석-준비" class="headerlink" title="시계열 데이터 분석 준비"></a>시계열 데이터 분석 준비</h3><p>데이터 분석 준비는 기존 머신러닝 데이터를 준비하는 것과 비슷하지만, 시계열 데이터만의 특징이 있기 때문에, 머신러닝 데이터 준비와 다른 몇 가지 특징들이 존재합니다.<br><img src="/images/cv_step.png" alt="1step and 2step"></p><ul><li>가장 옛날 것을 훈련데이터로 사용하고, 그 다음 것을 Validation 데이터, 가장 최근 것을 test데이터로 사용합니다.</li><li>기간을 두고 훈련셋을 만듭니다. 시간축을 잘 보존해야합니다.</li><li>훈련세트에서 하나 건너서 Validation set을 만들면 ⇒ 1스텝 교차검사</li><li>훈련세트에서 두 개 건너서 Validation set을 만들면 ⇒ 2스텝 교차검사</li><li>모델이 월 마다의 예측력이 다를 수 있기 때문에, 월별 모델을 만들기도 한다. (실제로 이렇게 모델을 만드는 경우가 많음)</li></ul><h3 id="검증지표와-잔차진단"><a href="#검증지표와-잔차진단" class="headerlink" title="검증지표와 잔차진단"></a>검증지표와 잔차진단</h3><p>분석하고 예측만 잘하면 될까요? 큰 착각일 수 있습니다. 중요한 것은 예측이 잘 되었는지 평가하는 것 그리고 데이터의 시간패턴이 잘 추출되었는지 확인하는 작업입니다.</p><p>검증지표는 예측값과 실제값이 얼마나 유사한지 측정하는 것이고, 잔차진단은 데이터의 시간패턴을 잘 뽑아내었는지 알아보는 작업입니다. </p><p><strong>검증지표</strong> 에는 흔히 사용되고 눈에도 익숙한 MSE가 대표적입니다. 그 외에 MAE, MAPE, MAPE, MPE(y가 %로 나올 때) 등 다양한 검증지표가 사용되며, 왠만하면 다 사용해보고 수치가 안정될 때까지 모델을 만드는 것이 좋습니다. 또한 기존의 검증지표가 맘에 들지 않는다면, 분석가나 데이터 사이언티스트가 직접 고안해서 검증지표를 사용해도 됩니다.</p><p>훌륭한 데이터 사이언티스트라면 검증지표를 직접 만들어서 사용해야하는 경우가 많을 것입니다. 왜냐하면 검증지표만 가지고 사용하면 결과에 대한 해석을 다르게 할 수 있기 때문입니다. 예를들어 MSE의 경우는 오차에 대한 가중치를 확 올려버리는 검증지표입니다. 오차에 대해서 제곱을 하기 때문에 가중치가 급격하게 증가하게 됩니다. 분석가가 보기에 이 가중치가 너무 과하다고 생각되면 절대값을 사용할 수 있습니다. 이렇게 되면 MAE를 사용하는 것이 되겠습니다.<br>$$MSE = {1\over n}\sum(y-\hat{y})^2$$<br>$$MAE = {1\over n}\sum|y - \hat{y}|$$</p><p><strong>잔차진단</strong> 는 말 그대로 잔차, $$y-f(x)$$ 에 대한 값을 보고 진단하는 것을 말합니다. 회귀분석을 해보신 분들을 아시겠지만, 잔차는 정의가 존재합니다.</p><h3 id="잔차의-정의"><a href="#잔차의-정의" class="headerlink" title="잔차의 정의"></a>잔차의 정의</h3><ol><li>잔차들은 정규분포이고, 평균 0과 일정한 분산을 가져야 한다.</li><li>잔차들이 시간의 흐름에 따라 상관성이 없어야 한다.<ul><li>자기상관 함수를 통해 Autocorrelation이 0인지 확인</li><li>공분산</li><li>자기상관함수</li><li>편자기상관함수</li></ul></li></ol><p>자기상관 정도를 확인하기 위해서는 일반적으로 랜덤으로 epsilon을 두 개 뽑아서 자기상관성이 있는지 확인합니다. 자기상관성 뿐 아니라, 잔차가 잘 뽑혔는지 확인하기 위해서는 시각화를 통해서 확인하고, 자기상관성이나 평균, 분산과 같이 통계량을 계산해서 확인하는, 두 가지 방법을 모두 사용해야 합니다.</p><p>잔차를 보고 더 뽑아내야 할 것이 있는지, 분석이 잘되고 있는지 안되고 있는지를 확인해 나가야 합니다. </p><p><img src="/images/autocor.png" alt="White Noise"><br>예를 하나 들어서 보겠습니다. 여기에 다양한 모델들이 있네요. Autocorrelation에 대해서 가정을 설정했고, 가정에 대한 결과표가 나왔습니다. 대중 주장은 모델의 잔차가 White Noise라는 것이고, 내 주장은 모델의 잔차가 White Noise가 아니라는 것입니다. 모델을 쭉 보니, SARIMA 모델은 p-value값이 굉장히 높고 나머지는 0에 가깝습니다. 이것을 어떻게 해석하면 될까요? p-value가 크다는 것은 대중 주장이 맞다는 것입니다. 대중 주장이 맞으니까 SARIMA의 잔차는 White Noise이고 모델이 잘 만들어졌다고 추측해 볼 수 있습니다. 나머지 모델들은 영 꽝이네요. 이런 식으로 모델에 대한 잔차를 검증해 나가면 됩니다.</p><h2 id="시간영역-선택하기"><a href="#시간영역-선택하기" class="headerlink" title="시간영역 선택하기"></a>시간영역 선택하기</h2><p>시계열 분석이 머신러닝 분석 방법과 다른 것은 <code>시간 축</code>입니다. 이 시간 축을 어떻게 두느냐에 따라 분석 결과가 급격하게 달라집니다. 따라서 시계열이 분석효과에 도움이 될 시간영역(해상도)를 선택해야 합니다. 일종의 window size를 정한다고 생각하면 이해가 잘 되실 겁니다. 시간축을 년 단위로 할지, 월로 할지, 일주일로 할지는 사실 다 해보는 수 밖에 없습니다. 알 수가 없기 때문입니다. 그래서 다 해보고 잘되는 시간 영역을 선택해야 합니다. 물론 선택하는 기준은 있습니다. 바로 통계량과 잔차를 기준으로 잘 나오는 시간축을 선택하는 것입니다. 분석은 항상 이 방법으로 진행이 됩니다. <strong>통계량과 잔차!</strong></p><h2 id="회귀분석-요약-시계열-분석-요약"><a href="#회귀분석-요약-시계열-분석-요약" class="headerlink" title="회귀분석 요약 / 시계열 분석 요약"></a>회귀분석 요약 / 시계열 분석 요약</h2><p>계수 추정 방법은 두 가지 방법이 있습니다. 수학자(수식)방법과 통계학자(확률)의 방법입니다. 수학자의 방식은 결정론적 모형입니다. 잔차벡터를 구하고, 잔차 제곱합을 구한 후, 그레디언트를 계산합니다. 그 이후에 미분을 하여 최적점을 구하고 추정된 계수를 얻는 방법입니다. 이 때, $X^T X$ 행렬은 역행렬이 존재해야 합니다. $X$ 가 full rank가 아니면 계산이 되지 않겠습니다. 결국 $X^TX$ 행렬은 Positive Definite이 아니면 계산 되지 않습니다.</p><p>확률론적 모형은 다음과 같습니다. Main Equation에서 $X$ 가 있을 때의 $Y$ 의 기대값과 분산을 구합니다. 이를 통해 분포를 추정할 수 있게됩니다. 구한 평균과 분산을 가지고, $Y$ 값 각각의 확률을 구합니다. 그 이후에는 $Y$ 의 발생가능성에 대해서 Likelihood를 구합니다. Maximum Likelihood Estimation을 하려는 것입니다.<br><img src="/images/likelihood.png" alt="Maximum Likelihood Estimation"><br>Log를 취해 계산을 편리하도록 만들어주고 미분으로 0값이 되는 지점을 구합니다. 통계학자의 방식은 이렇게 분포에 대한 계산을 해놓는 다는 점입니다. 이런 식으로 계산이 되면 구간추정도 가능해지게 됩니다. $beta$ (coefficient)값에 대한 분산은 $Covariance$ 를 통해서 구합니다.</p><p>역행렬이 없을 수록 에러값이 증가하게 되고 이것은 구간이 넓어지는 것을 의미하게 됩니다. 결국 넓어진 구간을 좁히는 것이 목표가 되겠습니다. beta값의 분포에 대해서는  $t$ 분포 따르는 게 증명이 되었기 때문에 $t$를 사용하면 됩니다. $beta$ 가 $t$ 에서 나온다는 성질은 가설검정을 할 때 이용됩니다. 추가로 딥러닝에 대해서 덧붙이자면, Neural Network는 신뢰구간의 범위가 너무 넓습니다. 이론적 근거가 없기 때문입니다. 검증을 할 때는 P-value를 보고 값이 높은 것은 coefficient가 몇이든 다 0으로 보면 됩니다. $R^2$ 값이랑은 별개의 문제입니다.</p><hr><p>시계열 데이터 분석은 분석하고 검증하고 모델링하는 것도 중요하지만, 데이터에 따라 높은 정확도나 높은 에러를 가지게 됩니다. 시계열 분석은 단기적인 상황에서는 성능이 좋지만 중 장기 예측에 대해서는 잘 맞지 않는 다는 단점이 존재합니다. 물론 변화가 별로 없고 일반적인 패턴을 지닌다면 잘 맞추겠지만 변화가 굉장히 극심하다면 잘 맞추기 못 하게 됩니다. 애초에 데이터를 정렬할때도 시간축을 잘 살려서 정제를 해야하니 데이터 정렬 자체, 데이터 준비하기도 굉장히 어려운 작업이라고 할 수 있겠습니다.</p>]]></content:encoded>
      
      <comments>http://tkdguq05.github.io/2019/09/20/Time-Series-03/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
