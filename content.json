{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"Ensemble_Model","text":"Ensemble에 대해 자세히 알아보자 (Bagging, Bootstrap, 그리고 RandomForest)앙상블 모델에 대해서 공부하기 전에, 그 배경부터 알아볼 필요가 있다. NFL (No Free Lunch)No Free Lunch 이론은 David H. Wolpert가 정리한 이론으로 모든 문제에 대해 다른 모든 알고리즘을 능가하는 모델은 없다는 이론이다. ‘어떤 특정 정책에 의해 얼핏 보면 이득을 얻는 것 같지만, 그것은 한 측면의 이득일 뿐이고 반드시 이면에 다른 측면이 있고 그 측면에서 손해가 발생한다.’는 것이 핵심이다. 이 이론에 따라서 혼성모델의 필요성이 대두되었다. 혼성모델이란 여러 알고리즘을 결합하는 모델이다. 이 모델은 특정 문제가 주어진 상황에서 그 문제를 가장 높은 성능으로 풀 수 있는 알고리즘에 대한 필요성에 의해서 제시되었다. Resampling리샘플링은 데이터가 부족할 때 같은 샘플을 여러번 사용하는 것을 말한다. 성능 통계치의 신뢰도를 높이기 위해 사용된다. Resample을 하는 이유는 다음과 같다. 실제 상황에서는 만족할 만한 큰 샘플을 얻기가 힘들다. Bias-Variance Trade off를 통해 큰 샘플이 중요하다는 것을 알 수 있다. sample의 집합이 커지면 variance가 감소한다!, MSE도 감소한다! 모델의 선택은 별도의 검증이 필요하다.(검증용 데이터, 큰 샘플의 필요성) Bootstrap, Statistical term for “roll n-face dice n times”부트스트랩은 Resampling을 이용하여, 분류기의 성능을 측정하는 방법 중 하나이다. 통계에서는 추정치에 대한 검증용(가설 검증)으로 많이 사용된다. 부트스트랩의 장점은 한번도 뽑히지 않은 데이터가 발생한다는 것이다. 이를 통해 데이터를 아낄 수 있게 된다. Ensemble앙상블 모델은 혼성모델 중 하나이다. 앙상블은 두가지 방식이 존재한다. 같은 문제에 대해 서로 다른 여러 알고리즘이 해를 구하고, 결합 알고리즘이 그들을 결합하여 최종 해를 만드는 방식 문제와 유사한 여러 하위 문제들에 대해 하나의 알고리즘이 해를 구하고, 결합 알고리즘이 그들을 결합하여 최종 해를 만드는 방식 앙상블의 동기는 단순히 통계적, 수학적일 뿐만 아니라, 사람들의 심리 등 여러부분을 관통하는 내용이기도 하다.12어느 도시에서는 소를 광장에 매어 놓고 참가자들에게 체중을 추정하여 적어 내게 하고 실제 체중에 가장 가까운 사람에게상품을 주는 대회가 있다고 한다. 수백 명이 참가하는데 그들이 적어낸 숫자들을 평균해 보면 답과 아주 근사하다고 한다. 사람들은 중요한 결정을 할때 여러 사람의 의견을 들어보고 결정하려는 경향이 있고, 이런 경향은, 통계학이 아닌 다른 분야에서도 사용되는 개념이다. 다양성앙상블 모델의 핵심은 다양성이다. 앙상블에 참여한 모델이 모두 같은 결과를 낸다면, 그것은 앙상블 모델로써 어떠한 장점도 갖고 있지 않다. 한 분류기가 틀리는 어떤 문제를, 다른 분류기에서는 맞출 수 있어야 앙상블 모델로써 가치가 있을 것이다. 앙상블 분류기 시스템은 앙상블 생성, 앙상블 선택, 앙상블 결합의 단계를 거친다. 앙상블 생성 Resample을 이용해서 (Bagging, Bootstrap) 샘플 집합들을 생성하고, 분류기를 훈련한다. Feature Vector의 subspace를 이용해서 샘플 부분 집합을 생성하고 분류기를 훈련한다. 앙상블의 분류기는 요소분류기와 기초분류기로 구분된다. 앙상블 결합요소 분류기(기초 학습기)들의 출력을 결합하여 하나의 분류 결과를 만드는 과정이다.요소 분류기의 출력은 세가지의 방식으로 나뉜다. Class Label Majority Vote : class 라벨이 많이 나온 쪽으로 분류한다. Weighted Majority Vote : 성능 좋은 분류기에 가중치를 부여한다.(Adaboost) Behavior knowledge space(BKS/행위지식공간) : 경험한 케이스를 테이블로 갖고 분류기 결과를 보고 경험적으로 결정한다(테이블에서 찾아서). 다수결 방법의 성능을 고도화 할때 사용됨 Class Ranking Borda 계수 Class Probability Softmax Bagging (Bootstrap + Aggregating)부트스트랩을 다중 분류기 생성 기법으로 확장한 것이다. 부트스트랩 된 샘플 집합에서 훈련을 하고, 입력 값에 대해 분류기들의 평균값이나, 다수결 투표를 취한다. 샘플링은 복원추출하는 방식으로 하고, 훈련된 분류기의 결과를 모두 종합하기 때문에 Bagging이라고 부른다. 반복적인 복원 추출 (Bootstrap) 결과를 모두 종합 (Aggregation) Bagging, 배깅은 언제 사용할까?배깅은 편향이 작고 분산이 높은 모델에 사용하면 효과적이다. 트리 분류기와 같이 불안정성을 보이는 분류기에 큰 효과를 발휘 훈련 집합이 달라지면 차이가 큰 트리가 생성 ⇒ 다양성 확보 Bias를 변화시키지 않고 variance를 감소시킨다.(Bias를 쪼오오오오끔 희생한다.) 배깅은 분산을 감소시키기 위해, 훈련데이터에서 많은 샘플링을 하고(Bootstrap), 샘플들로 별도의 Decision Tree를 구성한 후, 회귀나 분류문제를 푸는데 사용된다. 회귀는 분류기 결과의 평균값을 사용하고, 분류는 최빈값을 취한다. 배깅은 이미 저분산 모델인 경우 별로 효과가 없다. Bias-Variance Tradeoff 를조금만 생각해보자. 분산이 이미 줄어있는 상태에서는 더 줄일 분산이 없다. 배깅은 오직 분산을 줄이는 데 효과적이다. Out-of-Bag (OOB) Error Estimation샘플에 대해 Bootstrap을 하게 되면 부트스트랩 샘플은 전체 훈련데이터의 약 63.2%를 차지하게 된다.(왜 그러한가에 답은 $\\lim_{n\\to\\infty} (1-{1\\over n})^n$을 풀면 답이 나온다. $1\\over e$로 0.378이 나온다. 자세한 내용은 링크를 참조하면 된다. [https://www.quora.com/Why-is-the-limit-1-frac-1-n-n-equal-to-frac-1-e]) 부트스트랩되지 않은 샘플들은 한번도 사용되지 않은 샘플들로 검증데이터에 활용할 수 있다. 이런 training observations은 out-of-bag observations이라고 불린다. OOB estimate of test error 부트스트랩 샘플을 이용하여 개별 학습기를 학습한 후, OOB에 속하는 샘플들에 대한 예측값을 모두 구한다. OOB의 실제 라벨값과 OOB의 예측값을 이용하여 OOB error를 구한다. 모든 부트스트랩 샘플 sets에 대하여 위의 과정을 반복하면, 샘플 sets 수 만큼의, errors를 모을 수 있다. OOB errors의 평균값을 이용하여 bagging 모델의 최종 테스트 error를 계산한다. Weakness of Bagging배깅은 엄청나게 효과적인 것처럼 보이지만 약점이 존재한다. 배깅은 feature를 모두 사용하고, row를 랜덤하게 선택하는 것이다. Decision Tree를 만든다고 해보자, 만약 영향력이 높은, Information Gain이 높은 모델을 사용한다고 했을때, 특정 Feature만 계속 선택되서 트리가 만들어질 가능성이 있다. 즉, 중요한 칼럼들이 트리의 초기 분기때 모든 표본에 그대로 존재하게 된다. 이렇게 되면 만들어진 대다수의 트리들의 결과가 비슷해진다. 이것이 반복되면 트리간의 상관관계가 발생해서 분산 감소의 효과가 줄어들게 된다.(배깅의 약점은 IID condition이다. IID 조건을 만족하는 경우 분산은 $Var={\\sigma^2\\over n}$이 되지만, IID를 만족하지 못하는 경우, 상관관계가 발생하여 $Corr = p$이라고 할때, $Var = p\\sigma^2$가 된다.) 그래서 혁신적인 아이디어와 함께 등장하게 된 것이 Random Forest이다. Random Forest랜덤 포레스트는 일반적으로 bagging 방법(또는 pasting)을 적용한 결정 트리의 앙상블이다.랜덤 포레스트 알고리즘은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 무작위로 선택한 특성 후보 중에서 최적의 특성을 찾는 식으로 무작위성을 더 주입한다. 트리를 더욱 다양하게 생성하고 (트리의 의존성을 낮추고, 다양성을 증가) 편향을 손해 보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 생성한다. Random Forest는 쉽게 말해 Tree 모델에 Bagging과 Subsampling기법을 사용한 모델이다.훈련 데이터에서 bootstrap 샘플을 뽑아내고, 노드 분기 시, 모든 Feature가 아니라, 일정 Feature만 사용하는 것이 특징이다. 이를 통해 Tree간의 Correlation을 줄이고, 분산을 감소시킬 수 있다. Subspace Sampling샘플링 시에는 일반적으로 전체 변수가 p라고 할 때, $m = \\sqrt{p}$를 사용한다.(m = p이면 Bagging이다. 또한 회귀에서는 경험적으로 $m ={p\\over3}$를 사용한다.) Random Forest 모델의 장단점?장점 : 굉장히 간편하다. 스케일링도 필요없고 파라미터 튜닝을 많이 안해도 성능이 뛰어나다. 의사결정의 트리의와 배깅의 단점은 극복하고 장점만을 가져온 것이라고 할 수 있다. 단점 : 차원이 높고 매우 희소한 데이터에서는 잘 작동하지 않는다. 이런 희소한 데이터에는 선형 모델이 더 적합할 수 있다.","link":"/2019/05/05/Ensemble-Model/"},{"title":"Multi Armed Bandit 알고리즘?","text":"이 글의 주 소스 링크를 먼저 밝힙니다. 원작자에게 먼저 허락을 구하고 글을 작성했습니다.https://www.kaggle.com/ruslankl/how-to-deal-with-multi-armed-bandit-problem Multi Armed Bandit(MAB) 란?마케팅이든 아니면 의학적인 실험에서든 사용자에게 어떤 게 가장 좋은 것 인지 확인하는 방법은 무엇일까요?바로 Multi Armed Bandit Algorithm입니다. 특히 Thompson Sampling이라는 기법과 같이 사용된다면 굉장히 효과적으로 가장 좋은 선택이 무엇인지 알아낼 수 있습니다.(실제로 추천 알고리즘의 Cold Start 문제 등에 효과적으로 적용되고 있는 알고리즘 중 하나입니다.) 마케팅 캠페인을 한다고 합시다. 마케팅 캠페인에서는 보통 CTR(Click Through Rate)을 이용해서 광고가 효과적인지 판단하곤 합니다.(물론 마케팅 회사마다 케이스 바이 케이스이긴 합니다만, 일단 CTR이라고 가정하고 넘어가 봅시다) CTR 예시, 어떤 광고가 100번 노출되고 유저가 10번 클릭을 한다면, 이 광고의 CTR은 10/100으로 0.1입니다. 이야기가 나온김에 Regret도 같이 설명하자면, Regret은 가능한 CTR중 최고의 CTR과 지금 있는 CTR을 빼준 값입니다. 광고 A의 CTR이 0.1이고 B가 0.3이라고 할 때, A를 보여줬을 때 Regret은 $0.3 - 0.1 = 0.2$가 됩니다. 이제 광고에 대한 여러 안들이 있고, 어떤 광고가 가장 효과적인지 확인하려고 합니다. 하지만 광고에 대해서 어떤 사전 정보도 없다면 어떨까요?, 어떻게 여러 대안중에 효과적인 광고를 골라낼 수 있을까요? 이럴 때는 보통 사용하는 방법이 A/B test입니다. A/B 테스트는 말 그대로 A안과 B안을 노출시켜서(노출 비율은 정할 수 있다) 두 집단의 각각 다른 효과를 확인하기 위해서 사용되는 방법입니다. (wiki 설명 : A/B 테스트는 변수 A에 비해 대상이 변수 B에 대해 보이는 응답을 테스트하고, 두 변수 중 어떤 것이 더 효과적인지를 판단함으로써 단일 변수에 대한 두 가지 버전을 비교하는 방법이다, https://ko.wikipedia.org/wiki/A/B_%ED%85%8C%EC%8A%A4%ED%8A%B8) ‘아 그럼 A/B 테스트 하고 좋은 거 그냥 뽑으면 되겠네!’라고 생각할 수 있겠지만, 회사에서 이 테스팅을 진행한다고 생각해 봅시다. 주의할 점이 있습니다. A안을 기존에 하던 광고라고 하고 B를 실험하는 광고라고 해봅시다. A안 광고를 통해서는 꾸준히 매출을 기록하고 있고, B안은 아직 확실하지 않습니다. B가 아마 효과적이라고 하는데 아직 의심스럽습니다. 만약 테스팅을 하는데 B의 효과가 너무 떨어진다면 어떨까요? 기존 광고 효과의 목표치에 도달하지 못한다. 매출이 떨어진다. 고객이 실망하고 이탈한다. 이런 상황이 가능하지 않을까요? 그래서 MAB에서 중요한 것은, Exploration과 Exploitaion입니다. 한국어로 쉽게 말하면, 탐색하기와 뽑아먹기 입니다. 쉽게 탐색과 이용이라고 하겠습니다. Exploration은 탐색하는 것입니다. 새로운 안에 대해서 계속 테스트하고 실험해 보는 것입니다.Exploitation은 이용하는 것입니다. 즉, 기존에 효과적이었던 광고를 계속 하는 것입니다.결국 A/B테스트이든, MAB이든 중요한 것은, 이 비율을 적절하게 맞춰서 탐색을 간결하게 하고 최대한 효과적으로 이용할 수 있는 대안을 선정하는 것입니다. MAB, 즉 Multi Armed Bandit 알고리즘은 여러 대안들(슬롯머신의 Arm에서 이름을 따왔습니다)을 자동으로 실험하고 최적의 광고를 탐색과 이용사이에서 균형을 잡으면서 빠르게 찾는데 좋은 알고리즘입니다. Multi Armed Bandit 알고리즘들은 몇 가지 종류가 있습니다만 거의 모든 알고리즘은 위에서 소개한 Regret을 줄이는 것을 목표로 하고 있습니다. 주요 알고리즘들은 다음과 같습니다. Random Selection Epsilon Greedy Thompson Sampling Upper Confidence Bound (UCB1) 이 알고리즘들을 가지고 실험을 하기 전에 CTR을 사전에 설정해 둘 필요가 있습니다. 설정해둔 CTR로 광고가 주어졌을 때 클릭에 대한 시뮬레이션을 진행할 수 있습니다. 먼저 CTR을 비현실적이지만 0.45와 0.65로 설정하겠습니다. 1ACTUAL_CTR = [.45, .65] 1. Random SelectionRandom Selection은 말그대로 탐색을 하지 않고 동전 튕기기를 이용해서 앞면이면 광고0, 뒷면이면 광고1을 보여주는 알고리즘입니다. 정말 간단합니다! 123456789101112131415161718192021222324252627282930313233343536373839n=1000regret = 0total_reward = 0regret_list = []ctr = {0: [], 1:[]} #lists for collecting the calculated CTRindex_list = [] # lists for collecting the number of randomly choosen Ad#initial values for impressons and clicksimpressions = [0,0]clicks = [0,0]for i in range(n): random_index = np.random.randint(0,2,1)[0] # randomly choose the value between [0,1] index_list.append(random_index) impressions[random_index] += 1 did_click = bernoulli.rvs(actual_ctr[random_index]) if did_click: clicks[random_index] += did_click if impressions[0] == 0 : ctr_0 = 0 else : ctr_0 = clicks[0]/impressions[0] if impressions[1] == 0: ctr_1 = 0 else : ctr_1 = clicks[1]/impressions[1] ctr[0].append(ctr_0) ctr[1].append(ctr_1) ## calculate the regret and reward regret += max(actual_ctr) - actual_ctr[random_index] regret_list.append(regret) total_reward += did_click 123Ad #0 has been shown 48.4 % of the time.Ad #1 has been shown 51.6 % of the time.Total Reward (Number of Clicks): 546 CTR이야 0.65, 0.45를 잘 찾아간다지만, 중요한 것은 Regret입니다. Regret함수를 보면 함수값이 거의 100대에 육박하는 것을 볼 수 있습니다. 좀 더 좋은 알고리즘을 통해서 Regret을 낮출 필요가 있겠습니다. 마케팅 예산이 무한대라면 그냥 마구잡이로 보여주고 CTR을 측정해서, 높은 CTR을 보이는 광고안을 선정하면 그만입니다. 하지만 일개 사원인 우리들은 예산을 최대한 아껴서 좋은 효율적인 광고를 통해 매출을 극대화 해야하는 사람들입니다. 그렇다면 좀 더 좋은 알고리즘을 살펴보겠습니다. 2. Epsilon GreedyEpsilon Greedy 알고리즘은 Random Selection에서 한 단계 업그레이드 된 모델입니다.이 알고리즘은 탐색과 이용의 비율을 어느정도 조정한다는 것이 큰 특징입니다. ~15%까지는 Exploration ~85%까지 Exploitation 로직은 다음과 같습니다. 초기 몇번 까지는 Exploration(초기 값이 중요!) 각 Exploration마다 최고 점수를 받는 variant 고르기 Epsilon 설정 (1-E)%의 winning variant를 고르고 다른 옵션에는 E%를 설정한다. 1234567891011121314151617181920e = 0.05n_init = 100impressions = [0,0]clicks = [0,0]for i in range(n_init): random_index = np.random.randint(0,2,1)[0] impressions[random_index] += 1 did_click = bernoulli.rvs(actual_ctr[random_index]) if did_click: clicks[random_index] += did_click ctr_0 = clicks[0] / impressions[0]ctr_1 = clicks[1] / impressions[1]win_index = np.argmax([ctr_0, ctr_1])print('After', n_init, 'initial trials Ad #', \\ win_index, 'got the highest CTR', round(np.max([ctr_0, ctr_1]),2), '(Real CTR value is', actual_ctr[win_index], ')' 12345678910111213141516171819202122232425262728293031323334regret = 0total_reward = 0regret_list = []ctr = {0 : [], 1: []}index_list = []impressions = [0,0]clicks = [0,0]for i in range(n): epsilon_index = random.choices([win_index, 1-win_index], [1-e, e])[0] index_list.append(epsilon_index) impressions[epsilon_index] +=1 did_click = bernoulli.rvs(actual_ctr[epsilon_index]) if did_click : clicks[epsilon_index] += did_click if impressions[0] == 0 : ctr_0 = 0 else : ctr_0 = clicks[0]/impressions[0] if impressions[1] ==0 : ctr_1 = 0 else : ctr_1 = clicks[1]/impressions[1] ctr[0].append(ctr_0) ctr[1].append(ctr_1) regret += max(actual_ctr) - actual_ctr[epsilon_index] regret_list.append(regret) total_reward += did_click 123Ad #0 has been shown 6.2 % of the time.Ad #1 has been shown 93.8 % of the time.Total Reward (Number of Clicks): 642 Random Selection model보다는 훨씬 괜찮은 결과가 나왔습니다. 간단한데 결과는 훨씬 좋아지네요. 하지만 탐색시의 winning variant는 최적의 variant가 아닐 수 있습니다. 사실 suboptimal variant로 탐색 한 것입니다. 이것은 regret을 올리고 보상을 감소시킬 수 밖에 없습니다. 큰 수의 법칙에 따르면, 초기 시도를 많이 할수록, winning variant를 찾을 가능성이 커집니다. 하지만 마케팅에서는 큰 수의 법칙에 결코 따를 수가 없을 겁니다. 우리는 일개 사원… 이 알고리즘의 좋은 점은 어떤 비율을 설정할 수 있다는 것입니다. 각기 다른 epsilon값을 선택함으로써 얼마나 자주 winning ad를 보여줄 수 있는지 조정할 수 있는 것입니다. 좀 더 좋은 알고리즘을 살펴볼까요? 3. Thompson Samling 50% Exploration 50% ExploitationThompson Sampling의 탐색 부분은 Epsilon-greedy알고리즘보다 복잡합니다. 이 알고리즘은 단순히 epsilon을 정하는 것이 아니라, Beta distribution을 이용하기 때문입니다. 왜냐하면 광고를 클릭하는 것은 베르누의 과정에 속하기 때문입니다.(클릭했다, 안했다는 1,0으로 표현 가능합니다) 하지만 톰슨 샘플링은 일반적으로 어떤 분포, 어떤 파라미터에서든지 샘플링이 가능하다. 이게 가장 큰 장점 중 하나라고 생각합니다. 참고로 Beta 분포는 alpha와 beta 파라미터로 분포의 모양을 조절한다.(prior 조정 가능) 로직은 다음과 같습니다. alpha와 beta를 고른다. $\\alpha=prior+hits$, $\\beta=prior+misses$로 계산한다. 우리의 경우는 hits는 클릭 수를 말하고, misses는 클릭없이 impression된 경우를 말합니다(클릭 없는 노출). prior는 CTR에 대한 prior정보가 있으면 유용합니다. 우리는 갖고 있지 않으므로 1.0을 사용할 것 입니다.. CTR을 추정합니다. 실제 CTR을 베타 분포에서 샘플링하고 $B(\\alpha_i,\\beta_i)$에서, 추정 CTR이 가장 높은 것을 선택한다. 2-3을 반복한다. 1234567891011121314151617181920212223242526272829regret = 0total_reward = 0regret_list = []ctr = {0 : [], 1: []}index_list = []impressions = [0,0]clicks = [0,0]priors = (1,1)#randomly choose the first shown adwin_index = np.random.randint(0,2,1)[0] for i in range(n): impressions[win_index] += 1 did_click = bernoulli.rvs(actual_ctr[win_index]) if did_click : clicks[win_index] += did_click ctr_0 = random.betavariate(priors[0] + clicks[0], priors[1] + impressions[0] - clicks[0]) ctr_1 = random.betavariate(priors[1] + clicks[1], priors[1] + impressions[1] - clicks[1]) win_index = np.argmax([ctr_0, ctr_1]) index_list.append(win_index) ctr[0].append(ctr_0) ctr[1].append(ctr_1) regret += max(actual_ctr) - actual_ctr[win_index] regret_list.append(regret) total_reward += did_click 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556x = np.arange (0, 1, 0.01)y = beta.pdf(x, priors[0]+clicks[0], priors[1] + impressions[0] - clicks[0])y /= y.max() ## normalizedata1 = go.Scatter(x=x, y=y, name='(Ad #0)', marker = dict(color=('rgba(10, 108, 94, 1)')), fill='tozeroy', fillcolor = 'rgba(10, 108, 94, .7)')data2 = go.Scatter(x = [actual_ctr[0]] * 2, y = [0, 1], name = 'Actual CTR #0 Value', mode='lines', line = dict( color = ('rgb(205, 12, 24)'), width = 2, dash = 'dash'))y = beta.pdf(x, priors[0]+clicks[1], priors[1] + impressions[1] - clicks[1])y /= y.max()data3 = go.Scatter(x=x, y=y, name='(Ad #1)', marker = dict(color=('rgba(187, 121, 24, 1)')), fill='tozeroy', fillcolor = 'rgba(187, 121, 24, .7)')data4 = go.Scatter(x = [actual_ctr[1]] * 2, y = [0, 1], name = 'Actual CTR #1 Value', mode='lines', line = dict( color = ('rgb(205, 12, 24)'), width = 2, dash = 'dash'))layout = go.Layout(title='Beta Distributions for both Ads', xaxis={'title': 'Possible CTR values'}, yaxis={'title': 'Probability Density'})fig = go.Figure(data=[data1, data2, data3, data4], layout=layout)# fig = tools.make_subplots(rows=1, cols=2, print_grid=False, shared_xaxes=False,# subplot_titles=('Beta Distribution (Ad #0)','Beta Distribution (Ad #1)'))# fig.append_trace(data1, 1, 1)# fig.append_trace(data2, 1, 1)# fig.append_trace(data3, 1, 2)# fig.append_trace(data4, 1, 2)# fig['layout'].update(showlegend=False)iplot(fig, show_link=False) 123Ad #0 has been shown 4.2 % of the time.Ad #1 has been shown 95.8 % of the time.Total Reward (Number of Clicks): 647 지금까지 본 regret중 가장 낮은 regret을 확인할 수 있습니다. 이 알고리즘은 지속적으로 탐색합니다. 자연스럽게 Beta distribution을 이용해 가장 가치가 높은 샘플을 가져와서 이용할 수 있습니다. Beta distribution Ad#1은 더 높고 좁은 분포를 갖고 있습니다. 이것은 샘플된 값들이 항상 Ad#0보다 높을 것이라는 것을 의미합니다. 결국 Ad#1이 우리가 원하는 광고임을 빠르게 파악할 수 있습니다. UCB (Upper Confidence Bound) 50% Exploration 50% ExploitationThompson Sampling과 달리 UCB는 불확실성에 더 초점을 맞춥니다. 한 variant에 대해 더 불확실 할 수록, 더 탐색을 해야만 하는 알고리즘입니다. 알고리즘은 가장 높은 UCB가 나오는 variant를 선택합니다. UCB를 통해 가장 높은 보상이 나올 것이라고 생각되는 variant를 고르는 것입니다. $$UCB = \\bar x_i + \\sqrt{\\frac{2 \\cdot \\log{t}}{n}}$$이 수식을 따르며 뒤에 term에 따라 UCB의 파생 알고리즘들이 등장하게 됩니다. $\\bar x_i$ CTR이 i번째 단계일 때,$t$ - 모든 variant에 대해 impression을 다 더한 숫자이다.$n$ - 선택된 variant에 대해 impression을 다 더한 숫자이다. 로직은 직관적입니다. UCB를 모든 변량들에 대해 구합니다. 가장 높은 UCB를 가진 변량을 선택합니다. 1번으로 다시 돌아갑니다. 123456789101112131415161718192021222324252627282930313233regret = 0 total_reward = 0regret_list = [] index_list = [] impressions = [0,0] clicks = [0,0]ctr = {0: [], 1: []}total_reward = 0for i in range(n): index = 0 max_upper_bound = 0 for k in [0,1]: if (impressions[k] &gt; 0): CTR = clicks[k] / impressions[k] delta = math.sqrt(2 * math.log(i+1) / impressions[k]) upper_bound = CTR + delta ctr[k].append(CTR) else: upper_bound = 1e400 if upper_bound &gt; max_upper_bound: max_upper_bound = upper_bound index = k index_list.append(index) impressions[index] += 1 reward = bernoulli.rvs(actual_ctr[index]) clicks[index] += reward total_reward += reward regret += max(actual_ctr) - actual_ctr[index] regret_list.append(regret) 123Ad #0 has been shown 19.2 % of the time.Ad #1 has been shown 80.80000000000001 % of the time.Total Reward (Number of Clicks): 596 결과는 다음과 같습니다. Regret이 생각보다 높네요, UCB알고리즘도 현업에서 자주 사용하는 알고리즘입니다만, 이 알고리즘은 가장 기본적인 알고리즘이기 때문에 그런 것 같습니다. 결론 및 성능 비교이제 살펴봤던 모든 알고리즘의 성능을 비교해 볼 시간입니다. 기대가 되네요, 나온 결과를 시각화를 해서 살펴 보겠습니다. 1000번의 시도에 어떤 광고를 얼마나 노출시켰는 지에 대한 막대그래프입니다.Random Selection은 CTR이 낮은 광고를 너무 많이 노출 시켰네요, 그 다음은 UCB1, Epsilon Greedy, Thompson Sampling 순 입니다. Thompson Sampling이 가장 좋네요! 하지만 놀라운 것은 Epsilon Greedy입니다. 정말 간단한 알고리즘인데 성능이 좋군요. 다음 자료는 Regret에 대한 것입니다. 시도가 늘어날 수록, Random Selection이나 UCB는 쭉 쭉 증가하는 것이 보입니다. 하지만 Thomspon Sampling은 굉장히 안정적으로 Regret이 유지되네요. 마지막은 1000번의 시도에서 총 몇번의 클릭을 받았는가에 대한 시각화 자료입니다. 클릭이 많다면 더 효과적으로 실험을 하면서 광고를 했다고 할 수 있겠네요. 역시 Thompson Sampling이 가장 많은 클릭 수를 얻었습니다. 그 다음은 Epsilon Greedy, UCB1, Random Selection 순 입니다. 물론 Regret이 낮다고 가장 높은 보상이 있는 것은 아닙니다. 이 실험에서는 우연히 Thompson Sampling이 Regret도 낮고, 높은 보상을 얻었습니다. 알고리즘은 적절한 광고를(right ads) 보여줄 뿐 이고, 유저가 클릭하는 것은 보장하지 않습니다. 일반적으로 Thompson Sampling이 좋은 결과를 보여줍니다. 하지만 다른 알고리즘을 보면서 어떻게, 그리고 언제 그 알고리즘이 유용할지 생각해봐야 합니다. 어떤 문제를 풀 지는 각 개인 마다 다르기 때문에, 여러 알고리즘들 중에서 문제에 적합한 것을 선택할 수 있어야 합니다. 어떤 사전 정보를 갖고 있고, 알고리즘 적용 후에 어떤 정보를 알고싶은지를 명확하게 설정하는 것이 더 중요하다고 할 수 있겠습니다.","link":"/2019/07/18/Bandit/"},{"title":"Elice_Coding_Word_Pattern","text":"엘리스 코딩 단어 패턴을 풀어봤다 단어 패턴 문자열(패턴) 하나와 문자열의 배열 하나가 주어집니다.패턴 문자열의 각각의 문자 하나는, 두번째 문자열 배열의 각각의 문자열 하나에 대응 될 수 있습니다.해당 배열이 해당 패턴으로 표현 되는지 아닌지의 여부를 확인하는 함수를 만들어 보세요. 예를 들어서, aabb 와 [‘elice’, ‘elice’, ‘alice’, ‘alice’] 가 주어졌을 경우에는 함수가 True를 반환해야 합니다. 이 경우에는 a가 elice에, b가 alice에 대응되도록 하면 배열을 해당 패턴으로 표현 하는 것이 가능하기 때문이죠. 반면, aabb 와 [‘elice’, ‘alice’, ‘elice’, ‘alice’] 가 주어졌을 경우에는 함수가 False를 반환해야 합니다.모든 문자는 영어 소문자라고 가정합니다. 문제를 보고 쉬울거라고 생각했다. 패턴을 쪼개서 각 단어에 매칭을 시켜주면 간단히 해결될 것 같았다.하지만 패턴을 쪼개서 단어에 매칭 시키는 게 간단한 문제가 아니었다. 지금은 a와 b뿐이지만 만약에 단어리스트가 주어지는개 100개라면 abcd…로 매칭시키는게 힘들다.물론 그 정도까지로 테스트 케이스가 나올 것 같지는 않지만… 따라서, 일일이 패턴을 매칭시켜서 판단하는 건 힘들다고 판단해서 이렇게 가는 건 아니라고 생각했고다른 방향을 모색했다. 그러던 중에 불현듯 패턴과 단어리스트를 zip해보고 싶어졌다. 일단 길이는 서로 무조건 같을 거니까.그리고 패턴이 일치하는 것을 찾는 거니까 set을 하면 의미있는 결과가 나올 듯 싶었다.코드와 결과는 다음과 같다.123pattern = \"aabb\"strList = [\"elice\", \"elice\", \"alice\", \"alice\"] 1set(zip(pattern, strList)) 1Out : {('a', 'elice'), ('b', 'alice')} 잘 생각해보니, set을 한 pattern하고 길이가 똑같을 것 같았다. 직관적으로 그런 생각이 들었다.일단 테스트로 다음과 같은 코드를 작성해봤다.12pattern1 = \"abab\"strList1 = \"elice\", \"elice\", \"alice\", \"alice\" 1set(zip(pattern1, strList1)) 1Out : {('a', 'alice'), ('a', 'elice'), ('b', 'alice'), ('b', 'elice')} 역시 단어리스트의 패턴이 다르면 주어진 패턴의 set과는 길이가 달랐다. 그래서 전체코드로는 다음과 같이 작성했다.1234567891011def wordPattern(pattern, strList): return len(set(pattern)) == len(set(zip(pattern, strList)))def main(): print(wordPattern(\"aabb\", [\"elice\", \"elice\", \"alice\", \"alice\"])) # should return True print(wordPattern(\"abab\", [\"elice\", \"elice\", \"alice\", \"alice\"])) # should return False if __name__ == \"__main__\": main() 제출결과 빵끗 웃는 토끼가 나왔고, 테스트 점수 100점이 나왔다.","link":"/2019/04/17/Elice-Coding-Word-Pattern/"},{"title":"Boosting","text":"부스팅 기법(Boosting)에 대해 알아보자Weak Learner부스팅 기법에 대해 알아보기 전에 알아야 할 몇가지 용어들이 있다. 그 중 하나가 Weak Learner이다.Weak Learner는 다른 말로 Simple Learner이라고도 불리우며, 간단한 학습기 정도로 보면 될 것이다. 대표적인 Weak Learner는 다음과 같다. Decision stumps : depth가 1인 decision tree Shallow decision trees Naïve Bayes Logistic regression 우리는 Weak Learner를 많이 가질 것이고, 많은 학습기들을 이용해서 예측작업을 할 것이다.Weak Learner들의 앙상블을 통해 어떤 결과를 예측해보려는 것이다. 이렇게 다수의 Weak Learner들을 이용해서 학습하면, input space의 다른 부분들을 보완해줄 수 있다. 부스팅에 대해서 공부할 때 배깅이 자주 등장하는데 차이를 비교해보자면 다음과 같다. Bagging 훈련 데이터에서 다수의 표본을 추출하고, 개별 트리의 결과를 병합하여 단일 예측 모델을 생성 각 bootstrap 과정이 독립적이므로 병렬 처리가 가능 Boosting Bagging과는 달리 순차적으로 시행되며 bootstrap 과정이 없음 Original dataset에서 약간의 수정을 거친 데이터를 개별적으로 학습한 후, 결합되어 강력한 분류기 생성 부스팅의 아이디어는 간단하다. 약한 학습기들을 이용해서 학습된 학습기들을 결합해 strong learner를 만드는 것이다. Classifier의 경우 학습기들의 결합방법은 Majority voting방식이 될 것이고, Regressor의 경우 학습기들의 결합방법은 평균이 될 것이다. 부스팅의 알고리즘도 심플하다. 앙상블 내, $t$번째 분류기 $c_t$와 $t+1$번째 분류기 $c_{t+1}$이 연관성을 가지고 생성하는 것이다. 훈련데이터 X의 샘플을 $c_t$가 옳게 분류하는 것과, 그렇지 않은 것으로 나눈다. 옳게 분류하는 샘플들은 인식이 가능하므로 가중치를 낮춘다. 틀리게 분류하는 샘플들은 가중치를 높인다. $c_{t+1}$학습시키기 위한 정책으로 sampling과정에서 가중치가 높은 샘플이 뽑힐 확률이 높아지게 한다. Ada Boost(Adaptive Boosting)Yoav Freund &amp; Robert Schapire가 제안하였고, Weak learner를 반복적으로 사용하고, 그 결과를 합하여 모델의 accuracy를 향상시킨다. AdaBoosting은 위에서 살펴본 알고리즘이 작동하는 방식과 거의 비슷하게 동작한다. 그림과 함께 살펴보자첫번째 그림에서 약한 학습기인, 결정 그루터기가 하나의 결정경계를 가지고 +와 -를 나누고 있다. 이렇게 나눴을 때 위쪽의 3개의 +들은 잘못 분류가 되어 가중치가 높아진다. 두번째 그림에서는 오른쪽의 - 두 개만 잘 분류가 되었고 결정경계 왼쪽의 대개의 -들은 잘못 분류가 되어버렸다. 이 역시 가중치가 높아지고 학습된 3개의 Weight를 결합해서 + -를 잘 분류해내는 하나의 강 분류기를 만들어낸다. 결국 Weak Learner들의 앙상블이다. 가중치 업데이트 규칙은 다음과 같다.$w^{(i)} = w^{(i)}$, $\\hat{y_j}{(i)}= y_{(i)}$ 일때$w^{(i)} = w^{(i)}exp(\\alpha_j)$, $\\hat{y_j}{(i)}\\neq y_{(i)}$ 일때그런 다음 모든 샘플의 가중치를 정규화 한다.(즉, $\\sum_{i}^{m}w^{(i)}$로 나눠준다.) 마지막으로 새 예측기가 업데이트된 가중치를 사용해 훈련되고 전체 과정이 반복된다. 새 예측기의 가중치가 계산되고 샘플의 가중치를 업데이트해서 또 다른 예측기를 훈련시키는 방식이다. Adaboost는 지정된 예측기 수에 도달하거나 완벽한 예측기가 만들어지면 중지된다. Adaboost의 예측은 $\\hat{y}(x)=\\sum_{i=1}^{N}\\alpha_j$로 이루어진다.($N$은 예측기의 수) Gradient Boosting그래디언트 부스팅은 Ada부스팅처럼 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다. 하지만 Ada처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차(residual error)에 새로운 예측기를 학습시킨다. 다시말해서 약한 분류기가 이전 학습에서 발견하기 어려웠던 문제성 관측값, 즉, 예측이 틀린 관측값에 집중하게 하는 것이다. 다른 boosting 기법처럼 모델을 단계적으로 구축해 나가는 것은 같지만 임의의 미분 가능한 손실 함수를 최적화하는 문제로 일반화한 방법이다. GB는 여러개의 간단한 모델의 ensemble을 학습한다. Motivation of Gradient Boosting($x_1$,$y_1$),($x_2$,$y_2$) …, ($x_n$,$y_n$) 총 n개의 데이터가 있고, 이 데이터를 이용하여 회귀모형 $F(x)$ 를 학습하는 프로젝트를 진행한다고 생각해보자. 팀원이 모델 $F$를 만들었다. 하지만 성능이 그다지 좋지 않다. $F(x_1)$ = 0.8의예측값을 생성한다. 하지만 실제 $y_1$ = 0.9이다. $y_2$ = 1.3인데, $F(x_2)$ = 1.4의 값이 나온다. 이 모델의 성능을향상시켜야 하는데 한가지 제약조건이 있다. 팀원이 만든 모델 $F$는 절대 건드리지 않고, 모델을 향상시켜야한다. 어떤 방법이 있을까? 방법은 간단하다. 원래 모델은 그냥 두고, 차이만큼을 더해주는 함수 $h(x)$를 만들어주면 되는 것이다.완벽하게 우리의 목적을 달성시키지는 못하지만, 근사적으로 달성할수는 있다. 그렇다면$h(x)$는 어떻게 구할 수 있을까?$h(x_1) = y_1 - F(x_1)$$h(x_2) = y_2 - F(x_2)$$…$$h(x_n) = y_n - F(x_n)$ 이므로,$(x1, y_1-F(x_1))$, $(x2, y_2-F(x_2))$,$…,$,$(x_n, y_n-F(x_n))$을 학습하면 된다. 학습데이터를 이용하여 75%정도의 정확도까지 모델을 학습하고, 나머지 미설명 부분은 오차항에 남겨둔다.$Y = F(x) + E$오차항을 이용하여 다른 모델을 학습시킨 후, 그 전 모델에서는 미설명 부분이었으나 이번 학습에서는 설명이 되는 부분을 찾아내 원 모델에 추가한다. 단, 추가 모델은 반드시 전체 정확도를 향상시켜야만 한다.$Gradeint(E) = G(x) + E2$ 모델이 약 80%의 정확도를 갖게 되면 식은 다음과 같게 된다.$Y + F(x) + G(x) + E2$ 이런 방법을 계속해서 사용해 나가고, GB는 단순 합보다 가중 평균을 사용하여(다른 모델보다 정확도가 높은 예측 결과를 가진 모델에 더 높은 중요도 점수를 부여) 모델의 정확도를 더 개선할 수 있다$Y=\\alpha{F(x)}+\\beta{G(x)}+\\gamma{H(x)} + E$ $…$ Gradient Boosting의 Loss Function손실 함수는 해결하려는 문제에 따라 다르다. 부스팅에서는 처음부터 최적화를 하는 것이 아니라, 각 단계별로 이전 단계에서 설명되지 못한 손실에 관해 최적화를 수행한다. 회귀 문제 : Least squares method (최소 자승법) 분류 문제 : Log loss function (로그 손실 함수) 손실 함수를 최소화하기 위해 약한 분류기를 추가할 가법 모델(additive model) 기존 트리는 변동이 없고 새로운 트리가 하나씩 추가된다. 기울기 하강 절차가 사용되어 트리가 추가될 때의 손실을 최소화한다. Leaf node마다 가중치, score가 부여가 된다. Gini계수 등을 사용하지 않는다.분류 / 회귀 : Sklearn에서는 (friedman) mse를 사용한다.","link":"/2019/05/15/Boosting/"},{"title":"2019!","text":"첫 시작!데이터 사이언스 공부를 시작한지 1년이 훌쩍 넘었다. 기록을 하지 않으니까 정리도 되지않고 뭘 배웠는지도 잘 기억나지 않는다.꾸준히 기록해봐야겠다. 글을 꾸준히 쓰자 주 3회 목표","link":"/2019/01/11/2019/"},{"title":"Information_Theroy_Entropy","text":"정보이론 기초, 정보량(Information)과 엔트로피(Entropy)에 대해 알아보자정보량 (Information)정보량은 말 그대로 얼마나 정보를 갖고 있는 지를 뜻하는 말이고 정보이론이란 불확실성을 다루는 학문이다. 하지만 일상에서 정보량에 대해서 접하기는 상당히 힘들고, 정보량이라는 단어를 일상에서 사용하는 사람은 매우 드물다. 알기 쉬운 예를 들어보자. 12간만에 친구들과 약속을 잡아서 놀기로 했다. 12일 13일 14일 15일 중으로 날짜를 잡기로 했고카카오톡 투표를 통해서 가장 많이 나온 날짜를 약속날로 잡자고 했다. 흔히 있는 상황이다. 약속날 후보로 5월 12일 13일 14일 15일이 있다고 해보자. 총 4개의 옵션이 있는 것이다. 근데 투표를 만든 사람이 자비롭게 중복투표를 허용해놨고, ‘음 난 다좋은데~’라고 생각하는 주관없는 친구가 모든 날짜를 다 눌러놨다고 생각해보자. 이 친구의 투표가 가진 정보량은 얼마일까?직관적으로 생각했을 때 0이다. 하지만 수학적으로 왜 그런 것일까? 정보량의 공식을 보자.정보량 $h(x) = \\sum_{x}log_2p(x)$ 이다. 이 공식을 토대로 주관없는 친구의 5월 12일 날짜에 대한 정보량을 구해보면,$p(x) = 1$이므로, $log_21 = 0$이란 값이 나온다.13일, 14일, 15일 모두 같은 결과가 나오고, 주관없는 친구의 투표에 대한 정보량은 0이다. 어떻게 보면 어떤 사람의 주관은 일정의 정보량을 뜻하는 듯하다. 아무거나 빌런은 결국 어떤 정보도 갖고 있지 않다는 것이다. 정보량은 여기서 주관을 뜻하기도 하지만, 보통 정보량은 놀라움의 정도를 뜻한다.축구 경기중에 골키퍼가 골을 넣는 사건은 굉장히 놀랍다. 이는 굉장히 정보량이 많다는 것을 뜻한다.왜냐면 정보량은 확률에 반비례하기 때문이다. 이번에는 예를 바꿔서, 우리가 쉽게 알 수 있는 주사위 case를 갖고 와 보자.주사위를 던져서 짝수가 나타날 사상 $E_1$의 정보량은 몇일까?공식에 의해서 $p(x) = {1\\over2}$이므로$P(E_1) = {1\\over2}\\longrightarrow I = -log_2{1\\over2}=1(bit)$ 가 된다. 엔트로피(Entropy)엔트로피는 흔히 열역학에서 자주 볼 수 있는 단어지만, 정보이론에서도 사용되는 말이기도 하다. 엔트로피라는 말에 대해서는 정보이론의 아버지인 Shannon이 정립하였다. 엔트로피의 공식을 먼저 확인해보자.$H(X) = -\\sum_{X}P(X)log_2P(X)$이다. 확률과 통계를 기본부터 잘 다져온 사람이라면 익숙한 공식이 눈에 들어올 것이다.바로 기댓값이다. 수식을 그럼 천천히 다시봤을때, 엔트로피 공식이 뜻하는 것은 바로 확률분포 $P(X)$에 대한 기댓값이다. 확률분포가 있어야 정의가 될 수 있다. 확률 분포의 불확실한 정도를 뜻하는 것이라고 생각하면 된다. 엔트로피는 정보이론에서 사용되는 단어이므로, 이 역시 불확실도를 나타내는 척도로 사용된다.직관적으로 이해하기 위해 그림을 통해 살펴보자. 위 그림에서 보면 왼쪽의 분포는 몰려있고, 즉 정규분포로 따지자면 표본오차가 작은 모양이고, 오른쪽의 분포는 넓게 퍼진, 표본오차가 매우 큰 모습이다. 정보이론을 따라 분포를 다시 보면 왼쪽의 그림은 불균형한 분포로 불확실성이 적은 모양이다. 다시말해 엔트로피 값이 낮은 분포이다. 반면에 오른쪽 그림은 균등한 분포이며, 어떤 값이 나올지 모르는, 불확실성이 높은 모양이다. 즉, 엔트로피 값이 높은 분포라고 할 수 있다. 결론적으로, 엔트로피는 확률분포 P(X)에서 일어날 수 있는 모든 사건들의 정보량의 기댓값으로, P(X)의 불확실성 정도를 평가하는 척도로 사용된다. 엔트로피와 관련된 것으로 크로스 엔트로피(Cross-Entropy)가 있는데, 이것은 다음 포스트에 적도록 하겠다. P.S 다시 엔트로피와 크로스 엔트로피에 대해 공부한 이유는, 면접을 최근에 보게 되었는데 이 부분에 대해서 제대로 공부를 하지 못해 대답을 우물쭈물 했기 때문이다. 데이터 사이언스를 공부하면서 느끼는 것은 항상 이런 것이다. 내가 진짜 알고있는지 아닌지 확인하기 어렵다는 것이다. 최대한 많이 부딪혀 봐야겠다. 그것이 캐글이 되었든, 아니면 면접이 되었든, 실제로 일을 하는 것이든, 직접 경험해 봐야 많이 필요성을 느낄 수 있고 많이 배울 수 있게 되는 것 같다.","link":"/2019/05/14/Information-Theroy-Entropy/"},{"title":"My SQL Workbench Bug issue","text":"MySQL Workbench에서 쿼리를 날렸는데 결과창이 안나온다면!, For MAC OSXsource from : [https://stackoverflow.com/questions/45967413/results-grid-not-showing-on-mysql-workbench-6-3-9-for-macos-sierra]Thank you Yogev! 빅데이터 시대에 SQL공부가 필수적이다. 머신러닝, 딥러닝 뭐 할게 너무 많지만 일단 데이터를 이해하기 위해서는 SQL공부를 먼저 해야한다고 생각한다. 이런 마음에 SQL을 공부하기로 마음먹고 세달전에 샀던 데이터 분석을 위한 SQL 레시피를 다시 폈더랬다. 책은 매우 훌륭했다. 기본적인 SQL뿐 아니라, 내가 관심있었던 SparkSQL, Big Query에 대해서도 다뤄주고 있었다. 눈으로 SQL을 슬슬 할 때쯤, SQL을 직접 입력해보고 결과를 보고 싶어졌다. 소스코드가 있나 뒤져봤더니, 한빛 미디어에서 제공해 주는 코드가 있었다. 신나게 받아놓고 1년전에 세팅해둔 Mysql 서버를 실행시켰다. 뭐 잡 에러 덩어리가 많았지만 우여곡절 끝에 해결하고 드디어 MySQL Workbench로 들어가서 쿼리를 날렸다. Success!가 나왔다. 음, 근데 결과창이 보이지 않는다. 바로 StackOverFlow를 뒤져봤다.쿼리 옆에 있는 돋보기를 눌러보고 Result Grid를 눌러보랜다. 안된다. Mac에 있는 버그라며 쿼리 박스에 마우스를 신중히 갖다대고 바를 늘려보랜다. 회색화면만 나온다. 껐다 다시 키면 될 것이란다. 바뀐 게 없다. 아, 모든 게 거짓말 같았다. 오늘은 만우절ㅎㅎ*** 이렇게 6시간 넘게 삽질을 지속하다가, 빛 갓 Yogev의 Stackoverflow 글을 보게 되었다. 요약하자면 수정된 버전이 올라와 있다는 말이다.‘아니 최신 버전을 받았는데 왜 또 안됐었던 거지???’ 이해가 안되긴 하지만빛요게프 선생님께서는 친절히 공유경제의 장점에 대해 설파하고 계시었다. upvote를 찍어드리기 위해서 stackexchange에 가입했고 upvote를 꾸우우욱 눌러드렸다. 해결방법 : [https://dev.mysql.com/downloads/workbench/] 다운 후그대로 덮어쓰기 (Workbench 삭제 안해도 된다!)","link":"/2019/04/01/My-SQL-Workbench-Bug-issue/"},{"title":"Elice_Start","text":"엘리스 코딩을 구독했다.프로그래머스 문제를 쭉 풀면서, 코딩테스트를 몇번 보게 되면서 느낀 점은, 내가 알고리즘 기초가 부족하다는 것이었다. 문제를 보고 고민하는 시간이 너무 길었다. 고수들의 코드를 보면 고민한 시간은 잘 모르겠지만, 생각하는 방향이 딱딱 정해져서 배운 알고리즘과 자료구조 지식을 활용해서 답을 작성한 것이 눈에 띄었다. 반면에 나는 너무 막코딩하는게 아닌가 하는 생각이 들었다. 막코딩의 단점은 문제를 풀었어도 머리 속에 정리가 되지 않는다는 점이다. 머리에 남지 않으면 비슷한 문제가 나와도 틀릴 확률과 문제에 푸는 시간이 증가한다. 막코딩의 이러한 악효과를 차단하기 위해서 프로그래머스도 뒤져보고 엘리스도 뒤져보던 도중 엘리스의 프로그램 중 구독 시스템이 맘에 들어서 신청하게 되었다. 사실 엘리스는 양재 RNCD AI 실무자 양성 과정을 참여하고, 각종 무료 교육을 들으면서 친숙했다. 엘리스의 플랫폼에서는 메세지를 통해서 소통을 빠르게 할 수 있었던 것이 기억이 났다. 답답한 게 있으면 빠르게 물어보고 답을 얻어야 하는데 이런 점을 통해서 내가 원하는 바를 만족시킬 수 있을 것 같았다. 또한 개인적으로 문제를 다 풀고 100점이 나오면 토끼 애니메이션이 나오는데 이게 은근히 성취감을 불러일으킨다. 혼자 알고리즘 공부하다보면 문제를 깔끔하게 풀어도 ‘칭찬해 주는 사람도 없는데…’라는 생각이 항상 드는데, 토끼 애니메이션이 문제를 더 잘 풀고 싶게 하는 자극을 주는 게 참 맘에 들었다. 칭찬은 고래도 춤추게 한다고 하지 않은가. 나는 칭찬에 약하고 인정욕이 강한 동물이다. 어쨌든 큰 맘 먹고 10만원으로 퍼플키를 질렀다. 퍼플키를 지르게 되면 곧 튜터가 배정된다. 튜터는 쉽게 말해서 막히는 문제에 대해서 도움을 줄 수 있는 사람이다. 메세지를 통해서 소통하고, 문제가 있으면 답을 해준다. 아직 튜터에게 완전한 답을 받은 적은 없지만, 맘에 드는 시스템 중 하나다. 현재 수강 신청한 건 알고리즘 트랙(트랙으로 강의를 보거나, 구독을 하면 원하는 강의를 한달 동안 볼 수 있다.), 자료구조, 알고리즘1, 알고리즘2이다. 맘 같아서는 다 보고 싶은데, 실습 문제를 풀어야 하니 은근히 시간이 소요가 된다. 자료구조 먼저 끝내고 싶지만, 문제가 있는 것 같아서 알고리즘 트랙부터 끝내고 다른 강의들을 마저 들어야겠다.","link":"/2019/04/17/Elice-Start/"},{"title":"Programmers 124 나라의 숫자를 풀어보자","text":"프로그래머스 코딩테스트 연습문제 124 나라를 풀어봤다124 나라참고[https://thisisablog.tistory.com/14]124 나라가 있습니다. 124 나라에서는 10진법이 아닌 다음과 같은 자신들만의 규칙으로 수를 표현합니다. 124 나라에는 자연수만 존재합니다.124 나라에는 모든 수를 표현할 때 1, 2, 4만 사용합니다.예를 들어서 124 나라에서 사용하는 숫자는 다음과 같이 변환됩니다. 10진법 124 나라 1 1 2 2 3 4 4 11 5 12 6 14 7 21 8 22 9 24 10 41 자연수 n이 매개변수로 주어질 때, n을 124 나라에서 사용하는 숫자로 바꾼 값을 return 하도록 solution 함수를 완성해 주세요. 문제를 보고 패턴을 찾아야겠다는 생각부터 했다.3의 배수로 끊어지고, 끝자리가 1, 2, 4로 반복된다는 것을 파악했다. 끝자리는 그럼 1,2,4를 돌려주는 것으로 끝낼 수 있는데, 이제 앞자리가 문제가 된다.앞자리 패턴을 찾기 위해서 문제 표에는 10까지만 나와있지만, 21까지 구해봤다.21까지 쭉 따라 쓰다보니, 앞자리 역시 1,2,4가 반복되고 있다는 것을 파악했다.맨 뒷자리 1,2,4가 끝나면 그 다음 자리 index가 하나 올라가고 그 앞의자리도 마찬가지였다. 그렇다면, 뒤에서부터 자리 수를 채워주는 게 낫다고 생각했고다 채워준 다음에 뒤집어 버리는 방식을 택했다.그래서1[::-1] 을 이용해 줬고 코드는 다음과 같이 작성했다. 1234567891011121314151617181920212223def solution(n): if n&lt;3: return n elif n==3: return 4 result='' index=['4','1','2'] rem=0 quo=0 while n&gt;3: rem=n%3 quo=n//3 print(rem,quo) result+=index[rem] print('res = ',result) if rem==0: quo-=1 n=quo print('n={}, rem={}, result={}'.format(n, rem, result)) result+=str(n) result=result.replace('3','4') return result[::-1]","link":"/2019/02/25/Programmers/"},{"title":"2019 GDG DEVFEST SEOUL 행사를 다녀오다","text":"GDG Devfest 행사를 다녀왔다 2019 GDG DEVFEST SEOUL2019 GDG DEVFEST SEOUL 행사가 세종대학교 광개토관 지하 2층 컨벤션홀에서 열렸습니다. 2주 전에 행사 소식을 접했는데 타임 테이블을 보니 ML/AI관련 세션이 많이 준비되어 있고 관심있는 내용이 있어서 바로 티켓을 구매했습니다. 특히 캐글 코리아에서 활동하시는 이유한 님의 BERT in Kaggle, 분자대회에서 BERT를 이용해서 어떻게 금메달을 획득했는지가 궁금해서 질러버렸습니다.(캐글 코리아 짱짱) 타임테이블은 다음과 같았습니다. 쉽게 따라할 수 있는 한국어 임베딩 구축 by 이기창님맨 첫 번째로 Google Cloud Korea 양승도님께서 키노트 발표를 해주셨고 30분간의 쉬는시간을 가졌습니다. 그리고 제가 첫번째로 들으려고 했던 세션은 Naver 이기창님의 쉽게 따라할 수 있는 한국어 임베딩 구축 이었습니다. 쉬는시간에 밥을 빨리 먹으려고 하는데, 식사가 너무 늦게나오는 바람에 세션의 앞부분은 놓쳤습니다(ㅠㅠㅠㅠㅜㅠ). 아쉬운 부분이 있었지만, 핵심은 기존의 워드 기반의 임베딩보다 최근에는 문장 수준의 임베딩이 잘 된다는 것이었습니다. 문장 수준의 임베딩이 나온 이후로는 워드 기반의 임베딩에 관한 페이퍼가 거의 등장하지 않는다고 하셨습니다. 그 만큼 문장 수준의 임베딩의 결과가 훌륭하다는 것이겠습니다. 실제로 ELMo가 등장한 이후에 리서치 트렌드가 아예 문장 수준의 임베딩으로 바뀐것이 관찰되었습니다. 문장 수준의 임베딩에는 대표적으로 두 가지 알고리즘이 있는데 바로 앞서 말씀드린 ELMo와 그 유명한 BERT입니다. BERT는 transformer network이며 attention 기반으로 만들어진 것입니다. BERT는 기존의 일방향성의 문맥 흐름에서 양방향성의 문맥을 읽어냄으로써 동음이의어 분간을 가능하게 만들었습니다. BERT는 pretrain하고 fine tuning 하는 작업이 필요한데, pretrain하는 작업은 추천하지 않는다고 하였습니다. 그 이유는 이 모델을 훈련시키는데 GPU 8개로 2주의 시간이 필요하기 때문입니다. GPU리소스가 충분하다면야 가능하겠지만, 그 보다 공개된 BERT를 사용하면 좋겠습니다. 공개된 BERT 성능이 나쁘지 않기 때문입니다. 그리고 세션에서 마지막으로 강조한 부분은 임베딩 하고 그 위에 무엇을 올릴까에 관한 것이었습니다. 보통 Bi-Direction만 사용하곤 하는데, 그 위에 활용하는 부분은 자유롭게 바꿔보면서, 실험해보며 다양한 모델들을 만들어 볼 수 있다는 것으로 세션이 종료되었습니다. Clean Code for ML/AI by 한성민님15분의 휴식시간이 끝나고 두 번째 세션에 들어갔습니다. 두 번째로 들은 것은 Naver 한성민님의 Clean Code for ML/AI 였습니다.클린 코드에 대한 중요성을 말하기전에 설명해주신 개념은 깨진 유리창 이론에 관한 것이었습니다. 품질이 떨어지는 코드를 쌓아올리는 시작하는 순간 부터 다른 협업자의 코드 품질도 떨어지기 시작합니다. 그렇기 때문에 보이 스카웃 규칙대로 코딩을 하는 것이 중요합니다. 그 규칙은 간단합니다. 떠날 때는 찾을 때보다 캠프장을 더욱 깨끗이 할 것 손을 거치게 된 코드는 원래 상태보다 더 낫게 만들고 떠나라 훌륭한 도입부와 더불어 기존에 제가 짠 코드가 더럽다고 계속 생각을 했었고, Clean Code에 대해서 중요성을 항상 들어왔기 때문에 더 집중하게 되었습니다. ML/AI 관련 코드를 짤 때면 If else구문을 자주 활용했었고, indentation이 복잡해져 가독성이 많이 떨어지는 일이 빈번했습니다. 역시 한성민님이 지적한 부분도 같았습니다.일명 코드 악취라고 불리는 더러운 코드들을 소개해주셨고, 깨끗하게 정리된 코드들을 보여주셨습니다.보면서 느끼는 것이 너무 많았고, 회사에 있는 코드를 빨리 정리하고 싶다는 생각이 들었습니다. 최근에 한 프로젝트의 코드들이 눈앞에 지나가면서 다시보기 싫었던 그 느낌이 생각났습니다. 이 외에도 다양한 코드 악취 Case들을 보여주시고 정리한 내용들을 보여주셨습니다. 대표적으로 너무 많은 분기는 좋지 않으므로, return이나 continue를 이용해 가드 클로져 구문을 사용하는 것을 추천해 주셨고, 주ㅜ석 남용도 코드 악취에 해당하기 때문에 함수로 정의하는 것이 좋다고 말씀해 주셨습니다. 한 함수에는 너무 많은 기능을 담지 않고, 50줄 안으로 쓰는 것이 적정하며, 동일한 작동을 너무 많이 반복하는 것을 지양해야 함을 알려주셨습니다. 그 외에 연구자와 개발자끼리 코드 컨벤션이 맞지 않는 문제, IDE로 사용하지 않는 변수들 제거, 수정이 빈번한 변수들은 전역 변수로 추출, 복잡성이 높은 로직에 대해서 함수 추출해 optimizer과 hyperparameter로 뽑아내기 등을 설명해 주셨고google/gin-gonfig나 Lint와 Quality Gate와 같은 클린코드 툴을 알려주셨습니다. 개인적으로 느끼기에는 바로 써먹을 수 있고, 코드의 퀄러티가 높아질 수 있기에 너무 좋은 세션이었습니다. 클린 코드에 대해서 제대로 느끼게 만든 훌륭한 강의였습니다. BERT in Kaggle by 이영수님, 송원호님, 이유한님세번째 세션은 듣고싶었던 캐글 세션이었습니다.먼저 이영수님께서 BERT의 개념에 대해서 간략하게 설명해 주셨습니다. 맨 처음에 들었던 세션인, 이기창님의 세션에서 이해가 잘 안되었던 부분이 이영수님의 세션을 통해 어느정도 해결이 되었습니다.설명이 어느정도 끝나고 다음 바통을 잡은 분은 송원호님이었습니다. 송원호님은 Toxic 대회를 진행하면서 BERT를 통해 스코어를 올렸었던 경험에 대해서 설명해 주셨습니다. Toxic대회는 2017년에 한번 진행했었던 대회인데, 어떤 코멘트에 대해서 이 코멘트가 정상인지 악성인지 분류하는 문제입니다. 2019년에 다시 열리기 된 것은 바로 Unintended Bias가 추가되었기 때문입니다. 특정 키워드가 들어가게 되면 악성으로 분류되어버리는 문제 때문에, 이것을 더 정밀하게 분류해 내는 모델이 필요했고, 더 정밀해진 Metric을 만족시켜야 하는 대회였습니다. 그렇기 때문에 Metric에 대한 설명이 쭉 진행되었고 Metric에 맞는, 기존 loss와 다른 loss function을 재 정의했어야 했던 점을 설명해 주셨습니다. 결구 새로 열린 이 대회에서 중요했던 것은 문맥이었습니다. 문맥을 잘 살려서 코멘트를 분류해야 했었기 때문에 BERT로 접근을 했고 그 결과가 Baseline을 사용할 때에 비해서 좋아질 수 있었습니다. Leaderboard 상으로는 10등으로 in-money권에 있었지만 결과는 26등이었습니다. 쉐이크업으로 인해 등수가 떨어지게 되었는데, 캐글의 discussion에서 그 이유가 등장했습니다. 원호님과 비슷하게 접근을 했지만 높은 등수를 기록했던 분과의 차이점은 모델에서 BERT large, small, medium을 다 사용했고 GPT 2 small도 활용하며, XLNet까지 사용했다는 점이었습니다. 크고 다양한 언어 모델을 사용했고 속도 문제에 있어서는 Sequence Bucketing을 통해 해결한 점이 큰 차이점이었습니다. 결론은 다음과 같았습니다. NLP 에서 좋은 머신은 꼭 필요하다. 작은모델 큰 모델 다 중요하다. Evaluation에 맞는 loss를 잘 정의해야한다. 파이프라인을 일찍 구성하고 실험을 많이 해봐야 한다. 제한된 시간하에 높은 점수를 위해 속도 줄이기 위한 여러 기술이 필요하다. 그 다음으로는 생일을 맞으신 이유한님이 발표해주셨습니다.이유한님게서는 분자대회에서 BERT를 사용한 경험에 대해서 전달해주셨습니다. 먼저 과학에서 머신러닝이 어떻게 사용되는지에 대해 간략히 설명해 주셨고 과학과 머신러닝의 유사점에 대해서 정리해주셨습니다. 본격적으로 분자대회에 대해서 설명을 해주셨는데, 정확하게는 이해하지 못했고 아무튼 양자 계산으로 너무 힘든 분자에 관한 어떤 예측 문제를 딥러닝을 이용해서 풀어보는 대회였습니다. 보통 분자는 그래프 네트워크와 유사한 부분이 있습니다. 그래서 유한님도 처음에 GNN을 통해 접근을 했었다고 합니다. 하지만 분자를 sentence로 풀어서 BERT에 넣어봤더니 성적이 갑자기 뛰기 시작했습니다. 그래프 네트워크보다 오히려 더 좋은 성적이 나오기 시작한 것입니다. 분자의 정보를 문장화 시켰고, 각 정보를 임베딩하고 벡터화했습니다. 그리고 데이터에 기반해 자발적으로 학습된 임베딩을 뽑아냈습니다. 결국 임베딩도 학습된 것이고 임베딩된 결과도 학습되었습니다. end to end 모델로 구성한 것이고 결국 딥러닝 모델이 알아서 다 하게 되는 모델을 만들어 낸 것입니다. 여기에 Auxiliary targets을 이용해서 5개의 타겟으로 학습시켰고, 세밀한 정보가 도출되어 이것을 이용해서 점수를 끌어내었다고 설명해 주셨습니다. 여기까지 대회에 대해서 정리해 주셨고, 캐글 코리아 운영자답게 캐글 대회에서 높은 점수를 받는 방법에 대해서 알려주셨습니다. 캐글에서 높은 점수를 얻으려면?- Diversity를 살려서 Ensemble하자 - 다양한 모델의 사용 - 다양한 시드, 아키텍쳐 같아도 시드가 다르면 다른 모델 - Pseudo Labeling, 예측한 값을 타겟으로 한 테스트 셋으로 새로 학습을 진행한다.(대회니까 가능함) - 뭘 쓸까 항상 생각해야 한다. 학습 방식을 선택해야 하는데, EDA를 통해서 특성을 파악하고 모델을 선택해야 한다. - Loss functionn을 잘 찾아야 함, - BCE, MAE, MSE, CORAL, DICE 등, 다 해보고 좋은 것을 선택한다 - 학습이 잘 될 수 있는 학습 스케쥴링 구글 번역이 크라우드소스로 어떻게 언어를 배워요? by Anh Tuan Nguyen마지막으로는 구글 브레인의 안 투앙님의 발표였습니다. 프랑스분이셨는데, 한국어로 발표해주신 점이 인상깊었습니다.발표는 간단했습니다. 머신러닝에 대해서 설명하고, 학습과 추론에 대해서 짚었습니다. 지도학습에 대해서만 말씀해주셨고 이 모델을 활용하는 것에서는 Google의 photo에서 텍스트를 감지하는 것, 그리고 동물 감지, 예를 들어 강아지를 포토에서 검색하면 찍은 사진중에 강아지가 감지된 사진이 선택되는 것을 말씀해 주셨습니다. 그리고 번역에 대해서 말씀을 하기 시작하셨는데, 가끔 번역이 잘 안되는 내용이 있다고 하셨습니다. 위의 사진들이 그 예를 찍은 것입니다. 고치는 방법은 여러가지가 있겠습니다만, 많은 데이터를 통해서 해결해 나갈 수 있습니다. Google Crowdsource라는 앱을 통해서 가능하다는 것입니다. 일종의 게임을 만들어서 사용자가 해석에 대한 맞는 라벨을 골라주는 것입니다. 이 게임을 통해서 뱃지를 얻을 수 있고 레벨을 올릴 수 있다고 설명해 주셨습니다. 발표시간이 조금 남아서 질문 답변 시간을 길게 가졌고 이 마지막 세션을 끝으로 GDG DEVFEST행사가 종료되었습니다. 처음 가본 GDG행사였는데, 알고 싶었던 BERT모듈에 대해서 일부 알게 되었고, Kaggle에 대한 경험담과 피와 살이 될 것 같은 Clean code에 대해서 제대로 알 수 있게 되어 좋았습니다. 일찍 예매해서 만원정도에 티켓을 구매할 수 있었는데, 값이 아깝지 않았습니다.(물론 GDG에 참여한 회사들의 굿즈들을 많이 받고 간식도 많이 먹긴 했습니다.) 다음에 또 이런 행사가 있으면, 꼭 참여하고 싶다는 생각이 들었고 언젠가는 저기서 발표하고 싶다는 작은 목표를 기록해두면서 글을 마무리합니다.","link":"/2019/10/20/GDG-DEV-FEST/"},{"title":"Fearuture Selection with Information Value","text":"Feature Selection에 Information Value를 이용해보자.Information Value를 이용한 방법 Kaggle이나 데이터 분석을 하다보면 성능을 높이기 위해 여러가지 feature들을 만들어낸다. 그런데 feature를 무조건 많이 만든다고 성능이 올라갈까? 아니다. target에 대한 영향력이 큰 feature들이어야 성능에 영향을 줄 수 있을 것이다. 그렇다면 중요한 건, 만들어낸 feature들을 어떻게 평가할 것인가이다. Kaggle에서 Feature Selection 하는 방법들을 보면 gbm모델들의 Feature Importance를 이용하거나 DecisionTree나 RandomForest의 Classifier 객체의 feature_importances_ 메서드를 활용해 Feature Importance를 구해서 비교하는 모습들이 자주 보인다. 하지만 또 다른 방법으로, Information Value를 이용한 Feature Selection을 소개해보고자 한다. 1. Information Value (정보 가치)모델에서 변수의 사용유무를 판단하는 feature selection에서 유용한 방법이다. 주로 모델을 학습하기전 첫 단계에서 변수들을 제거하는 데 사용한다. 최종 모델에서는 대략 10개 내외의 변수를 사용하도록 한다(여러개 만들어 보고 비교해보는 것이 좋다). IV와 WOE 신용채무능력이 가능한(good) 고객과 불가능한(bad) 고객을 예측하는 로지스틱 회귀 모델링과 밀접한 관계가 있다. 신용 정보 관련분야에서는 good customer는 부채를 갚을 수 있는 고객, bad customer는 부채를 갚을 수 없는 고객을 뜻한다. 일반적으로 이야기할 때는 good customer는 non-events를 의미하고 bad customer는 events를 의미한다. 신용 관련 분야 $${WOE} = ln{\\frac{\\text{distribution of good}}{\\text{distribution of bad}}}$$ $${IV} = \\sum{(\\text{WOE} \\times (\\text{distribution of good} - \\text{distribution of bad}))}$$ 일반적 $${WOE} = ln{\\frac{\\text{distribution of non-events}}{\\text{distribution of events}}}$$ $${IV} = \\sum{(\\text{WOE} \\times (\\text{distribution of non-events} - \\text{distribution of events}))}$$ Information Value 값의 의미 Information Value 예측력 0 to 0.02 무의미 0.02 to 0.1 낮은 예측 0.1 to 0.3 중간 예측 0.3 to 0.5 강한 예측 0.5 to 1 너무 강한 예측(의심되는 수치) Information Value를 통해서 ‘이 feature를 꼭 사용해야하나?’에 대해 어느정도 답을 내릴 수 있다. Information Value가 0.5~1.0인 구간을 보면, 강한 예측이지만 의심되는 수치라고 되어있다. 처음보면 이게 무슨 의미인지 잘 이해가 안될 것이다. ‘너무 예측을 잘하는데 수치를 의심하라고?’ 하지만 잘 생각해보자. IV는 WOE를 활용한다. WOE는 good과 bad의 분포를 이용하는데, 데이터가 good으로 쏠려있을 경우 WOE는 무조건 잘 나올 수 밖에 없고, 이에 따라 IV값도 잘 나오게 된다. 따라서, IV의 값을 볼 때는 데이터가 어떻게 되어있는지 먼저 살펴보는 게 중요하다. 2. German Credit Data 이용해보기12path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data'dataset = pd.read_csv(path, delimiter=' ', header=None) Column이 제대로 되어있지 않기 때문에 노가다로 넣어준다.1234567891011121314151617181920212223COL = [ 'Status_of_existing_checking_account', 'Duration_in_month', 'Credit_history', 'Purpose', 'Credit_amount', 'Savings_account_bonds', 'Present_employment_since', 'Installment_rate_in_percentage_of_disposable_income', 'Personal_status_and_sex', 'Other_debtors_guarantors', 'Present_residence_since', 'Property', 'Age_in_years', 'Other_installment_plans', 'Housing', 'Number_of_existing_credits_at_this_bank', 'Job', 'Number_of_people_being_liable_to_provide_maintenance_for', 'Telephone', 'foreign_worker', 'Target'] Target값을 0, 1로 만들어준다.1dataset['Target'] = dataset['Target'] - 1 Information Value를 구하는 코드12345678910111213141516171819202122232425262728293031max_bin = 10 # 전체 데이터의 예측력에 해를 가하지 않는 한에서 구간을 테스트하였습니다. 통상적으로 10개로 나눔, 15개 20개 다 나눠보고# 값이 잘나오는 bin을 선택함def calc_iv(df, col, label, max_bin = max_bin): \"\"\"IV helper function\"\"\" bin_df = df[[col, label]].copy() # Categorical column if bin_df[col].dtype == 'object': bin_df = bin_df.groupby(col)[label].agg(['count', 'sum']) # Numerical column else: bin_df.loc[:, 'bins'] = pd.qcut(bin_df[col].rank(method='first'), max_bin)# bin_df.loc[:, 'bins'] = pd.cut(bin_df[col], max_bin) bin_df = bin_df.groupby('bins')[label].agg(['count', 'sum']) bin_df.columns = ['total', 'abuse'] bin_df['normal'] = bin_df['total'] - bin_df['abuse'] bin_df['normal_dist'] = bin_df['normal'] / sum(bin_df['normal']) bin_df['abuse_dist'] = bin_df['abuse'] / sum(bin_df['abuse']) bin_df['woe'] = np.log(bin_df['normal_dist'] / bin_df['abuse_dist']) bin_df['iv'] = bin_df['woe'] * (bin_df['normal_dist'] - bin_df['abuse_dist']) bin_df.replace([np.inf, -np.inf], 0, inplace=True) bin_df = bin_df[bin_df['total'] &gt; 0] iv_val = sum(filter(lambda x: x != float('inf'), bin_df['iv'])) return bin_df, col, iv_val``` ```pythonch_df, ch, ch_i_val = calc_iv(dataset,'Credit_history', 'Target')ch_df total abuse normal normal_dist abuse_dist woe iv Credit_history A30 40 25 15 0.021429 0.083333 -1.358123 0.084074 A31 49 28 21 0.030000 0.093333 -1.134980 0.071882 A32 530 169 361 0.515714 0.563333 -0.088319 0.004206 A33 88 28 60 0.085714 0.093333 -0.085158 0.000649 A34 293 50 243 0.347143 0.166667 0.733741 0.132423","link":"/2019/04/25/Fearuture-Selection-Information-Value/"},{"title":"PCA","text":"Dimensional Reduction에 쓰이는 PCA에 대해 알아보자PCA(Principal Component Analysis)데이터 분석을 하다보면 답답한 경우가 자주 발생한다. 모델을 돌려야 하는데 feature가 너무 많아서 연산 코스트가 너무 많이 들고, 계산하는데 너무 오랜 시간이 걸리는 것이다. 결과를 봤더니, 복잡한 feature때문에 지저분하게 나오고, 노이즈도 많이 껴있는 것 같다. PCA는 이런 경우에 자주 사용되는 알고리즘이다. PCA는 그러니까 relative하지만 redundant한 feature를 제거하는데 자주 사용되거나 데이터를 단순화 할때 사용된다. 데이터를 단순화하는데는 다음의 두 가지 방법이 있다. 차원 축소(Dimensional Reduction) : 데이터를 표현하는 속성의 수를 축소 요소 분석(Factor Analysis) : 관찰 가능한 데이터 = 잠재적인 변수(latent variable)와 noise의 선형결합 우리는 차원 축소에 대한 내용을 살펴볼 것이다. 아까의 상황을 다시 가져와보자. 이전의 예에서 복잡한 feature들은 사실 highly correlated 되어 있기 때문에 문제가 있는 것이다. 변수들의 서로 연관되어 있으면 설명량은 올라가지만, 좋은 모델이라고 볼 수 없고, 어떤 변수가 타겟에 어떻게 영향을 주는지 알 수 없다. 불필요한(서로 연관되어 있거나, 결과와 상관없는) 변수들은, 변수들을 모으고 분석하는데 드는 비용을 증가시켜서, 예측 모델 또는 분류 모델을 만들 때 전체 비용을 증가시키는 원인이 된다. 따라서 불필요한 변수들을 제거할 필요가 있고, Machine Learning 영역에서는 본래 모델이 가지고 있던 성질이나 정확도를 최대한 유지하면서 차원을 줄이는 방법을 통하여 위에서 설명된 문제점을 해결하려고 한다. 모델의 차원(dimensionality)은 모델에 사용되는 독립(independence) 변수 또는 입력(input) 변수의 개수(number)를 의미한다. 우리가 GLM(Generalized Linear Model)을 사용하는 이유처럼 독립변수가 타겟에 미치는 영향을 제대로 알기 위해서 독립적인 변수가 필요한 것이다. 통계에서 항상 IID 조건을 사용하는 것과 의미가 비슷할 것이다. 정리하자면 PCA를 사용하는 이유는 다음과 같다. feature가 너무 많으면 연산에 사용되는 cost가 너무 높고, 시간도 너무 오래 걸리기 때문에, 변수들을 줄여줄 필요가 있다. 많은 feature들 중에서는 상관관계가 높은 feature들이 있다(high correlated). 이런 feature들은 모델의 설명량은 높일 수 있지만, 모델의 성능은 떨어트릴 수 있다. 또한 우리가 흥미 있어 하는 결과와 상관없는 변수들이 존재할 수 있는 상황이 발생할 수 있다. Dimensional ReductionDimensional Reduction의 핵심 아이디어는, 상관도가 높은(interrelated, correlated) 변수들이 많이 존재하는 데이터 집합의 차원(Dimensionality)을 줄이면서, 동시에 데이터 집합에 존재하고 있는 차이(Variation, 정보)를 최대한 유지하는 것이다. 즉, 차원을 줄이되 “정보 손실을 최소화”하는 것이다. 여기서 정보란 데이터간의 거리, 실제적인 위치를 정보라고 표현한다. 다시말하면 위치, 상대적인 거리를 뜻한다. 하지만 차원축소는 정보의 손실을 어느 정도 감수해야 한다. Dimensional Reduction은 원래 공간에서 데이터가 퍼져 있는 정도를 변환된(축소된) 공간에서 얼마나 잘 유지하느냐를 척도로 삼는다. 원래 공간의 정보가 변환된 공간에서 얼마나 잘 유지하는지는 변환된 공간에서의 데이터의 분산으로 측정한다. 따라서, 변환된 공간에서 데이터의 분산을 최대로 유지 할 수 있는 좌표축을 찾아야 한다. 즉, PCA는 원래 공간에서 데이터가 분산되어 있는 주요한 방향(Principal direction)을 찾는 것이 목표가 된다.여러축으로 구성되어 있는 데이터를 주성분 분석으로 통해 기존의 feature들과는 다른 새로운 축으로써 다시 구성해보되, 분산을 최대로 유지한다. PCA 수행방법PCA에서 데이터가 분산되어 있는 주요한 방향(Principal Component)을 찾는 단계는 다음과 같다. 데이터를 투영(Projection)하기 투영된 공간에서 분산 측정하기 분산의 최대치는 어떻게 찾는가? 데이터를 여러 축에 투영해보면서 투영된 공간에서 분산을 측정하고, 가장 분산이 큰 축을 선택하는 것이 바로 PCA이다.새로운 축이 $u$이라고 했을때, 축으로 이동된 새 데이터 포인트 $X_{new} = u^TX$이다. (𝕦t 𝕩= 𝕦 ⋅ 𝕩 cos𝜃= 𝕩 cos𝜃) 이제 투영된 공간에서 분산을 측정해보자. 먼저, PCA를 실행하기 전에 데이터의 평균(mean)과 분산(variance)를 정규화(standardization) 해 준다.(Pre-process the data) 데이터는 특성 벡터로 표현되는데, 특성 벡터의 각 원소들에서 각각의 평균과 빼 주고, 분산의 제곱근으로 나누어 준다. 정규화 과정에서 데이터에서 평균을 빼는것:데이터의 평균이 0이 되도록 만든다. 데이터에서 분산의 제곱근을 나누어 주는 것 : 데이터의 값들이 unit variance를 갖게 해 준다. 새 축으로 이동된 데이터의 분산 구하기각각의 attribute의 평균이 0이 되고, 분산이 1이 된다. 즉 같은 “scale”을 가지게 되어, attribute간의 비교가 가능 해진다.데이터 포인트 $x_{1}, x_{2}, x_{3}, x_{4}, x_{5}$가 있을 때, u의 축으로 투영된 데이터 포인트$x_{1}^Tu, x_{2}^Tu, x_{3}^Tu, x_{4}^Tu, x_{5}^Tu$의 분산을 구해보자. 먼저 평균값을 구해놓자. $$\\mu={1\\over{m}}\\sum_{i=1}^{m}x_i^Tu = 0$$투영된 공간에서의 기댓값은 0이다. 왜냐하면 데이터 포인트들은 이미 standardizing을 한 상태이기 때문이다. 평균의 평균을 구하니까 0이 되는 것이다. 분산을 구해보자. $$\\sigma^2={1\\over{m}}\\sum_{i=1}^{m}(x_i^Tu - \\mu)^2 ={1\\over{m}}\\sum_{i=1}^{m}(x_i^Tu)^2$$($\\mu$가 0이므로) $$={1\\over{m}}\\sum_{i=1}^{m}(u^Tx_ix_i^Tu) = u^T({1\\over{m}}\\sum_{i=1}^{m}(x_ix_i^T))u$$ ($u$는 unit vector이다.) 이것은 결론적으로$$=u^T({1\\over{m}}\\sum_{i=1}^{m}(x_i-\\mathbb{o})(x_i-\\mathbb{o})^T)u$$ $$=u^T\\sum u$$식이 도출된다. Σ는 공분산 행렬로 기존 데이터의 공분산 행렬을 사용한다.결국 투영하려고 하는 축과 기존 데이터의 공분산 행렬의 곱으로 간단하게 새 축의 분산을 구할 수 있다. 분산의 최대치 구하기우리는 Principal Component, 즉, 주성분을 구하는 것이 목적이므로, 데이터의 분산이 최대가 되도록 만드는 축을 구해야 한다. 분산의 최대치를 구하기 위해서 변환된(투영된) 공간에서 분산을 최대화 해 줄 수 있는 벡터 $u$를 찾아야 한다. $u$는 unit vector라고 생각하자. 우리가 구하고자 하는 $u$는 방향이 중요하기 때문이다. 즉, $u^Tu = 1$이다. 따라서 문제는 $u$가 unit vector일 때의 $u^T\\sum u$의 최대값을 구하는 조건부 최적화 문제가 된다. $$\\max u^T\\sum u$$ $$s.t \\space u^Tu=1$$ 이 문제는 라그랑지 승수 (Laglange Multiplier)를 이용해 해결 할 수 있다.$$\\mathcal{L}(u,\\lambda)= u^T\\sum u - \\lambda(u^Tu-1)$$ $\\mathcal{L}(u,\\lambda)$를 미분해서 $u$의 최대치를 구한다. 이렇게 구한 식을 나타내면$$u^T\\sum u = u^T\\lambda u=\\lambda u^T u=\\lambda$$ 즉, 분산을 최대화 하는 문제는 Σ의 eigenvalue를 최대화 하는 문제가 된다.$$argmax_{u}u^T\\sum u = argmax_{u}\\lambda$$ 따라서, 변환된(축소된) 공간에서 분산의 최대값은 Σ의 eigenvalue의 최대값이다. 분산의 최대값은, 𝕦가 Σ의 eigenvalue 중 가장 큰 값을 가지는 eigenvalue에 대응되는 eigenvector일 때 달성된다. 우리는 이것을 주성분이라고도 부른다. 이 다음의 주성분을 구하는 것은 간단하다. D차원에서 주성분은 데이터 공분산 행렬의 가장 큰 eigenvalue에서 부터 D번째로 큰 eigenvalue까지에 대응되는 D개의 eigenvector가 될 것이다.","link":"/2019/05/23/PCA/"},{"title":"SQL_Recipe_01","text":"데이터 분석을 위한 SQL 레시피 with MySQL데이터 분석을 위한 SQL 레시피의 3장에 있는 코드 내용들을 실습하고 MySQL코드로 변형시켜보았다.데이터 분석을 위한 SQL 레시피 책에서는 PostgreSQL, Redshift, BigQuery, Hive, SparkSQL의 코드를 다룬다. 책에 있는 코드는 어떤 건 그대로 쳤을 때 돌아가고, 몇몇 개는 MySQL 쿼리대로 수정을 해주어야 한다. 3장의 mst_users_with_dates 테이블을 가지고 실습을 진행한다.실습 진행 전에 테이블을 만들어준다. 123456DROP TABLE IF EXISTS mst_users_with_dates;CREATE TABLE mst_users_with_dates ( user_id varchar(255) , register_stamp varchar(255) , birth_date varchar(255)); 1select * from mst_users_with_dates; 위 코드로 테이블이 잘 만들어졌는지 확인해본다. 잘 만들어졌으면, 데이터를 삽입한다.123456INSERT INTO mst_users_with_datesVALUES ('U001', '2016-02-28 10:00:00', '2000-02-29') , ('U002', '2016-02-29 10:00:00', '2000-02-29') , ('U003', '2016-03-01 10:00:00', '2000-02-29'); 먼저 날짜 데이터들의 차이를 계산해보자. 현재 날짜와 등록한 날짜를 빼주는 방식이다.1234select user_id, CURRENT_DATE AS today, date(timestamp(register_stamp)) AS regitser_date,datediff(CURRENT_DATE(), date(register_stamp)) AS diff_daysfrom mst_users_with_dates ; 이렇게 만들어주면 원하는 결과가 나온다. 책에 있는 결과와 조금은 다를 수 있는데, 왜냐하면 CURRENT_DATE를 하면 현재의 날짜를 가져와 주기 때문에, 책에 있는 2017-02-05가 아니라, 지금 작성하고 있는 2019-05-21로 계산된다. 여기까지는 datediff함수가 MySQL에도 있기 때문에 책에 있는 그대로 쳐도 잘 돌아간다. 이번에는 사용자의 생년월일로 나이를 계산해보자.나이를 계산하기 위한 전용함수가 구현되어 있는 것은 PostgreSQL뿐이다. PostgreSQL에는 age함수가 구현이 되어있어 편하게 나이를 구할 수 있다. MySQL의 경우에는 책에 있는 코드를 MySQL의 언어로 변형시켜 주어야 한다. 12345SELECT user_id, CURRENT_DATE AS today, date(register_stamp) as register_date, birth_date,(YEAR(CURRENT_DATE)-YEAR(birth_date))- (RIGHT(CURRENT_DATE,5)&lt;RIGHT(birth_date,5)) AS age,(YEAR(register_stamp)-YEAR(birth_date))- (RIGHT(register_stamp,5)&lt;RIGHT(birth_date,5)) AS register_ageFROM mst_users_with_dates; 책에 있는 코드와 다른점은 EXTRACT를 사용하지 않았다는 것이다. MySQL에는 EXTRACT가 없기 때문에 년도를 이용해 일일이 계산해 주어야 한다. YEAR함수를 이용해 년도만 가져와서 계산해준다. 주의해야 할 점은, YEAR함수에 today를 넣어주는 게 아니라 CURRENT_DATE를 넣어주어야 한다는 것이다. today를 넣어주면 syntax에러가 발생한다. 하지만 YEAR로 계산한 경우 연 부분만의 차이가 계산되므로, 해당 연의 생일을 넘었는지 제대로 계산이 되지 않는 문제가 발생한다. 12345SELECT user_id,substring(register_stamp, 1, 10) as register_date, birth_date, floor(( cast(replace(substring(register_stamp, 1,10), '-', '') AS unsigned) - cast(replace(birth_date, '-', '') AS unsigned)) / 10000) as register_age, floor(( cast(replace(CAST(CURRENT_DATE as signed), '-', '') as unsigned) - cast(replace(birth_date, '-', '') AS unsigned)) / 10000) as current_agefrom mst_users_with_dates; 이 코드로 실행시켜주면 문제가 해결된다. MySQL에서는 CAST함수 실행시에 주의해야 할 점이 있는데, 보통 프로그래밍 언어에서는 Integer나 String등으로 타입을 정해주는데, MySQL에서는 UNSIGNED–&gt;INTEGER이고, SIGNED–&gt;STRING임을 명심해야 한다. 코드를 바꿔주고 실행하면 문제없이 돌아가는 것을 확인할 수 있다.","link":"/2019/05/21/SQL-Recipe-01/"},{"title":"시계열 분석 시리즈 두 번째","text":"시계열 데이터 분석의 특성들 Dive into 시계열 데이터 분석시계열 데이터 분석에 대해서 공부해보자 02시계열 데이터도 그렇고 데이터 분석 자체도 그렇습니다만, 내가 어떤 문제를 풀어야 하는지 알게 되면, 문제에 대한 기획/접근/해결 방향은 단순해집니다. 문제가 무엇일까? 내가 사용가능한 알고리즘은? 알고리즘의 입력과 형식은? 알고리즘의 출력은? 보통 이런 4단계를 거치게 되겠습니다. 항상 알고리즘을 사용하기 전에 위의 4가지 물음에 대해서 고민해 보는 게 좋을 것 같습니다. 학습하는 방향에 따라서 알고리즘은 달라집니다. 대표적인 알고리즘 유형은 다음과 같습니다. Supervised Learning Regression Instance Based Regularization Tree Bayesian ANN Unsupervised Learning Clustering Association Rule Dimensionality Reduction Ensemble Deep Learning 머신러닝에 대해서는 어느정도 알고 있습니다. 그렇다면 머신러닝과 시계열 분석의 차이는 무엇일까요? 큰 차이점은 바로 시간축을 고려하는가? 아닌가?에 관한 것입니다. 시계열 분석은 시간축에 대해서 고려를 해야하고, 학습데이터와 테스트 데이터를 나눌때도 시간축을 신경 써서 나누어야 합니다. 시계열 데이터의 독자적인 성분들 7종시계열 데이터를 다루다 보면 시계열 데이터만의 독특한 성분들이 발견됩니다. 대표적인 7종을 알아보겠습니다. 빈도빈도는 말 그대로 frequency를 나타내는 말입니다. 하지만 조금 다른게 있다면, 계절성 패턴이 나타나기 전까지의 데이터 갯수로 사람이 정해야 한다는 점입니다. 일 단위, 월 단위, 연 단위로 나누어서 빈도를 측정합니다. 나누는 기준은 따로 없지만, 비즈니스 사이클이 복잡할수록 작은 시간축, 단순하다면 크게 월로 해도 어느정도 맞아들어갑니다. 혹시 작업을 하다가 시간축이 안맞아 Null이 발생할 수 있는데, 파이썬의 여러 메서드를 사용한다면 금방 채울 수 있습니다. bfill(Backward fill)이나 ffill(Forward fill)등을 이용하면 됩니다. 추세추세는 트렌드를 말합니다. 시계열이 시간에 따라 증가, 감소 또는 일정 수준을 유지하는 경우를 말하며, 아마존의 주식 챠트를 보면 우상향하는 그래프를 볼 수 있는데, 그런 경향을 바로 트렌드라고 합니다. 확률과정(Yt)이 추정이 가능한 결정론적 추세함수 f(t)와 정상확률과정(Yt)의 합으로 나타내어집니다.$$Y_t = f(t) + Y_t(Seasonal)$$추세를 제거하게 되면 추세를 제외한 함수로 예측하는 것이 일반적이고 상한, 하한을 만들어 놓습니다. X에 대해서는 일/월/년 인지 모르기 때문에 frequency 세팅을 꼭 해주어야 합니다. 계절성계절성은 일정한 빈도로 주기적으로 반복되는 패턴(m), 특정한 달/요일에 따라 기대값이 달라지는 것을 말합니다. 주기적 패턴이 12개월마다 반복된다면 m=12로 설정합니다. $Sin$함수나 $Cos$함수를 그려보면 이해가 빠를 것 같습니다. 주기주기는 계절성과 많이 헷갈릴 수 있습니다. 주기는 일정하지 않은 빈도로 발생하는 패턴을 말합니다. 계절성과는 일정한 빈도라는 부분에서 차이가 납니다. 빈도가 1일 때도 발생가능하며, 쉽게말해 Variance가 비슷하면 계절성이고, 다르면 주기(Cycle)이라고 볼 수 있습니다. 더미변수더미변수는 이진수의 형태로 변수를 생성하는 것으로 휴일, 이벤트, 캠페인, Outlier 등을 생성할 수 있습니다. 범주형 변수의 기준값을 미리 결정해야 하며, 기준 값을 제외한 채 더미변수를 생성합니다.(N개의 변수라면 하나는 빼주어야 합니다.) 각 더미변수의 값을 0 또는 1로 채우며 1은 각 더미변수의 정의와 같음을 의미합니다. 물론 더미변수는 N-1개를 만들어야 하지만, 더미변수는 해석에 자신이 있을때의 경우이고 자신이 없다면 N개 만들어도 됩니다. 지연값(Lag)Lag는 변수의 지연된 값을 독립변수로 반영합니다. 과거의 X를 현재에 반영하고 싶을 때 사용하고 python의 shift를 이용해서 간단하게 Lag를 만들 수 있습니다. shift(1) 바로 이전의 것, shift(2) 두 단계 전의 값. ARIMA, VAR, NNAR 등이 활용합니다. 시간변수시간 변수를 미시/거시적으로 분리하거나 통합하여 생성된 변수를 말합니다. 시계열 구성요소는 각 변수의 시간패턴을 알아내는 데 중요합니다. Feature Engineering을 통해 생성된 변수의 input형태로 모델 선택을 하는 데 필요합니다. 생성된 변수의 패턴이 이전 모델에서 발견되지 않은 패턴이라면 모델의 성능을 높일 수 있습니다. 또한, 예측 성능 뿐 아니라, 결과를 해석하고 해당 속성을 분석하며 가능한 원인 식별에 도움이 됩니다. 기본적으로 시계열 데이터 분석은 Numpy와 Pandas를 가지고 분석을 하게 됩니다. statsmodels라는 패키지의(sm)sm.tsa.seasonal_decompose를 통해 분석을 하게 됩니다. 여기서 parameter를 살펴보면, additive와 multiplicative모델을 고르는 부분이 있습니다. 뭘 선택해야 할지 고민을 할 수 있습니다. 일반적으로 additive모델을 주로 사용합니다. multiplicative모델은 언제 사용하느냐 하면, y가 %, 즉 비율로 표현되어 있을 때 사용하면 됩니다.sm.tsa.seasonal_decompose를 통해서 개략적으로 파악하고, 잔차까지 확인합니다. Residual을 확인하면서 추출해 나가야합니다. Residual을 봤는데, Y = T + S + Residual이므로Residual은 Y-T-S가 됩니다. 이런식으로 잔차를 확인하고, 잔차에 트렌드가 있으면 트렌드를 잘못 추출한 것이고 잔차에 계절성이 있으면 계절성을 잘못 추출한 것이라고 판단하면서 분석의 방향을 잡아나아가면 됩니다.","link":"/2019/09/08/Time-Series02/"},{"title":"Classification Metrics","text":"Classification Metrics분류 모델의 평가지표에 대해서 알아보자Classification 모델을 만든 후에 모델의 성능이 어떤지 알기 위해서는 성능 평가 지표가 필요하다.분류 모델의 성능지표를 알아보면서, 데이터의 상태에 따라서 어떤 지표를 사용해야하는지 공부해보자. 0과 1로 결정값이 한정되는 이진 분류 성능 평가 지표에 대해서 집중적으로 다뤄보자. 분류 성능 평가 지표 정확도(Accuracy) 오차행렬(Confusion Matrix) 정밀도(Precision) 재현율(Recall) F1스코어 ROC AUC 정확도(Accuracy)정확도는 실제 데이터에서 예측 데이터가 얼마나 같은지를 판단하는 지표이다.$$Accuracy = {TP+TN\\over TP+TN+FP+FN}$$정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표이며, 기본적으로 많이 사용하는 지표중 하나이다.하지만 정확도는 치명적인 약점이 존재하는데, 바로 불균형한 데이터 셋에서는 제대로 평가가 안된다는 것이다. 예를 들어보자 1000개의 샘플에 10개만 문제가 있는 샘플이다. 이럴 경우에 엉터리 분류기, 즉 모든 샘플에 대해서 정상이라고 분류하는 분류기를 이용해서 분류하고 정확도로 성능 평가를 한다면, 결과는 990/1000, 99%의 정확도를 보이게 된다.엉터리 분류기가 과연 좋은 분류기일지 생각해보자. 만약 이 분류기에 문제가 있는 샘플을 더 추가한다면, 정확도는 기하급수적으로 떨어지게 될 것이다. 오차행렬(Confusion Matrix)오차행렬은 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리고 있는지도 함께 보여주는 지표이다. 즉, 이진 분류의 예측 오류가 얼마인지와 어떤 유형의 예측 오류가 발생하고 있는지를 같이 나타내 주는 지표이다. 오차행렬은 다음과 같이 표현한다. Negative(0) Positive(1) Negative(0) TN(True Negative) FP(False Positive) Positive(1) FN(False Negative) TP(True Positive) 위의 표에서 진하게 표시된 것이 예측 클래스에 대한 것이고(Predicted Calss) 옅게 표시된 것이 실제 클래스(Actual Class)이다. TP는 예측값을 Positive값 1으로 예측했고, 실제 값 역시 Positive값 1 TN는 예측값을 Negative 0으로 예측했고, 실제 값 역시 Negative값 0 FP는 예측값을 Positive값 1으로 예측했고, 실제 값은 Negative 값 0 FN는 예측값을 Negative값 0으로 예측했고, 실제 값 역시 Positive값 1 오차행렬을 기반으로 해서 정확도의 식을 다시 보면 결국, True에 해당하는 값인 TP와 TN에 값이 좌우되고 있다는 것을 알 수 있다. 정확도 = 예측 결과와 실제 값이 동일한 건수 / 전체 데이터 수 라고 다시 말할 수 있다. 불균형한 이진 분류 데이터 셋에서는 Positive 건수가 매우 작기 때문에 이러한 데이터로 학습된 ML 알고리즘은 Positive보다는 Negative로 예측 정확도가 높아지는 경향이 발생한다. TN값이 높아진다는 것이다. 결과적으로 불균형 데이터 셋에서는 Positive에 대한 예측 정확도를 판단하지 못하고 Negative에 대한 예측 정확도만으로 분류의 정확도가 매우 높게 나타나는 수치적인 판단 오류를 일으키게 된다. 이런 판단 오류를 극복하기 위해서 정밀도(Precision)와 재현율(Recall)이 성능지표로 사용된다. 정밀도와 재현율 (Precision and Recall)정밀도와 재현율은 다음과 같은 공식으로 정의된다.$$Precision = {TP \\over FP+TP}$$$$Recall = {TP \\over FN+TP}$$정밀도는 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율을 뜻한다. 정밀도의공식에서 분모는 예측을 Positive로 한 모든 데이터 건수이다. Positive 예측 성능을 더욱 정밀하게 측정하기 위한 평가 지표로 양성 예측도라고 불린다. 재현율은 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율을 뜻한다. 공식의 분모는 실제 값이 Positive인 모든 데이터 건수이다. 민감도 또는 TPR(True Positive Rate)라고도 불린다. 정밀도와 재현율은 중요하게 생각하는 부분이 서로 다르기 때문에, 주어진 업무 특성에 따라서 특정 평가 지표가 더 중요한 지표로 간주될 수 있다. 재현율이 중요한 경우를 생각해보자. 재현율이 중요 지표로 사용되는 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면 크리티컬한 영향이 발생하는 경우이다. 예를 들어 암 판단 모델은 재현율이 중요한데, 실제 Positive인 경우, 즉, 암환자를 Negative, 정상으로 분류하는 경우 오류의 대가가 생명이 될 수 있을 정도로 치명적이다. 만약 정상환자를 암환자로 분류하는 경우에는, 재검진을 하는 정도의 비용이 소모된다.(Positive–&gt;Negative로 잘못분류) 정밀도가 중요한 경우를 생각해보자. 스팸메일 여부를 확인하는 예를 들어보면, 실제 Positive인 스팸 메일을 Negative 정상 메일이라고 분류하게 되면 사용자가 불편함을 느끼는 정도지만, 정상메일을 Spam으로 분류해 버리면 업무메일 등이 스팸으로 처리되어 메일을 받지 못하게 돼 업무에 차질이 생길 수 있다.(Negative–&gt;Positive로 잘못분류) 정리하자면, 재현율이 더 중요한 경우, 실제 Positive 양성 데이터 예측을 Negative로 잘못 판단하게 되면 업무 상 큰 차질이 발생하는 경우 정밀도가 더 중요한 경우, 실제 Negative 음성 데이터 예측을 Positive로 잘못 판단하게 되면 업무 상 큰 차질이 발생하는 경우 공식을 다시살펴보면, Precision은 FN이 분모에 사용되고, Recall은 FP가 분모에 사용된다. 재현율은 FN을 낮추는 데, 정밀도는 FP를 낮추는 데 초점이 맞춰진다. 가장 좋은 것은 둘다 높은 것인데, 두 성능 지표가 상호 보완적이기 때문에 Trade off가 존재한다. 정밀도/재현율 트레이드 오프정밀도나 재현율은 분류의 결정 임계값을 조정해 정밀도나 재현율의 수치를 높일 수 있다. sklearn의 분류 모델들에서 threshold를 조절할 수 있는 파라미터를 찾아보면 된다. threshold값을 낮추면 보통 재현율 값이 올라가고 정밀도 값이 떨어진다. threshold값은 Positive 예측값을 결정하는 확률의 기준이 되고 낮출 수록 True값이 많아지기 때문이다. Positive 예측값이 많아지면 상대적으로 Recall 값이 높아진다. 양성 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 횟수가 상대적으로 줄어들기 때문이다(FN값이 떨어진다). 임계값 증가하면 Negative 예측 값이 증가한다(FP값이 떨어짐) ==&gt; Precision 증가 임계값 감소하면 Positive 예측 값이 증가한다(FN값이 떨어짐) ==&gt; Recall 증가 정밀도와 재현율의 맹점Positive 예측의 임계값을 변경함에 따라 Precision과 Recall의 수치가 변경되는 것을 확인해 봤다. Threshold의 이런 변경은 업무 환경과 목적에 맞게 두 수치를 상호 보완할 수 있는 수준에서 적용되어야 한다. 단순히 성능지표로서 숫자를 올리는 수단으로 사용되면 안된다. 정밀도 100% 만들기확실한 기준이 되는 것만 Positive로 예측하고 나머지는 모두 Negative로 예측한다. 정밀도 = TP / (TP+FP) 이다. 예를 들어 암환자를 예측한다고 해보자. 전체환자 1000명 중에 확실한 Positive 징후만 가진 환자가 단 1명이라면(죽기 일보직전의) 한명만 Positive로 예측하고 나머지는 모두 Negative로 예측하더라도 FP는 0, TP는 1이기 때문에, 정밀도는 1/(1+0)으로 100%가 된다. Precision은 100%지만, 초기 암진단을 예측하는 경우는 희박하고, 위험한 정도의 암환자도 정상이라고 분류할 수 있기 때문에 좋은 분류기라고 할 수 없을 것이다. 재현율 100% 만들기모든 환자를 Positive로 예측하면 된다. 재현율 = TP / (TP+FN)이므로 전체 환자 1000명을 다 Positive로 예측하는 것이다. 이 중 실제 양성인 사람이 30명 정도라도 TN이 수치에 포함되지 않고 FN은 아예 0이므로 30/(30+0)으로 100%가 된다. 이렇게 되면 재현율은 100%지만 모델을 정말 신뢰할 수 있는지에 대해 의심이 발생할 것이다. 이런 모델은 정상인 사람도 암 환자로 예측하게 되므로, 재검사 비율을 매우 높이게 된다. 병원에서 재검사 비용을 대줘야 한다면, 혹은 환자로 분류된 사람이 재검사 비용을 내야 한다면, 병원이 손해를 막심하게 보거나, 고객들이 병원에 대해 신뢰를 하지 않을 것이다. 따라서 정밀도와 재현율을 적절하게 고려한 평가 지표가 필요하게 된다. F1 ScoreF1-Score는 정밀도와 재현율을 조화 평균한 지표이다. F1-Score는 정밀도와 재현율이 어느 한 쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. 공식은 다음과 같다.$$F1={2\\over{1\\over{recall}}+{1\\over{precision}}}=2\\times{precision*\\space recall\\over precision+recall}$$ 만일 A 예측 모델의 경우 Precision이 0.9, Recall이 0.1로 극단적인 차이가 나고, B 예측 모델은 Precision과 Recall이 0.5로 큰 차이가 없다면 A의 F1-Score는 0.18이고, B의 F1-Score는 0.5로 B의 모델이 좋은 점수를 얻게 된다. 사실 F1 Score는 Precision과 Recall에 동일한 가중치인 0.5를 적용한 값이다. F-Measure는 $\\beta$를 이용해 가중치를 조절한다. 공식을 살펴보자. $F_\\beta=$$(1+\\beta^2)(Precision * Recall)\\over{\\beta^2 Precision + Recall}$ $\\beta$가 1보다 크면 Recall이 강조되고 1보다 작으면 Precision이 강조된다. 1일때의 점수를 $F_1$점수라고 한다. ROC &amp; AUCROC곡선(Receiver Operation Characteristic Curve)은 수신자 판단 곡선으로, 2차대전 때 통신 장비 성능 평가를 위해 고안된 수치이다. 요즘에는 이진 분류의 성능 평가 지표로 자주 사용된다. ROC Curve는 FPR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선이다. FPR을 x축으로, TPR을 y축으로 잡으면 FPR에 대한 TPR의 변화가 곡선 형태로 나타난다. TPR은 True Positive Rate의 약자이며, Recall을 나타낸다. 따라서 TPR은 TP/(TP+FN) 이다. 민감도라고도 불리며 민감도에 대응하는 지표로 TNR(True Negative Rate)이라고 불리는 특이성이 있다. 민감도(TPR)는 실제값 Positive가 정확히 예측되어야 하는 수준을 나타낸다.(질병이 있는 사람은 질병이 있는 것으로 양성 판정) 특이성은(TNR) 실제값 Negative가 정확이 예측되어야 하는 수준을 나타낸다.(정상인 사람은 정상으로 음성 판정) TNR은 TN/(TN+FP)이며 X축의 기준인 FPR은 FP/(FP+TN)이므로 1-TNR로 표현할 수 있다. ROC 곡선은 FPR을 0부터 1까지 변경하며 TPR의 변화 값을 구한다. Threshold값을 변경하면서, 처음에는 1로 지정해 FPR을 0으로 만든다. Threshold가 1일 때 Positive 예측 기준이 매우 높기 때문에 분류기가 Threshold보다 높은 확률을 가진 데이터를 Positive로 예측할 수 없다. 즉, 아예 Positive로 예측을 하지 않기 때문에 FP가 0이 되어 FPR이 0이된다. FPR = FP/(FP+TN) 반대로, FPR을 1로 만들려면 TN을 0으로 만들면 된다. Threshold를 0으로 지정하게 되면, 분류기가 모든 데이터에 대해서 Positive로 예측을 하게 된다. 이렇게 되면 Negative 예측은 없기 때문에 FPR이 1이 된다. 일반적으로 ROC Curve자체는 FPR과 TPR의 변화 값을 보는 데 이용하고, 분류의 성능 지표로 실제로 사용되는 것은 AUC(Area Under Curve)이다. 이 값은 ROC 곡선 밑의 면적을 구한 것으로, 일반적으로 1에 가까울수록 좋은 수치이다. AUC가 커지려면, FPR이 작은 상태에서 얼마나 큰 TPR을 구할 수 있는 지가 중요하다. 가운데 직선에서 멀어지고 좌상단 모서리로 곡선이 바짝 붙을 수록 직사각형에 가까운 곡선이 되어 면적이 1에 가까워진다. 가운데의 직선은 랜덤 수준의 이진 분류 AUC값으로 0.5이다.","link":"/2019/05/29/Metrics/"},{"title":"2019년도 회고, 2020년을 맞이하며","text":"2019년을 돌아보는 글 2019 회고.2019년은 감사할 일이 많았었던 해, 많은 변화가 있었고, 사람들을 제대로 챙기지 못했었던 해. 상반기 - 예상치 못한 취준상반기는 한 단어로 요약할 수 있을 것 같다. ‘취준. 1월부터 6월까지 패스트 캠퍼스 데이터 사이언스 익스텐션 2학기 조교생활을 하면서 동시에 취업준비를 했다. 사실 1월 2월에는 취업을 당장 준비하지 않을 계획이었다. 강의 들은 것과 프로젝트 진행했었던 내용들을 차분히 정리하고 하반기부터 본격적으로 취업에 도전하려고 했었다. 하지만 상반기 서류 지원이 시작되었고, 한번 넣어보자는 생각으로 데이터 분석 관련해서 인턴 몇개랑 정규직 원서를 제출했다. 인턴이라도 될 줄 알았었는데, 다 떨어져버렸다. 말그대로 광탈. 그래도 교수님께 첨삭받은 지원서들이었는데 어안이 벙벙했다. 그렇게 취업시장의 높은 벽을 느껴버리고는 바로 취업 준비를 시작했다. 패캠 강사분에게도 조언을 구하면서 6월까지 원서를 고치고 제출하고, 시험을 보고 면접만 봤었다. 몇 번을 쓴물을 삼키다가 마지막 남은 회사의 면접에서 ‘여기 안되면 다시 준비해서 내년에 도전해야겠다’라는 생각으로 마음 놓고 들어갔다. 지금까지 했었던 데이터 프로젝트를 차분히 설명하고, 2차 면접까지 진행한 후에 최종 합격 통보가 내게 전달되었고 7월 1일 첫 출근 하게 되었다. 하반기 - 회사에서 혼자 살아남기입사7월부터 지금까지 정신없이 살아오고 있는 것 같다.(글또를 시작한 것도 7월!!!!) 입사한 회사는 이제 데이터 프로젝트를 진행하려는 회사였다. 데이터 사이언스 프로젝트를 위한 준비가 잘 되어있는 회사는 아니었기 때문에, 데이터를 분석하고 모델만 만드는 일은 할 수 없었다. 물론 처음에는 아무것도 몰랐다. OJT를 1주동안 받고 2주차부터 A/B 테스트 자동화 프로젝트를 맡게 되었다. 데이터 조직에 관련된 사람이 나 혼자 뿐이었기 때문에 혼자 리서치하고 코딩하고 결과를 분석했다. 리서치하는 일은 좋았다. 논문이나 글들을 정리해서 방향을 설정하는 것이었기 때문이었다.(물론 수식을 이해하는 부분은 어려웠다) 코딩을 하다 막히면 정말 도와줄 사람이 아무도 없었기 때문에 그 때는 정말 답답해서 자주 옥상으로 올라가 바람을 쐬었다. 처음으로 담배를 필까 고민을 했다. 여차저차 모델을 만들고 정말 싫어하는 Flask를 이용해서 API를 만드는 작업을 했다. 백엔드 쪽은 공부를 얉게 했기 때문에 어떻게 데이터를 전달받고 결과를 출력해서 줘야하는지 이해하는 데 시간이 많이 들었지만, 개발팀의 도움으로 회사 서비스와 이어 붙이는 데 성공했다.(정말 이어 붙이는 데에만…) 이 작업을 진행하면서 어떻게 회사 제품이 서비스 되고 있는지 깊이 알 수 있게 되었고 백엔드와 데이터 엔지니어링에 흥미가 가기 시작했다. 이전에는 무슨 말인지 이해가 하나도 안되어서 해야되는 건 알았지만 하기가 싫었다… 10월 이후10월쯤에 되어서야 혼자있는 데이터팀에 대리님이 오시게 되었고 둘이서 열심히 A/B 테스트 베타 서비스한 결과를 가지고 수정에 수정을 거쳐 드디어 서비스할 수 있는 정도의 제품이 나오게 되었고 화면기획만 되면 내년 초에 오픈할 수 있게 되었다. 동시에 추천 시스템 개발을 10월부터 시작해서 12월 말까지 돌릴 수 있는 알고리즘을 선별해 놓았고 아키텍쳐를 짜는 중이다. A/B 테스트 자동화에 시간을 많이 써서 추천 쪽에 시간을 못 써서 아쉽긴 하지만 데이터 파이프라인을 붙이는 쪽에 신경을 많이 써볼 생각이다. 데이터 엔지니어링…데이터 엔지니어링…잘하고 싶다… 여가 및 개인 공부취미생활은 따로 나가서 하는 건 없는 것 같다. 일주일에 한번 정도 하는 풋살? 그 외에는 집 앞 헬스장에서 살기 위해서 하는 운동과 축구를 잘하기 위해서 운동을 한다. 축구 잘하려고 운동은 정말 가끔했었는데, 입사하고 삼개월 정도 되었을 때 3키로 넘게 쪄버려서 꾸준히 운동을 해야겠다고 마음 먹었다. 운동하는 걸 제외하면 영화보고 음악듣는 게 내 취미생활의 전부인데 뭔가 새로운 걸 해보고 싶은 마음이 있다. 회사에서 마카롱 만드는 걸 배워서 직접 만든 걸 먹어본 적이 있었는데 굉장히 맛있어서 클래스를 한번 들어볼까? 생각해봤다. 생각만했다… 향수를 만들어 보거나 목공예를 하고 싶기도 한데 시간은 없고ㅠㅜㅜㅠㅠㅠ 입사한 첫 달을 제외하면 개인 공부 시간이 많이 줄었다. 처음에야 할 일도 많지 않고, 엄청난 열정을 갖고 시작했기 때문에 출근하는 버스에서도 관련 논문도 챙겨보고 했었지만, 5개월차에 벌써 피곤에 쩔어 버스만 타면 졸게 되어 버려서 기껏해야 동영상으로 설명해주는 영상 몇개 보는 정도에 그치고 있다. 그러다 딥러닝 공부를 제대로 해보고 싶기도 했고, 일해서 번 돈으로 딥러닝 서버를 집에 하나 두고 싶었던 작은 소망이 있어서 거금 200만원을 투자해서 딥러닝용 PC를 사들였다. sanghyub machine이라는 특징없는 이름을 갖고 있지만 아주 애착이 간다. 딥러닝 모델을 돌리기 위해서 세팅은 다 끝났고, 케라스 창시자에게 배우는 딥러닝을 읽으면서 모델을 한번 돌려보고 있다. 기존 노트북에서 하루종일 돌아가던게 쌩쌩 돌아간다. 역시 돈이 최고인 것 같다. 하지만, 진짜 하고 싶었던 건 12월까지 sanghyub machine과 함께 캐글 대회에 참가해보는 것이었다. 이것저것 세팅도 하고 회사일도 조금 바빴기에 미뤄두다가 연말이 되어버렸다. 내년에는 꼭 대회에 참가해서 좋은 결과를 만들어 내고 싶다. 20202020년에는 하고싶은? 목표라고 생각할 만한 것들이 몇개 고정되어 있다. 일적인 부분도 있고 내 개인 생활과 관련된 목표도 있다. 2019년은 닥치는 대로 살다보니 계획적으로 살지 못한 것 같아 아쉬운 것이 있어서, 할 걸 제대로 설정해서 똑바로 살아봐야겠다.(하지만 글또 3기를 한 건 2019년의 큰 계획 중에 하나였다. ‘글을 꾸준히 써보자’, 아주 성공적이진 않지만, 나름 한달에 글은 부끄럽지만 2개는 꾸준히 썼다!) 2020년에 나는 추천 시스템이나 다른 서비스를 위한 데이터 파이프라인 구성을 성공적으로 한다. 퇴근 후 개인 공부시간을 매일 30분 이상 갖는다. 캐글 대회에 도전한다. 캐글 대회에서 동메달 이상의 성과를 낸다. 일주일에 3일 이상 운동한다. 3대 운동 300에 도전한다. 돈을 모아서 피렌체에 간다. 꾸준히 글을 쓴다. 책을 꾸준히 읽자. 일을 즐겁게 한다. 소중한 사람들을 잘 챙기자. 더 구체적인 목표를 더 쓰고 싶은데, 글로 적으려니 제대로 써지지가 않는다. 아직 2020년까지 8일 더 남았으니까 그 전까지 하고싶은 리스트를 더 늘리고, 더 구체적으로 작성해봐야겠다. 2019년 재밌게 살았다. 이번해와는 또 다르게 2020년을 계획하고 기록해서, 2020년 말에는 꽉 찬 느낌이 드는 해로 만들어 봐야겠다. 그리고 글또 3기가 끝나면 4기에 다시 도전해보고 싶다. 처음에 목표했었던 것은 10만원 deposit을 온전히 연말에 돌려받는 것이었는데, 리뷰하는 것을 까먹어서 10만원 돌려받기는 글러버렸다… 리뷰도 꼼꼼히 하고 지금 글쓰는 것과는 조금 다르게 글 구성에 신결을 써서 작성해보고 싶다.","link":"/2019/12/22/adios-2019/"},{"title":"CRAFT 요약 Character Region Awarenness for Text Detection)","text":"이미지 효과가 있는 글자를 인식하는 CRAFT에 대해서 파헤쳐보자 CRAFT (Character Region Awarenness for Text Detection)Clova AI Research, NAVER Corp. CRAFT 논문 이미지에서 정보를 얻기 위한 방법을 찾기 위해 SOTA 논문을 뒤지던 중에 발견한 논문. Naver Clova AI Research 팀에서 작성했다. 이 논문의 요약은 다음과 같다. 어떤 텍스트가 논문에 있는 글처럼 바르게 작성되어 있지 않고, 그래픽 효과로 휘어져 있거나, 크기가 각각 다른 경우에 CRAFT를 사용하면, text detection이 잘 된다는 것이다. 실제 OCR 프로젝트를 해 본 경험이 있다면, 글자가 조금만 틀어져도 인식이 제대로 되지 않는 다는 것을 느낄 수 있다. 그러한 점에서 이 논문이 눈에 띄었고 집중해서 볼 수 밖에 없었다. 그럼 도대체 왜 잘되는 것 일까? 하나 하나 파헤쳐 보자. 1. IntroductionCRAFT는 기본적으로 CNN으로 디자인되어 있고, 여기서 $region$ $score$과 $affinity$ $score$이 나오게 된다. $region$ $score$은 이미지에 있는 각각의 글자들을 위치 시키는 데에 사용되고, $affinity$ $score$은 각 글자들을 한 인스턴스에 묶는데에 사용된다. 문자 수준의 annotation이 부족하기 때문에 CRAFT에서는 약한 정도의 supervised leargning 프레임 워크를 제안한다. 일반적으로 사용하는 텍스트 이미지 데이터 셋에 글자 수준의 groud-truth 데이터가 없기 때문이다. 데이터 셋으로는 ICDAR을 사용했고 그 외에 MSRATD500, CTW-1500 등을 통해 실험했다. 2. Related Work생략 3. MethodologyCRAFT의 주 목표는 일반 이미지에 있는 각 글자들을 정확하게 포착하는 것이다. 이를 위해 딥러닝을 학습시켜서 글자의 위치와 글자들 간의 affinity(글자 옆에 글자가 있는 것 정도로 해석)를 예측한다. 데이터를 학습시킬 때, 이와 관련된 데이터 셋, 즉 글자 하나하나 학습시키는 데이터가 없으므로, 이 모델은 약한 지도학습 방법으로 학습된다. 3.1 ArchitectureVGG-16에 Batch Normalization이 적용된 네트워크를 backbone 모델로 사용했다. 이 모델에는 디코딩 파트에 skip-connection이 있는데 이것은 U-Net과 비슷한 구조로 Low-level의 feature들을 취합한다. 최종 아웃풋에는 score map에 해당하는 두 개의 채널이 있고, 여기에서 위에서 말한 $region$ $score$과 $affinity$ $score$이 나오게 된다. 3.2 Training3.2.1 Ground Truth Label GenerationTraining Image를 위해서 $region$ $score$과 $affinity$ $score$에 대한 ground thruth 라벨을 생성해야 한다. 이 라벨은 글자 단위의 박스를 이용해 만든다. $region$ $score$은 주어진 픽셀이 글자의 중앙에 있을 확률이고, $affinity$ $score$는 인접한 글자들의 공간의 가운데에 있을 확률이다. 이 가운데에 있을 확률을 가우시안 히트맵으로 인코딩한다. 왜냐하면 ground truth 지역이 엄격하게 경계쳐져 있지 않기 때문이다. 이 히트맵은 $region$ $score$과 $affinity$ $score$ 모두에 사용된다. 제안된 ground truth는 receptive field의 크기가 작은걸 사용함에도 불구하고, 모델이 크거나 또는 긴 텍스트를 찾아내는 것을 가능케 한다. 3.2.2 Weakly-Supervised Learning학습에 사용되는 데이터는 단어 단위의 annotation을 가지고 있다. weakly-supervised learning을 사용해서 실제 이미지에 단어 단위의 annotation이 들어오면, 이미지에 글자 부분을 crop하고, 학습된 모델이 이미지에 있는 글자 지역을 예측해 글자 단위의 bounding-box를 만든다. 위의 사진에서 그 구조를 파악할 수 있다. 먼저 단어 단위의 이미지들이 crop된다. 학습된 모델이 $region$ $score$를 예측한다. watershed 알고리즘이 글자 단위로 쪼갠다. crop단계에 사용했던 inverse transform을 사용해서 글자 박스의 좌표들이 원래의 이미지 좌표로 변형해 넣는다. 만약 모델이 부정확한 region score로 학습되게 된다면, 결과의 글자들은 blurred 되어 나타나게 된다. 이것을 막기 위해서, psuedo-GT(제안된 Ground Truth)의 퀄러티를 측정한다. text annotation의 강력한 시그널인 word length를 이용해서 측정하게 되는데, 이것을 이용하면 psuedo-GT의 confidence를 계산할 수 있다. 먼저 첫 번째 식에 있는 변수들에 대해서 설명해보자. $s_{conf}(w)$는 우리가 구하려는 샘플 $w$에 대한 confidence 값이다. $R(w)$와 $l(w)$는 각각 샘플 $w$d에 대한 bounding box 영역과 단어의 길이이다. 글자단위로 쪼개는 과정에서 글자들의 추정된 bounding box들과 이에 상응하는 글자들의 길이를 알아낼 수 있다. 이 길이는 $l^c(w)$로 표현된다. 결국 이 식에서 confidence라 함은, 단어의 길이라는 정보를 이용해서 얼마나 단어의 길이를 잘 인식했는지를 나타내는 수치라고 할 수 있다. 이제 두 번째 식을 보자. $S_c(p)$는 pixel-wise의 confidence map이다. 이 픽셀이 $R(w)$에 속하면 아까 구한 $s_{conf}(w)$를 사용하고, 그렇지 않으면 confidence를 1로 준다. 마지막으로 L을 구하는 식을 보면, $S_{r}^\\star(p)$와 $S_{a}^*(p)$는 각각 pseudo-GT의 region score과 affinity map을 의미한다. 그리고 $S_{r}(p)$와 $S_{a}(p)$는 각각 예측된 region score과 affinity map을 의미한다. 합성 데이터로 훈련시킬 때, 우리는 진짜 ground truth를 얻을 수 있으므로, $S_{c}(p)$는 1로 설정된다. 훈련이 수행되면, CRAFT모델은 각 글자들을 더 정확하게 예측할 수 있고, confidence 점수인 $s_{conf}(w)$ 는 점진적으로 증가하게 된다. 즉, 학습할 수록 confidence가 올라가게 되어 글자가 더 잘 인식된다는 말이다. 학습을 하면서, 만약 confidence score가 0.5보다 낮으면 추정된 글자의 바운딩 박스들은 모델을 학습하는데 악영향을 주기 때문에 무시하도록 설정된다. 3.3 Inference추론 단계에서, 결과물이 다양한 모양으로 전달된다. 예를들어, 단어 박스나 글자 박스들이나 다른 다각형 등으로 나오게 된다. 평가를 위해서 IoU(word-level intersection-over-union)을 사용한다. $QuadBox$를 만들어서 글자를 큰 박스로 인식하고 각 글자들을 polygon을 생성한다. 이 방식을 통해서 휘어져 있는 text 전체에 대해서 효과적으로 다룰 수 있게 된다. 이것은 OpenCV에 있는 $connectedComponents$와 $minAreaRect$를 사용해 만들 수 있다. 4. DiscussionsRobustness to Scale Variance 비록 데이터 셋에 있는 텍스트의 사이즈는 매우 달랐지만, 모든 데이터에 대해서 단일 스케일의 실험을 진행했다. 상대적으로 작은 receptive fiel는 큰 이미지에 있는 작은 글자를 잡는데 적합했고, CRAFT는 이를 통해서 다양한 글자들을 잡아내는데 robust하다는 결과이다. Multi language issue IC17에는 Bangla와 Arabic 글자들이 있지만, synthetic text 데이터 셋에는 Bangla와 Arabic 글자들이 포함되어 있지 않다. 게다가, 두 언어는 각각의 글자들을 개별적으로 세그먼트하기가 힘들다. 왜냐하면 모든 글자들이 필기체로 이루어져있기 때문이다. 그러므로 이 모델을 두 글자들을 분간해 내지 못하고, Latin이나 한글, 중국어, 일본어 등을 찾지 못한다. Generalization ability 3개의 다른 데이터 셋들로, fine-tuning을 하지않고 실험 해 보았을 때, SOTA의 퍼포먼스를 보여주는 것을 확인했다. 이 모델이 한 데이터 셋에 오버피팅 되는 것 보다, 여러 글자들에 일반적인 성능을 내는 것을 증명하는 결과이다. 5. 결론글자 단위의 annotation이 주어지지 않았을 때 각 글자들을 detect하는 CRAFT 모델을 제시했다. $region$ $score$과 $affinity$ $score$로 다양한 모양의 텍스트 모양들을 커버할 수 있다. 여기에 weakly supervised learning을 사용해 pseudo-ground truth를 만들어 내었다. CRAFT는 SOTA에 해당하는 퍼포먼스를 보여줬으며 일반화 성능과 scale 변동에 덜 민감하지만, multi language를 다루지는 못한다는 점에서 한계를 보인다.","link":"/2020/03/29/craft/"},{"title":"Airflow Basic","text":"Airflow의 기본적인 컨셉에 대해서 이해해보자 Workflow 관리! Airflow 컨셉을 알아보자요즘 왠만한 회사에서 Airflow를 안 쓰는 곳이 없습니다. 파이프라인 관련 세션을 들으면 심심치 않게 들을 수 있는 것이 Airflow를 이용한 워크플로우 관리일 것 입니다. 최근 저희 회사에서도 Airflow를 도입했습니다. 여러 세션에서 관련 내용을 기억해두고 정리해 두었다가, 잡 스케쥴을 관리할 필요성이 생겨서 Airflow 도입을 제안했습니다. 현재 Airflow를 이용해서 추천 쪽에 적용하고 있고, 대략 전처리-모델링-Prediction의 플로우를 돌리려고 합니다. test를 계속해서 진행 중이고 시행착오법 끝에 DAG들이 잘 돌아가는 것을 확인하고 있습니다. 오늘 글은 간단하게 Airflow의 컨셉에 대해서 알아보는 내용입니다. 위에서도 등장한, Airflow의 핵심인 DAG에 대해서 알아보고, DAG안에 들어가는 요소들을 살펴보겠습니다. Workflow? Airflow?Airflow는 AirBnB에서 만든 Workflow 관리 툴입니다. workflow라고 하면, 대략적으로 ‘아 작업의 흐름’이라고 할 수 있겠습니다만, workflow를 조금 더 자세히 설명하자면 워크플로우는 작업 절차를 통한 정보 또는 업무의 이동을 의미하며, 더 자세히 말하면, 워크플로는 작업 절차의 운영적 측면이라고 할 수 있습니다.(출처 위키피디아) 여기서 업무라는 것이 등장합니다. Airflow에서 업무는 Task라고 합니다. 이 Task들이 연결 된 것이 Workflow고 이것을 관리하는 것이 Airflow입니다. 쉽게 말하자면, Airflow는 Task들을 잘 연결시키고 관리하는 툴이라고 볼 수 있겠습니다. Airflow를 활용하는 예는 다양합니다. 데이터 분야에서는 ETL 파이프라인에 사용해 데이터들을 관리할 수 있고, 모델의 학습주기를 관리할 수도 있습니다. 학습된 모델을 이용해 prediction해 배치성으로 결과들을 주기적으로 저장해 놓을 수도 있겠네요. 마케팅 도메인에서 활용한다면, 마케팅 자동화에 적용해서, 이메일을 자동으로 보내주는 일 등에도 활용할 수 있겠습니다. 어떤 작업이 성공했을때 다음 작업을 하게 한다거나(의존성, branching) 등의 작업도 가능하기 때문에 굉장히 다양하게 활용할 수 있습니다. DAG방금 전까지 Airflow가 어떻게 흐르는지 알아봤습니다. 이 흐름은 어떻게 만들까요? Airflow에 대해서 찾아보면 다음과 같은 가지들을 볼 수 있습니다. 간단한 DAG라 보기 굉장히 편합니다. 하지만 DAG를 어떻게 짜느냐에 따라, 작업이 얼마나 복잡하냐에 따라 DAG는 복잡하게 변할 수 있습니다.그래도 이렇게 눈으로 보고 확인할 수 있으니 얼마나 편한지 모르겠습니다. 이걸 airflow없이 코드로 일일이 보고 작업 스케쥴 관리를 하려면 몸과 마음이 지쳐 월요일부터 글또 채널에 pass권을 사용할지도 모릅니다. 다행히도, 우리에겐 Airflow가 있고 Task들을 DAG를 통해서 이어주면 한 눈에 알아볼 수 있습니다. DAG는 Directed Acyclic Graph의 약자입니다. 번역하자면, ‘방향성 비순환 그래프’입니다. 노드와 노드가 단방향으로 연결되고 한번 노드로 향하면, 돌아오지 않는 특성을 가진다는 정도만 알면 될 것 같습니다. DAG는 Python script로 작성되어 있습니다. 따라서 python에 익숙한 분들이라면 DAG작성은 별로 어렵지 않을 것입니다. DAG를 구성하는 요소들에 대한 개념만 알면 쉽게 쉽게 짤 수 있습니다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import jsonfrom datetime import timedelta, datetimefrom utils.slack_alert import SlackAlertfrom airflow import DAGfrom airflow.models import Variablefrom airflow.contrib.operators.bigquery_operator import BigQueryOperatorfrom airflow.contrib.operators.bigquery_check_operator import BigQueryCheckOperatoralert = SlackAlert('#test')# Config variablesdag_config = Variable.get(\"bigquery_github_trends_variables\", deserialize_json=True)BQ_CONN_ID = dag_config[\"bq_conn_id\"]BQ_PROJECT = dag_config[\"bq_project\"]BQ_DATASET = dag_config[\"bq_dataset\"]default_args = { 'owner': 'Jose Lee', 'depends_on_past': True, 'start_date': datetime(2018, 12, 1), 'end_date': datetime(2018, 12, 5), 'email_on_failure': True, 'email_on_retry': False, 'retries': 2, 'retry_delay': timedelta(minutes=5), 'on_failure_callback' : alert.slack_fail_alert}# Set Schedule: Run pipeline once a day. # Use cron to define exact time. Eg. 8:15am would be \"15 08 * * *\"schedule_interval = \"00 21 * * *\"# Define DAG: Set ID and assign default args and schedule intervaldag = DAG( 'bigquery_github_trends', default_args=default_args, schedule_interval='@once' # schedule_interval )# Config variablesBQ_CONN_ID = 'my_gcp_conn'# BQ_PROJECT = 'airflow-268501'# BQ_DATASET = 'github_trends'## Task 1: check that the github archive data has a dated table created for that datet1 = BigQueryCheckOperator( task_id='bq_check_githubarchive_day', sql=''' #standardSQL SELECT table_id FROM `githubarchive.day.__TABLES_SUMMARY__` WHERE table_id = \"{{ yesterday_ds_nodash }}\" ''', use_legacy_sql=False, bigquery_conn_id=BQ_CONN_ID, dag=dag ) 제가 연습하느라 사용했던 DAG를 보면서 설명해보겠습니다. import를 뭘하는지 보면 방금까지 설명한 DAG, 그리고 Variable과 operator가 보입니다. VariableVariable은 Airflow UI를 보면서 설명을 해야 할 것 같습니다. UI에 Variable의 탭이 별도로 존재하기 때문입니다. 위의 화면이 Variable을 보여주고 있습니다. Key와 Value로 이루어진 걸 알 수 있습니다. Key Value로 이루어진 데이터를 이용하고 싶다면, Variable에 등록해두면 편하게 dict로 값을 받듯이 Variable.get를 이용해서 값을 얻을 수 있습니다. 위의 예시에서는 dag_config = Variable.get(&quot;bigquery_github_trends_variables&quot;, deserialize_json=True)BQ_CONN_ID = dag_config[&quot;bq_conn_id&quot;]이 부분이 되겠습니다. 저희 회사의 경우에는 각 고객사의 서비스키와 캠페인키를 Variable에 넣어두고 사용하고 있습니다. 신규 고객사가 발생하면 코드를 일일이 만들 필요없이 간단하게 Variable에 넣어두고 DAG를 태우면 끝입니다! default_argsdefault_args는 dictionary로 이루어져 있고 DAG에 들어가게 됩니다. 여기에 작성한 내용들이 operator들에 적용될 것입니다. 일일이 operator에 넣을 필요없이 default_args에 넣어서 수정하면 되니까 굉장히 간편하게 여러 오퍼레이터에 들어갈 파라미터들을 관리 할 수 있습니다. OperatorDAG가 워크 플로우를 어떻게 run할지를 설명한다면, Operator는 실제 Task가 수행하는 작업을 결정합니다. 실제 Task가 돌아가는 것을 Operator를 통해서 지정해준다고 보면 됩니다. operator의 종류는 굉장히 다양합니다. 대표적으로는 BashOperator : bash 커맨드를 실행시킨다 PythonOperator : python 함수를 호출한다 EmailOperator : 이메일을 보낸다 SimpleHttpOperator : HTTP 요청을 보낸다이런 것들이 있습니다. 위의 코드에서는 BigQueryCheckOperator를 사용했습니다. Bigquery에 접근해서 쿼리를 보내고 싶었기 때문입니다. Google Cloud Platform에 지원되는 다양한 Operator들이 있고, 저희 회사는 AWS를 주로 사용하기 때문에 AWS operator를 많이 사용하고 있습니다. 추가로 SlackAPIOperator 등도 있으니 아이디어만 있다면 왠만한 작업은 다 처리가 가능할 것 입니다. 슬랙 이야기가 나와서 덧붙이자면, default_args에 보면 'on_failure_callback' : alert.slack_fail_alert이 있습니다. 대략 유추가 가능하겠지만, 실패시에 슬랙에 알럿을 띄우려고 만들어 둔 파라미터 입니다. Airflow 작업을 하다가 실패가 나면 슬랙을 통해서 알럿을 보고 대응하기 위해서 작성해 두었습니다. 슬랙에 알람을 보내는 내용은 다음 글에서 다뤄보도록 하겠습니다. 이렇게 간단한 DAG가 만들어지고 airflow_home에 있는 dag폴더에 업로드가 되면 airflow UI에서 DAG가 떠있는 것을 확인할 수 있습니다.(Airflow UIhttp://localhost:8080) Task들의 상태는 다음과 같이 나눠집니다. 이제 이 상태들을 확인해 보면서 워크플로우를 관리해 나가면 됩니다. 이번 글에서는 Airflow의 정말 기본적인 개념들에 대해서 다뤄봤습니다. 간단한 DAG를 직접 작성해보고 success를 한번 띄워보면 Airflow가 어떤 건지 대략적인 감을 잡을 수 있을 것 이라고 생각합니다. 다음 글에서는 슬랙을 통해서 메세지 알람을 보내는 걸 작성해 보려고 합니다. 저도 공부를 하는 입장이라 부족한 내용일 수 있지만, 보시면서 틀린 부분이나 수정할 부분 알려주시면 감사하겠습니다.","link":"/2020/03/14/airflow-start/"},{"title":"클래스에 대해서 알아보자","text":"Class는 객체지향 프로그래밍에서 가장 중요하고도 까다롭다.흔히 말하는 상속이 무엇인지, 어떤 상황에서 상속을 하는지, 상속을 할 수 없을 때는객체 관계를 어떻게 표현하는지 알아보자. 클래스 관계클래스 관계를 나타내는 방법으로 IS-A와 HAS-A가 있다. 1.1 IS-A : 상속IS-A는 ‘은 ~의 한 종류다’를 말한다. 노트북과 컴퓨터를 예를 들어보자. 노트북은 컴퓨터의 한 종류일까?그렇다. 이런 관계일 경우 Computer와 laptop 클래스는 IS-A관계라고 말할 수 있다.IS-A관계 인지 아닌지 분간이 안된다면, ‘한 종류다’라는 의미가 있는지 생각해 보자. 이런 IS-A관계를 프로그램에서 표현할 때는 상속을 사용한다. 상속은 IS-A관계에서 설계가 쉽다. 상속을 하는 클래스와 상속을 받는 클래스를 나눠 볼 수 있는데 표현은 다음과 같다. 상속을 하는 클래스 기본 클래스 부모 클래스 슈퍼 클래스 상속을 받는 클래스 파생 클래스 자식 클래스 서브 클래스 코드로 laptop과 computer 클래스를 설계해 보자.12345678910class Computer: def __init__(self, cpu, ram): self.CPU = cpu self.RAM = ram def browse(self): print('browse') def work(self): print('work') 이 코드에서 인스턴스 멤버는 CPU와 RAM이다. 인스턴스 메서드는 browse()와 일을 하는 work()이다.노트북은 컴퓨터의 모든 멤버와 메서드를 가진다. 노트북에도 CPU와 RAM이 있고, 같은 일을 하기 때문이다.어떤 객체가 다른 객체의 모든 특성과 기능을 가진 상태에서 그 외에 다른 특성이나 기능을 가지고 있다면 상속해서 쓰는게 편하다. 노트북 클래스를 설계해보자1234567class Laptop(Computer): def __init__(self, cpu, ram, battery): super().__init__(cpu, ram) self.battery = battery def move(self, to): print('move to {}'.format(to)) 노트북의 클래스 옆에 Computer가 붙은게 보인다. 이는 컴퓨터 클래스를 상속하겠다는 뜻이다.이렇게 되면 노트북은 컴퓨터 클래스가 가진 모든 멤버와 메서드를 가지게 된다. 노트북도 browse()와work()가 가능하다는 말이다. 확실히 손이 덜 피곤하다는 게 느껴질 것이다. super는 무엇일까? 이것은 기본 클래스를 의미한다. 기본 클래스는 위에서 써놨듯이, 상속을 하는 클래스, 즉컴퓨터 클래스를 가리킨다. CPU와 RAM은 기본 클래스의 생성자를 이용해 초기화가 되었기 때문에 남은 한 멤버인 battery만 할당해 주면 된다. 그리고 노트북에만 있는 move메서드를 입력해준다. 이렇게 되면, 노트북만의 메서드를 하나 갖게 된다. 테스트 코드는 다음과 같다.123456if __name__ == \"__main__\": lap = Laptop('intel', 16, 'powerful') lap.browse() lap.work() lap.move('office') ` 1.2 HAS-A : 합성 또는 통합HAS-A관계는 ‘~이 ~을 가진다 혹은 포함한다’를 의미한다. Computer는 CPU와 RAM을 가지는데,여기서 이 관계를 HAS-A관계라고 부를 수 있다. 경찰과 총의 관계를 생각해보자. 경찰은 총을 가지고 있다. 경찰과 총은 HAS-A관계가 성립한다.주의해야할 점이 있는데, HAS-A관계에는 합성과 통합이라는 표현방법이 존재한다. 컴퓨터와 CPU의 관계를 합성으로 표현하고, 경찰과 총의 관계를 통합으로 표현해보자.12345678910class CPU : pass class RAM : pass class Computer : def __init__(self): self.cpu = CPU() self.ram = RAM() Computer는 인스턴스 멤버 cpu를 가진다. 생성자에서 CPU 객체를 생성해서 멤버 cpu에게 할당한다.이렇게 되면 Computer라는 객체가 생성이 될 때, CPU와 RAM이 같이 생성이 되고, 사라질때 같이 사라지게 된다. 이 둘의 관계는 매우 강한 관계를 맺고 있다고 할 수 있다. 이런 관계를 합성이라고 한다. 경찰과 총의 관계를 살펴보자123456789101112131415161718192021222324class Gun : def __init__(self, kind): self.kind = kind def bang(self): print('bang bang') class Police : def __init__(self): self.gun = None def acquire_gun(self, gun): self.gun = gun def release_gun(self): gun = self.gun self.gun = None return gun def shoot(self): if self.gun: self.gun.bang() else : print(\"Unable to shoot\") 이 관계에서는 Police 객체가 만들어질 때 Gun 객체를 가지고 있지 않다. 이후 acquire_gun()메서드를통해서 Gun 객체를 멤버로 가지게 된다. 이 관계 역시 HAS-A이다. 또한 release_gun()으로 가진 총을반납할 수도 있다. 이 두 메서드를 이용해 총을 가진 경찰, 총이 없는 경찰 모두를 표현할 수 있다. 하지만 컴퓨터 클래스와 다른 점은, 경찰은 언제든지 Gun을 가질 수 있고 해제할 수 있다는 점이다.관계가 컴퓨터에 비해 훨씬 약하다는 느낌이 들 것이다. 이런 약한 관계를 통합이라고 부른다. 2. 메서드 오버라이딩과 다형성(Polymorphism)OOP에서 가장 중요한 개념은 다형성이다(polymorphism). 나는 이 ‘폴리몰피즘’에 대해 노이로제가 걸렸었던 적이 있다.고려대에서 진행한 Bigdata X Campus 교육에서였다. 파이썬 강의를 들으면서 강사는 “뽈리몰피즘! 뽈리몰피즘이 중요하죠!” 라고 열변을 토했고, 매 강의마다 항상 강조되었었다.‘도대체 polymorphism이 뭐길래’ 라는 생각이 들었었고, 이 책을 보면서 그 갈증이 어느정도 해결이 되었다. 다형성이란 ‘상속 관계에 있는 다양한 클래스의 객체에서 같은 이름의 메서드를 호출할 때, 각 객체가 서로 다르게 구현된 메서드를 호출함으로써 서로 다른 행동, 기능, 결과를 가져오는 것’을 의미한다. 이를 구현하기 위해서는 파생 클래스 안에서 상속받은 메서드를 다시 구현하는 메서드 오버라이딩이라고 부른다. 2.1 메서드 오버라이딩먼저 코드를 살펴보자.123456789101112131415class CarOwner: def __init__(self, name): self.name = name def concentrate(self): print('{} can not do anything else'.format(self.name))class Car: def __init__(self, owner_name): self.owner = CarOwner(owner_name) def drive(self): self.owner.concentrate() print('{} is driving now.'.format(self.owner.name)) drive()메서드를 보면 Car 객체는 반드시 차 주인인 CarOwner객체가 운정해야 하고 차 주인은 운전에만집중해야 한다. drive()메서드가 나오자마자 CarOwner객체의 concentrate()메서드를 호출해서 차 주인이 운전외에는 아무것도 못하게 한다. 이번에는 자율주행차를 만들어보자123class SelfDrivingCar(Car): def drive(self): print('Car is driving by itself') 자율주행차에는 상속받은 drive가 어울리지 않는다. 새롭게 바꿔줄 필요가 있다. drive()메서드를 제외하고는 나머지 멤버와 메서드는 그대로 사용한다. 이러한 경우에는 drive()메서드만 클래스 안에서 재정의해준다. 이렇게 클래스 안에서, 맘에 들지 않는 메서드를 재정의 하는 것을 메서드 오버라이딩이라고 한다.자율주행차의 차 주인은 더 이상 운전에 집중하지 않아도 된다. 따라서 오버라이딩된 drive()메서드에서는concentrate()를 호출하지 않는다. 여기서 정리해보자면, drive()메서드는 같은 이름이지만 객체에 따라 다른 기능을 하게 된다. 이처럼같은 이름의 메서드를 호출해도 호출한 객체에 따라 다른 결과를 내는 것을 다형성이라고 한다. 2.2 다형성다형성에 대해 좀 더 깊이 알아보자.12345678910111213141516171819202122232425262728class Animal: def eat(self): print('eat something')class Lion(Animal): def eat(self): print('eat meat') class Cow(Animal): def eat(self): print('eat grass') class Human(Animal): def eat(self): print('eat meat and grass')if __name__ == \"__main__\": animals = [] animals.append(Lion()) animals.append(Cow()) animals.append(Human()) for animal in animals: animal.eat() 이 코드의 Animal 클래스에는 eat()메서드가 있다. 모든 동물은 반드시 먹어야 한다는 가정이다.하지만 동물마다 먹는 종류는 다르기 때문에 육식 동물의 대표로 사자를 설정했고, 초식 동물의 대표로 소를설정했다. 그리고 잡식 동물로 사람을 설정했다. 나는 소고기를 쌈싸먹는 것을 좋아한다. 코드의 반복문의 마지막 부분에서 animal.eat()은 다형성을 구현한 부분이다. animals 리스트에서 객체를 하나씩 불러와 eat()메서드를 호출할 때, 메서드를 호출한 쪽에서는 육식동물인지 초식동물인지 잡식인지 고민할 필요가 없다. 각 객체는 오버라이딩된 메서드를 호출하기 때문이다. 그렇게 되면 여기서는 그냥 무엇인가를 먹는 동물은 없다. 그러니까 Animal은 eat something하는 게 있는데 사용하는 동물이 아무도 없다. 안써버리자니 문제가 되고, 뭔가 낭비같다. 이럴 때는 Animal 클래스를 추상 클래스로 만들면 된다. 추상 클래스는 독자적으로 인스턴스를 만들 수 없고 함수의 몸체가없는 추상 메서드를 하나 이상 가지고 있어야 한다. 또한 이 클래스를 상속받는 파생 클래스는 추상 메서드를 반드시 오버라이딩 해야한다. 당연히 아무것도 없으니까! Animal을 추상 클래스로 변경해보자.1234567from abc import *class Animal(metaclass = ABCMeta): @abstractmethod def eat(self): pass ... 먼저 abc모듈을 가져온다.(abstract base class) 그 후 @abstractmehod 데코레이터를 붙여준다.여기서 메서드 구현하는 부분을 pass로 비워두면 eat()은 추상 메서드가 된다.이제 Animal을 상속받는 모든 파생 클래스는 eat()을 오버라이딩 해야한다. 클래스 설계 예제클래스를 설계할 때느느 다음 두가지를 고려해야 한다. 공통 부분을 기본 클래스로 묶는다. 부모가 추상클래스인 경우를 제외하고, 파생 클래스에서 기본 클래스의 여러 메서드를 오버라이딩한다면 파생 클래스는 만들지 않는 것이 좋다. Character 클래스 만들기게임 캐릭터를 만들어보면서 클래스를 정리해보자.게임에 등장하는 캐릭터는 플레이어 우리 자신과 몬스터이다.모든 캐릭터(추상 클래스)는 다음과 같은 특성을 지닌다. 인스턴스 멤버 : 이름, 체력, 공격력을 가진다. 인스턴스 메서드 : 공격, 공격당했을 때는 피해를 입는다.(모두 추상 메서드로 구현한다.) 123456789101112131415161718from abc import *class Character(metaclass = ABDMeta): def __init__(self, name, hp, power): self.name = name self.hp = hp self.power = power @abstractmethod def attack(self, other, attack_kind): pass @abstractmethod def get_damage(self, power, attack_kind): pass def __str__(self): return '{} : {}'.format(self.name, self.HP) 3.2 Player 클래스 만들기플레이어는 다음과 같은 특성이 있다. 추가되는 멤버 : 플레이어는 다양한 공격 종류를 담을 수 있는 기술 목록이 있다. attack : 플레이어는 공격할 때 공격 종류가 기술 목록 안에 있다면 상대 몬스터에게 피해를 입힌다. get_damage : 플레이어가 피해를 입을 때 몬스터의 공격 종류가 플레이어의 기술 목록에 있다면 몬스터의 공격력이 반감되어 hp의 절반만 깎인다. 12345678910111213141516171819class Player(Character): def __init__(self, name = 'player', hp = 100, power = 10, *attack_kinds): super().__init__(name, hp, power) self.skills = [] for attack_kind in attack_kinds: self.skills.append(attack_kind) def attack(self, other, attack_kind): if attack_kind in self.skills: other.get_damage(self.power, attack_kind) def get_damage(self, power, attack_kind): if attack_kind in self.skills: self.HP -= (poewr/2) else : self.HP -= power 코드에서 살펴보면 플레이어는 캐릭터를 상속했다. 3.3 Monster, IceMonster, FireMonster 클래스 만들기몬스터에는 불 몬스터와 얼음 몬스터가 있으며 다음과 같은 특징이 있다. 추가되는 멤버 : 공격 종류를 가진다. 불 몬스터는 Fire, 얼음 몬스터는 Ice를 가진다. 공통 메서드 : 두 몬스터는 같은 행동을 한다. attack : 공격 종류가 몬스터의 속성과 같다면 공격한다. get_damage : 몬스터는 자신과 속성이 같은 공격을 당하면 체력이 오히려 공격력만큼 증가한다. 그렇지 않으면 체력이 공격력만큼 감소한다. 여기서 고민해야 할 점이 있다. FireMonster 클래스와 IceMonster 클래스를 Character 클래스에서 상속받아 구현할지, Moster 클래스라는 부모 클래스를 따로 만들어야 할지.설명에 따르면 추가되는 멤버도 겹치고, fireball()메서드를 제외한 나머지 메서드도 겹친다. 그러면 공통되는 부분을 기본 클래스로 만들고 이를 상속받는 게 좋을 것 같다. 몬스터를 만들고 몬스터는 캐릭터 클래스를 상속받을 것이다. 그리고 몬스터 클래스를 상속받아 각 몬스터를 만들어보자. 1234567891011121314151617181920212223242526272829303132class Monster(Character): def __init__(self, name, hp, power): super().__init__(name, hp, power): self.attack_kind = 'None' def attack(self, other, attack_kind): if self.attack_kind == attack_kind: other.get_damage(self.power, attack_kind) def get_damage(self, power, attack_kind): if self.attack_kind == attack_kind: self.HP += power else : self.HP -= power def get_attack_kind(self): return self.attack_kind class IceMonster(Monster): def __init__(self, name = 'Ice monster', hp = 50, power = 10) super().__init__(name, hp, power) self.attack_kind = 'ICE' class FireMonster(Monster): def __init__(self, name = 'Fire monster', hp= 50, power = 20) super().__init__.(name, hp, power) self.attack_kind = 'FIRE' def firebreath(self): print('firebreath') 불 몬스터와 얼음 몬스터는 몬스터 클래스를 상속 받았고 추가되거나 변경되는 부분만 수정했다. 12345678910111213141516171819202122232425if __name__ == \"__main__\": player = Player('sword master', 100, 30, 'ICE') monsters = [] monsters.append(IceMonster()) monsters.append(FireMonster()) for monster in monsters : print(monster) for monster in monsters: player.attack(monster, 'ICE') print('after the plater attacked') for monster in monsters: print(monster) print('') print(player) for monster in monsters: monster.attack(player, monster.get_attack_kind()) print('after monsters attacked') print(player) 4. 연산자 오버로딩연산자 오버로딩은 클래스 안에서 메서드로 연산자를 새롭게 구현하는 것으로 다형성의 특별한 형태이다.연산자 오버로딩을 사용하면 다른 객체나 일반적인 피연산자와 연산을 할 수 있다. 12345678910111213141516171819class Point: def __init__(self, x=0, y=0): self.x = x self.y = y def set_point(self, x, y): self.x = x self.y = y def get_point(self): return self.x, self,y def __str__(self): return '({x}, {y})'.format(x = self.x, y = self.y) if __name__ == \"__main__\": p1 = Point(2,2) p2 = p1 + 3print(p2) 결과를 실행해 보면 에러가 발생할 것이다. Point와 int 객체 사이는 덧셈을 할 수 없다고 나온다. 123456789 def __add__(self, n): x = self.x + n y = self.y + n return Point(x,y) if __name__ == \"__main__\": p1 = Point(2,2) p2 = p1 + 3 print(p2) 이렇게 add메서드를 추가해보자. 예약한 함수를 사용해서 x좌표와 y좌표에 인자 n을 더한 새로운 x와 y로새로운 객체를 만들어 반환한다.실행 결과는 (5,5)가 나오게 된다. 1234if __name__ == \"__main__\": p1 = Point(2,2) p2 = 3 + p1 print(p2) 계산이 안된다. int와 Point의 순서가 바뀌면 에러가 발생한다.연산자 오버로딩을 하나 더 해주자. 123456789 def __radd__(self, n): x = self.x + n y = self.y + n return Point(x,y)if __name__ == \"__main__\": p1 = Point(2,2) p2 = 3 + p1 print(p2) 1(5,5) __radd__()메서드를 이용해서 연산이 돌아가도록 만들었다.","link":"/2019/01/28/class/"},{"title":"cross_entropy_KL_divergence","text":"Cross Entropy와 KL-Divergence에 대해서 알아보자Cross Entropy크로스 엔트로피에 대해서 알아보기 전에, 엔트로피 식을 다시한번 확인해 보자엔트로피는 $H(x)=-\\sum P(x)log_2P(x)$로 확률분포 $P(X)$에 대한 기댓값이다. 엔트로피는 확률분포가 있어야 정의가 될 수 있다. 확률 분포의 불확실한 정도를 뜻하는 것이라고 생각하면 된다. 이제 크로스 엔트로피 식을 확인해 보자$H(P,Q)=-\\sum_{X} P(x)logQ(x)$(자연로그 또는 이진로그)식을 자세히 보면, $P(x)$가 들어갈 자리에 $Q(x)$가 들어가 있다. 어떤 의미가 숨어져 있는 것 같은데, 이 수식이 의미하는 것이 무엇일까? 크로스 엔트로피(Cross Entropy)는 실제 데이터는 $P$의 분포로부터 생성되지만, 분포 $Q$를 사용해서 정보량을 측정해서 나타낸 평균적 bit수를 의미한다. 이제 수식이 눈에 들어오기 시작할 것이다. 실제 데이터는 분포 P로 부터 생성되는데, 우리가 실제 P에 대해서 몰라서 분포 Q의 정보(or 코딩 스킴)을 대신 활용하면 어떨까?에 대한 답으로써 만들어졌다고 생각하면 편할 것이다. 크로스 엔트로피는 $H(P,Q)$와 같이 나타내고 일반적으로 $H(P,Q) &gt;=H(P)$이다. 항상 크로스 엔트로피가 크거나 같을 수 밖에 없는 것은 데이터의분포를 Q로 가정한 코딩방식을 사용하게 되면, 실제의 분포 P를 가정한 코딩방식 보다 질의응답에 필요한 코드의 수(code length)가 많아지게 되기 때문이다. KL-DivergenceKL-Divergence는 쿨백 라이블러 발산이라고 불리기도 한다. 이 역시 수식으로 먼저 확인해 보자$D_{KL}(P||Q)=\\sum_{X}P(x)log {P(x)\\over{Q(x)}}$이다. 이 수식을 자세히 보면, Cross entropy 식이 들어가 있는 것을 확인 할 수 있다. 좀 더 풀어서 써보면 $D_{KL}(P||Q)=\\sum_{X}P(x)log{1\\over Q(x)}-P(x)log{1\\over P(x)}$로결국 $H(P,Q) - H(P)$, 즉 P와 Q의 크로스엔트로피에서 P의 엔트로피를 빼준 식이다. 이것은 결론적으로 Q를 이용한 정보량이 P의 분포와 얼마나 차이가 나는 지를 알려주는 것이다. 일종의 분포사이의 거리로 이해를 하면 된다. (KL divergence는 두 확률 분포 P와 Q의 차이를 측정한다. 하지만 엄밀히 말해서 거리는 아니다.) 다른 표현으로 데이터 인코딩 관점에서 보면 KL divergence는 데이터 소스의 분포인 P 대신 다른 분포 Q를 사용해서 인코딩하면 추가로 몇 bit의 낭비가 생기는지 나타낸다고 이해할 수 있다. KL-Divergence는 거리함수가 아니다. 왜냐하면 교환법칙이 성립하지 않기 때문이다. Reverse KL은 별도의 개념으로 사용된다. 하지만, 두 분포가 다를수록 큰 값을 가지며 둘이 일치할 때에만 0이 되기 때문에 거리와 비슷한 용도로 사용할 수 있다.[https://wiseodd.github.io/techblog/2016/12/21/forward-reverse-kl/] Cross Entropy와 KL-Divergence가 어떤 관계에 있느냐고 묻는다면, KL-Divergence의 앞쪽 수식에 크로스 엔트로피가 있으므로, 크로스 엔트로피가 작을 수록, KL-Divergence값이 작아진다. 즉, 두 분포가 가까워진다고 말할 수 있겠다.","link":"/2019/05/15/cross-entropy-KL-divergence/"},{"title":"Time Series Analysis Begins","text":"시계열 데이터 분석의 시작 Dive into 시계열 데이터 분석시계열 데이터 분석에 대해서 공부해보자!저번 주부터 고대하던 시계열 분석에 대해서 본격적으로 공부해보게 되었습니다. 공부한 내용에 대해서 차근차근 정리를 해보는 시간을 갖겠습니다. 분석적 사고의 필요성시계열 분석에 앞서 강조하는 부분이 있었습니다. 데이터 분석에는 6가지 사이클이 있습니다. 문제정의 데이터 수집 데이터 전처리 데이터 정리 데이터 분석 결과 정리 그 중 가장 중요한 것은, 데이터 분석이 아니라 분석적 사고의 필요성입니다. 특히 데이터 사이언티스트에게 중요한 덕목으로서 강사님께서 강조해 주셨는데요, 결국은 문제 정의를 잘하는 것이 중요하다는 것이었습니다. 문제가 잘 정의되면 데이터를 어디서 갖고 올지 어떤 알고리즘을 사용할지 등이 정리가 될 수 있습니다. 사실 캐글 = 데이터사이언스로 보는 사람들이 많은데, 엄밀히 따지면 캐글은 알고리즘을 공부할 수 있는 대회일 수 있습니다. 이미 사전에 문제정의가 끝나고 데이터를 잘 모으고 정리되어서 캐글쪽에 전달되기 때문에 정말 필요한 능력인 문제정의 능력을 키우기는 힘들 수 있습니다.(개인적으로 가장 신선한 충격을 받았던 설명이었습니다) 문제 정의 후에는 이것을 고객들이나 관리자에게 잘 전달해주어야 합니다. 그래서 중요한 것은 설득 및 설명 능력입니다. 사실, 데이터분석 관련지식(이론), 프로그래밍 활용능력(실습) 등은 금방 늘어날 수 있는 능력이지만, 잘 설득하고 이해하기 쉽도록 설명하는 능력은 정말 키우기 어렵습니다. 어쩌면 데이터 사이언티스트에게 커뮤니케이션 능력은 잘 키울 수 없기에 더 중요하지 않을까 생각되기도 합니다. 정확한 문제 정의의 중요성앞서 말씀 드린 것처럼, 정확하게 문제를 정의하고 분석을 시작해야 합니다. 이를 위해서는 가설을 설정하고 검정하는 것이 필요하게 됩니다. 가설 설정의 세가지 조건은 다음과 같습니다. 가설 설정 세가지 조건 상호배반적 증명가능성 구체성 상호배반성은 나의 주장과 대립 주장이 모호함이 없어야 한다는 것으로, 서로 겹치는 영역이 없어야 된다는 것을 말합니다. 증명가능성은 성급화 일반화에 빠지지 않기 위해서는 증명 가능한 것이나, 범위로 내세워야 한다는 것을 말합니다. 마지막으로 구체성은 충분히 구별되고 실현가능한 표현으로 정의되어야 한다는 것입니다. 검정가설을 세웠으면, 이제 그 가설이 맞는지 검정을 해야 합니다. 주의해야할 점은 모집단에 대한 것입니다. 모집단은 논란이 있을 수 있지만, 관찰이 불가능한 것입니다. 이상적인 샘플 집단이기 때문입니다. 그렇기 때문에 우리는 항상 샘플을 갖고 분석을 할 수 밖에 없습니다. 샘플에 Bias가 있다면, Bias를 제거하고 사용합니다. 귀무가설과 대립가설은 통계에서 정말 자주 등장하는 개념입니다. 하지만 익숙하지 않은 단어들이기 때문에 항상 헷갈리는에요, 귀무가설은 기존의 주장(대립 주장), 대립가설은 내 주장 이라고 생각하면 사고하기 편리합니다. 귀무가설과 대립가설을 세웠다면, 이제 통계량을 보고 어떤 가설이 맞는지 확인해야 합니다. 이때 우리가 확인하는 통계량을 검정 통계량이라고 합니다. 대립가설과 귀무가설을 비교하기 위한 검증지표값으로 흔히 ‘점추정’이라고 부릅니다. 검정통계량이 발생가능한 구간에 대해서도 용어가 정리되어 있는데, 이것을 신뢰 구간이라고 부릅니다. 또 귀무가설이 참일 때 검정통계량으로 대립가설이 발생활 확률을 말하는 p-value가 있습니다. 일반적으로 p-value 기준으로 0.05보다 크면 대립가설을 기각하고, 0.05보다 작으면 대립가설을 채택합니다. 통계량분석 단계별 의사결정을 위해서는 수학/통계적 언어를 이해하는 것이 필요합니다. 변동/산포 특성, 지표의 변동성을 나타내는 통계량에는 분산과 표준편차가 있습니다. 분산은 편차제곱의 합을 데이터의 수로 나눈 값이고, 표준편차는 분산에 루트를 씌운 값입니다. (참고로 편차는 관측값과 평균의 차이입니다) 분포의 형태 특성을 나타내는 것으로는 대표적으로 Skewness와 Kurtosis가 있습니다. 왜도와 첨도라고 부를 수 있습니다. 왜도는 평균을 중심으로 데이터가 좌우로 편향되어 있는 정도를 말하고, 첨도는 뾰족한 정도로, 사실 더 중요한 것은 tail의 fat함을 보는 데 사용될 수 있습니다. fat-tail하다면 정규성 가정이 깨지게 되므로(정규성 가정 중 하나 : 분포의 tail은 슬림하다, kurtosis값은 0에 가까울 수록 좋다) 만든 모델이 제대로 작동하지 않을 가능성이 높습니다. 시계열 데이터와 횡단면 데이터이제 데이터에 대한 얘기를 시작하겠습니다. 횡단면 자료(Cross-Sectional data)는 일정시점에서 하나 이상의 변수에 대해 수집된 자료를 말합니다. (예: 2016년 전국 16개 시도의 GRDP와 최종소비) 시계열 데이터는 일별, 주별, 월별, 분기별, 연도별 등 시간에 걸쳐 수집한 자료로 거시경제변수를 측정한 자료에서 많이 발생하는 데이터 입니다. 시계열 데이터는 보통의 데이터에 비해서 레코드(또는 로우)에 타임스탬프 또는 각 시간구간에 따른 집계 레벨(분별, 시간대별, 일별, 주별, 월별, 분기별, 년도별)에 대한 순서가 있는 시간값을 함께 가지고 있습니다. 시계열 데이터는 횡단면 데이터에 비해 고려해야 할 시간축이 하나 더 있는 것이 문제이며 시간축이 선후관계를 가지는 것, 그리고 시간축에 대한 것을 드릴다운하거나 다시 롤업(roll-up)해서 집계 응집도를 높여야 할 수 있습니다.http://intothedata.com/02.scholar_category/timeseries_analysis/ 시계열 데이터 분석을 위한 준비는 이것으로 어느정도 마무리 된 것 같습니다. 이외에도 Anaconda 설정이나, Numpy, Pandas를 다루는 부분이 있지만 블로그 글에서는 생략하겠습니다. 다음 포스팅 부터는 본격적인 시계열 데이터의 계절성이나 주기의 차이점, Residual을 주의깊게 관찰해야 하는 이유 등에 대해서 다뤄보겠습니다.","link":"/2019/08/25/Time-Series-Begins/"},{"title":"글또 3기에 들어서면서... 상반기 회고와 다짐","text":"상반기 회고와 나의 다짐상반기 회고 (2019.01.01 ~ 2019.06.24, 도서관)새해 첫 날은 스페인에서 보냈었네요, 공부만 하다가 처음으로 짬이 나서! 계획했었던 영국~스페인 여행을 2주일 정도 갔었습니다. 항상 아침에 조깅을 하면서 ‘한국 돌아가서 뭘해볼까…’ 이런 고민들을 했었고 그 중 제일 처음으로 해야겠다고 생각했던 것이 블로그였습니다. 일단 블로그부터 제대로 구축하자! 라는 계획으로 hexo 블로그를 만들었고, 여러 테마들을 돌려보면서 괜찮은 것들을 살펴봤습니다. 한 한달정도 블로그랑 씨름하다보니 어느정도 구축이 되었고, 배운 내용들을 글로 정리하기 시작했습니다. 사실 올해에는 취업 생각이 없었습니다. 데이터 이론이나 알고리즘 등 준비해야 할 것도 많다고 생각했고, 개인 프로젝트도 더 필요하다고 느꼈습니다. 하지만 3월에 상반기 대기업 취업 공고가 나니까 마음이 급해지기 시작했죠. ‘내가 공부는 정말 많이 했지만 내가 진짜 제대로 알고 있는걸까?’, ‘이 상태로 취업은 가능할까?’, ‘공부를 이렇게 하는게 맞나?’ 이런 고민들이었습니다. 이런 고민들로 3월부터 6월 동안 취업준비를 급하게 시작했습니다. 결국 데이터 관련 일은 데이터를 직접 만져봐야 얻는 게 있다는 결론을 내렸기 때문입니다. 이력서도 많이 쓰고 면접도 많이 봤습니다. 첫 면접부터 마지막 면접까지 하나하나 기억이 다 나네요, 쓰라렸지만 좋은 경험을 많이 했다고 생각합니다. 사실 상반기 회고를 하면서 면접에 관해서 글이 길어졌는데, 너무 무거운 내용들이 많아서 일단 나중에 정리해서 업로드 할 생각입니다. 이번 글은 조금 가벼운 느낌으로! 그래서 상반기 회고를 다시 하자면 저는 도서관에서 거의 살았었습니다. 아침 일찍 나가서 저녁 늦게 까지 책을 쌓아놓고 노트북 두들기면서 한 자리에만 있었습니다. 그때 공부를 하면서 느낀 건 공부를 오래하고 싶더라도 체력이 부족하면 불가능 하다는 것이었습니다. 운동도 시작했고, 식단 조절도 해보면서 건강을 챙겼습니다. 우연히 운동 좋아하는 후배들을 알게 되면서 좋은 습관들을 쌓게 된 것 같네요. 글쓰는 습관취업 준비도 준비지만 또 다른 좋은 습관을 들이려고 노력한 것은 글쓰는 습관이었습니다. 배운 내용을 혼자 공부해서 갖고 있는 것보다, 글을 쓰고 공유하고, 얘기하는 것이 저에게 훨씬 더 큰 가치를 가져다 줄 수 있겠구나 하는 생각이 들었습니다. 부족하지만 이론을 정리한 걸 글로 작성하고, 알고리즘 문제 푼 것들도 어떤 생각의 흐름으로 풀었는지 기록했습니다. 이외로 면접 준비하면서 이걸 다시 보게 되니까 정리하는데 도움이 많이 되는 것 같았습니다. 특히 이론에 관해서 글을 쓸때는 완벽하게 알지 못하면 글을 쓰지 못하기 때문에, 어디가 부족한지 스스로 알 수 있게 되어서 더 좋은 것 같습니다. 그 외에 상반기에 했던 것들은 다 취업 준비가 대부분이었던 것 같네요. 회고를 해보니 너무 정신없이 살았던 것 같습니다. 정리 안되고 정신없는 거 별로 안 좋아하는데 상반기를 정리해보니 제 자신이 정리 안하고 살았었네요. 하반기에는 계획을 제대로 세워서 하나씩 클리어 하는 재미로 살아봐야겠습니다. 상반기 회고를 두 번 해보니 얻어지는 것이 있었습니다. 사실 취업준비를 하면서 많이 지쳤었거든요, 데이터 얘기만 들어도 싫고, 개발이나 알고리즘, 코드만 봐도 어지럽고 도망치고 싶었습니다. 하지만 취업도 했고, 상반기를 냉철하게, 처절하게 다시 들여다보고 나니, 다시 시작할 힘이 나는 느낌입니다. 바닥에 다시 내려왔고, 어디부터 공부를 해야할지 이제 감이 잡히는 느낌입니다. 어떤 글을 쓸까? / 다짐글또 3기를 하면서 가끔은 넋두리 같은, 오늘 같은 이야기를 하게 될 것 같고, 캐글 대회에 참여하고 잘 되거나, 안되었던 것들을 정리할 계획입니다. 또 일하면서 필요한 부분을 공부하면서, 예를들어 AWS(사실 GCP를 더 공부하고 싶었는데 ㅠㅠ 회사는 AWS를 쓰는군요…)나 Apache Spark, NoSQL(MongoDB) 등을 정리한 내용을 공유할 것 같습니다. 하반기에 계획했었던 일 중 하나가 글또 3기 들어가는 것이였는데요, 벌써 체크 하나하게 되어서 너무 기쁩니다. 최소 12개의 글을 쓰게 될텐데 그 과정이 의외로 도전적일 것 같아 재밌을 것 같네요. 도전적인 자세로 하반기를 살아봐야겠습니다. 내일은 월요일, 도전이 생각보다 꽤 빨리 시작되는 느낌입니다.","link":"/2019/07/07/geultto/"},{"title":"Python의 실수형에 대해서 알아보자","text":"컴퓨터 사이언스 부트캠프 with Python1. 실수 연산의 함정데이터 사이언스 공부를 하다보면 가끔 머리로 이해되지 않는 것이 생기곤 한다. 그 중 하나가 실수 연산에 대한 것이다.다음 예를 살펴보자 python(3.6.4)으로 다음과 같이 입력하자. 12345a = 0.01result = 0.0for i in range(100): result += a result result는 값이 어떻게 나오게 될까?위의 코드는 쉽게 말하자면 0.01을 100번 더한 것과 다를 게 없다.그렇다면 답은 1일 것이다. 하지만12&gt;&gt;&gt; result 1.0000000000000007 답은 1이 아니다. 만약 내가 조건문을 이용해서1==result 판단을 내렸다면 결과는 False로 나올 것이다. 1234a = 0.015625for i in range(100): result += aresult 1&gt;&gt;&gt;1.5625 그런데 이번 경우에는 생각과 같이 1.5625라는 결과가 나온다. 왜 갑자기 오차 하나 없이 깔끔하게 값이 나오는 것일까?왜 이런 일이 발생하는 것일까? 2. 부동소수점이 현상에 대해 이해하기 위해 부동소수점에 대해서 이해를 해야한다. 부동소수점에서 ‘부’는 부유한다는 말, 즉 떠다닌다는 말이다. 123.456을 다르게 표현해 보는 경우를 생각해보자123123.456 = 1.23456 * 10^2123.456 = 12.3456 * 10 ........ 위의 예시 말고도 다양한 방식이 있다. 소숫점이 둥둥 떠다니는 것 같이 움직인다. 그래서 이러한 실수 표현 방식을 부동소수점이라고 부른다. 3. 단정도와 배정도단정도(single-precision)는 실수를 32비트로 표현하며 부호 1비트, 지수부 8비트, 가수부 23비트로 구성되어 있다.배정도(double-precision)는 실수를 64비트로 표현하며 부호 1비트, 지수부 11비트, 가수부 52비트로 구성되어 있다.배정도가 단정도보다 두 배 정도의 비트 수가 많은데, 비트 수가 많은 만큼 정밀도가 높다고 할 수 있겠다. 파이썬은 배정도를 사용한다. 4. 1바이트 실수 자료형 설계하기$$\\pm 1.man \\times 2^{(exp-bias)}$$ 위의 수식은 실수 자료형을 표현한 수식이다. 1.man은 가수, 2는 밑수, exp-bias는 지수를 의미한다. 이 식을 이용해 7.75라는 10진수 실수를 1바이트 부동소수점으로 표현해보자. 4.1 10진수 실수를 2진수 실수로 바꾸기$$7.75 = 4 + 2 +1 +0.5 + 0.25 $$$$= 2^2 + 2^1 + 2^0 + 2^{-1} + 2^{-2}$$$$=111.11$$2진수로 바꾸면 111.11이란 값이 나온다. 4.2 정규화아 숫자를 정규화 해보자. 정규화란, 소수점 왼쪽에 위치한 가수 부분을 밑수보다 작은 자연수가 되도록 만드는 것이다.111.11을 정규화 하면 다음과 같다. $$ 111.11 = 1.1111 \\times 2^2$$ 4.3 메모리 구조정규화된 부동소수점 $1.111 \\times 2^2$를 앞의 수식과 비교해 보면man은 1111이고 exp-bias는 2이다.이제 메모리 구조를 정하고 man과 exp값만 저장하면 설계가 끝난다.이때 지수부와 가수부에 할당하는 비트 수에 따라 표현 범위와 정밀도가 결정된다. 1바이트 부동소수점 구조는 다음과 같다.$$ 0 \\ 0000 \\ 000 \\ [부호(sign) \\ 지수부(exp) \\ 가수부에서 \\ man에 \\ 해당되는 \\ 부분] $$ 첫번째 비트 : 부호 0은 양수, 1은 음수 가운데 4비트 : 지수부에 해당하며 exp 값이다. 0~15의 양수를 표현할 수 있다 $bias = 2^{지수의 비트수} -1$ 맨 뒤 3비트 : 가수부로 man 값을 저장함 $1.1111 \\times 2^2$를 1바이트의 메모리 구조로 변경해 보자. 부호비트는 0이다. $exp-bias$는 2이다. $bias$값이 7이므로 $exp$는 9가 된다.이것을 이진수로 나타내면 $1001_{(2)}$가 된다. 가수부에 할당된 비트는 3비트이다. 1111을 3비트에 넣을때는 뒷자리 1을 생략한다. 가수부는 111이다.$$ 0 \\ 1001 \\ 111 = 0100 \\ 1111 $$이것을 16진수로 나타내면 0x4f가 된다.정리하자면 10진수 7.75를 $ 0100 \\ 1111 $로 나타낼 수 있고 이것을 다시 16진수로 나타내면 0x4f이다. 4.4 1바이트 부동소수점의 표현 범위 표현할 수 있는 가장 작은 수(지수부0001) $1.0000 \\times = 0.0156256$ 표현할 수 있는 가장 큰 수(지수부 1110) $1.111 \\times = 240$ 단, 지수부 비트가 모두 0일때와 모두 1일때는 0.0, 정규화 불가능, 무한대, NaN 같은 특별한 상황이므로 제외한다. 4.5 1바이트 부동소수점의 정밀도7.75를 변환하는 과정에서 3비트의 가수부데이터에 1을 누락해 가면서 가수부 공간에 담았던 것을 기억할 것이다.1을 누락하게 되면 0x4f는 7.75를 완벽하게 표현하지 못하게 된다.$$1.111 \\times 2^2 = 1 \\times 2^2 + 1 \\times 2^1 + 1 \\times 2^{-1} = 7.5$$여기서 0.25만큼 차이가 나게되고, 그만큼 정밀도도 떨어지게 된다. 5. 정밀도에 대한 고찰5.1 엡실론실수 자료형에서 엡실론이란 1.0과 그 다음으로 표현 가능한 수 사이의 차이를 말한다. 12import syssys.float_info.epsilion 위 코드로 엡실론 값을 확인해 보자.배정도의 가수부는 52비트인 것을 기억할 것이다. 1.0을 배정도에 맞춰 표현하면$1.0000 ….. 0000(0:52개) \\times 2^0$배정도에서 1.0다음으로 표현할 수 있는 수는$1.0000 ….. 0000(0:51개) \\times 2^0$두 수의 차이는$1.0000 ….. 0000(0:51개) \\times 2^0$이 숫자를 10진수로 바꾸면 엡실론 값이 등장한다.$2.220446049250313 \\times 10^{-16}$ 5.2 엡실론과 정밀도엡실론을 이용하면 해당 실수 다음에 표현할 수 있는 수를 알아낼 수 있다.9.25라는 수를 부동소수점 방식으로 표현하면 $1.00101 \\times 2^3$이다.여기서 지수부분만 떼서 엡실론을 구하면 이 실수와 다음 표현 가능한 수 사이의 차이를 구할 수 있다.코드로 살펴보면, 123456789import sysep = sys.float_info.epsiliona= 9.25diff = (2**3)*epdiff&gt;&gt;1.7763568394002504e-15b = a + diffb&gt;&gt;&gt;9.250000000000002 0.000000000000002만큼 차이가 난다. 그렇다면 9.25에 diff보다 작은 값을 더하면 어떻게 될까?추측으로는 9.25가 나올 것 같다.확인해보자 12345a = 9.25half_diff = diff/2c = a + half_diffa == c&gt;&gt;&gt; True 추측과 같이 half_diff를 더하더라도 값의 변화가 없다.diff보다 작은 값을 더한 수를 부동소수점 방식에서는 표현할 수 없다는 말이다.다르게 말하자면 정밀도가 떨어진다는 말이다. 다음의 내용은 혼자서하는 괴발개발 블로그 https://aisolab.github.io/computer%20science/2018/08/07/CS_Real-number 에서 가져온 글이다.다음의 방법을 이용하면 상대오차(relative error) 가 엡실론보다 작으면 서로 같은 수라고 판단하는 function을 만듦으로써 위와 같은 문제를 해결할 수 있다. 1234a = 0.1 * 3b = 0.3print(a == b) 1False $$relative \\ error = {\\left\\vert x - y\\right\\vert \\over \\max(\\left\\vert x \\right\\vert , \\left\\vert y \\right\\vert)}$$ 123456789import sysdef is_equal(a, b): ep = sys.float_info.epsilon return abs(a - b) &lt;= max(abs(a), abs(b)) * epa = .1 * 3b = .3print(is_equal(a,b)) 1True","link":"/2019/01/19/float/"},{"title":"글또 4기를 시작하며 다짐하기","text":"글또 Notion 구경하기 글또 3기가 끝나고 돌아보기글또 3기를 성공적으로 마치고! 글또 4기를 시작하게 되었다. 작년 7월부터 연말까지 글을 쓰는 대장정이었던 3기에서 pass권 2번 쓴 걸 제외하면, 모든 주차에 글을 작성했었다. 피드백을 기간내에 까먹고 못해서 5000원이 차감되었지만, 원래 목표했었던 10만원 그대로 돌려 받기에는 성공했다(?). 그 동안 쓴 글들을 쭉 돌아보니, 글또하길 잘했다는 생각이든다. 블로그 자체의 콘텐츠도 많이 늘었을 뿐만 아니라, 항상 어떤 걸 배우면 기록하는 습관이 생겼다. 블로그에 적어야 겠다는 것이 머리 안에 자리잡힌 것 같고 일요일 저녁만 되면 ‘글 써야 되는데’ 하는 생각이 슬쩍 스쳐지나간다. 글쓰기 중독 초기 증세일까? 하루에 한번은 애드센스에 들어가서 예상 수입에 떠라 그 날 기분이 왔다 갔다 하니, 이쯤되면 중독이 맞는 것 같다. 무엇을 썼을까?초기에는 강의를 들었던 내용을 쭉 정리해서 작성했고, 행사 다녀온 내용과 딥러닝 컴퓨터를 사서 서버로 만든 글, MAB 등 굉장히 다양한 글을 썼다. 좋게 말해서 다채로운 글을 쓴 것이지만, 포커싱 되지 않았다고 해야할까? 한 주제에 대해서 깊이 파헤치고 그 분야에 대해서 성장하는 느낌이 들지 않는 느낌이었다. 어떤 글을 쓰면서 부족한 건 나중에 채워넣어야 겠다고 생각한 글도 있었는데, 결국 채우지 못했다. 완성도 있는 글을 써야하지 않았을까? 최근에는 딥러닝 서버 구축기를 다시 읽을 일이 생겼다. 비슷한 문제 상황이 발생했고 내가 정리한 글을 보고 해결하기로 한 것이다. 그런데 그대로 따라해도 문제가 해결이 되지 않았다. 구축 환경도 비슷하다고 생각했고 똑같은 작업을 하는데 왜 안되는지, 당황스러웠다. 내가 인터넷에서 도움받은 만큼 트러블 슈팅한 내용을 작성해서 다른 사람들에게 도움을 주고 싶었는데, 내 글로 인해 괜히 망치는 건 아닐까?, 성급하게 글을 작성한 건 아닐까?, 좀 더 많이 찾아보고 깔끔하게 정리된 내용으로 글을 작성해야 하지 않았을까? 란 생각이 들었다. 무엇을 쓸까?3기 때 목표는 명확했다. 예치금 깎이지 않기! 하지만 이미 이룬 목표를 다시 사용하는 것은 성장하는 과정에서, 목표로서의 기능을 다 하지 못하는 것이라고 생각한다. 예치금을 다 받는 것은 기본 조건으로 하고, 완성도 있는 글을 작성하기 위해서 노력해야겠다. 이를 위해서는 특별한 과정이 필요할 것 같다. 기존에 글을 쓸 때는 마감시간에 닥쳐서 쓴 글이 많았었다. 물론 글로 쓰려고 작업한 내용이 예상한 것보다 작업시간이 길어져서, 금요일부터 ubuntu 서버를 구축하기 시작해서 일요일 11시까지 질질 끄느라 그랬었던 적도 있긴 하지만, 어쨌든 마감을 맞추려고 급하게 쓴 글은 다시보면 티가 나는 것 같다. 결국 준비를 해야 좋은 글을 쓸 수 있는 것 같다. 어떻게 준비를 해야할까? 1-2주 정도의 텀을 두고 쓸 글의 주제를 정해놓고 초고를 작성한다 초고를 적어도 3번이상 수정한다 잘 읽히는지 확인한다 시간이 지나서도 가치가 있는지 확인한다 업데이트 주기에 대해서도 생각한다 일단 생각나는 것은 이 정도다. 8월까지 진행하게 되면서 매번 완성된 좋은 글을 작성할 수는 없겠지만, 최소한 한달에 한번은 공들여서 다른 사람에게 도움이 되는, 좋은 글을 작성해보는 게 목표다. 세부적으로, 작성하고 싶은 글감들 Pyspark Airflow AWS(S3, EC2, Load Balancer) 궁극적인 목표에 대한 고민 최근에 일하면서 만나게 된 아이들에 대해서 정리해서 글을 써 보고 싶다. 그리고 최근에 학교 후배와 교수님과 만나서 얘기를 하던 중에 궁극적인 목표에 대한 주제가 나왔다. 그 후배는 목표가 확실했고, 목표에서 굉장한 동기부여를 받는 듯 했다. 취업한 이후에 동기부여가 되었다가 안되었다 하는 일이 자주 있는데 여기에 대해서 넋두리일지라도, 삶의 고민에 대한 흔적으로 남겨두고 싶다. 그 외에 데이터 엔지니어링 채널이나 백엔드 쪽 채널을 자주 기웃 거리면서 꼭 피드백을 해야하는 글이 아니더라도, 꾸준히 챙겨보고 관심있는 것들에 대해서 공부를 해봐야겠다.(사실 요즘 데이터 엔지니어링에 관심이 많고 잘하고 싶다는 마음이 마구마구 든다!) 글또 4기가 3월 1일부터 본격적으로 시작된다. 6개월 동안 꾸준하게 블로그를 채워나가봐야겠다.","link":"/2020/02/24/geultto4/"},{"title":"Failed to get convolution algorithm. This is probably because cuDNN failed to initialize 해결하기, feat. 케라스 창시자에게 배우는 딥러닝","text":"케라스 책을 따라하다 보면 나오는 에러에 대한 트러블 슈팅 Failed to get convolution algorithm. This is probably because cuDNN failed to initialize케라스 창시자에게 배우는 딥러닝 책 진도를 쭉쭉 나가는 중이었다. 챕터5의 CNN코드를 돌리던 중 다음과 같은 에러가 등장했다. 123(0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above. [[{{node conv2d_1/convolution}}]] [[metrics/acc/Mean/_99]] 황금같은 쉬는 날에 cudnn과 CUDA의 꼬임으로 인한 에러인 줄 알고 굉장히 낙담했다. 실제로 저 에러를 복사해다가 구글에 붙여넣어서 답을 찾아보니 다시 설치하라는 의견이 많았다. 하지만 정말 귀찮아서 내 설치 환경을 다시 살펴보고 이상이 있으면 수정하기로 했다. 일단 내가 설치한 CUDA와 CuDnn 환경은 저번 글들에 나와 있다. ubuntu 딥러닝 서버 구축기 01ubuntu 딥러닝 서버 구축기 02ubuntu 딥러닝 서버 구축기 03 CUDA와 CuDNN의 호환성은 문제가 없었다. 그렇다면 뭐가 문제였을까? 하던 중에 Python의 버젼을 살펴봤다.콘다 가상환경에 올리고 쓰고 있었는데 python버전을 확인해보니 python 3.7버전이었다. 여러 글들을 보다보니 python 3.6버전이 keras와 궁합이 잘 맞는다는 것을 알게되었다. 그래서 바로 1conda create -n [환경 이름] python=3.6 가상환경을 만들어주고 만든 가상환경을 jupyter kernelspec에 넣어주었다.jupyter kernelspec 추가하기 python 3.6으로 갈아끼워 준 뒤에123456model = models.Sequential()model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu'))model.add(layers.MaxPooling2D((2, 2)))model.add(layers.Conv2D(64, (3, 3), activation='relu')) 123model.add(layers.Flatten())model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(10, activation='softmax')) 1234567891011121314151617181920from keras.datasets import mnistfrom keras.utils import to_categorical(train_images, train_labels), (test_images, test_labels) = mnist.load_data()# train_images = train_images.reshape((60000, 28, 28, 1))train_images = train_images.reshape((60000, 28,28, 1))train_images = train_images.astype('float32')/255test_images = test_images.reshape((10000, 28, 28, 1))test_images = test_images.astype('float32')/255train_labels = to_categorical(train_labels)test_labels = to_categorical(test_labels)model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])model.fit(train_images, train_labels, epochs=5, batch_size=64) 코드를 돌렸더니 잘 돌아간다!!!","link":"/2020/01/01/keras-trouble-shooting/"},{"title":"programmers Carpet","text":"프로그래머스 코딩테스트 연습문제 카펫을 풀어봤다카펫 문제 설명 Leo는 카펫을 사러 갔다가 아래 그림과 같이 중앙에는 빨간색으로 칠해져 있고 모서리는 갈색으로 칠해져 있는 격자 모양 카펫을 봤습니다. Leo는 집으로 돌아와서 아까 본 카펫의 빨간색과 갈색으로 색칠된 격자의 개수는 기억했지만, 전체 카펫의 크기는 기억하지 못했습니다. Leo가 본 카펫에서 갈색 격자의 수 brown, 빨간색 격자의 수 red가 매개변수로 주어질 때 카펫의 가로, 세로 크기를 순서대로 배열에 담아 return 하도록 solution 함수를 작성해주세요. 제한사항 갈색 격자의 수 brown은 8 이상 5,000 이하인 자연수입니다.빨간색 격자의 수 red는 1 이상 2,000,000 이하인 자연수입니다.카펫의 가로 길이는 세로 길이와 같거나, 세로 길이보다 깁니다. 문제는 완전탐색으로 풀라고 하는 것 같았지만, 이 문제는 수학적으로 풀 수 있을 것 같았다.Brown과 Red를 이루는 수를 Red의 m과 n으로 표현해보고 (Red = (m x n)꼴, m&gt;n) 나온 (m,n)꼴에 +2를 해주면,return값이 (m+2,n+2) 나오게 된다는 것을 깨달았다. 하지만 문제가 있었다. 이 경우는 Red가 Square꼴이 아닐 때만 해당했던 것이었다.Red가 Square꼴일 경우, m과 n으로 문제를 풀 수 없다. 이 경우는 다른 케이스를 생각해 봐야 한다.R을 (nxn)꼴이면 Brown이 4(n+1)로 나온 다는 것을 알아야 한다.return은 처음의 케이스와 같이 2만 더해주면 된다. 첫번째 케이스의 경우를 n에 대해서 쭉 풀어주면 이차방정식 꼴이 나온다.아마도 테스트 케이스는 근이 정수로 나올 것 같아서 중근이나 허근이 나올 경우를 제외한, 근의 공식을 코딩해서 함수화 하였다.12345678910def fun(a,b,c): D=b*b-4*a*c if D&gt;0: x1=round((-b-D**0.5)/2*a) x2=round((-b+D**0.5)/2*a) if x1&gt;x2: return [x1+2,x2+2] else : return [x2+2,x1+2] 그 다음 두번째 케이스로 넘어가는 것이 중요했는데, Brown과 Red를 받았을 때, 특히 Red를 가지고 제곱수인지 판별하는 함수가 필요했다. 만약 Red가 제곱수라면 Red에 루트를 씌워서 값을 받아 2만 더해주면 될 것이고, 제곱수가 아니라면 위의 함수를 이용해서 return을 받으면 된다.그래서 제곱수 판별하는 함수를 다음과 같이 작성했다.1234import numpy as npdef issquare(n): if int(n ** 0.5) ** 2 == n : return int(np.sqrt(n)) 마지막으로 solution 함수에서는 이 함수들을 모두 합쳐주고 조건문을 통해서 return값을 다르게 받아준다.123456789def solution(brown, red):# return이 제곱 수 아닐 때 if issquare(red) : return [issquare(red) + 2,issquare(red) + 2] else: a = 1 b = (4-brown)/2 c = red return fun(a,b,c) 정리 : 코딩 연습을 꾸준히 해야하는 것이 느껴진 문제였다. 제곱수를 판별하는 문제나, 이차방정식의 해를 구하는 문제는 연습문제로 간간히 나오던 것이었다. 기초적인 문제가 제대로 학습이 되어있지 않으면, 문제 푸는데 굉장히 오랜 시간이 걸리지 않을까 생각했다. 기본적인 문제도 중요하다!","link":"/2019/04/12/programmers-carpet/"},{"title":"Time Seires, 시계열 분석 세번째","text":"시계열 데이터 분석과 기계학습의 차이점, 회귀분석과의 차이점 Dive into 시계열 데이터 분석시계열 데이터 분석에 대해서 공부해보자 03시계열 데이터 분석과 기계학습의 차이점에 대해서 본격적으로 들어가보도록 하겠습니다. 시계열 알고리즘에는 기계학습과는 다른, 2가지 차별화 방향이 있습니다. 첫번째는 ‘과거데이터로 미래 데이터 뽑아낼 수 있는가’ 로 시간 축 기반의 예측이 가능하다는 것이고, 두번째는 시계열 알고리즘은 점추정이 아닌 구간추정 알고리즘으로 설명력 효과에 뿌리를 두었다는 것입니다. 대부분의 기계학습 모델은 통계분포에 기반하지 않끼 때문에 점추정 알고리즘이고, 시계열 알고리즘은 구간추정을 하기 때문에 점추정 보다 더 다양한 해석이 가능합니다. (특별한 모델들) Dynamic Modeling : y가 여러개인 모델입니다. y에 대한 영향을 비교하고 싶다면 y를 두 개로 둬서 모델을 두개 만들고 실험하는 것을 생각해 볼 수 있습니다. XAI, LIME : 최근에 등장한 개념으로, 설명가능한 Aritificial Intelligence, 설명가능한 딥러닝에 대한 것입니다. Random Forest의 feature importance와 비슷하게 측정이 가능하다고 합니다. 시계열 데이터 분석 준비데이터 분석 준비는 기존 머신러닝 데이터를 준비하는 것과 비슷하지만, 시계열 데이터만의 특징이 있기 때문에, 머신러닝 데이터 준비와 다른 몇 가지 특징들이 존재합니다. 가장 옛날 것을 훈련데이터로 사용하고, 그 다음 것을 Validation 데이터, 가장 최근 것을 test데이터로 사용합니다. 기간을 두고 훈련셋을 만듭니다. 시간축을 잘 보존해야합니다. 훈련세트에서 하나 건너서 Validation set을 만들면 ⇒ 1스텝 교차검사 훈련세트에서 두 개 건너서 Validation set을 만들면 ⇒ 2스텝 교차검사 모델이 월 마다의 예측력이 다를 수 있기 때문에, 월별 모델을 만들기도 한다. (실제로 이렇게 모델을 만드는 경우가 많음) 검증지표와 잔차진단분석하고 예측만 잘하면 될까요? 큰 착각일 수 있습니다. 중요한 것은 예측이 잘 되었는지 평가하는 것 그리고 데이터의 시간패턴이 잘 추출되었는지 확인하는 작업입니다. 검증지표는 예측값과 실제값이 얼마나 유사한지 측정하는 것이고, 잔차진단은 데이터의 시간패턴을 잘 뽑아내었는지 알아보는 작업입니다. 검증지표 에는 흔히 사용되고 눈에도 익숙한 MSE가 대표적입니다. 그 외에 MAE, MAPE, MAPE, MPE(y가 %로 나올 때) 등 다양한 검증지표가 사용되며, 왠만하면 다 사용해보고 수치가 안정될 때까지 모델을 만드는 것이 좋습니다. 또한 기존의 검증지표가 맘에 들지 않는다면, 분석가나 데이터 사이언티스트가 직접 고안해서 검증지표를 사용해도 됩니다. 훌륭한 데이터 사이언티스트라면 검증지표를 직접 만들어서 사용해야하는 경우가 많을 것입니다. 왜냐하면 검증지표만 가지고 사용하면 결과에 대한 해석을 다르게 할 수 있기 때문입니다. 예를들어 MSE의 경우는 오차에 대한 가중치를 확 올려버리는 검증지표입니다. 오차에 대해서 제곱을 하기 때문에 가중치가 급격하게 증가하게 됩니다. 분석가가 보기에 이 가중치가 너무 과하다고 생각되면 절대값을 사용할 수 있습니다. 이렇게 되면 MAE를 사용하는 것이 되겠습니다.$$MSE = {1\\over n}\\sum(y-\\hat{y})^2$$$$MAE = {1\\over n}\\sum|y - \\hat{y}|$$ 잔차진단 는 말 그대로 잔차, $$y-f(x)$$ 에 대한 값을 보고 진단하는 것을 말합니다. 회귀분석을 해보신 분들을 아시겠지만, 잔차는 정의가 존재합니다. 잔차의 정의 잔차들은 정규분포이고, 평균 0과 일정한 분산을 가져야 한다. 잔차들이 시간의 흐름에 따라 상관성이 없어야 한다. 자기상관 함수를 통해 Autocorrelation이 0인지 확인 공분산 자기상관함수 편자기상관함수 자기상관 정도를 확인하기 위해서는 일반적으로 랜덤으로 epsilon을 두 개 뽑아서 자기상관성이 있는지 확인합니다. 자기상관성 뿐 아니라, 잔차가 잘 뽑혔는지 확인하기 위해서는 시각화를 통해서 확인하고, 자기상관성이나 평균, 분산과 같이 통계량을 계산해서 확인하는, 두 가지 방법을 모두 사용해야 합니다. 잔차를 보고 더 뽑아내야 할 것이 있는지, 분석이 잘되고 있는지 안되고 있는지를 확인해 나가야 합니다. 예를 하나 들어서 보겠습니다. 여기에 다양한 모델들이 있네요. Autocorrelation에 대해서 가정을 설정했고, 가정에 대한 결과표가 나왔습니다. 대중 주장은 모델의 잔차가 White Noise라는 것이고, 내 주장은 모델의 잔차가 White Noise가 아니라는 것입니다. 모델을 쭉 보니, SARIMA 모델은 p-value값이 굉장히 높고 나머지는 0에 가깝습니다. 이것을 어떻게 해석하면 될까요? p-value가 크다는 것은 대중 주장이 맞다는 것입니다. 대중 주장이 맞으니까 SARIMA의 잔차는 White Noise이고 모델이 잘 만들어졌다고 추측해 볼 수 있습니다. 나머지 모델들은 영 꽝이네요. 이런 식으로 모델에 대한 잔차를 검증해 나가면 됩니다. 시간영역 선택하기시계열 분석이 머신러닝 분석 방법과 다른 것은 시간 축입니다. 이 시간 축을 어떻게 두느냐에 따라 분석 결과가 급격하게 달라집니다. 따라서 시계열이 분석효과에 도움이 될 시간영역(해상도)를 선택해야 합니다. 일종의 window size를 정한다고 생각하면 이해가 잘 되실 겁니다. 시간축을 년 단위로 할지, 월로 할지, 일주일로 할지는 사실 다 해보는 수 밖에 없습니다. 알 수가 없기 때문입니다. 그래서 다 해보고 잘되는 시간 영역을 선택해야 합니다. 물론 선택하는 기준은 있습니다. 바로 통계량과 잔차를 기준으로 잘 나오는 시간축을 선택하는 것입니다. 분석은 항상 이 방법으로 진행이 됩니다. 통계량과 잔차! 회귀분석 요약 / 시계열 분석 요약계수 추정 방법은 두 가지 방법이 있습니다. 수학자(수식)방법과 통계학자(확률)의 방법입니다. 수학자의 방식은 결정론적 모형입니다. 잔차벡터를 구하고, 잔차 제곱합을 구한 후, 그레디언트를 계산합니다. 그 이후에 미분을 하여 최적점을 구하고 추정된 계수를 얻는 방법입니다. 이 때, $X^T X$ 행렬은 역행렬이 존재해야 합니다. $X$ 가 full rank가 아니면 계산이 되지 않겠습니다. 결국 $X^TX$ 행렬은 Positive Definite이 아니면 계산 되지 않습니다. 확률론적 모형은 다음과 같습니다. Main Equation에서 $X$ 가 있을 때의 $Y$ 의 기대값과 분산을 구합니다. 이를 통해 분포를 추정할 수 있게됩니다. 구한 평균과 분산을 가지고, $Y$ 값 각각의 확률을 구합니다. 그 이후에는 $Y$ 의 발생가능성에 대해서 Likelihood를 구합니다. Maximum Likelihood Estimation을 하려는 것입니다.Log를 취해 계산을 편리하도록 만들어주고 미분으로 0값이 되는 지점을 구합니다. 통계학자의 방식은 이렇게 분포에 대한 계산을 해놓는 다는 점입니다. 이런 식으로 계산이 되면 구간추정도 가능해지게 됩니다. $beta$ (coefficient)값에 대한 분산은 $Covariance$ 를 통해서 구합니다. 역행렬이 없을 수록 에러값이 증가하게 되고 이것은 구간이 넓어지는 것을 의미하게 됩니다. 결국 넓어진 구간을 좁히는 것이 목표가 되겠습니다. beta값의 분포에 대해서는 $t$ 분포 따르는 게 증명이 되었기 때문에 $t$를 사용하면 됩니다. $beta$ 가 $t$ 에서 나온다는 성질은 가설검정을 할 때 이용됩니다. 추가로 딥러닝에 대해서 덧붙이자면, Neural Network는 신뢰구간의 범위가 너무 넓습니다. 이론적 근거가 없기 때문입니다. 검증을 할 때는 P-value를 보고 값이 높은 것은 coefficient가 몇이든 다 0으로 보면 됩니다. $R^2$ 값이랑은 별개의 문제입니다. 시계열 데이터 분석은 분석하고 검증하고 모델링하는 것도 중요하지만, 데이터에 따라 높은 정확도나 높은 에러를 가지게 됩니다. 시계열 분석은 단기적인 상황에서는 성능이 좋지만 중 장기 예측에 대해서는 잘 맞지 않는 다는 단점이 존재합니다. 물론 변화가 별로 없고 일반적인 패턴을 지닌다면 잘 맞추겠지만 변화가 굉장히 극심하다면 잘 맞추기 못 하게 됩니다. 애초에 데이터를 정렬할때도 시간축을 잘 살려서 정제를 해야하니 데이터 정렬 자체, 데이터 준비하기도 굉장히 어려운 작업이라고 할 수 있겠습니다.","link":"/2019/09/20/Time-Series-03/"},{"title":"객체 지향 프로그램에 대해서 알아보자","text":"Reference : https://aisolab.github.io/computer%20science/2018/08/09/CS_Object-oriented-programming/ [김보섭님 블로그] 1. 프로그래밍 패러다임프로그래밍 패러다임으로는 다음 3가지가 대표적이다. 절차 지향 프로그래밍(procedural programming) 객체 지향 프로그래밍(object-oriented programming) 함수형 프로그래밍(fuctional programming) 2. 절차 지향 프로그래밍절차를 의미하는 procedure는 서브 루틴, 메서드, 함수라고 불린다.함수는 입력을 받아 연산을 하고 출력을 내보낸다. 함수를 한 번 정의해 두면 다시 호출해서 쓸 수 있고 이름으로 어떤 일을 하는지 쉽게 알 수 있다.이처럼 함수를 사용해 프로그래밍 하는 것을 절차 지향 프로그래밍이라고 한다. 3. 절차 지향으로 학급 성적 평가 프로그램 만들기우리가 담임 선생님이 되었다고 가정하고 엑셀에 저장된 학생들의 점수를 이용해 평균과 표준편차를 구하고 전체의 평균과 비교하여 평가하는 프로그램을 만들어 보자. 3.1 openpyxl모듈 설치하기pip install openpyxl 을 입력한다. 3.2 openpyxl 모듈로 데이터 읽어 들이기exam.xlsx name score greg 95 john 25 yang 50 timothy 15 melisa 100 thor 10 elen 25 mark 80 steve 95 anna 20 function.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from openpyxl import load_workbookfrom functools import reduceimport mathdef get_data_from_excel(filepath): wb = load_workbook(filename = filepath) ws = wb.active rows = ws.rows raw_data = {name_cell.value : score_cell.value for name_cell, score_cell in rows} scores = raw_data.values() return scoresdef get_average(scores): avrg = reduce(lambda score1, score2 : score1 + score2, scores) / len(scores) return avrgdef get_variance(scores, avrg): tmp = 0 for score in scores: tmp += (score - avrg)**2 else: var = tmp / len(scores) return vardef get_std_dev(var): std_dev = round(math.sqrt(var),1) return std_devdef evaluate_class(avrg, var, std_dev, total_avrg, total_std_dev): \"\"\" evaluate_class(avrg, var, std_dev, total_avrg, total_std_dev) -&gt; None Args: avrg : 반평균 var : 반분산 std_dev : 반표준편차 total_avrg : 학년평균 total_std_dev : 학년분산 \"\"\" print(\"평균:{}, 분산:{}, 표준편차:{}\".format(avrg, var, std_dev)) if avrg &lt; total_avrg and std_dev &gt; total_std_dev: print('성적이 너무 저조하고 학생들의 실력 차이가 너무 크다.') elif avrg &gt; total_avrg and std_dev &gt; total_std_dev: print('성적은 평균 이상이지만 학생들의 실력 차이가 크다. 주의 요망!') elif avrg &lt; total_avrg and std_dev &lt; total_std_dev: print('학생들의 실력 차이는 크지 않지만 성적이 너무 저조하다. 주의 요망!') elif avrg &gt; total_avrg and std_dev &lt; total_std_dev: print('성적도 평균 이상이고 학생들의 실력 차이도 크지 않다.') main.py123456789101112131415161718from functions import *import argparse parser = argparse.ArgumentParser(prog = '평가프로그램', description = '엑셀에 저장된 학생들의 점수를 가져와 평균과 표준편차를 구하고, 학년 전체 평균과 비교하는 프로그램')parser.add_argument('filepath', type = str, help = '엑셀파일 저장경로')parser.add_argument('total_avrg', type = float, help = '학년평균')parser.add_argument('total_std_dev', type = float, help = '학년표준편차')args = parser.parse_args()def main(): scores = get_data_from_excel(filepath = args.filepath) avrg = get_average(scores = scores) var = get_variance(scores = scores, avrg = avrg) std_dev = get_std_dev(var = var) evaluate_class(avrg, var, std_dev, args.total_avrg, args.total_std_dev)if __name__ == '__main__': main() 메인 함수에는 책과 다른점이 있다. argparse부분이다. argparse 라이브러리를 임포트해서함수에 argument들을 넣었다. argument를 가지고 좀 더 세밀한 부분을 다뤄볼 수 있게 되었다. 이처럼 함수를 이용하면, 코드가 심플해지고, 쉽게 다시 불러와 사용할 수 있다.프로그램이 무슨 일을 하는지 알 수 있고, 한눈에 프로그램의 실행 흐름을 파악할 수 있다.절차 지향의 특징과 장점이라고 할 수 있겠다. 4. 객체 지향 프로그래밍객체 지향은 ‘현실 세계에 존재하는 객체를 어떻게 모델링할 것인가?’에 대한 물음에서 시작한다.데이터 사이언티스트들에게 익숙한 표현이 아닌가 싶다. 4.1 캡슐화현실 세계의 객체를 나타내려면 변수와 함수만 있으면 된다. 객체가 지니는 특성 값에 해당하는 것이 변수이고,행동 혹은 기능은 함수로 표현할 수 있다. 이처럼 현실 세계를 모델링하거나 프로그램을 구현하는 데 변수와 함수를 가진 객체를 이용하는 패러다임을 객체 지향 프로그래밍이라고 하며, 변수와 함수를 하나의 단위로 묶는 것을 캡슐화라고 한다. 4.2 클래스를 사용해 객체 만들기객체와 함수에 대해서 사람들은 어떤 중요한 의미를 부여하게 된다. 하지만 컴퓨터의 입장에서는 어떨까?컴퓨터는 의미가 전달이 되지 않는다. 메모리의 한 단위로만 저장될 뿐이다. 객체라는 메모리 공간을 할당한 다음 객체 안에 묶인 변수를 초기화하고 함수를 호출하는 데 필요한 것이 클래스일 뿐이다. 클래스는 객체를 생성해내는 템플릿이고(그 유명한 붕어빵 틀) 객체는 클래스를 이용해 만들어진 변수와 함수를 가진 메모리 공간이다. 둘은 서로 다른 존재이고 메모리 공간도 다르다. 객체와 매우 유사한 개념으로 인스턴스가 있다. 객체와 인스턴스의 차이점은 객체는 객체 자체에 초점을 맞춘 용어이고(붕어빵) 인스턴스는 이객체가 어떤 클래스에서 만들어졌는지에 초점을 맞춘 용어이다.(어떤 붕어빵 틀에서 나왔니) 사람이라는 클래스를 만들어보면서 이해해보면 쉬울 것이다.구현 코드는 아래와 같다.1234567891011121314class Person: def __init__(self, name, money): self.name = name self.money = money def give_money(self, other, money): other.get_money(money) self.money -= money def get_money(self, money): self.money += money def __str__(self): return 'name : {}, money : {}'.format(self.name, self.money) 12greg = Person('greg', 5000)john = Person('john', 2000) 12345print(greg, john)name : greg, money : 5000 name : john, money : 2000greg.give_money(john, 2000)print(greg, john)name : greg, money : 3000 name : john, money : 4000 4.3 파이썬의 클래스4.2에서 구현한 클래스를 가져와서 살펴보자12345678type(Person.__init__)= &lt;class 'function'&gt;type(Person.give_momey)= &lt;class 'function'&gt;type(Person.get_money)= &lt;class 'function'&gt;type(Person.show)= &lt;class 'function'&gt; 모두 함수라는 충격적인 결과가 나온다. 이번에는 객체가 가진 메서드를 살펴보자123456type(g.give_money)= &lt;class 'method'&gt;type(g.get_meney)= &lt;class 'method'&gt;type(g.show)= &lt;class 'method'&gt; 객체 g의 메서드는 메서드인 것을 알 수 있다. 비슷한 것 같은데 둘의 차이는 무엇일까? 1234567dir(g.give_money)g.give_money.__func__g.give_money.__self__g.give_money.__self__ is g 위의 코드를 실행 시켜보면 차이를 확인해 볼 수 있다.g가 가진 메서드의 속성을 dir을 통해 확인해보면, __func__, __self__가 등장하는 것을 볼 수 있다. __self__를 확인해 보면 Person객체라고 나온다. __func__는 또한 Person클래스의 give_money()함수라는 것을 확인할 수 있고, __self__가 이 메소드를 가진 객체 자신을 참조하고 있다는 것도 알 수 있다. 객체에서 메서드를 호출할 때 self를 전달하지 않아도 되는 이유를 여기서 알 수 있게 된다. 메서드 내부에 함수와 객체의 참조를 가지고 있으므로, 함수에 직접 객체의 참조를 전달할 수 있기 때문이다. [혼자서하는 괴발개발 블로그]https://aisolab.github.io/computer%20science/2018/08/09/CS_Object-oriented-programming/classmethod와 staticmethod를 한눈에 정리가능한 코드가 있어서가져와봤다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344class Person: # 여기에 class variable (또는 class member) # instance 모두가 공유하는 동일한 값 # instance를 생성하지않고도, class만 선언한 상태에서 호출이 가능하다. # oop에서 global variable을 대체하기위하여 사용 __whole_population = 0 # name mangling technique 사용, 외부에서 ## Person.__whole_population으로 접근 불가 ## Person._Person__whole_population으로 접근 가능 # class method # oop에서 global에 선언된 function을 대체하기위해 사용 # 대체 생성자를 만들 때, 더 많이씀 (여긴 대체생성자 구현하지 않음) @classmethod def __birth(cls): cls.__whole_population += 1 @staticmethod def check_population(): return Person.__whole_population # instance method def __init__(self, name, money): # 생성자(constructor) Person.__birth() # instance variable (또는 instance member) # instance마다 값이 다른 변수, instance가 가지는 고유한 값 # 여기에서는 self.name, self.age self.name = name self.money = money def get_money(self, money): self.money += money def give_money(self, other, money): # message passing # 다른 인스턴스(객체)랑 상호작용을 할 때, 상대 인스턴스(객체)의 인스턴스 변수를 바꿔야한다면 other.get_money(money) # 이렇게하세요 # other.money += money 이렇게하지마세요 self.money -= money def __str__(self): return '{} : {}'.format(self.name, self.money) 주석처리도 너무 잘되어 있기 때문에 쭉 보고 따라 쳐보면서 이해하면 아주 좋을 것 같다. 클래스 메서드의 특징 중 하나는 인스턴스를 만들지 않고도 불러낼 수 있다는 것이다.12# 클래스 메소드는 인스턴스를 생성하지않고도 호출할 수 있다.print(Person.check_population()) 10 만들어놓은 클래스를 사용해보자.1234567891011121314151617# 클래스 변수는 인스턴스간에 모두 공유한다.# 인스턴스를 통해서도 클래스 변수나 클래스 메소드를 호출할 수 있다.mark = Person('mark', 5000)greg = Person('greg', 3000)steve = Person('steve', 2000)print(Person.check_population())print(Person._Person__whole_population)print(mark._Person__whole_population)print(greg._Person__whole_population)print(steve._Person__whole_population)steve._Person__birth()print(mark._Person__whole_population)print(greg._Person__whole_population)print(steve._Person__whole_population) 1234567833333444 마크와 그렉 스티브가 인스턴스로 만들어졌다. 만들어지자마자 클래스메서드의 __birth가 실행되어서인구가 총 3명이 된다.스티브 인스턴스를 통해 클래스 메서드로의 접근이 가능해서인구가 4가 되었고인스턴스로 접근해 전역변수 __whole_population을 요청하면 4가 나오게 된다. 4.4 객체 지향으로 은행 입출금 프로그램 만들기123456789101112131415161718192021222324252627282930313233343536class Account: __num_acnt = 0 @staticmethod def get_num_acnt(): return Account.__num_acnt def __init__(self, name, money): self._user = name self._balance = money Account.__num_acnt += 1 def deposit(self, money): assert money &gt; 0, '금액이 음수입니다.' self._balance += money def withdraw(self, money): assert money &gt; 0, '금액이 음수입니다.' if self._balance &gt;= money: self._balance -= money else: pass def transfer(self, other, money): assert money &gt; 0, '금액이 음수입니다.' self.withdraw(money) if self._balance &gt;= 0: other.deposit(money) return True else: return False def __str__(self): return 'user : {}, balance :{}'.format(self._user, self._balance) 4.5 정보 은닉결론부터 말하자면, 파이썬은 정보 은닉을 지원하지 않는다. 정보은닉은 캡슐화할때 사용된다. 캡슐화하는 과정에서 어떤 멤버와 메서드는 공개해서 유저 프로그래머가 사용할 수 있게 해야하고, 어떤 멤버와 메서드는 숨겨서, 접근하지 못하도록 해야한다. 캡슐화는 그래서 정보 은닉까지 포함하는 개념이다. 파이썬이 그나마 제공하는 방법은 두 가지이다. 숨기려는 멤버 앞에 언더바 두개 붙이기(name mangling) 프로퍼티 기법 첫번째 방법을 사용해보자123456789101112class Account: def __init__(self, name, money): self.__name = name self.__balance = money def get_balance(self): return self.__balance def set_balance(self, new_bal): if new_bal &lt; 0: return self.__balance = new_bal 12my_acnt = Account(name = 'hyubyy', money = 5000)print(my_acnt.__dict__) 1{'_Account__name': 'hyubyy', '_Account__balance': 5000} 내 계좌에 5000을 넣어놨다.12my_acnt.__balance = -5000print(my_acnt.get_balance()) 내 계좌에 직접 접근해서 -5000을 하는 코드이다. 그런데print를 하게되면 결과는 5000이 나오게된다.어? 정보 은닉이 된게 아닐까?1print(my_acnt.__dict__) 1{'_Account__name': 'hyubyy', '_Account__balance': 5000, '__balance': -5000} -5000은 __balance라는 형태로 저장되어 있다. 숨겨진 형태로 저장될 때_Account__balance로 원래의 5000이 따로 저장된다. 클래스 안에서 멤버 앞에 언더바를 두 개 붙이면 이 멤버는 객체가 만들어질 때 이름이 변한다. 하지만 __dict__로 확인이 가능해서, 언제든지 접근해서 변경할 수 있다.1my_acnt._Account__balance = 8888 1print(my_acnt.__dict__) 1{'_Account__name': 'hyubyy', '_Account__balance': 8888, '__balance': -5000} 다음은 프로퍼티 기법이다.1234567891011121314151617181920class Account: def __init__(self, name, money): self.__name = name self.balance = money @property def balance(self): # getter function return self.balance @balance.setter def balance(self, money): # setter function if money &lt; 0: return self._balance = money if __name__ == '__main__': my_acnt = Account('greg', 5000) my_acnt.balance = -3000 print(my_acnt.balance) 실행 결과는 5000이다. 놀랍게도 balance가 2000으로 나오지 않았다. 위의 코드에서 특이한 점이 있는데 @property와 @balance.setter라는 부분이다.@property를 붙여주면 이 함수는 getter 함수가 되며, @balance.setter는 setter함수로 사용된다.따라서, my_acnt 객체에는 balance라는 멤버가 없다. balance라는 이름의 getter와 setter밖에 존재 하지 않는다. my_acnt.balance = -3000은 값을 변경하는 것처럼 보이지만, 실제로는 setter가 실행되고 그 결과로 _balance값은 변경되지 않는다. 하지만 프로퍼티 기법 역시 유저가 접근하는 것을 막을 수는 없다.마찬가지로1my_acnt._balance = -3000 으로 바꿔버리면 그만이기 때문이다.이처럼 파이썬은 완벽한 정보 은닉을 제공하지 않는다. 5. 객체지향으로 다시 만드는 학급 성적 평가 프로그램이제 사용자 프로그램을 더 심플하게 작성할 수 있게 되었다.statistics.py1234567891011121314151617181920212223from functools import reduceimport mathclass Stat: def get_average(self, scores): avrg = reduce(lambda score1, score2 : score1 + score2, scores) / len(scores) return avrg def get_variance(self, scores, avrg): tmp = 0 for score in scores: tmp += (score - avrg)**2 else: var = tmp / len(scores) return var def get_std_dev(self, var): std_dev = round(math.sqrt(var),1) return std_dev datahandler.py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from openpyxl import load_workbookfrom statistics import Statclass DataHandler: evaluator = Stat() @classmethod def get_data_from_excel(cls, filepath): wb = load_workbook(filename = filepath) ws = wb.active rows = ws.rows raw_data = {name_cell.value : score_cell.value for name_cell, score_cell in rows} scores = raw_data.values() return scores def __init__(self, filepath): self.scores = DataHandler.get_data_from_excel(filepath = filepath) self.cache = {'scores' : self.scores} def get_average(self): if 'average' not in self.cache.keys(): self.cache.update({'average' : DataHandler.evaluator.get_average(self.cache.get('scores'))}) return self.cache.get('average') else: return self.cache.get('average') def get_variance(self): if 'variance' not in self.cache.keys(): self.cache.update({'variance' : DataHandler.evaluator.get_variance(self.cache.get('scores'), self.get_average())}) return self.cache.get('variance') else: return self.cache.get('variance') def get_std_dev(self): if 'std_dev' not in self.cache.keys(): self.cache.update({'std_dev' : DataHandler.evaluator.get_std_dev(self.get_variance())}) return self.cache.get('std_dev') else: return self.cache.get('std_dev') def evaluate_class(self, total_avrg, total_std_dev): avrg = self.get_average() var = self.get_variance() std_dev = self.get_std_dev() print(\"평균:{}, 분산:{}, 표준편차:{}\".format(avrg, var, std_dev)) if avrg &lt; total_avrg and std_dev &gt; total_std_dev: print('성적이 너무 저조하고 학생들의 실력 차이가 너무 크다.') elif avrg &gt; total_avrg and std_dev &gt; total_std_dev: print('성적은 평균 이상이지만 학생들의 실력 차이가 크다. 주의 요망!') elif avrg &lt; total_avrg and std_dev &lt; total_std_dev: print('학생들의 실력 차이는 크지 않지만 성적이 너무 저조하다. 주의 요망!') elif avrg &gt; total_avrg and std_dev &lt; total_std_dev: print('성적도 평균 이상이고 학생들의 실력 차이도 크지 않다.') main.py 123456789101112131415from datahandler import DataHandlerimport argparse parser = argparse.ArgumentParser(prog = '평가프로그램', description = '엑셀에 저장된 학생들의 점수를 가져와 평균과 표준편차를 구하고, 학년 전체 평균과 비교하는 프로그램')parser.add_argument('filepath', type = str, help = '엑셀파일 저장경로')parser.add_argument('total_avrg', type = float, help = '학년평균')parser.add_argument('total_std_dev', type = float, help = '학년표준편차')args = parser.parse_args()def main(): datahandler = DataHandler(filepath = args.filepath) datahandler.evaluate_class(total_avrg = args.total_avrg, total_std_dev = args.total_std_dev)if __name__ == '__main__': main()` 유저 프로그램인 main.py script를 실행시키면 아래와 같다.123456789101112$ python main.py --helpusage: 평가프로그램 [-h] filepath total_avrg total_std_dev엑셀에 저장된 학생들의 점수를 가져와 평균과 표준편차를 구하고, 학년 전체 평균과 비교하는 프로그램positional arguments: filepath 엑셀파일 저장경로 total_avrg 학년평균 total_std_dev 학년표준편차optional arguments: -h, --help show this help message and exit 1$ python main.py ./class_1.xlsx 50 25 12평균:51.5, 분산:1240.25, 표준편차:35.2성적은 평균 이상이지만 학생들의 실력 차이가 크다. 주의 요망!","link":"/2019/01/28/oop/"},{"title":"지도 학습과 쉬운 모델을 사용하는 이유에 대해서","text":"지도학습과 쉬운 모델을 사용하는 이유는 뭘까?출처 : [파이썬 라이브러리를 활용한 머신러닝] 지도 학습에 대해서 알아보자도마뱀 책의 지도 학습에는 상당한 분량의 내용이 있지만, 그 중 쉬운 모델, 선형모델과 나이브 베이즈 모델에 대해서 살펴볼 것이고, 왜 굳이 쉬운 모델을 사용해야 하는지에 대해서 알아보려 한다. 그 전에 잠깐 KNN(K-Nearest Neighbors)알고리즘에 대해서 살펴보자. K-NN (k-Nearest Neighbors)K-NN알고리즘은 가장 간단한 머신러닝 알고리즘 중 하나이다.이 알고리즘은 말 그대로 훈련 데이터셋에서 가장 거리가 가까운 데이터 포인트, ‘최근접 이웃’을 찾는다. K-NN을 이용해서 분류와 회귀를 할 수 있다.간단히 분류만 알아보자 123from sklearn.model_selection import train_test_splitX, y = mglearn.datasets.make_forge()X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) 12from sklearn.neighbors import KNeighborsClassifierclf = KNeighborsClassifier(n_neighbors=3) 데이터를 불러왔고, 알고리즘을 임포트 해서 인스턴스화 하였다.여기서 n_neighbors=3이라고 설정했는데 이 뜻은 데이터 포인트 주위의 이웃을 세 개만 보겠다는 뜻이다. 이웃의 수는 적을 수록 모델을 복잡하게 만드는 것이며, 많아질 수록 모델을 단순하게 만든다. 이웃의 수가 많아지게 되면 결정 경계는 부드러워진다. 이웃의 수가 하나라면, 이 모델은 훈련 데이터에서 완벽하게 예측을 할 수 있게 된다. 하지만 이웃의 수가 늘어나면, 모델은 단순해지고 훈련 데이터의 정확도는 떨어지게 된다. 일반화 되는 것이다. 1clf.fit(X_train, y_train) 1print('prediction of test set : {}'.format(clf.predict(X_test))) 이렇게 예측을 하고 결과값을 뽑아낼 수 있다. 예측을 하는 방법은, 테스트 세트의 각 데이터 포인트에 대해 훈련 세트에서 가장 가까운 이웃을 계산하고 가장 많은 클래스를 찾는 방식이다. 정리를 해보자면, K-NN 분류기의 중요변수는 두 개이다. 데이터 포인트 사이의 거리를 재는 방법 이웃의 수거리를 재는 방법은 주로 유클리디안 거리 방식을 사용한다. 일반적으로 노름(Norm)이라고 알려져 있는 방식이다. K-NN의 장점은 이해하기 매우매우매우 쉬운 모델이라는 것이다. 그리고 별로 조정할 것 없이 꽤 성능이 잘 나온다. 그래서, 이 알고리즘은 복잡한 알고리즘을 적용하기 전에 시도할 수 있는 좋은 시작점이 될 수 있다. K-NN을 한번 돌려보면서 데이터를 파악해 볼 수 있는 것이다. 하지만 K-NN은 데이터 셋이 매우 커지면 예측이 느려진다. 또한 전처리하는 과정이 중요하다. 유클리디안 거리를 재는 방식이기 때문에, 특성마다의 값의 범위가 달라지면 범위가 작은 특성에 영향이 매우 커지게 된다. 그래서 K-NN을 사용하기 전에는 Scaling해주는 작업이 필요하다. 또한 K-NN은 많은 특성을 가진 데이터 셋에는 잘 동작하지 않고, Sparse한 데이터 셋에서는 잘 동작하지 않는다. 결국 전처리가 중요한 모델이다. K-NN은 그래서 단점이 꽤 있는 모델 중에 하나이다. 이해하긴 쉬워도 예측이 느린편이고, 많은 특성을 처리해야 하는 작업에 어울리지 않아, 현업에서는 잘 사용되지 않는다. 그래서 단점이 별로 없는 모델을 사용하게 되는데, 그것이 바로 선형 모델이다. 선형 모델, Linear Model회귀의 경우 선형 모델을 위한 일반화된 예측 함수는$\\hat{y}$ = $w$$\\times$ $\\vec{x}$ + b 이다.여기서 w는 기울기이고 b 는 절편값이 된다. 회귀를 위한 선형 모델은 특성이 하나일 땐 직선, 두 개일 땐 평민이며, 더 높은 차원일 경우에는 hyperplane이 되는 특징을 갖고 있다. 특성이 많은 데이터 셋이라면 선형 모델은 매우 훌륭한 성능을 낼 수 있다. 특히 훈련 데이터보다 특성이 더 많은 경우에 선형 함수로 모델링이 잘 된다. 최소제곱법선형 회귀는 OLS(Ordinary Least Squares)라고도 불리며, 가장 간단하고 오래된 회귀용 알고리즘이다. 선형 회귀는 MSE(Mean Square Error)를 최소화 하는 파라미터 w와 b를 찾는다. 평균제곱오차는 예측값과 타깃값의 차이를 제곱하여 더한 후에 샘플의 개수로 나눈 것이다. 선형 회귀는 매개 변수가 없다, 이것은 장점이기도 하고 복잡도를 제어할 별 방법이 없다는 것을 뜻하기도 한다. 선형 모델은 다음과 같이 만들 수 있다. 12345from sklearn.linear_model import LinearRegressionX, y = mglearn.datasets.make_wave(n_samples=60)X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=43)lr = LinearRegression().fit(X_train, y_train) 데이터 셋이 복잡해 지면서 모델은 과대적합이 될 가능성이 높아진다. 이럴 때 규제가 필요하게 되는데,주로 사용되는 모델은 릿지 회귀이다. Ridge Regularization릿지 회귀에서의 가중치 선택은 훈련 데이터를 잘 예측하기 위해서와 더불어 추가 제약 조건을 만족시키기 위한 목적도 있다. 가중치의 절댓값을 가능한 한 작게 만드는 것이다. 즉, w의 모든 원소가 0에 가깝게(0이 되지는 않는다.) 되길 원한다. 이렇게 되면, 모든 특성이 출력에 주는 영향을 최소한으로 만들게 된다.(기울기가 작아진다.) 이것을 Regularization이라고 부른다. Ridge Regularization은 L2규제라고 부르기도 한다.L1은 Lasso이다. 123from sklearn.linear_model import Ridgeridge = Ridge().fit(X_train, y_train)print('훈련 세트 점수 :{:2f}'.format(ridge.score(X_train, y_train))) 릿지 회귀는 다음과 같이 사용할 수 있다.릿지를 사용하게 되면 모델이 더 일반화 되어서 테스트 셋에서 성능이 좋게 된다.사용자는 하이퍼 파라미터 alpha로 훈련세트의 성능 대비 모델을 얼마나 단순화할지를 정할 수 있다. alpha값을 높이면 규제를 높여서 일반화에 도움을 주는 것이고, alpha를 낮추면 규제를 낮춰서 모델을 복잡하게 만드는 것이다. Lasso라쏘는 L1규제라고도 하며, 릿지와의 차이점은, 라쏘는 어떤 계수는 정말 0이 된다는 점이다. 모델에서 완전히 제외되는 특성이 발생한다. Feature Selection이 자동적으로 이루어지게 되는데, 중요특성을 뽑고 싶다면 Lasso를 활용해도 된다. 실제로 라쏘와 릿지 중에서는 릿지를 더 선호한다. 하지만 특성이 많고 그중 일부분만 중요하다면 Lasso가 더 좋을 수도 있다. 또한 특성이 줄어들어 쉽게 해석할 수 있기 때문에, 라쏘가 사용되는 경우도 있다. 하지만 최상의 방법은 Elastic Net으로 L1과 L2를 섞은 것이다. 둘의 매개변수를 잘 조정하면 최상의 결과가 도출될 수 있다. Naive Bayes 분류기나이브 베이즈는 선형 모델과 매우 유사하다. 훈련 속도도 빠르고 단순하지만 성능이 좋은 편이다. 하지만 일반화 성능은 조금 뒤진다. scikit-learn의 나이브 베이즈에는 Gaussian, Multinomial, Bernoulli 총 세가지가 구현되어 있다. Gaussain은 연속적인 어떤 데이터에도 적용할 수 있고, Bernoulli는 이진 데이터를, Multinomial은 카운트 데이터에 적용된다. 가우시안은 연속, 베르누이와 다항은 이산 데이터에 적용된다고 생각하면 된다. MultinomialNB와 BernoulliNB는 모델의 복잡도를 조절하는 알파 하이퍼파라미터를 갖고 있다. 알파가 주어지면 알고리즘이 모든 특성에 양의 값을 가진 가상의 데이터 포인트를 알파 개수만큼 추가한다. 이렇게 되면 통계 데이터가 완만해진다. 알파가 크면 더 완만해지고 모델의 복잡도는 낮아진다. 하지만 알파는 성능에 크게 기여하지 않는다. GaussianNB는 대부분 고차원인 데이터셋에 사용하고, 다른 나이브 베이즈 모델들은 텍스트 같은 희소한 데이터를 카운트 하는데 사용된다. MultinomialNB는 0이 아닌 특성이 많은 데이터셋(큰 문서)에서 Bernoulli보다 성능이 좋다. 나이브 베이즈 모델과 선형 모델의 장단점은 비슷하다. 훈련과 예측 속도가 빠르고 훈련 과정을 이해하기가 쉽다. 일단 한번 빠르게 돌려보고 과정을 보면서 데이터에 대해 이해할 수 있게 된다는 것이다. 또한 희소한 고차원 데이터에서 잘 작동하고 비교적 하이퍼 파라미터에 민감하지 않다. 선형 모델로 일단 한번 훅 돌려보고 너무 오래 걸린다 싶으면 나이브 베이즈 모델을 시도해서 돌려볼만 하다.","link":"/2019/02/28/supervised-learning/"},{"title":"ubuntu를 이용한 딥러닝 Server 설치하기 1","text":"ubuntu 설치와 트러블슈팅 Installation of Ubuntu 18.04 LTS for Deep Learning Computer -1Ubuntu Installation - The Trouble Shooting평소에 캐글 대회에 관심이 있었고, 제대로 대회에 참여해봐야지 하는 생각이 있었습니다. 그러다가 IEEE 대회에 참여해 봤습니다. 결과는 노메달!, 동메달이라도 따보고 싶었는데 아쉽게 메달을 놓치게 되었습니다. 그때 정말로 캐글 대회에서 메달을 따보고 싶다는 생각이 들었고, 올해가 되기 전에 대회에 한번 더 참여하는 것과, 내년 초까지 Competition으로 Expert를 따보자는 계획을 하게 되었습니다. 계획을 위해서는 딥러닝용 사양 좋은 PC가 필수라는 생각이 들었고, 시원하게 2백만원 정도의 사양의 컴퓨터를 맞추게 되었습니다. 딥러닝용 PC서버를 만들기 위해서 OS를 정해야 했는데, 캐글 고수분들께서 Ubuntu로 세팅을 많이 하시는 걸 보고 바로 usb에 부팅용 iso를 받아서 시동 디스크를 만들었습니다. 제가 주로 보고 따라한 블로그 주소는 다음과 같습니다. 장지수님 블로그, 개인 딥러닝용 서버 설치 과정기 이 과정을 보고 쭉 따라했고, usb를 넣고 부팅을 시켰더니, Ubuntu 18.04 USB 설치 error (Mac OS 이용)쉽게 될 줄 알았더니 똭!! 에러가 떠버렸습니다. Google신을 답을 알고 계시겠죠? 하지만 답을 쉽게 주지는 않으시는 것 같습니다. 3시간의 서칭 끝에 ‘nouveau unknown chipset’, 누보(새 시작 이라는 말이라고 합니다)에 대한 해결법을 찾았습니다. 부팅 디스크를 넣고, ubuntu Grub화면까지 간 다음(grub 화면이 안나올 경우 shift를 누르면 된다고 하네요), e를 눌러서 텍스트 편집기로 갑니다. 이후 나타나는 화면에서 quiet splash 혹은 quiet ---로 표시된 부분으로 갑니다. quiet splash nomodeset 또는 quiet splash nomodeset을 입력 후 F10을 눌러 저장하고 리부팅 합니다. 보통 이렇게 하면 문제없이 ubuntu화면으로 넘어가서 설치가 될 것입니다. 하지만 신은 저에게 편한 삶을 허락하지 않는 것 같습니다. 일요일에 좀 쉬고싶었는데 눈치없이 등장하는 에러들… couldn't get size 0x80000000000e와 MODSIGN: Couldn't get UEFI db list 그리고 line 7: can't open /dev/sdc: No medium found. 또 다시 Google 신에게 달려가서 2시간 동안 서칭하고 실패하고를 반복했습니다. 그러다가 사막의 오아시스 같은 솔루션을 발견했습니다. line 7 no medium found 솔루션 3 보트업 받은 솔루션을 보면 약간의 야매 방법이 등장합니다. 아까 nouveau 메시지에 대한 해결을 하기 위해 nomodeset을 사용했는데 그것 대신 break debug를 입력하는 것 입니다. 이렇게 하면 debug 메시지가 쭈우욱 등장하고 엔터를 눌렀을 때 입력할 수 있는 터미널이 나오게 됩니다. 여기서 부팅 디스크인 USB를 빼고 다시 USB를 넣은 후에 exit을 입력하고 엔터를 누릅니다. 이렇게 되면 화면이 후루룩 쭉죽 지나가면서 된다 된다…라는 희망에 빠지게 됩니다. 성공입니다. ubuntu 설치 화면으로 정상적으로 이동하였습니다. 여기서부터 세팅은 안내에 따라서 착착 맞춰서 하면 되고, 쭉 진행 하면 성공적으로 username과 서버명이 등록된 ubuntu 화면을 볼 수 있게 됩니다. 잠시 달콤한 성공을 맛봤으니 원격 부팅 세팅을 해보아야겠습니다. 블로그 설명에 따라 1sudo vi /etc/ssh/sshd_config 를 쳐봅니다. ????? 그런건 없습니다. ssd_config를 오타냈나 싶어서 쳐보고 확인해보지만 블로그 설명과는 다른 값이 나옵니다. 찾아보니 1sudo apt-get install openssh -server 을 해야 만들어진다고 합니다. 자 그럼 sudo apt-get install openssh -server를 입력해봅니다. Unable to fetch ~~가 나올 것입니다. 이것은 현재 ubuntu 라이브러리를 받는 서버가 맛이 갔거나, 현재 인터넷 연결이 안되어 있다는 말입니다. 또 다시 문제에 봉착했습니다. 인터넷 연결이 안되어있습니다. Ubuntu에서는 인터넷 연결을 어떻게 하는 것일까요?또 다시 서칭을 시작했습니다. 인생은 깁니다. 주말은 짧습니다. 일단 떡볶이를 먹고 제정신을 차린 후 다시 시도해보았습니다. 여러 솔루션들이 나왔지만 고정 ip를 이용하는 글이 대부분이었고 제가 원하는 답은 없는 것 같았습니다. 먼저 1vi /etc/network/interfaces 명령어를 통해서 네트워크 설정이 어떻게 되어있는지 확인했습니다. 보통의 경우라면 12345678# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto eth0iface eth0 inet dhcp 이렇게 나와있는 게 정상이고 default 값이라는 얘기가 많아서 똑같이 설정을 해주었습니다. 하지만 리부트해도 여전히 인터넷 연결은 되지 않았습니다. 하지만 스스로 답을 구하는 자에게는 복이 주어집니다. fresh ubuntu installation을 한 사람이 인터넷 연결이 안된다는 글을 올린 것을 보았고 솔루션 내용은 다음과 같았습니다. 12sudo ifconfig eth0 up sudo dhclient eth0 물론 바로 되지는 않습니다. 세상에는 쉬운일이 없습니다. 항상 자신의 상황에 맞는지 확인하고 문제에 쏙 맞게 다듬을 필요가 있습니다. 문제는 eth0 부분이었습니다. eth0에 해당하는, 자신의 값을 알아야 했습니다. ifconfig -a를 이용하면 logical name을 알 수 있습니다. 저의 경우에는 enp34s0였습니다. 이 값을 다시 넣어서 엔터를 눌러주고1ping www.google.com 로 테스트를 해주면, 이제 구글에 ping을 날릴 수 있는 것을 확인할 수 있습니다. 일요일을 다 날렸지만, 이제 나름대로 세팅할 수 있는 환경을 만들어 놔서 뿌듯합니다. 월요일에 퇴근하고 이제 원격 부팅 접속을 만져봐야겠습니다.","link":"/2019/11/10/ubuntu/"},{"title":"programmers the Biggest Number","text":"프로그래머스 코딩테스트 연습문제 가장 큰 수를 풀어봤다가장 큰 수 문제 설명 0 또는 양의 정수가 주어졌을 때, 정수를 이어 붙여 만들 수 있는 가장 큰 수를 알아내 주세요. 예를 들어, 주어진 정수가 [6, 10, 2]라면 [6102, 6210, 1062, 1026, 2610, 2106]를 만들 수 있고, 이중 가장 큰 수는 6210입니다. 0 또는 양의 정수가 담긴 배열 numbers가 매개변수로 주어질 때, 순서를 재배치하여 만들 수 있는 가장 큰 수를 문자열로 바꾸어 return 하도록 solution 함수를 작성해주세요. 제한 사항 numbers의 길이는 1 이상 100,000 이하입니다.numbers의 원소는 0 이상 1,000 이하입니다.정답이 너무 클 수 있으니 문자열로 바꾸어 return 합니다. 처음 이 문제를 봤을 때, ‘어 permutation 쓰면 끝이네 개꿀ㅎㅎ’ 이런 생각이 들었다.바로 itertool을 import 해서 1234567891011from itertools import permutationsdef solution(numbers): str_list = [] for i in numbers: str_list.append(str(i)) first=list(map(''.join, permutations(str_list))) int_list = [] for li in first: int_list.append(int(li)) answer = sorted(int_list, reverse=True)[0] return str(answer) 이런 코드를 작성해서 제출했다. 결과는!! 시간초과가 떠 버렸다. 효율성이 제로라는 말이다.검색해보니 permutation은 필요하지 않은 부분까지 순열 조합을 만들어 내기 때문에굉장히 비효율적인 코드라는 것을 알아냈다. ‘그렇다면 순열같이 코드를 짜되 효율적으로 작성해야 한다는 것인가?’ 라는 고찰과 함께코딩을 시작했고 하루를 날렸다. 당연했다. 문제푸는 방향이 완전히 잘못되었었다. 효율적으로 순열조합 만드는 코드를 짠다면 내가 라이브러리를 새로 만드는 수준인 것이었다. 방향을 다시 생각해봤다.사실 이 문제를 풀다보면 list에 있는 원소를 편하게 처리하기 위해 str으로 바꿔야 하고 비교하기 위해int로 다시 바꿔줘야 하는 번거로움이 있다. 그런데 굳이 int–&gt;str 이런식으로 바꿔줄 필요가 없다. 왜냐하면 정수모양의 str도 정수 값이 증가함에 따라 메모리 값도 증가하기 때문이다. 이를 id()를 통해 확인해 볼 수 있다. 12345print(id('1'))print(id('2'))print(id('3'))print(id('4'))print(id('5')) 123456Out:43800979284380098040438011123243801112884379241192 이를 이용해서 문제를 푼다면 다음과 같은 코드를 작성할 수 있다. 12345def solution(numbers): numbers = list(map(str, numbers)) numbers.sort(key=lambda x : x*3, reverse = True) answer = str(int(''.join(numbers))) return answer 결론 : 메모리 값에 대한 지식이 있다면, 훨씬 간단하게 문제를 해결할 수 있다!","link":"/2019/04/12/programmers-the-Biggest-Number/"},{"title":"Zeppelin으로 Spark를 다뤄보자 01","text":"pyspark로 데이터를 읽으려면 어떻게 해야할까? Zeppelin 이용해서 pyspark로 데이터 읽기 01Spark는 고속 범용 분산 컴퓨팅 플랫폼으로 정의되곤 합니다. 대용량 데이터를 가져와 빠르게 분석해 낼 수 있다는 점에서 많은 기업들에서 도입을 검토하고 있고 실제로도 많이 사용되고 있습니다. 오늘은 이 유명한 Spark를 다운받고 Zeppelin으로 띄워서 pyspark를 이용해 데이터를 읽어보는 작업까지 해 보겠습니다. 먼저 Spark를 다운받아 줍니다. Spark 설치 링크로 들어가면 다음과 같이 나오는데 다운받는 버전은 아무거나 받아도 상관 없지만 저는 AWS EMR로 Spark를 도입하기 전에 연습하는 용으로 사용하는 것이기 때문에 AWS EMR 버전과 같은 2.4.4버전을 다운받았습니다. 하둡 버전은 사진 그대로 2.7버전으로 진행 했습니다. 다운이 완료되면 폴더를 만들어서 그곳에 저장해주고 압축을 풀어줍니다. tgz로 되어있는 파일은1tar -xvf [filename] 이렇게 풀어줍니다. 다음은 Zeppelin입니다. Zeppelin 설치 제플린도 역시 두 가지 버전이 등장하는데, 저는 용량이 작은 버전으로 받았습니다. 큰 용량의 버전은 카산드라 등이 다 포함된 버전이기 때문에 굳이 받지 않았습니다. 제플린도 특정 폴더에 저장해주고 압축을 풀어줍니다. Spark 경로 지정Spark의 경로를 잘 지정해줘야 Zeppelin이 실행되고 코드를 돌렸을 때 오류가 나지 않습니다.먼저 쉘의 프로파일을 열어줍니다. 저는 zsh을 사용하기 때문에 zshrc를 열겠습니다.1vi ~/.zshrc 그 다음 설정해야 할 것은 java home 경로입니다. jdk가 없다면 jdk 1.8이상 버전을 다운받아 설치합니다.java home 경로는1echo $JAVA_HOME 이 명령어로 알아낼 수 있습니다. java home의 경로를 알아냈다면 zshrc에 이 위치를 알려줘야합니다. 1export JAVA_HOME=\"/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\" 저의 경우는 위치가 다음과 같아서 zsh의 아래쪽에 작성해 주었습니다. 그리고 설치된 Spark의 위치도 알려줘야 합니다. 아까 저장했던 폴더의 주소를 입력해 줍니다.1export SPARK_HOME=/Users/sanghyub/spark-2.4.4-bin-hadoop2.7 저의 경우는 이렇게 되어있습니다. 절대경로로 작성해 주시면 됩니다. Spark 세팅은 일단 여기까지 하고 Zeppelin으로 넘어가겠습니다. Zeppelin 환경 설정Zeppelin이 저장된 폴더로 들어가서 conf로 들어가줍니다. conf에는 ls를 입력해보면 여러 파일들이 있는 것을 볼 수 있습니다. 이 파일들 중에서 zeppelin-env.sh와 zeppelin-site.xml을 사용해야 하는데, .template으로 된 파일들이 보일 것 입니다. template를 cp를 이용해서 바꿔줍니다. 1cp zeppelin-env.sh.template zeppelin-env.sh cp는 복사하는 것도 있지만, 이렇게 이름을 바꿔주는데에도 사용됩니다.zeppelin-env.sh와 zeppelin-site.xml을 얻었다면 vi를 이용해서 zeppelin-env.sh로 들어갑니다.아까 작성한 자바 경로와 스파크 홈 경로를 그대로 갖고와서 작성해줍니다. zeppelin이 이 위치를 보고 Spark와 jdk를 이용할 수 있도록 적어두는 것 입니다. zeppelin의 포트도 수정해 줍니다. 기본 포트는 8080포트인데 혹시 충돌될 수 있으니, 저는 안정적으로 9999포트로 변경하겠습니다. 이제 기본적인 세팅은 끝났고 zeppelin을 실행시켜 봅니다. Zeppelin 실행Zeppelin 실행은 jupyter notebook여는 것과는 조금 다릅니다. zeppelin을 입력해도 아무일도 일어나지 않습니다. Zeppelin을 열기 위해서는 zeppelin daemon을 실행시켜줘야 합니다. zeppeliln daemon은 bin폴더에 있습니다. conf에서 빠져나와서 bin으로 들어가줍니다.1cd ../bin ls를 입력하면 찾았던 daemon이 보일 것 입니다. 너무 반갑지만 쉘이 익숙하지 않다면 실행하는 방법을 모를 것입니다. 저도 그랬고 같이 공부했던 사람들도 눈치만 봤었습니다. 백날 눈치를 보고 째려봐도 실행은 되지 않습니다. 1./zeppelin-daemon.sh start 이렇게 데몬을 실행시켜줍니다. 확인은 (https://localhost:9999)로 들어가서 해 보면 됩니다. 짠! 제플린의 날개가 등장했습니다. 이제 노트를 만들고 데이터를 로드해보는 작업을 하겠습니다.","link":"/2020/01/06/spark-zeppelin/"},{"title":"Spark에서 데이터 분석 시, RDD로 연산하면 안되는 이유","text":"Spark를 사용해서 데이터를 읽고 분석하자 데이터 분석하기 전, 데이터부터 읽자Spark Session, conf 설정기존 python의 pandas를 이용해서 데이터를 읽으려면 pd.DataFrame(‘…….’)를 통해 파일을 읽으면 간단히 해결 되었다. 하지만 spark에서 데이터를 읽기 위해서는 조금 더 손을 거쳐야 한다. 물론 Zeppelin을 이용한다면 바로 파일을 읽어들일 수 있겠지만, pycharm을 이용해서 pyspark application을 만드는 작업을 할 것이기 때문에 직접 spark세팅을 해주어야 한다. pycharm에서는 Spark Session을 설정해줘야 spark를 사용할 수 있다. 이 Spark Session에 대한 설정값으로 Spark Conf를 설정해주어야 한다.먼저 필요한 라이브러리를 불러들이고 Spark conf와 session을 설정한다.123456from pyspark.conf import SparkConffrom pyspark.sql import SparkSessionconf = SparkConf()conf.set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.11:2.3.1')spark = SparkSession.builder.appName(\"spark test\").config(conf=conf).getOrCreate() conf.set에는 mongoDB와의 연결을 위한 spark connector를 넣어줬다. 이렇게 하면 mongoDB에 있는 데이터를 바로 읽을 수 있을까? 아직 할 작업이 조금 남았다. data를 불러오기 전에 스키마 지정을 해줘야 하기 때문이다. 스키마 지정스키마란 간단하게 말해서 데이터 구조와 제약 조건에 대한 명세(Specification) 기술한 것을 의미한다.여기서 설정할 스키마는 이 데이터의 칼럼이 어떤타입으로 들어갈 것인지(string, integer, double …)를 주로 뜻하게 될 것이다.mongoDB에서 사용자들이 거래한 내용 중 카트에 어떤 상품을 담았는지 알기 위해서 다음과 같이 코드를 작성했다. 1234567891011121314cartSchema = StructType([ StructField(\"cartGoodsName\", StringType(),True), StructField(\"cartGoodsCode\", StringType(),True), StructField(\"cartGoodsAmount\", IntegerType(),True), StructField(\"cartGoodsCount\", IntegerType(),True), ])userSchema = StructType([ StructField(\"cookieId\", StringType(),True), StructField(\"currentTime\", StringType(),True), StructField(\"sessionSeq\", StringType(),True), StructField(\"userSeq\", StringType(),True), StructField(\"cart\", ArrayType(cartSchema),True) ]) 이렇게 카트 데이터에 대한 스키마를 작성해서 유저스키마의 cart 부분에 넣어준 뒤 합쳐진 userSchema를 이용해 데이터를 읽었다. 123456df = spark.read.schema(userSchema).format(\"com.mongodb.spark.sql.DefaultSource\") \\ .option(\"spark.mongodb.input.uri\", \"mongodb://******/*****.userDataInfo.******\") \\ .option(\"spark.mongodb.output.uri\", \"mongodb://******/*****.userDataInfo.******\") \\ .load() 읽은 결과는 따로 dataframe을 지정할 필요없이 바로 dataframe으로 떨어진다. 이제 바로 데이터에 대해서 작업을 수행할 수 있게 되었다.카트에 담은 상품이 무엇인지 알고 싶어서 actionType이 viewCart인 부분을 가져왔다. 1view_cart_df = df.filter(df.actionType =='viewCart') 가져오고 나서 전처리 작업을 하려고 했는데, 데이터프레임에 대한 이해가 적었었던 때라 어떻게 작업해야 할지 몰랐다. 그래서 먼저 RDD로 작업을 했고 뼈저리게 후회했다. 절대 발생하면 안되는 일이 일어났기 때문이다. RDD를 사용한 결과RDD를 사용해서 전처리를 해보고 Cart에 담긴 Top N개의 상품을 가져와보기로 했다.12345def get_info(x): for i in x: for k in i: test = Row(code=k[1],cart_count=k[3]) return test RDD를 이용해 전처리를 할 때 쓸 함수를 지정해 놓고 작업을 하기로 했다. 함수는 다음과 같이 작성했고 상품의 코드와 그 상품이 얼마나 담겼는지를 Row로 생성했다. 1234df = df.filter(df.cart.isNotNull()).withColumn(\"currentTime\", to_timestamp(\"currentTime\", \"yyyy-MM-dd HH:mm:ss\"))view_cart_count = df.select('cart').rdd.map(get_info).toDF()view_cart_count.groupBy('code').count().show() 이렇게 만든 함수를 df의 cart에서 rdd의 map을 이용해서 결과를 가져왔다. 그리고 상품코드 별로 그룹화 하고 sum을 해서 결과를 출력했다. 그런데 뭔가 이상했다. sum을 했으면 결과값이 적어도 100은 넘어야 했는데, 100넘는 값이 너무 적었다. 그래서 특정 상품코드에 대해서 python으로 데이터 분석을 실시해서 결과를 매칭시켜 비교해보기로 했다. 결과가 너무 차이가 났다. 이렇게 나온 결과로 아이템을 추천하게 되면 제대로 된 상품이 추천되지 않을 것이다. RDD에 함수를 map하는 것에 뭔가 문제가 있는 것이 분명했다. 방법을 찾다가 Dataframe으로 작업을 해보기로 했다. Dataframe을 사용한 결과데이터 프레임으로 작업해야 결과값이 바뀌지 않는 다는 정보를 알게 되어 기존에 있던 df에 filter를 걸어 새 DF를 만들고 이걸 가지고 전처리 해보기로 했다.123456789cartDF = df.filter(df.cart.isNotNull()).withColumn(\"currentTime\", to_timestamp(\"currentTime\", \"yyyy-MM-dd HH:mm:ss\")).select(\"cart\") \\ .withColumn(\"cart\", explode(\"cart\"))cart_all = cartDF.withColumn(\"goodsCode\", cartDF[\"cart\"].getItem(\"cartGoodsCode\"))\\ .withColumn(\"goodsCount\", cartDF[\"cart\"].getItem(\"cartGoodsCount\"))results_df = cart_all.groupby('goodsCode').sum().orderBy('sum(goodsCount)', ascending=False) 작업은 다음과 같이 실시했고 상품 갯수를 정렬하기 위해서 orderBy를 사용했다.결과는 어떻게 나왔을까? python을 사용한 결과와 똑같은 값이 등장했다. 성공했다!!! 왜 값이 다를까?그렇다면 왜 RDD를 사용해서 함수를 적용할 때랑 Dataframe을 갖고 작업한 결과가 다른 것일까?일단1view_cart_count = df.select('cart').rdd.map(get_info) 이 코드에서 rdd.map한 부분까지 가져와서 확인해보니 결과값이 많지 않았다. rdd에서 df로 바꿀때 데이터가 변하는 일은 없다는 것이다.그렇다면 이 코드 전에 rdd.map(get_info)하는 부분에서 변형이 일어난 거라고 추측할 수 있다. 하지만 spark 이론에서 map을 적용할 때는map 자체가 narrow transformation에 해당되기 때문에, shuffle이 일어나지 않는다고 나와있다. 결국 shuffle에 의한 데이터 변형의 가능성도 없다고 할 수 있는 것이다. 함수 자체에 이상이 있는 것일까? 그렇다고 보기엔 어렵다. 이 코드를 갖고 구매-할인율에 대한 것을 집계했을 때는 정확한 값이 나왔기 때문이다. 조금 더 공부해보고 왜 값이 다른지에 대해서는 추후에 계속 수정을 해 나가야겠다. 결국은 spark에서는 RDD를 사용할지 Dataframe을 사용할지, 그리고 Dataset을 사용할지 먼저 생각하고 작업하는 것이 중요하다.이것에 관련해서는 Databricks에서 나온 문서가 있는데, 이것은 추후에 번역해서 업로드할 예정이다.","link":"/2020/01/16/spark-in-action/"},{"title":"ubuntu를 이용한 딥러닝 Server 설치하기 2","text":"서버에 원격 접속하기와 원격 파일 전송하기 Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2Ubuntu with mac, 원격 접속 연결하기간밤에 평안하셨습니까? 지난번까지는 ubuntu설치를 열심히 했고, 원격 접속은 쉽게 해결할 것이라고 생각하며 가볍게 노트북을 열어서 신나게 원격접속 연결을 시도했습니다. 퇴근 후에 저녁을 먹고 여유롭게 9시부터 만지기 시작했는데, 어느새 11시가 넘어가고 새벽 1시가 넘어가곤 했습니다. 그렇게 며칠을 지내고서야 맥북으로 ubuntu서버에 접속을 할 수 있었습니다. 이번 글을 쓰기 까지 간밤에 한 일들은 다음과 같습니다. ubuntu 인터넷 연결 KT 공유기 고정 아이피 설정 포트 포워딩 원격 접속 원격 파일 전송 nvidia 그래픽 드라이버 설치 Cuda 설치 CuDnn 설치 tensorflow, keras 설치 jupyter notebook 연결 각 단계에서 다음 단계로 넘어가기 까지 우여곡절이 많았는데, 해결할때마다 은근 쾌감이 있어서 기분 좋았습니다. 리눅스, 그러니까 검은 화면만 보면 겁을 실컷 먹었었는데, 그 두려움을 극복해가는 스스로가 대견스럽다고 생각을 해봅니다. 이번 글은 ubuntu 인터넷 연결부터 원격 파일 전송까지가 되겠습니다. 가장 힘들었던 부분은 네트워크를 이해하지 못해서 삽질을 여러번해서 애먹었던 부분입니다. 이 부분은 KT 공유기 고정 아이피 설정에서 민낯을 샅샅이 밝힐 예정입니다. 1. ubuntu 인터넷 연결저는 공유기에 있는 랜선을 서버용 컴퓨터에 끼워주면 알아서 인터넷 연결이 될 줄 알았지만 그렇지 않았습니다. 그런 것은 검은 화면이 아닌 곳에서나 가능하다는 것을 왜 그때는 제대로 깨닫지 못했을까요? ubuntu 서버 화면으로 넘어가서 vi 편집기를 이용해 다음 명령어를 날려줍니다.1vi /etc/network/interfaces 그러면 무언가 입력할 것이 있는 창이 하나 나와 있을 것 입니다.1234567# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto lo# The primary network interfaceauto eth0 저의 경우에는 이렇게 들어있는 내용이 얼마 없었던 걸로 기억합니다.(당시에 경황이 없어서 화면 사진을 하나도 찍지 못했네요ㅜㅜ)ifconfig명령어를 이용해 현재 이더넷의 주소를 알아내야합니다. 저의 경우에는 enpxxxx이었습니다. 이 값을 기억해두고 다시 interface로 들어가서 123456789# This file describes the network interfaces available on your system# and how to activate them. For more information, see interfaces(5).# The loopback network interfaceauto loiface lo inet loopback# The primary network interfaceauto enpxxxxiface enpxxxx inet dhcp 이런식으로 입력해 줍니다. 이 방식은 유동 ip일때 사용하는 방식이기는 하지만, 일단 인터넷 연결이 되어야 ubuntu 라이브러리를 받을 수 있기 때문에 설정해 줍니다. 설정이 다 되었다면1sudo service networking restart 를 이용해서 network를 재시작해 반영시켜줍니다. 2. KT 공유기 고정 ip 설정이제 KT공유기의 ip를 고정시켜줄 차례입니다. 고정 아이피를 잡아주는 이유는 간단합니다. 외부 통신을 통해(예를 들어 맥북을 통해) 서버로 접속한다고 했을 때, 고정 ip가 잡혀있지 않다면, 동적 아이피로 인해 서버의 ip 주소는 계속 바뀔 것이고 결국 접속이 안될 것입니다. 정말 간단하게 설정할 수 있기 때문에 제가 보고 설정한 링크를 첨부해 놓겠습니다.KT 공유기 고정IP 설정 이렇게 고정 ip를 잡아주고 난 뒤에 서버 컴퓨터로 와서 ifconfig 명령어를 쳐서 ip를 확인하고 설정한 ip대로 나온다면 성공입니다. 이제 아까 dhcp설정을 static으로 고쳐줄 차례입니다.1vi /etc/network/interfaces 명령어를 통해 수정할 부분으로 가서123# The primary network interfaceauto enpxxxxiface enpxxxx inet static 이렇게 고쳐줍니다. 3. 포트포워딩포트포워딩을 하는 이유는 외부에서 접속을 하게 될 때 공유기로 인해 내가 접속하고 싶은 컴퓨터를 제대로 연결을 못 해주게 됩니다. 공유기가 많은 컴퓨터에 대해서 ip를 뿌려주고 있기 때문입니다. 그래서 어떤 포트로 접속을 해줘야 내가 원하는 컴퓨터에 연결이 되는지를 알려줘야 제대로 연결을 할 수 있게 됩니다. 따라서 포트포워딩은 특정 컴퓨터에게 특정 포트로 연결해 주는 작업을 말하는 것입니다. 포트가 일종의 이정표 역할을 하게 되겠습니다. 다시 KT 홈허브 관리페이지로 가서 장치 설정에 트래픽 관리로 가면 바로 포트포워딩을 볼 수 있습니다. 포트 포워딩 설정에서 다른 건 건드리지 않고 외부/내부 포트, 내부 ip 주소만 작성해줍니다. 외부/내부 포트에는 내가 열고 싶은 포트 번호를 입력해주고, 내부 ip에서는 고정 ip를 넣어줍니다. 일단 8888포트를 열어줍니다.(jupyter notebook이 켜지는 포트가 주로 8888이니까 이렇게 설정을 했습니다)사진출처 4. 원격접속원격접속을 위해서 우리가 연결할 포트가 필요합니다. ssh접속은 22번 포트를 사용하고, 보통 처음 산 컴퓨터에는 이런 설정이 제대로 되어있지 않은 경우가 많습니다. 그렇기 때문에 직접 포트를 열어주도록 하겠습니다. 서버컴퓨터로 돌아와서1sudo vi /etc/ssh/sshd_config 명령어를 입력해 줍니다. 여기서 sshd_config가 없다면, 1apt-get install openssh-server 를 통해 ssh를 설치해주면 됩니다. sshd_config를 보면 Port라고 써있는 부분이 있습니다. 해당 라인이 #로 주석 처리 되어 있거나, 포트 넘버가 적혀있지 않은 경우에는, i를 눌러 insert모드로 바꾸고 port에 22를 넣어줍니다. 그리고 esc를 누르고 :wq(저장 후 종료)로 빠져나옵니다. 그러면 원격접속을 위한 준비는 끝났습니다. 이제 맥북으로 돌아와 터미널을 열어줍니다.1ssh -p 22 [servername]@[ip address] 이 명령어를 입력해주면 연결이 됩니다. ssh로 22번 포트에 접속해 준다는 것이고, 설정한 서버이름과 서버가 사용하는 ip주소를 입력해주고 설정한 비밀번호를 입력해주면연결에 성공하게 됩니다.(*주의, 혹시 집에서 노트북도 같은 아이피를 쓰고 있는 상태에서 연결을 시도하게 되면 100% 연결 성공했다고 나오게 된다. 공유기를 통해 내부적으로 연결이 되어 있기 때문이다. 정확한 확인을 위해서 잠시 노트북의 wifi를 끄고 테더링을 걸어서 다시 확인해보자, 이렇게 해서 연결이 된다면 진짜 된 것이다.) 5. 원격 파일 전송원격 파일 전송은 로컬에 있는 파일을 서버에 보내고 싶을 때 혹은 서버에 있는 파일을 로컬로 가져오고 싶을 때 사용할 수 있습니다. 특히 Nvidia 그래픽 드라이버를 서버에 보내서 설치하고 싶을 때 아주 유용하게 사용할 수 있습니다. scp를 사용해도 되고, sftp를 사용해도 된다. 개인적으로 sftp를 더 선호하기 때문에 sftp로 파일 전송하는 법을 간략히 설명해보면,아까 원격접속을 했던 것과 마찬가지로1sftp [servername]@[ip_address] 앞에만 sftp로 바꿔주고 나머지는 똑같이 입력한다. 그러면 패스워드를 입력하라고 나오고 정상적으로 패스워드를 입력했다면 sftp&gt; 로 시작하는 간단한 화면이 등장한다. sftp는 쉘을 오픈한 폴더 위치에서 시작하게 되므로 파일이 있는 위치로 이동한 다음 쉘을 여는 것을 권장한다. 여기서 put명령어를 입력하고 보낼 파일이름을 끝까지 적으면, 서버에 파일이 전달되는 것을 확인할 수 있다. 이제 진짜 딥러닝 PC를 만들기 위한 기본 세팅이 끝났다. 퇴근 하고 집에와서 잠도 적게 자면서 연결했는데 생각보다 금방 끝나서, 그리고 회사에서도 연결이 되어서 뿌듯했었다. 이제 간밤에 악몽을 꾸게 만든 Nvidia 그래픽 드라이버 설치, CUDA 설치 그리고 CuDNN 설치가 남았다.","link":"/2019/11/22/ununtu2/"},{"title":"ubuntu를 이용한 딥러닝 Server 설치하기 3","text":"본격적으로 딥러닝을 하기 위한 준비과정 Installation of Ubuntu 18.04 LTS for Deep Learning Computer -2Ubuntu with mac, 원격 접속 연결하기저번 글에서는 공유기를 이용한 고정ip설정과 포트포워딩, 원격 접속, 원격 파일 전송에 대해서 알아봤습니다. 이번 글에서는 본격적으로 딥러닝을 하기 위한 Cuda와 CuDnn, tensorflow, keras 설치에 이어 jupyter notebook 연결까지 해보겠습니다. 설치하는 순서는 다음과 같습니다. nvidia 그래픽 드라이버 설치 Cuda 설치 CuDnn 설치 tensorflow, keras 설치 jupyter notebook 연결 1. nvidia 그래픽 드라이버 설치그래픽 드라이버 설치는 겜돌이들에게는 너무나 익숙한 일입니다. 예전엔가 카트를 하고 싶었는데 화면이 까맣게 나와서 네이버 지식인에 물어봤더니 드라이버 설치를 하라고 했던 기억이 나네요. 그래픽 드라이버 회사의 사이트로 들어가서 그래픽 드라이버를 받아서 설치하면 끝입니다. 같은 방식으로 Ubuntu 서버에도 그래픽 드라이버 종류를 알아본 뒤에, 이에 맞는 드라이버를 받아서 설치해주면 됩니다. 하지만 설치를 잘못하게 되면 검은화면에서 뚝뚝뚝 에러메서지만 흘러나오며 엔터키도 먹지 않는 악몽을 경험하게 될 것입니다. 자신의 그래픽 카드가 뭔지는 컴퓨터를 사신 분이 잘 아실 것입니다. 그 정보를 가지고 http://www.nvidia.com/Download/Find.aspx?lang=en-us nvidia 홈페이지에 접속한 후에 그래픽 카드 정보를 입력하고 알맞은 그래픽 드라이버의 Version을 확인 합니다. 1$ apt-cache search nvidia 서버로 돌아와 다음 명령어를 이용해 설치가능한 드라이버를 확인해 보고 본격적으로 설치해봅니다. 1$ sudo apt-get install nvidia-xxx 이제 xxx에 들어갈 적절한 드라이버 Version을 입력하고 설치를 해주면 됩니다. 보통은 정상적으로 설치가 되는데, 12345Errors were encountered while processing: nvidia-xxx libcuda1-xxx nvidia-opencl-icd-xxxE: Sub-process /usr/bin/dpkg returned an error code (1) 이런 식의 에러가 날 수 있습니다. 이런 경우에는12$ sudo mkdir /usr/lib/nvidia$ sudo apt-get install nvidia-xxx 를 이용해서 nvidia 폴더를 만들어주고 설치를 진행하면 됩니다. 1sudo apt-get install dkms nvidia-modprobe 그리고 드라이버 종류에 상관없이 위의 패키지를 설치해주고 그래픽 드라이버가 로드 될 수 있도록 reboot 시켜줍니다. 설치 가이드 출처 Trouble shooting설치 후에 뿌듯 뿌듯한 마음으로 reboot 명령어를 내리고 화면을 천천히 지켜보고 있다 보면, 심상치 않은 메세지가 뚝뚝뚝 나오고 엔터키도 듣지 않는 것처럼 보일 때가 있습니다. 저 또한 멘탈이 깨져버려서 10붕동안은 멍하니 쳐다보고만 있었습니다. 이유는 여러가지가 있겠습니다만, 저의 경우에는 설치된 드라이버 버전이 서버와 맞지 않아서 생기는 문제였습니다. 이럴 경우에는 설치된 드라이버를 삭제하고 다시 설치하는 방법을 생각해 볼 수 있겠습니다. 1sudo apt purge nvidia* 위의 명령어로 nvidia관련 드라이버를 싹 날려주고 reboot하여 맞는 드라이버를 설치해 줍니다. 2.Cuda, CuDnn 설치이제 CUDA 10.0과 cuDNN 7.5 설치를 진행해봅니다. Cuda는 다음의 명령어로 설치해 줍니다.1sudo apt install cuda-10-0 설치가 되는데 시간이 좀 걸릴 수 있습니다. 설치가 끝나면 제대로 설치가 되었는지를 확인하기 위해서 1nvcc --version 위의 명령어를 이용해 Cuda Compiler정보를 확인합니다. nvcc정보가 잘 나오면 설치가 잘 된 것입니다. 이제 CuDnn을 설치해야 하는데, CuDnn은 Nvidia에서 공개적으로 다운받을 수 있게 해놓지 않았습니다. Nvidia에 회원으로 등록을 해야 CuDnn을 다운 받을 수 있게 해줍니다. 따라서 https://developer.nvidia.com/cudnn 이 사이트로 들어가 회원가입을 해주고 우리는 Cuda 10.0을 설치해줬으니까 10.0에 맞는 Cudnn 7.5를 다운로드 해줍니다. 다운이 되었으면 저의 경우는 Mac을 이용해서 받았기 때문에, 이전 글에서 다루었던 sftp를 이용해서 파일을 전송했습니다. 파일을 이동시킨 후에 tar로 말려져 있는 파일을 풀어줍니다.1tar -zxvf cudnn-10.0-linux-x64-v7.5.x.x.tgz 7.5뒤의 버전은 다를 수 있으니 잘 확인하시고 수정해 넣으시면 됩니다. 이제 압축을 푼 파일을 cuda폴더에 복사해 넣어줍니다. 123sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-10.0/lib64/sudo cp cuda/include/cudnn.h /usr/local/cuda-10.0/include/ 마지막으로 이동시킨 파일들에 대한 권한을 부여해주면 됩니다.1sudo chmod a+r /usr/local/cuda-10.0/include/cudnn.h /usr/local/cuda-10.0/lib64/libcudnn* *추가12#Nvidia에서 필요한 파일이라고 하네요sudo apt-get install libcupti-dev bashrc에 환경변수를 추가해주는 글을 많이 봤는데, 저의 경우에는 굳이 입력을 안해도 잘 인식하는 걸 봐서, 넣지 않았습니다. 제대로 불러오지 못한다면1sudo vi ~/.bashrc 123export PATH=/usr/local/cuda-10.0/bin${PATH:+:${PATH}}export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64\\${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}} 를 추가해 주세요 3. Tensorflow, Keras 설치Keras 설치에 앞서, 백엔드로 사용하는 Tensorflow를 설치해 줍니다. 이제 설치는 정말 간단합니다. 1pip3 install --upgrade tensorflow-gpu python3버전의 tensorflow를 설치해줍니다. cpu 버전을 설치하고 싶다면 -gpu를 빼주면 됩니다. conda 가상환경을 만들었다면, 실행시킨 후에 필요 라이브러리를 설치하고 Keras를 최종적으로 설치하면 끝입니다! 12345conda activate your_name(your_name) conda install h5py(your_name) conda install graphviz(your_name) conda install pydot(your_name) conda install keras 4. Jupyter Notebook 설치먼저 가상환경으로 들어가주고 pip를 이용해 jupyter notebook을 설치해 줍니다.12conda activate your_namepip install jupyter notebook 설치가 되었지만, 외부에서 접속을 해서 jupyter notebook을 사용하려면 아직 몇가지 설정이 더 남았습니다.1jupyter notebook --generate-config 를 이용해서 jupyter 설정파일을 만들어 준 후에 12345ipython #입력 후 In[1]이 나오면,from notebook.auth import passwdEnter password: #원하는 비밀번호 입력Verify password: Out[2]: 'sha1:f24baff49ac5:863dd2ae747212ede58125302d227f0ca7b12bb3' 이렇게 하고 나면 shai로 시작하는 암호문 같은 게 있습니다. 이걸 복사해서 잘 적어두시고, jupyter_notebook_config.py를 열어서12345# The IP address the notebook server will listen on. c.NotebookApp.ip = &apos;0.0.0.0&apos; c.NotebookApp.port_retries = 50 c.NotebookApp.port_retries = 8888c.NotebookApp.open_browser = False 위와 같이 수정해 주고 jupyter notebook --ip=0.0.0.0으로 실행해주면 jupyter notebook이 짠 하고 등장할 것입니다.","link":"/2019/12/08/ubuntu3/"},{"title":"What is Transformer?","text":"보는 논문 마다 Transformer와 Attention가 빠지지 않고 등장하곤 합니다.이에 대해서 공부하고 정리해 봤습니다. Transfomer글 미리보기 : Transformer는 번역에서 RNN 셀을 이용하지 않고 순차적 계산도 하지 않는다. 이를 통해 속도를 크게 향상 시켰다 성능도 크게 오르게 되었다. RNN을 사용 안하는데 단어의 위치와 순서 정보도 활용할 수 있다. 인코더 디코더 방식을 활용한다 Attention은 뭐지?번역을 하는 상황을 가정해보자논문참고* Attention is All you Need RNN을 통해 번역을 하는 상황을 가정해 보자. 한 단어를 다른 언어로 번역하는 일을 하기 위해서는 word embedding작업이 필요하다. 일단 임베딩에 관한 설명은 간단하게만 하고 넘어가자면, 텍스트를 수치화 하는 개념이다. 워드 임베딩이 끝난 후에 단어는 정해놓은 차원의 공간으로 임베딩 된다. RNN은 인풋 벡터와 히든 state의 벡터를 받아 아웃풋 벡터를 뱉어 낸다. 신경망 번역기의 구성은(Seq 2 Seq) Encoder와 Deoder로 이루어져 있다. 인풋이 인코더로 들어오면 이를 기반으로 hidden state를 만들어내고 업데이트를 하게 된다. 업데이트 된 hidden state는 차례로 인코더에 input값과 들어가 최종 hidden state를 만들어내고, 이것이 디코더로 들어가서 인풋에 대한 번역된 아웃풋을 출력하게 된다. 이 과정에서 업데이트 된 마지막 hidden state는 디코더에게 전달되는 context라고 할 수 있다. decoder 역시 hidden state를 갖고 있고 time step이 지나가면서 하나씩 이 hidden state를 다음으로 넘기는 과정이다. Attention!je suis etudiant라는 문장을 i am a student로 바꿀 때, je와 i라는 단어를 연결시켜 해석 하는게 더 정확한 결과를 가져오는 게 당연할 것이다. 여기서 특정 문맥을 더욱 상세히 보게 해주는 것이 바로 Attention이다. 즉, 순차적으로 계산되는 각각의 RNN cell에서 나오는 state를 모두 활용해보자는 것이다.(보통 RNN번역에서는 최종 hidden state만을 context로 이용해서 번역한다.)이 Attention은 보통의 Seq2Seq모델과는 두 가지 점에서 차이를 보인다. Seq2Seq model with Attention 인코더는 디코더에 더 많은 데이터를 보낸다. 마지막 hidden state를 디코더에 보내는 대신, 모든 hidden state를 디코더에 보낸다. Attention 디코더는 아웃풋을 내기 전에 추가적인 작업을 거친다. 각 타임스텝에 있는 디코더에 해당하는 인풋의 부분들에 집중을 하기 위해서, 디코더는 다음과 같은 작업을 한다. 각 인코더의 hidden state는 인풋 문장의 특정 단어와 관련된 부분을 가지고 있다. (먼저 step 4에 해당한다고 하자, step 3까지 만들어진 h1,h2,h3 인코더 hidden state가 존재) 각 hidden state에 대해서 점수를 매긴다.(점수를 매기는 법에 대해서는 일단 무시하고 넘어감) 점수를 softmax화 해서 이 점수로 각 hidden state를 곱한다. 이를 통해서, 높은 점수를 가진 hidden state를 더 상세히 보고, 점수가 낮은 hidden state는 빼낸다 가중치가 적용된 hidden state 벡터들을 합한다. 여기서 만들어진 context vector는 step4에 있는 디코더를 위한 것이다. 점수를 매기는 작업은 각 time step의 디코더에서 진행된다.이제 정리를 하자면, Attention 디코더 RNN은 임베딩된 토큰을 받고 디코더의 시작 hidden state를 받는다. RNN은 인풋을 처리하고 아웃풋과 새로운 hidden state 벡터(h4)를 만든다. output은 버려진다. Attention 단계에서, 우리는 인코더의 hidden state와 h4 vector를 이용해 context vector(C4)를 만들어내고 이것은 다음 time step에 사용된다. h4와 C4를 하나의 벡터로 합친다. (concatenate, 갖다 붙인다) 이 벡터를 Feed forward neural network에 넘긴다. feed forward neural network의 아웃풋은 이 time step에 대한 결과물을 가리킨다. 다음 time step까지 반복한다. Illustrated Transformertransformer는 Attention is All You Need의 논문에서 제안 되었다. 먼저 high level에서 살펴보자. A High level look번역기 모델을 가정하고, transfomer을 사용한다고 하면, 인풋은 transformer를 통해 처리되고 output 이 나오게 된다. transformer의 구성을 보면, 인코딩과 디코딩 파트, 그리고 이를 연결해주는 부분으로 이루어져 있다. 인코딩 파트는 encoder를 stack시켜놓은 구성이다. 인풋 벡터와 아웃풋 벡터가 같기 때문에 쌓는 것이 가능한 게 transformer의 특징이다. 디코딩 부분 역시 decoder가 stack되어 있는 모습이다. 인코더들은 모두 같은 구조를 갖지만 weight를 공유하지는 않는다. 각각은 self-attention과 Feed Forword Neural Network의 sub layer로 구성되어 있다. 인풋은 첫번째로 self-attention layer로 들어간다. 이 layer는 인코더가 인풋 문장에 있는 다른 단어들을 볼 때, 특정 단어로 인코드 하는 것을 돕는다. 이 self-attention을 중점적으로 보도록 하자. self-attention의 아웃풋은 feed forward NN으로 들어간다. 똑같이 생긴 feed forward NN이 독립적으로 각 포지션에 들어가 있다.(인코더 또는 디코더에 다 들어가 있음) 디코더는 특이한 점이 있는데, self-attention과 FFNN 모두를 갖고 있지만, 이 사이에 Encoder-Decoder Attention이라는 layer를 추가적으로 갖고 있다는 것이다. 이것은 attention layer로써 인풋 문장에 대한 적절한 부분들에 대해 집중할 수 있도록 도와주는 역할을 한다. 이 구조로 번역을 해보자번역을 위해서, 위에서 했던 것과 같이 word embedding부터 실시한다. 임베딩은 맨 아래 인코더에서부터 시작한다. 모든 인코더에 공통되는 부분은, 512차원의 벡터를 받는다는 것이다. 벡터의 길이는 하이퍼 파리미터로서 우리가 설정할 수 있는 부분이다. 보통 이 값은 훈련 셋에 있는 가장 긴 문장을 기준으로 설정된다. 단어 임베딩이 끝나면, 각 단어들은 인코더의 두 레이어로 들어가게 된다.(self attention layer, FFNN layer) 여기서 Transformer의 큰 특징이 드러난다. 각 포지션에 있는 단어는 지정된 인코더의 path를 따라 간다는 것이다. self-attention에는 이런 의존성이 존재한다. FFNN에는 의존성이 없지만 그러므로 FFNN을 타면서 여러 path들이 병렬로 처리가 가능하게 된다. Encoding인풋 벡터를 받으면 인코더는 이 벡터들을 self-attention 층으로 보낸다. 그리고 FFNN을 통하고, 결과물을 만들어 다음 인코더로 보내게 된다(구조상 위로 보낸다). 문장 번역을 하는 예를 들어 보자.”The animal didn’t cross the street because it was too tired”it은 무엇에 해당되는 것일까? street일까 아니면 animal일까? 사람들에게는 아주 쉬운 질문이지만, 알고리즘 상으로 답을 내기에는 어려운 질문이다. it을 처리할때, self-attention은 it을 animal과 연결하는 것을 허용한다. 모델이 각 단어를 처리할 때, self-attention은 다른 위치에 있는 인풋 시퀀스를 보는 것을 허용해서 이 단서들을 이용해 단어를 잘 인코딩 하도록 돕는 역할을 한다. 결국 self-attention은 인코딩 파트에서 Transformer가 다른 연관된 단어를 갖고 우리가 지금 처리 중인 것에 대해서 잘 이해할 수 있도록 하는 방법이다. 다른 말로하면, self-attention은 self-attention 점수를 각 단어마다 매겨서 단어와 단어끼리의 매칭 점수를 이용해 연관정도를 파악하는 것 이라고도 할 수 있다. 더 자세히 알아보자. Self-attention in detailself-attention이 벡터들을 갖고 어떻게 계산하는지 보도록 하자. 첫 번째로 self-attention의 계산에서는, 각 인코더의 인풋 벡터들에서 세 개의 벡터들을 생성한다. 각 단어에서 이제 Query Vecotor를 만들어낸다. Key Vector, Value Vector로 이루어진다. 이 벡터들은 훈련단계에서 학습된 3개의 행렬들을 곱하여 생성된다.(Q, W, K) 주목할 점은 이 새 벡터들이 임베딩 벡터보다 차원이 작다는 것이다. 이것들의 차원은 64이고, 임베딩과 인풋/아웃풋 벡터들은 512차원이다. 이 행렬들이 작아질 필요는 없다. 이것은 단순히 multihead attention 상수를 계산하기 위한 것이기 때문에 선택의 문제다. query, key, value 벡터들?이 세 벡터들은 attention에 대해 생각할 때 유용하게 계산되는 abstraction들이다. 하단에 attention이 어떻게 계산되는지를 진행해보면, 이 세 벡터들의 역할에 대해서 잘 이해할 수 있다. 두 번째로 self attention점수를 내는 것이다. Thinking이라는 단어에 대해서 self-attention 점수를 계산한다고 해보자. 우리는 문장에 있는 이 단어에 대한 각 단어들의 점수가 필요하다. 점수는 특정 위치에서 단어를 인코딩 할 때 입력 문장의 다른 부분에 집중할 정도를 결정한다. 이 점수는 각 단어에 점수를 매길 때 query vector와 key vector의 dot product로 계산된다. 그래서 만약 우리가 #1포지션에서 self-attention을 한다면 첫 점수는 q1과 k1의 dot product로 계산될 것이다. 두 번째 점수는 q1과 k2의 dot product로 계산된다. 세 번째와 네 번째는 점수를 8로 나눠주는 것이다.(key value의 차원에 루트 씌운 값) 이렇게 하게되면, Key 벡터의 차원이 늘어날수록 dot product 계산시 값이 증대되는 문제를 막아주게 되어 안정적으로 gradient를 흐르게 만들어 줄 수 있다. 그리고 결과를 softmax 처리에 보낸다. softmax는 점수를 normalize해서 그 값들이 모두 1까지 갖는 양수로 만든다. softmax 스코어는 이 위치에서 각 단어가 얼마나 표현될지를 보여주는 점수인데, 분명히 그 위치의 단어는 가장 높은 점수를 갖겠지만 가끔은 현재 단어와 관련된 다른 단어를 위치시키는 것이 좋을 때도 있다. 예를 들어 it이 어떤 걸 의미 하는가에 대해서 궁금할 때. 다섯 번째는 각 value 벡터를 softmax 점수로 곱하는 것이다. 여기서의 포인트는 우리가 집중하고 싶은 단어의 값을 유지하고 관련없는 단어의 값을 떨어트리는 것이다. 여섯 번째는 가중치가 곱해진 벡터들을 더하는 것이다. 이 과정을 통해 이 위치의 self-attention 레이어 값을 얻게된다. 이것을 통해 self-attention 계산이 완료된다. 결과로 나온 vector는 FFNN에 보낼 수 있는 벡터이다. 이 계산은 matrix form으로 되어있다면 더 빠르게 가능하다.(Matrix Factorization으로 한번에 계산이 가능함) 이제 단어 수준에서 계산을 살펴 보자. Matrix Calculation of self-attention첫 번째는 Q, K, V 행렬을 계산하는 것이다. 임베딩을 matrix X로 만들 때 훈련된 가중치 행렬(WQ, WK, WV)를 곱해서 이 세가지 행렬을 이미 만들어 냈다. X와 WQ를 곱해서 Q가 나오고 WK를 곱해서 K, WV를 곱해서 V가 나오게 된다. 행렬들에 관련된 것이기 때문에, 이 2-6댠계를 하나의 공식으로 응축하여 self-attention의 아웃풋을 계산할 수 있다. Matrix Factorization을 통해서. multi head attentionmulti head attention은 attention layer를 head의 수 만큼 병렬로 수행하는 것을 말하는데, 이를 통해서 모호한 문장을 해석하는데 연관된 정보를 다른 관점에서 바라보게 만들어서 퍼포먼스를 상승시키는 효과가 있다(it 구분하기 등). attention layer의 퍼포먼스는 다음 두 가지 방법을 통해 향상시킨다. multi head attention은 단어의 위치를 잡는 능력을 여러 위치들로 잡는 것으로 확장한다. 이전에 실시해서 얻은 값에는 다른 인코딩 값이 있긴 하지만, 단어 그 자체의 값에 의해 dominate 될 수 있다. multi head attention을 이용하면 “The animal didn’t cross the street because it was too tired”라는 문장에서 우리는 it이 의미하는 것이 무엇인지 번역할 때 유용할 것이다. multi head는 attention layer에 representation subspace들을 제공한다. multi head attention을 통해 여러개의 QKV 가중치 곱 행렬을 가질 수 있다. 각 셋들은 랜덤하게 값이 들어가고, 훈련이 끝나면, 각 셋들은 input 임베딩들을 다른 representation subspace에 투영하는데 사용된다. 만약 input X를 8개의 다른 attention head에 넣고 계산하게 되면, 8개의 다른 Z 행렬들이 등장하게 된다. FFNN은 사실 8개의 행렬에 대해 예상하지 못한다. FFNN은 single 행렬을 기대하게 되는데 그래서 우리는 이 여덟개의 행렬을 응축해 하나의 matrix로 만들어야 한다. concatenate 한다 W0 행렬을 가중치 matrix와 곱해 훈련한 데이터를 만든다 결과는 정보를 가진 모든 atttention head의 값을 가진 Z matrix이다. 이걸 FFNN에 보낸다 이 과정을 그림으로 요약하면 다음과 같다 모든 attention head를 더하면 굉장히 다양한 해석이 등장한다. Positional encoding을 통한 시퀀스 순서 나타내기모델을 설명하면서 하나 놓친 부분은, 인풋 시퀀스의 순서를 어떻게 설명하느냐이다. RNN에서는 문장의 길이가 짧긴 하지만 각 단어의 sequence 정보를 잘 활용할 수 있었다. 하지만, Transformer는 속도가 느린 RNN을 사용하지 않고 Matrix Factorization을 활용하기 때문에 시퀀스 정보를 전달해 주는 과정이 필요하다. transfomer는 각 인풋 임베딩에 벡터를 더하고, 이 벡터들은 상대적인 위치 정보들을 갖게 되어 모델이 학습하는 패턴을 따르게 된다. 이것을 통해 각 단어의 위치를 결정하고 또는 시퀀스에 있는 다른 단어들과의 거리를 결정한다. 여기서 포인트는 이러한 벡터 값을 임베딩에 더하면 Q / K / V 벡터로 투영 된 후 내적시 임베딩 벡터간에 의미있는 거리를 제공할 수 있다는 것이다. positional encoding을 통해서 얻을 수 있는 또 하나의 장점은 훈련 셋보다 긴 문장이 들어왔을 때에도 scale이 가능하다는 것이다. Residuals인코딩 구조에서 한 가지 더 설명할 부분은 residual이다. positional encoding은 학습이 진행될 수록 역전파에 의해서 정보가 많이 손실된다. 각 인코더에 있는 각 sub-layer에서 residual connection을 취해주고 더해줌으로써 이 정보 손실을 막아준다. Residual connection 후에는 layer-normalization 단계를 통해서 학습의 효율을 증진시켜준다. 인코더는 이렇게 작업이 끝난다. 정리하자면, 인코더는 임베딩 - multi head attention - FFNN 그리고 Residual Connection으로 이루어진다. Decoder가장 위의 encoder의 아웃풋은 attention 벡터 셋 K, V로 변환된다. 이 두 벡터들은 각 디코더의 질문에 사용된다. K와 V는 encoder-decoder attention 층에서 디코더가 인풋 시퀀스에 적절한 자리에 집중하는 것을 돕는다. 디코더는 masked Multi head attention - multi head attention - FFNN의 구조로 이루어져 있다. 디코더 입력값은 Query로 사용되고 Encoder의 최종 결과값을 Key, Value로 사용한다. 이것은 디코더의 현재값을 Query로 encoder에 질문하는 모습이 되겠고, 인코더 출력값에서 중요한 정보를 K, V로 획득해서 디코더의 가장 적합한 다음 단어를 출력하는 과정이라고 볼 수 있다. 이 과정이 Decoder layer 에서 쭉 이어지게 되고, 이후에는 linear layer와 softmax layer를 지나게 된다. 일반적으로 softmax를 이용해서 가장 높은 확률값을 전달해주는 과정이 여기서도 이루어 지게 된다. 가장 높은 확률값을 지닌 단어가 다음 단어로 오게 된다. 하지만 여기서도 Lable smoothing을 통해 한층 더 성능을 높여준다. one hot encoding으로 값을 확확 죽이는 것 보다, 정답은 1에 가깝게, 오답은 0애 가깝게 만들어 주는 과정이다. Thanks가 고맙다와 감사하다로 label된 것을 예로들면, 고맘다와 감사하다는 둘 다 잘못 label된 것이 아니다. 하지만 one hot encoding을 시켜버리면, 이 둘은 완전히 다른 결과값을 갖게 될 것이다. 이렇게 되면 학습이 효율적으로 학습이 진행되지 않게 되는데, 이를 방지하는 것이 label smoothing인 것이다. 위의 과정은 &lt;EOS&gt;가 나올 때까지 반복된다. 각 스텝의 아웃풋이 가장 아래의 디코더에 주입되고, 디코더는 디코딩된 결과를 bubble up한다. 인코더가 그랬던 것 처럼 디코더의 인풋에 임베딩하고 임베딩 벡터에 positional 인코딩을 취해 각 단어의 위치를 가리킨다. encoder-decoder attention 층은 multihead self-attention과 비슷하게 작동하지만, Query Matrix를 층 아래에 생성한다는 것과 K, V matrix만 인코더 스택의 아웃풋에서 취한다는 것에서 차이가 난다. The Loss Function‘merci’를 “thanks’로 바꾸는 작업을 한다고 해보자. 이 작업은 아웃풋이 확률 분포에서 thanks 단어를 가리키는 걸 원한다는 것을 뜻한다. 하지만 이 모델은 훈련되지 않았고, 제대로 번역되지 않을 것이다. 어떻게 잘 학습된 분포와 학습이 안된 분포를 비교할까? 간단하게, 하나를 잡고 다른 걸 빼면 된다. cross entropy나 Kullback-Leibler divergence를 살펴보자. 하지만 이 예는 너무 단순화한 예이다. 보통 우리는 문장단위의 번역을 한다. “je suis étudiant”를 “i am a student”로 번역한다고 해보자. 각 확률분포는 vocal_size의 길이로 표현된다. 여기서는 까지 포함하여 6개이다. target output은 1과 0으로 나오게 되지만 학습에서는 값이 확률로써 등장한다. ReferenceJay Alammar : https://jalammar.github.io/허민석님 유튜브 : https://www.youtube.com/watch?v=mxGCEWOxfe8&amp;t=786s)medium 글 : https://medium.com/@adityathiruvengadam/transformer-architecture-attention-is-all-you-need-aeccd9f50d09","link":"/2020/04/12/transformer/"},{"title":"Studio3T trouble shooting","text":"Studio3T에서 bson file을 import하려고 했다. 근데 왜 안될까?? Studio3T에 bson 파일 import하기Pyspark를 쓰던 중, MongoDB에서 데이터를 끌어다 써야 하는 일이 있어서 bson 파일을 Studio3T에 import 하기로 했다.그냥 끌어다가 놓으면 될 줄 알았는데, 그런 건 되지 않았다. 먼저 콜렉션을 선택하고 import 버튼이 보여서 눌러봤다. 내가 import 하려는 파일은 bson이고 파일 하나만 있으니까 mongodump archive를 눌렀다.그럼 여러 옵션들이 나오는데 무시하고 경로만 설정해서 Execute시켜줬다. 하지만 진행이 되지 않았고 Import mongodump folder groobee_for_feature_engineering: error creating collectiongroobeefor_feature_engineering: error running create command: The field ‘background’ is not valid for an _idindex specification. Specification: { v: 2, name: “_id“, ns: “groobee_****_for_feature_engineering”, background: true,key: { _id: 1 } } 위와 같은 에러가 등장하게 되었다. 대충 내용을 보아하니 ‘background’라는 필드가 유효하지 않다는 내용이었고 json형식처럼 보이는 게 확인 되었다. 열심히 구글링을 해보았지만, 관련된 내용은 거의 없었고 좌절하고 있던 찰나에 bson파일을 받을 때 같이 등장하는 metadata.json 파일이 생각났다. metadata.json바로 확인해 보니 12345{\"options\":{},\"indexes\":[{\"v\":1,\"key\":{\"_id\":1},\"name\":\"_id_\",\"ns\":\"groobee_****.userDataInfo.****\"},{\"v\":1,\"key\":{\"cookieId\":1,\"sessionSeq\":1},\"name\":\"cookieId_1_sessionSeq_1\",\"ns\":\"groobee_****.userDataInfo.****\",\"background\":true},{\"v\":1,\"key\": ..... 아까 에러에서 본 내용들이 여기에 있었다. 에러가 왜 났는지 보아하니, DB안에 중복된 파일명이 있을까봐 bson파일명을 바꿨는데, metadata 안에는 이전의 파일명과 DB, collection명이 적혀있어서 Studio3T가 제대로 인식을 못한 것이었다. metadata에서 DB와 Collection을 일일이 수정해준 후, 다시 한번 시도 해봤다. mongodump folder이번에는 아까와는 다르게 mongodump folder로 해봤다. 왜냐면 옆에 계시는 매니져님이 folder로 할때 더 잘되더라는 얘기를 들었기 때문이다. 이 방식은 아까와는 조금 다른데, 아무이름의 폴더를 하나 만들어 주고, 그 안에 DB에 있는, 내가 import하려는 대상collection명으로 폴더를 만들어야 한다. 폴더를 만들고 경로를 지정해 주자. Execute지정을 해주고 Execute를 시켜주면! 진행이 되기 시작한다.그렇게 시간이 지나서 100%가 될 때까지 기다리다 보면, Restoring은 100%로 끝났는데 완료가 되지 않는 걸 볼 수 있다.‘아 뭐야 다시해야 되나’ 하고 중지를 누른다면 소중한 시간을 날리게 된다. 이 bson데이터의 경우 780만 row로 꽤 큰 데이터였는데, 30분 정도 넣고 이상해서 중지하고 다시 시작하고… 이런 작업을 3번정도 반복했다. 소중한 시간을 낭비하지 말고 기다리자. Restoring이 끝났다면, Restoring index작업이 남아있다. index를 restore하는 작업은 생각보다 오래걸린다. 인내심을 갖고 화장실을 다녀오든, 커피를 한잔하든 여유롭게 기다리다 보면. 초록색 동그라미와 함께 done이라는 메세지를 볼 수 있을 것이다.","link":"/2020/05/10/trouble-shooting_studio3t/"},{"title":"pyspark trouble shooting, schema","text":"pyspark로 작업 하던 중 파일이 읽히지 않는다면?????cannot cast DOCUMENT into a ArrayType Pyspark Trouble Shooting간만에 pyspark로 작업할 일이 생겼다. 거의 한 달 만에 쓰는 거라 조금 어색했다. purchase관련 데이터와 view데이터를 갖고 작업을 해야 했었다. Purchase 데이터로 분석을 끝내고, view 데이터를 열어봤다. 파일이 커서 돌려놓고 dataframe을 만든 후 show()를 통해서 잘 불러왔는지 확인하려고 했었다. cannot cast DOCUMENT into a ArrayType 자주 보지만 정이 안드는 친구가 등장했다. 도대체 이해가 되지 않았다. purchase데이터를 읽어올 때와 똑같은 방식으로 schema를 지정해줬고 변수명도 다 바꿔서 문제가 없을 줄 알았는데 에러가 발생한 것이다. DOCUMENT를 ArrayType으로 바꿀 수 없다는 내용인데… 왜 아까는 됐고 지금은 안되는지 참… 답답했다. 에러메세지를 복붙해서 구글을 뒤져봤지만, 문제를 해결할 만한 소스는 없었다. 결국 혼자 답을 찾아보기로 했다. 데이터의 구조그 전에 사전지식으로 알아야 할 점은, 데이터가 어떻게 구성되어 있느냐이다. 글을 읽는 사람들의 이해를 돕기위해 간단하게 설명을 해보자면, purchase데이터에는 사용자의 구매내역이 들어있고 이는 purchase라는 칼럼에 잘 담겨있다. 구매내역이란 구매한 상품 내용, 상품의 갯수, 상품의 가격, 상품의 이미지 등등이 들어있다. 이 데이터는 한 칼럼에 담겨있으므로 묶어줄 수 있는 자료구조가 필요하다. 여기서 사용되는 자료구조는 list이며, 리스트 안에는 dictionary형태로 담겨 있다. [{‘purchaseGoods’ : ‘값싸고질좋은 상품’}, …]예를 들면 이런식으로 담겨 있는 것이다. pyspark로 데이터를 불러올 때는 schema를 지정해서 가져온다. list로 묶여있는 경우에는 ArrayType(list가 pyspark에서는 ArrayType으로 나타난다)으로 지정하고 이걸 purchase 데이터에 적용했을 때에는 너무나 잘 불러와졌었다. printSchema를 쳐봐도 잘 나왔다. 데이터의 차이점view데이터를 불러오는 코드를 다시 들여보고 printSchema를 하니 역시 잘나왔다. spark의 신기한 점 중에 하나인데, spark는 직접 작업을 수행하는, collect나 show 등을 수행하기 전까지는 작업 스케쥴링만 해놓고 실제로는 돌지 않는다. 그렇게 지나가려는 순간 이상한 점을 발견했다. 둘의 구조를 비교해보자. 뭔가 다른 점이 보인다. 위의 goods에서는 그냥 dictionary로 묶여있고, 밑에 있는 purchase에는 list로 묶여있다. 이 차이점 때문에 동일한 schema를 사용하게 되면 에러가 발생하게 되는 것이다. list는 ArrayType으로 지정하면 된다면, dictionary는 무엇일까?dictionary는 StructType으로 지정하면 된다. 스키마를 제대로 지정하고 나면 제대로 데이터가 나오게 된다. purchase Schema123456789101112131415purchaseSchema = StructType([ StructField(\"purchaseGoodsName\", StringType(),True), StructField(\"purchaseGoodsCode\", StringType(),True), StructField(\"purchaseGoodsAmount\", IntegerType(),True), StructField(\"purchaseGoodsCount\", IntegerType(),True) ])userschema = StructType([ StructField(\"cookieId\", StringType(),True), StructField(\"currentTime\", StringType(),True), StructField(\"sessionSeq\", StringType(),True), StructField(\"userSeq\", StringType(),True), StructField(\"purchase\", ArrayType(purchaseSchema),True), StructField(\"actionType\", StringType(),True) ]) view Schema123456789101112131415viewSchema = StructType([ StructField(\"goodsName\", StringType(),True), StructField(\"goodsCode\", StringType(),True), StructField(\"goodsAmount\", IntegerType(),True), StructField(\"goodsCount\", IntegerType(),True) ])userschema = StructType([ StructField(\"cookieId\", StringType(),True), StructField(\"currentTime\", StringType(),True), StructField(\"sessionSeq\", StringType(),True), StructField(\"userSeq\", StringType(),True), StructField(\"view\", StructType(viewSchema),True), StructField(\"actionType\", StringType(),True) ]) 추가로 이런 타입들을 지정할 때는 1from pyspark.sql import StructType, ArrayType 이런식으로 불러와야 한다. import가 되지 않았다면, 에러가 발생한다.","link":"/2020/05/10/trouble-shooting_pyspark/"},{"title":"Google Cloud Platform - Task","text":"Google Task를 통해 비동기처리에 대한 Toy Project를 진행해보자. 1. Google Cloud Task먼저 Google Cloud Task가 무엇인지 살펴보자. Google Cloud Tasks는 대규모 분산형 태스크의 실행, 디스패치, 전송을 관리할 수 있는 완전 관리형 서비스이다. Cloud Tasks를 사용하게 되면 사용자 또는 서비스 간 요청 이외의 작업을 비동기적으로 수행할 수 있게 된다. 여기서 비동기적으로 작업을 수행한다는 말이 있다. 비동기적으로 작업을 수행한다는 것은 어떤 의미이고 어떤 상황에서 사용해야 할까? 2. 비동기 처리먼저 동기식 처리가 뭔지 알아보자. 동기식 처리 모델(Synchronous processing model)은 직렬적으로 태스크(task)를 수행한다는 의미이다. 즉, 태스크는 순차적으로 실행되며 어떤 작업이 수행 중이면 다음 작업은 대기하게 된다. 반면, 비동기식 처리는 (Asynchronous processing model 또는 Non-Blocking processing model)은 병렬적으로 태스크를 수행한다. 즉, 태스크가 종료되지 않은 상태라 하더라도 대기하지 않고 다음 태스크를 실행한다. 예를 들어 서버에서 데이터를 가져와서 화면에 표시하는 태스크를 수행할 때, 서버에 데이터를 요청한 이후 서버로부터 데이터가 응답될 때까지 대기하지 않고(Non-Blocking) 즉시 다음 태스크를 수행한다. 이후 서버로부터 데이터가 응답되면 이벤트가 발생하고 이벤트 핸들러가 데이터를 가지고 수행할 태스크를 계속해 수행한다. (출처 : https://poiemaweb.com/js-async) 다른 상황을 가정해보자. 어떤 API가 두 개 있다. 여기서 정보를 가져올 건데, get_user_list라는 API에서는 이름만을 가져올 수 있는 API이고, get_user_record에는 특정 유저의 구체적인 정보가 담겨 있다. get_user_record는 제한 조건이 있어 한번에 많은 데이터를 가져올 수 없다. 예를 들어 get_user_list에서 모든 user list를 받는다고 하더라도 bulk로 get_user_record에 쿼리를 보내 데이터를 갖고 오는 것은 불가능하다는 것이다. 이런 조건에서 빠르게 데이터를 확보하려면 어떻게 해야할까? for loop을 돌려 user name 하나씩 정보를 얻어 온다면 많은 시간이 걸릴 것이다. 비동기 처리를 활용해 보자. 워커를 여러 대 만들고 워커에 수행할 작업(get_user_record에서 name별 정보를 갖고 오는 일)을 넣어주면 훨씬 빠르게 데이터를 모을 수 있을 것이다. 그렇다면 파이프 라인은 다음과 같다. get_user_record에 쿼리를 날리고 실시간성 DB에 데이터를 담아두기 실시간성 DB에 있는 데이터를 배치성 DB에 넣기여기서 1번 작업은 Google Cloud Function에 작업 내용을 넣어주고 Google Cloud Task를 이용해서 비동기 작업을 통해 빠르게 수행하도록 할 것이다. 1234567https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_listparam: {}https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_by_nameparam: { \"name\": \"Braund, Mr. Owen Harris\"} 다음과 같이 API가 있다. 첫 번째 API는 user의 list를 받아올 수 있는 API이고, 두 번째는 user의 name별로 정보를 가져올 수 있는 API이다. 먼저 Local에서 정보를 제대로 받아오는지 테스트를 해보자. Toy ProjectToy1. get_user_list, get_user_by_name 확인API에서 정보를 받기 위해서 자주 사용하는 라이브러리인 requests를 이용한다. requests 쉽게 사용하기 1234567import jsonimport requestsurl = 'https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_list'response = requests.post(url)user_list = response.json()user_list = user_list['data'] user_list는 아주 잘 나온다. 이제 get_user_by_name을 확인해보자 1234url2 = 'https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_by_name'params = {'name' : user_list[0]}response = requests.post(url2, json=params)response.json() post로 parameter를 보내면 역시 값이 잘 나온다. Toy2. mongoDB 설정작업을 수행하고 나온 데이터를 임시로 넣을 DB는 mongoDB로 해볼 것이다. 무료로 사용할 수 있는 mongoDB가 있는데 https://mlab.com/ 여기를 이용하면 된다. 아니면 mongoDB Atlas를 사용해도 된다. mongoDB 구축하는 건 간단하니 Pass한다. Toy3. gcloud 설정google cloud의 다양한 서비스를 이용하려면 gcloud를 설정해야 한다. google cloud Task를 사용하기 위한 퀵 가이드 문서는 잘 되어 있는 편이다. Cloud Task Quick Guide 1pip install gcloud gcloud를 먼저 설치하고 따라하면 된다. SDK를 만들고 난 후에 Cloud Tasks API, API 라이브러리 페이지에 갔을 때 사용가능 하다는 초록 표시가 나오면 퀵 가이드 문서의 샘플 설정 및 하단의 내용을 수행하면 된다. 퀵 가이드 문서 내용을 다 따라하고 task페이지로 오게 되면 my-queue가 생성된 것을 확인할 수 있다. 이렇게 되면 성공이다. Toy4. Functions 설정Cloud Functions로 이동해보자. Cloud Functions는 AWS Lambda와 비슷하다(서버리스 아키텍쳐) .Functions를 세팅할 때 주의할 점이 있다. 위치나 메모리는 아무렇게나 설정해도 되지만 인증 부분을 꼭 체크하고 코드를 작성할 런타임을 python 3.7로 바꿔주도록 한다. 조금 기다리면 funtion이 만들어진다. funtion에 들어가서 수정 버튼을 누르면 작업을 수행할 소스코드가 등장한다. main.py에 작업할 코드를 넣어주고 requirements.txt에 필요한 라이브러리들을 작성해 import할 수 있도록 한다. 1234#requirements.txtpymongorequestsdnspython requirements에는 mongoDB 연결과 저장을 위한 pymongo, API를 위한 requests 그리고 기타 dnspython을 넣어준다. 12345678910111213141516171819202122232425262728#MAIN.pyfrom pymongo import MongoClientimport astimport pymongoimport requestsmongo_uri = \"mongodb://****:****@ds263018.mlab.com:63018/****?retryWrites=false\"client = pymongo.MongoClient(mongo_uri)db = client.****def hello_world(request): \"\"\"Responds to any HTTP request. Args: request (flask.Request): HTTP request object. Returns: The response text or any set of values that can be turned into a Response object using `make_response &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response&gt;`. \"\"\" payload = request.get_data(as_text=True) request_json = ast.literal_eval(payload) r = requests.post(url = \"https://us-central1-contxtsio-267105.cloudfunctions.net/get_user_by_name\", json=request_json) db.temp.insert_one(r.json()) return \"Success\" main 소스에는 local에서 테스트한 내용을 넣어준다. mongo_url에서 retryWrite=false는 mongoDB에서 MongoError: This MongoDB deployment does not support retryable writes. Please add retryWrites=false to your connection string.이런 에러가 나온다면 사용해보자. 소스를 넣은 후에 잘 세팅 되었는지를 확인해보기 위해 테스트를 이용해 볼 수 있다. 테스트를 누르면 트리거 이벤트라고 나오고 {}이렇게 나와 있다. 여기에 테스트 할 파라미터 값을 넣어보면 된다. 우리의 파라미터는 name이므로 1{\"name\": \"Braund, Mr. Owen Harris\"} 이렇게 파라미터를 적어주고 테스트한다. success가 나오면 성공이다. Toy5. Task이제 로컬로 돌아와서 task와 function을 붙여볼 차례다. 먼저 코드부터 소개한다. 1234567891011from google.cloud import storagefrom google.cloud import tasks_v2task_client = tasks_v2.CloudTasksClient() # Credentialdef dispatch_task(name): #json 같은 string payload = str({ \"name\": name, }) resp = create_task(project='affable-**********', queue='my-queue', location='asia-northeast3', payload=payload) dispatch_task를 통해서 수행할 작업을 어떤 프로젝트에 연결하고 어떤 큐에 보낼 것인지 선택할 수 있다. 이렇게 프로젝트와 Task에 연결하고 난 뒤에 create_task를 수행하게 된다. 1234567891011121314151617181920212223242526272829303132def create_task(project, queue, location, payload=None, in_seconds=None): parent = task_client.queue_path(project, location, queue) # Construct the request body. task = { 'http_request': { # Specify the type of request. 'http_method': 'POST', 'url': 'https://us-central1-affable-audio-277311.cloudfunctions.net/function-2' #function url } } if payload is not None: # The API expects a payload of type bytes. converted_payload = payload.encode() #여기서 인코딩 # Add the payload to the request. task['http_request']['body'] = converted_payload if in_seconds is not None: # Convert \"seconds from now\" into an rfc3339 datetime string. d = datetime.datetime.utcnow() + datetime.timedelta(seconds=in_seconds) # Create Timestamp protobuf. timestamp = timestamp_pb2.Timestamp() timestamp.FromDatetime(d) # Add the timestamp to the tasks. task['schedule_time'] = timestamp # Use the client to build and send the task. response = task_client.create_task(parent, task) print(response) return response create_task에서 본격적으로 funtions와 연결이 되어 작업들이 돌아가게 된다. task에 보면 url 파라미터가 보인다. 이 url이 작업을 수행할 funtions의 위치를 나타내는 것으로, functions의 트리거 부분에 있는 url을 넣어주면 된다. 이렇게 넣어주고 나면 payload를 받아서 인코딩을 해주고 task에서 request를 날려주게 된다. 함수를 다 지정하고 나서 user_list있는 name들을 dispatch_task에 넣어주게 되면 task가 돌아간다! 12345# dispatch_task('Braund, Mr. Owen Harris') #하나짜리로 먼저 테스트for user in user_list: dispatch_task(user) Trouble ShootingTrouble, Credential바로 성공이 되는 경우가 있는 반면, 에러가 터져나와 제대로 돌아가지 않는 경우가 발생하곤 한다. 보통은 credential 문제가 대부분이다. dispatch_task의 윗부분에 #credential이라고 작성한 부분에서 보통 에러가 나는데 DefaultCredentialsError: Could not automatically determine credentials. Please set GOOGLE_APPLICATION_CREDENTIALS or explicitly create credentials and re-run the application. For more information, please see https://cloud.google.com/docs/authentication/getting-started 이러한 에러가 발생한다면 Credential관련한 에러이며, 이런 에러가 나오지 않고 아래 for loop을 돌 때, AttributeError: ‘str’ object has no attribute ‘create_task’ 이런 에러가 나와도 task_client에서 credential이 제대로 되지 않았다는 뜻이다. Solution.11task_client = tasks_v2.CloudTasksClient(&lt;json&gt;) 다음과 같이 task_client에 cloud 프로젝트에 있는 json key를 받아서 절대경로를 에 넣어준다.json key는 다음과 같이 얻을 수 있다. 내 프로젝트로 들어가서 작업을 누르고 키를 받는다. Solution.2이렇게 해결이 되는 경우도 있지만 사용하는 컴퓨터의 종류나 gcloud설정에 따라 제대로 되지 않을 수 있다. 두 번째 방법은 구글 문서에 있는 방법이다. 123456789def explicit(): # Explicitly use service account credentials by specifying the private key # file. storage_client = storage.Client.from_service_account_json( &lt;json&gt;) # Make an authenticated API request buckets = list(storage_client.list_buckets()) print(buckets) 이 역시 아까 받은 json 키를 사용하는 방법이다. 이렇게 함수를 만든 뒤 explicit함수를 실행시키다. 그리고 다시 for loop을 돌려보자. Solution.3마지막 방법이다. os 라이브러리를 이용해 google credential에 사용하는 json 키 값을 직접 지정해 주는 방식이다. 본인 생각으로는 가장 확실한 방법이라고 생각한다. 12import osos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '&lt;json&gt;'","link":"/2020/05/19/google-task/"},{"title":"클라우드 서비스의 기초. Virtualization, 가상화에 대해서","text":"IT infrastructure, 클라우드 서비스의 기초인 가상화에 대해서 알아보자! 1.가상화에 대해 알기 전에 IT Infrastructure.가상화에 대해서 알기전에 IT infra는 어떤 종류가 있는지 알아볼 필요가 흔히 세미나를 돌다보면 흔하게 들리는 ‘온프레미스’라던가 ‘클라우드 서비스’라는 개념이 도대체 정확히 뭘 말하는 건지 알아야 실제로 사용하는 AWS가 어떻게 돌아가는지 이해해볼 수 있을 것이다. 1.1. IT InfrastructureIT 인프라는 크게 4가지로 나눠볼 수 있을 것이다. 온프레미스, 퍼블릭 클라우드, 프라이빗 클라우드, 하이브리드 클라우드 가 그 종류이다. 온프레미스(On-premise) : 쉽게 말해서 물리서버를 구축한다는 이야기이다. 회사에서 서버실을 따로 두고 있다면, 온프레미스 방식의 구조를 사용하고 있다는 걸 말한다. 온프레미스 방식은 직접 서버를 놓고 사용하기 때문에 관리가 용이하고 기밀성이 굉장히 높다는 장점을 갖고 있지만 고가의 장비를 사용해야 하고 사용량 예측이 불가하며, 재해 등에 취약하다는 단점이 있다. 퍼블릭 클라우드(Public Cloud) : 클라우드 제공자가 물리서버(데이터 센터, 인프라 등)를 구축하고 가상화 기술을 이용해 불특정 다수에게 제공하는 시스템을 말한다. AWS, GCP, Azure등이 퍼블릭 클라우드 프로바이더이며, 서비스 이용자들이 퍼블릭 클라우드를 이용하는 것이라고 볼 수 있다. 프라이빗 클라우드(Private Cloud) : 회사에서 자체적으로 물리서버를 구축하고 가상화하여 클라우드 기술을 사용하는 것을 말한다. 높은 기밀성이 필요한 회사에서 주로 사용한다. 고객 정보에 대한 데이터 등의 기밀성이 높은 데이터를 AWS나 기타 퍼블릭 클라우드에 올릴 수 없을 때 자체적으로 프라이빗 클라우드를 구축하곤 한다.(금융회사 또는 삼성) 하이브리드 클라우드(Hybrid Cloud) : 온프레미스와 클라우드를 동시에 사용하는 방식이다. 위에서 소개한 것처럼 클라우드에 대해서 다룰 때 가상화란 개념이 등장한다. 클라우드 서비스 유형에 따라 Iaas, Paas, Saas 등이 나뉘지만 이 글에서는 설명을 생략한다. 2. 가상화가상화는 간단하게 설명하자면 하나의 물리적인 하드웨어 위에 여러 대의 OS를 올려 운영하는 것이다. 물리적인 CPU/서버를 이용할 때 하나의 운영체제를 이용하여 구동하는 경우에는 CPU 자원을 최대한으로 활용하기가 어려울 것이다. 하지만 하나의 물리적 CPU에서 여러 대의 가상 OS를 운영한다면 CPU 자원을 최대한 활용하는 효과를 가져올 수 있다. 그리고 자원이 많이 남는다면 AWS나 GCP처럼 퍼블릭 클라우드 서비스를 제공하는 것도 가능할 것이다. 2.1 가상화의 구조 - Hypervisor한 컴퓨터를 가상화해서 여러 개의 가상 머신을 만들었다고 가정해보자.그림에서 보면 가상 머신은 Hypervisor위에서 돌아가고 Hypervisor는 OS 위에 위치하고 있는 것을 알 수 있다. 각 가상 머신은 각각의 OS(Linux, MacOS, WindowsOS…)를 갖고 돌아갈 것이며 Hypervisor 밑에 있는 OS는 물리서버의 OS이다. 이런 구조라면 Hypervisor의 기능은 여러 OS를 관리하는 것이 아닐까? 라고 눈치 빠르게 유추해 볼 수 있을 텐데, 정답이다. OS는 Kernel을 통해서 자원관리나 명령을 해석하고 컨트롤하는데 문제는 OS마다 Kernel의 규칙이 다르다는 점에 있다. 그러니까 가상머신에는 다양한 OS가 돌아가게 되고 이것들을 중재해 주는 것이 Hypervisor이다. Hypervisor 자체도 가상화 커널이기 때문에 Hypervisor는 운영체제를 스케줄 하는 가상화 커널이라고 볼 수 있겠다. 2.2 가상화 유형가상화 유형은 크게 두 가지로 볼 수 있다. Type-1과 Type-2형이다. OS를 사용하는가 안하는가에 따라 나뉜다.출처: Wikipedia - Virtualization Type-1(Bare-Metal) : 호스트 OS가 존재 하지 않는다. 베어메탈은 “운영체제가 없는 컴퓨터 하드웨어”를 의미한다. “깡통 컴퓨터”라고 표현하면 적당할 것 같다. 하이퍼바이저가 하드웨어를 제어하고 그 위에 게스트 운영체제(Guest OS)를 올리는 방식이다. 완전히 제어하는 방식을 베어 메탈 하이퍼바이저라고 부른다. Type-2 :호스트 OS가 존재한다. 전통적인 OS에 하이퍼바이저를 실행하고, 이 하이퍼바이저 위에서 게스트 운영체제를 실행하는 방식이다. 하이퍼바이저를 실행하는 운영체제를 HOST 운영체제라고 부른다. 기존에 사용하던 운영체제 위에, 애플리케이션을 실행 하듯이 새로운 운영체제를 올릴 수 있다. 기존 운영체제에 익숙한 일반 사용자들이 주로 접하는 하이퍼바이저다. 가상화는 다시 전가상화 반가상화로 나누어 볼 수 있다. 하드웨어를 완전히 가상화 하고 DOM0를 통해 모든 접근을 처리하는 것이 전가상화이고 완전 가상화 하지 않고 HyperCall이라는 Interface를 통해 접근을 처리하는 것이 반 가상화이다. 전가상화는 DOM0를 통해 모든 처리를 하므로 GuestOS를 수정할 필요가 없지만 Hypervisor가 모든 명령을 중재하므로 성능이 느린 편이다. 반면에 반가상화는 OS커널을 수정해 다른 OS에서 내리는 명령을 Hypercall에서 번역해서 처리하기 때문에 성능이 빠른 편이다. OS커널을 수정해야 하기 때문에 오픈소스 OS만 사용가능하다. 2,3 Hypervisor 기능본격적으로 Hypervisor에서 사용가능한 기능들에 대해서 알아보자. AWS콘솔을 다루다 보면 자주나오는 용어들이 보일 것이다. 대표적으로 Migration, Snapshot, Templet이다. 그 전에 용어 정리기능을 다루기 전에 용어 정리를 하고 넘어가야 이해가 쉬울 것 같다. 가상화의 구조에는 Host, Cluster, Datacenter가 있다. HostHost는 Storage 에 붙어있는 한 물리서버를 말한다. Host의 기준이 헷갈릴 수 있는데, Storage 기준이 아니라 물리서버라는 점을 명심하자. ClusterCluster는 여러대의 Host머신을 한 Storage로 묶어 놓은 것을 말한다. 클러스터는 한 Storage로 묶일 수 있다. DatacenterDatacenter는 여러대의 Cluster를 하나로 묶어 놓은 것을 말한다.정리하자면 데이터 센터 아래에 여러대의 클러스터가 있고 각 클러스터에는 여러대의 호스트 머신이 띄워져 있으며, 호스트 머신에는 가상머신들이 떠 있다. Provisioning쉽게 말해서 제공하는 것. 클라우드 서비스 업체에서 어떤 걸 제공받느냐에 따라서 Server Resource Provisioning : CPU, Memory, IO 등과 같은 실제 서버의 자원을 할당해주고 운영할 수 있게 제공해주는 것을 말한다. 출처 : 프로비저닝이란 OS Provisioning : OS를 서버에 설치하고 구성작업을 해서 사용할 수 있도록 제공하는 것을 말한다. Software Provisioning : WAS, DBMS 등의 소프트웨어를 설치하고 세팅하여 실행할 수 있도록 제공하는 것을 말한다. Account Provisioning : 접근 권한을 가진 계정을 제공해주는 것을 말한다. 클라우드 인프라 쪽에서는 해당 업무를 담당하던 관리자가 변경된 경우 권한의 인계를 Account Provisioning 을 통해 하는 경우가 많다. Storage Provisioning : 데이터를 저장하고 관리할 수 있는 Storage 를 제공할 수 있다. 특히 클라우드에서는 제공하는 Storage 의 종류와 용도에 따라 다양한 방식의 제공이 이루어진다. 2.3.1 MigrationMigration은 한 클러스터 내에서 어떤 호스트에 붙어있는 가상머신을 다른 호스트로 옮기는 것을 말한다. Storage가 같기 때문에 가상머신을 다른 호스트로 옮기는 것이 가능하고 데이터도 그대로 사용가능하다. 2.3.2 Storage MigrationStorage Migration이란 Storage를 갈아타는 Migration이라고 생각하면 된다. 한 클러스터에 있는 가상머신을 다른 클러스터로 옮기는 것이다. 중요한 점: 구조를 살펴보면 Storage가 가장 핵심인 것을 알 수 있다. Storage위의 가상머신 등은 얼마든지 죽어도 다시 살릴 수도 있고 다른 호스트로 옮길 수도 있지만 Storage는 죽으면 참…답이 없다. 이에 대해서 여러가지 처리를 해놓았지만 이 글에서는 다루지 않겠다. 2.3.2 Snapshot스냅샷은 특정 시간대의 가상머신 데이터와 설정 정보를 백업하는 기술이다. Snapshot으로 저장된 백업 데이터를 이용하면 VM에 장애가 발생하더라도 빠르게 복구 가능하다. 2.3.3 Templet서버 템플릿이란 서버 다수를 사용하는데 사용할 수 있도록 공통 요소들을 프로비저닝해 둔 서버 이미지를 말한다. 기능들에서 살펴보니 AWS에서 사용되는 여러가지 용어들이 익숙하게 보이는 것을 확인할 수 있다. 이런 지식들을 가지고 AWS나 기타 클라우드에서 활용할 수 있고 후에 Docker나 Kubernetes 등에도 활용할 수 있을 것이다.","link":"/2020/06/07/virtualization/"},{"title":"인프라의 기초, Docker에 대해서 알아보자","text":"여러 서비스에서 다양하게 사용되고 있는 Docker에 대해서 알아보자. main 출처 : 완벽한 it 인프라 구축을 위한 docker.완벽한 it 인프라 구축을 위한 docker을 읽고 정리한글임을 밝힙니다. sub 출처 : https://subicura.com/2017/01/19/docker-guide-for-beginners-1.html Docker?도커는 애플리케이션의 실행에 필요한 환경을 하나의 이미지로 모아두고, 이미지를 사용해 다양한 환경에서 앱 실행 환경을 구축하고 운영하기 위한 오픈소스 플랫폼입니다. 도커는 내부에서 컨테이너를 사용하는데, 일반적으로 생각하는 물류시스템에서의 컨테이너를 생각해도 좋습니다. 컨테이너로 실어서 다른 곳에 나르는 것처럼, 다양한 개발환경을 컨테이너로 추상화하기 때문에 동일한 환경을 누구에게나 제공할 수 있습니다. 이렇게 동일한 개발환경을 제공하게 되기 때문에 프로그램의 배포 및 관리를 쉽게 할 수 있게 됩니다. 하루종일 환경세팅만 하다가 하루를 날린 경험이 있다면 도커로 환경세팅하는 것이 얼마나 감사한 일인지를 잘 느낄 수 있을 것 입니다. 도커를 이용해서 개발을 하게 되면 폭포형 개발에서 벗어나서 지속적 딜리버리가 가능한 구조의 개발 스타일이 가능해집니다. 도커를 이용한 블루 그린 디플로이먼트 방법이 그 예시 중 하나인데. 블루 그린 디플로이 먼트는 글 하단에서 자세하게 다뤄보도록 하겠습니다. *참고로 도커 컨테이너를 가장 잘 사용하고 있는 회사는 구글이고, 모든 서비스들이 컨테이너로 동작하며 매주 20억 개의 컨테이너를 구동 한다고 합니다. Container도커는 컨테이너 기술을 활용합니다. 컨테이너 기술은 도커가 시작되면서 만들어진 기술은 아니고, 기존에 존재하던 기술이었습니다. 컨테이너란 호스트 OS상에 논리적인 구획, 즉, 컨테이너를 만들고, 어플리케이션을 작동하기 위해 필요한 라이브러리나 애플리케이션 등을 하나로 모아 마치 별도의 서버인 것처럼 사용할 수 있게 만든 것입니다. 호스트 OS의 리소스를 논리적으로 분리시키고 여러개의 컨테이너가 이것을 공유해 사용합니다. 컨테이너는 가볍고 속으로 작동합니다. 컨테이너는 가상화 기술을 사용해서 다양한 기능을 제공하게 되는데, 가상화에 대해서는 이 글을 참고하시면 좋습니다. 가상화. 컨테이너 기술의 장점을 잠깐 소개하자면, 기존의 컨테이너를 이용하지 않는 시스템에서는 하나의 OS 상에서 움직이는 여러 애플리케이션들에 대한 관리를 해주어야 합니다. 다양한 디렉토리와 IP주소를 공유하게 됩니다. 이런 개발 환경에서는 각 어플리케이션이 서로 영향을 받을 가능성이 높습니다. 반면에 컨테이너를 활용하면 OS나 디렉토리, IP 주소 등을 각 어플리케이션이 독립적으로 갖고 있는 것처럼 보이게 할 수 있습니다. 이런 개발환경에서는 마이크로 서비스가 구현될 가능성이 높습니다. Image개발자는 도커를 이용해서 자신이 개발한 프로그램에 필요한 모든 것이 포함되어 있는 도커 이미지를 작성합니다. 이 이미지는 도커의 가장 큰 특징 중 하나입니다. 개발자가 개발한 환경을 도커 이미지로 만들면 이 이미지를 기반으로 해서 컨테이너가 동작하게 됩니다. 이렇게 만들어진 이미지는 기본적으로 어디서든 동작합니다. ‘테스트 환경에서는 됐는데, 제품 환경에서는 안돈다’라는 리스크를 줄일 수 있게 됩니다. 이를 통해 지속적 딜리버리가 가능하게 되고 변화에 강한 시스템을 구축할 수 있습니다. 데이터 사이언스 분야에서는 대량의 컴퓨터 리소스를 사용하게 되고 다양한 라이브러리들을 사용하게 되는데, 환경 세팅에 너무 힘을 쓰다보면 모델 개발에 집중할 수 없게 됩니다.(공감하시는 분들 많으실 겁니다( xgboost설치 할 때를 생각해보십시오). 이런 경우 환경을 도커 이미지로 모아두면 어디에서나 다른 환경에서도 작동하는 실행환경을 만들 수 있게 됩니다. Docker의 기능도커에는 크게 세 가지 기능이 있습니다. Docker Build, 이미지 만들기 Docker Ship, 이미지 공유 Docker Run, 컨테이너 실행 Docker Build도커는 앞서 소개했듯이, 프로그램 실행에 필요한 프로그램 본체, 라이브러리, 미들웨어, OS, 네트워크 등을 하나로 모아서 Image로 만듭니다. 그리고 이 이미지는 컨테이너의 바탕이 됩니다. 보통 Docker에서 빌드시에 권장하는 내용은 하나의 이미지에 하나의 어플리케이션만 넣어 두고, 여러개의 컨테이너를 조합해 서비스를 구축하는 것입니다. 도커 이미지를 만드는 방법은 Docker 명령어를 사용해 수동으로 만들 수도 있고, Dockerfile이라는 설정 파일을 만들어서 작성한 내용을 바탕으로 자동으로 이미지를 만들 수 있습니다. Dockefile을 사용하여 관리하는 것이 지속적 integration과 지속적 딜리버리 관점에서 바람직해 보입니다. 또한 도커 이미지는 겹쳐서 사용할 수 있다는 것이 중요한 특징입니다. 도커에서는 변경이 있었던 부분을 이미지 레이어로 관리합니다. Docker Ship만들어진 이미지는 공유가 가능합니다. Docker Hub에 자유롭게 공개를 할 수 있습니다. 많은 이미지가 있으므로 원하는 내용이 있다면 받아서 사용이 가능합니다. 물론 가입은 해야합니다. 또한 도커는 github이나 bitbucket과 연계가 가능하기 때문에 github같은 곳에서 Dockerfile을 관리하고 거기에서 이미지를 자동으로 생성해서 Docker Hub에 공개 할 수도 있습니다. Docker RunDocker는 리눅스 상에서 컨테이너 단위로 서버 기능을 작동시킵니다. 이 때 사용되는 것이 도커 이미지입니다. 이미지만 있다면 여러대의 컨테이너를 기동시키는 것도 가능합니다. 도커는 다른 가상화 기술과는 다르게 떠 있는 OS 상에서 프로세스를 실행시키는 것과 같은 속도로 빠르게 실행을 할 수 있습니다. 도커는 하나의 리눅스 커널을 여러 개의 컨테이너가 공유하는 구조입니다. 각 그룹별로 프로세스나 파일에 대한 엑세스는 독립적으로 가져가게 됩니다. 이렇게 독립적으로 사용하게 위해 리눅스의 namespace나 cgroup등의 개념이 사용됩니다. 한 대의 호스트 머신에서 모든 도커 컨테이너를 작동시키고 운용하는 것은 힘들기 때문에 분산환경을 구축하고, 컨테이너 관리를 위해서 오케스트레이션 툴을 사용합니다. 컨테이너 오케스트레이션 툴 중 가장 핫하고 유명한 것은 Kubernetes입니다. Docker Image 사용해보기Docker Install먼저 Docker를 사용하기 위해서는, 도커를 먼저 설치해야 합니다. Mac이나 Windows를 사용하시는 분들은 https://docs.docker.com/get-docker/ 에서 다운받아 간단하게 설치하실 수 있습니다. 설치 후에 1docker version 명령어를 쳤을 때, 이상 없이 나오면 성공입니다. Docker Image1docker image pull [이미지명] 이미지를 받는 방법은 간단합니다. pull 명령어를 통해서 원하는 이미지를 받으면 됩니다.받고 싶은 이미지를 search를 통해서 검색하고 Pull해서 받아봅니다. 받은 이미지들의 목록은 ls를 이용해면 됩니다. 1docker image ls 생각보다 이미지가 너무 많습니다. 원하지 않는 이미지를 지우고 싶을 때는 rm을 사용하면 됩니다. 리눅스 명령어와 비슷합니다. 1docker image rm [이미지id] 이미지 id를 적어주고 명령어를 실행시키면, 해당 이미지가 없어진 것을 볼 수 있습니다. 사용하지 않는 이미지를 제거하고 싶다면 prune을 사용하면 됩니다. 이번에는 컨테이너로부터 이미지를 직접 작성해 봅시다. 작성자에 ‘hyub’이라는 정보를 설정하고 webserver라는 컨테이너를 hyuby/webfront라는 이름으로 태그명을 지정해서 새 이미지를 작성해 보겠습니다. 명령어는 다음과 같습니다. 1docker container commit -a 'hyub' webserver tkdguq05/webfront:1.0 이렇게 만들어진 이미지의 작성자 정보는 docker image inspect로 확인할 수 있습니다. 만든 hyuby/webfront 이미지를 그러면 업로드 해보겠습니다. 1docker image push [이미지명:태그명] 위와 같은 명령어가 기본값입니다. 1docker image push tkdguq05/webserver:1.0 혹시 denied: requested access to the resource is denied이런 에러가 나온다면, Docke Hub에 로그인이 되어있지 않았기 때문일 가능성이 높습니다. 1docker login 이 명령어를 통해 로그인 하도록 합시다. 그래도 에러가 난다면, Docker Hub와 이미지에 있는 작성자 정보가 다르기 때문입니다. 1docker image tag tkdguq05/webserver:1.0 [change user]/webserver:1.0 이런 식으로 작성자명을 변경한 후에 업로드 해보도록 합니다. 업로드가 완료되면! Docker Hub에 올린 이미지가 공개됩니다. Docker export컨테이너나 이미지나 모두 export하고 다시 import 할 수 있습니다. export를 하게되면 tar파일로 떨어지게 됩니다. 이 파일을 import하면 이미지나 컨테이너를 그대로 사용할 수 있습니다. 12#docker image save [옵션] &lt;파일명&gt; [이미지명]docker image save -o export.tar tensorflow 읽는 것도 간단합니다. 1docker image load -i export.tar 다만 컨테이너는 명령이 조금 다릅니다. 12#docker container export &lt;컨테이너 시별자&gt; &gt; 저장 파일명docker container export webserver &gt; latest.tar 이렇게 만들어 놓은 tar파일을 이용해서 이미지를 작성할 수 있습니다. 위에 있는 import 명령어를 활용하면 됩니다. 여기서 한 가지 이상한 점이 있습니다. export/import, save/load가 왜 따로 구분되어 있을까? 하는 점입니다. 둘의 차이는 무엇일까요? export/import, save/load의 차이컨테이너를 export하면 컨테이너를 실행하는데 필요한 파일을 모두 압축된 아카이브로 모을 수 있습니다. 이 tar파일을 풀면 컨테이너의 root파일 시스템을 그대로 추출할 수 있습니다. 반면 save는 이미지의 레이어 구조도 포함된 형태로 tar로 모을 수 있습니다. 바탕이 되는 이미지는 같아도 export를 사용할 때와 save 명령을 사용할 때는 내부적인 디렉토리와 파일 구조가 다릅니다.따라서 docker container export 명령에는 docker image import 명령을, docker image save명령에는 docker image load를 사용하는 게 좋습니다.","link":"/2020/07/03/docker/"},{"title":"2020년 상반기를 보내면서, 회고하기","text":"2020년 상반기 회고. (썸네일 사진은 대구의 앞산. 불꽃놀이도 운 좋게 구경하고 좋은 기억이었다.) 2020년 새해가 밝은 이후로 벌써 7월이 다 지나가고 있다. 아차 싶을 사이에 시간은 쏜살 같이 흘렀고 동시에 회사에 들어온지도 1년이 되었다. 작년의 나는 많은 것들을 다짐했다. 특히 글또를 하면서, 글에 대해서 고민을 많이 했고 완성도 있는 글을 쓰고자 노력했다. 2주에 한 번 글을 쓰는게 글또의 룰이지만, 스스로의 의지로 더 많은 글을 쓰고 싶었고 공부를 꾸준히 하면서 정리하고 성장하고 싶었다. 나는 많이 성장했을까? 내가 지키고자 한 약속들을 지키고 있는걸까? 글또를 시작하면서의 다짐 https://tkdguq05.github.io/2020/02/24/geultto4/#more2019년도 회고, 2020년을 맞이하며 https://tkdguq05.github.io/2019/12/22/adios-2019/#more “2020을 시작하며”를 다시 보면서2020년에 나는 추천 시스템이나 다른 서비스를 위한 데이터 파이프라인 구성을 성공적으로 한다. 퇴근 후 개인 공부시간을 매일 30분 이상 갖는다. 캐글 대회에 도전한다. 캐글 대회에서 동메달 이상의 성과를 낸다. 일주일에 3일 이상 운동한다. 3대 운동 300에 도전한다. 돈을 모아서 피렌체에 간다. 꾸준히 글을 쓴다. 책을 꾸준히 읽자. 일을 즐겁게 한다. 소중한 사람들을 잘 챙기자. 나는 2020년을 시작하면서 위와 같은 목표를 세웠었다. 보자마자 헛웃음이 나왔다. 11개의 목표 들을 하나하나 훑어보자. 먼저 파이프라인에 대해서는 잘 구성한 것 같다.오픈소스와 나름 최신 스택의 기술을 사용해서(Airflow, Kubeflow, Spark …) 파이프라인을 운영하고 있고 아직까지는 잘 돌아가고 있다. 파이프라인 구성을 잘 한 것도 기분이 좋았는데 새로운 영역을 공부하는 재미를 알게 된 것 같다. 아마 데이터 엔지니어링이라고 불리우는 이 영역에 대해서 항상 마음만 갖고 제대로 공부해 본 적이 없었고, 어떤 걸 먼저 공부해야 하는지 몰랐었다. 특히 기본 지식이 부족한 게 큰 것 같았다. 가상화와 리눅스에 대해서 알아야 이해가 빨랐었을 것 같은데 예전에는 전혀 몰랐다. 다행히 같이 일하시는 매니져님이 오픈소스도 많이 다뤄보시고 가상화와 리눅스에 대해서 잘 알려주셔서 다른 개념들을 이해하는데 큰 도움이 되었다. 공부한 내용을, 특히, 가상화와 도커에 대해서 최근에 정리를 했다. 사실 마음에 들지는 않는다. 더 필요한 내용들이 많다고 생각되었는데, Kubernetes나 Kubeflow를 빠르게 다뤄보고 싶은 사람들이 타겟이었기 때문에 가볍게 작성했다. 추후에 시간이 되면 스핀오프 글로 도커에 대한 내용을 더 자세하게 다루고 해당 개념이 Kubernetes나 Kubeflow에서 어떻게 적용되는지 자세하게 다뤄보고 싶다. 이미 스포가 된 것 같은데 앞으로의 글은 Kubeflow와 Kubernetes에 대한 글이 될 것이다. 처음 다뤄 보는 내용이지만 기존 시스템을 혁신적으로 바꿀 수 있을 만하기 때문에 흥미롭게 보고 있다. 퇴근 후에 공부시간을 갖고 있지 못하고 있다.공부도 공부인데 목하고 어깨가 너무 안좋아졌다. 조금만 무리하면 목디스크처럼 통증이 있어서 왠만하면 퇴근하고는 운동을 하거나 스트레칭을 하고 있다. 이렇게라도 하지 않으면 회사 업무에 신경쓰기 힘들어져서 일단 몸을 먼저 챙겨야 겠다는 생각이다. 주말에는 가끔 도수치료나 물리치료를 받고 있고 여유롭게 지내고 있다. 하지만 최근 들어서 공부하고싶은 내용도 생겼고, 좀 더 달려봐야겠다는 마음이라 앞으로는 공부시간을 갖게 되지 않을까 한다. 캐글 대회에는 도전하지 못했다.물론 동메달 이상의 성과도 내지 못했다. 하지만 회사 동료들과 챌린지에 나가서 나름 대회에 참가해보기는 했다. 회사에 들어오면서 내가 세운 다른 목표중에 하나였는데, 결과는 좋지 않았지만 훌륭한 출발이었던 것 같다. 추후에 캐글에 도메인 성격과 맞는 대회가 나오면 같이 참가해보기로 했다. 챌린지 결과 때문인지 다들 눈에 독기가 생긴 것 같다. 일주일에 3일 이상 운동을 하고 있다.3대 운동 300이라는 목표를 달성하려고 했는데 무리하게 하다보니까 몸이 너무 피곤해서 업무에 지장이 가는 것 같았다. 단백질을 많이 챙겨먹어야 하는데 그렇지도 못하고 있고… 일단 운동을 통해서 기본 체력을 올리고 스트렝스 훈련을 강화해야겠다. 무게가 잘 늘지 않는데 일단 유지하고 심페 능력부터 길러야겠다. 나이를 먹으면서 느끼는 것 중 하나는 내 상태를 유지하는 것도 힘든 일이라는 점이다. 피렌체에 가려고 했다.4월에 가는 티켓을 구매했고 모든 걸 준비해 놨었다. 하지만 코로나19로 인해……나이 서른이 되기 전에 유럽을 다시한번 가고 싶었는데 이건 뭐 어쩔 수 없으니깐^^ 하…결국 누나도 이탈리아에서 돌아왔고 앞으로 피렌체에 가려면 돈이 더 들거 같다. 언젠가 한번쯤은 꼭 가고 싶기 때문에 돈을 모아서 피렌체와 로마 밀란을 가봐야겠다. 글은 글또를 하면서 꾸준히 쓰고 있다.돈의 강제성이란 정말 무서운 것이다. 아 정말 쓰기 싫다라는 생각이 들어도 보증금을 생각하면 맘이 바뀐다. 애초에 목표를 ‘10만원 보증금 유지하기’로 잡고 있어서 더 그런 것도 있지만, 직접 돈을 벌다 보니 푼돈 몇 푼 나가는 게 모여서 10만원이 되고 카드값이 되었다. 티끌모아 태산이란 말은 내 통장의 잔고에는 해당이 되는 말이 아니었고 카드 값에 대한 말인 것 같았다.2주에 글 하나 쓰는 건 글또를 하면서 조금 아깝다는 생각이 들었다. 공부할 시간이 더 많아지면 정리할 내용도 많아지고 글 쓸 거리도 많아지겠지! 시리즈 물도 다시 한번 써보고 싶다. 확실히 시리즈를 써야 내 관련 글도 많이 보고 글을 쓰는 내 태도도 좋아 지는 것 같다. 책은 기술관련 서적 말고는 거의 못 보고 있다.‘인간의 무늬’나 ‘정확한 사랑의 실험’을 꼭 마저 보고 싶은데, 아… 책 읽는 거 힘들다. 아이패드를 항상 갖고 다니면서 버스나 지하철에서 봐야겠다. 유튜브 프리미엄을 끊었더니 유튜브가 너무 너무 좋아졌는데, 책 몇 페이지라도 보고 유튜브 보고 이래야겠다. 하루에 몇 페이지 꾸준하게 정해놓고 읽으면 모여서 책 한 권이되고 다섯 권이 되지 않을까? 책 많이 보고 싶어서 yes24 많이 들어가긴 하는데, 기술 서적이 더 눈에 띄고 그러면서 다른 책은 언제 보나 또 고민하고 있고… 꾸준히… 꾸준하게…! 일에 대해서 고민이 커지고 있다.사실 데이터 팀에서의 일을 너무 즐겁고, 이 일에 대해서 고민하는 것도 행복하다. 하지만 어떤 프로젝트가 늦어지고 그것 때문에 우리가 만들어 놓은 모델을 붙이지 못해서 하염없이 기다리고 있을 때 좀 힘들었다. 큰 불만이 없었는데 조금씩 불만이 쌓여가고 있는 것 같다. 아예 신경을 쓰지 말고 내 일만 해야할까? 고민한다고 상황이 나아지기는 하는 걸까? 잘 모르겠다. 일단 내 생각은 우리 일에 집중하자는 것이다. 고민해봤자 나아지는 건 없는 것 같았다.이런 고민 말고 다른 고민은 데이터 엔지니어와 모델러에서 어떤 커리어 패스를 잡아야 할까 하는 고민이다. 엔지니어 쪽 공부가 재밌긴 하지만, ‘내가 다른 사람보다 잘 할 수 있을까?’, ‘경력이 너무 모자른건 아닐까?’라는 생각이 많다. 물론 그렇다고 모델러의 역할을 잘 하는 것 같지도 않다ㅎㅎ. 팀원들이랑 얘기도 많이 해보고, 추후에 글또 모임을 또 갖게 된다면, 다양한 분들과 얘기를 하고 싶다. 글을 쓰다보니 마음이 정리되는 것 같은데, 이야기를 나누다 보면 고민도 어느정도 풀리겠지. 소중한 사람들을 잘 챙기려고 노력하고 있다.소중한 사람들을 챙길 때 마음이 좋아지고 행복해질 때가 있는 반면에, 썩 기분 좋지 않은 일도 있었다. 보통 어떤 사람을 챙겨줄 때 내가 기대한 만큼 고마워 한다거나 그 마음이 느껴지면 행복하고 기분이 좋아졌는데, 그냥 항상 있는 일인듯 양, 반응을 보이면 기분이 좋지 않았다. 실망했다고 해야하나? 난 소중하게 생각했는데, 상대방은 그렇지 않은 것 같다고 느낄 때 마음이 좋지 않았다. INFJ형 인간은 이렇때 과감하게 싹을 잘라내 버리는 편 이긴 하지만, 왠만하면 내가 더 잘해야지 라는 마음가짐으로, 차카게 살려고 노력하고 있다. 회고를 하면서. 내가 어떻게 살아왔는지에 대해서 고민하면서 생각을 많이 정리할 수 있어서 좋았다. 한 동안 이런 시간이 없었는데, 강제로라도 회고글을 쓰게 해주는 글또에 큰 감사함을 느낀다. 오랜 고민을 하고 생각을 정리한 끝에 내가 뭘 해야 하고 어떤 행동을 할때 행복한지 기분이 좋은 지에 대해서 덤으로 알게 되는 것 같다. 한 번쯤 뒤돌아 보는 삶은 괜찮은 것 같다. 더 바쁘게 살아야지 하면서 앞만 보고 달리다보면 지치게 된다고 생각한다. 가끔은 뒤돌아보고 내가 걸어온 길이 크게 돌아온 것은 아닌지, 오면서 놓친 것이나 잃어버린 건 없는지, 확인해보고 다음 걸음을 준비하는 게 어떨까 하고 글을 마무리해본다.","link":"/2020/07/18/retrospect2020/"},{"title":"SISG를 활용한 Fasttext에 대해서 알아보자","text":"자연어 처리 모델에 자주 사용되는 FastText를 뽀개보고 skipgram 모델과의 차이를 알아보자. Fasttext출처 : WikipediafastText는 Facebook의 AI Research lab에서 만든 단어 임베딩 및 텍스트 분류 학습을 위한 라이브러리입니다. 이 모델을 사용하면 단어에 대한 벡터 표현을 얻기 위해 비지도 학습 또는 지도 학습 알고리즘을 만들 수 있습니다. Fasttext는 위키피디아 설명에서 보듯이 Facebook에서 만들었고 그에 걸맞게 임베딩도 잘되고 성능도 우수한 편인 모델입니다. 실제로 지금 있는 회사에서도 자연어 처리를 할 때 Word2Vec이나 Fasttext의 도움을 받고 있습니다. 오늘 살펴볼 내용은 Fasttext가 어떻게 학습을 하는지, 그리고 Skip gram 모델과 어떤 차이점이 있는지 입니다.자연어 모델이나 Sequential 딥러닝 모델을 한번 쭉 살펴보신 분들은 이해가 빠를 수 있습니다. 하지만 글은 어렵지 않으니 천천히 따라오면 잘 이해할 수 있을 것입니다. 있을 것이라고 믿습니다. Introduction기존의 embedding model은 unique word를 하나의 vector에 할당할 수 있었습니다. 그러나 이와 같은 방식은 vocabulary의 크기가 커지거나 rare word(못 봤었던 단어)가 많을수록 한계점을 내포하게 됩니다. 한계점이란 이러한 word들은 good word representation을 얻기 힘들다는 점입니다. 특히 현재까지의 word representation 기법들은 문자의 internal structure를 고려하지 않고 있기 때문에 학습이 더욱 힘들었습니다. 더 나아가 스페인어나 프랑스어의 경우 대부분의 동사가 40개 이상의 변형된 형태를 지니고 있는 복잡한 언어이기 때문에, 이러한 언어에서는 rare word 문제가 더욱 부각 됩니다. 이런 이유로 이처럼 형태학적인 특징이 풍부한 언어의 경우에는 subword 정보를 활용하여 rare word에 대한 한계점을 극복하고자 합니다. subword를 이용해서 vector representation을 개선시키고자 하는 것입니다. General Model, Skip-gram먼저 일반적인 모델인 skip-gram의 프로세스를 알아야 fasttext에 대해 잘 알 수 있습니다.스킵 그램은 그림에서처럼 window size의 단어들을 슬라이딩 해 가면서 타겟 단어에 대해서 주변 단어들이 올 확률을 구해 나갑니다. 파란색으로 표시된 것이 타겟 단어이며 주위 단어들이 context word, 즉 주변 단어입니다. $w_t$와 $w_c$로 표현됩니다. 여기에 가정이 하나 추가되는데, context word는 조건부 독립(conditionally independent)이라는 가정입니다. 위의 그림은 아까 살펴봤었던 과정을 모델의 관점에서 자세히 살펴보겠습니다. 레이어의 구성은 인풋 레이어, 히든 레이어, 아웃풋 레이어로 되어있습니다. 인풋 레이어에는 단어들이 들어가고 히든 레이어에는 타겟 단어에 대한 벡터값이 남습니다. 아웃풋 레이어 까지 거치게 되면 가중치 벡터들과 예측한 값이 나오게 되고 가중치 벡터에는 타겟단어와 주변단어들에 대한 가중치들이 저장되어 있습니다. 예측된 결과를 실제 값과 비교해서 각 단어들에 대한 loss값을 뽑아내게 되고 이것을 더하면 전체 loss값이 됩니다. 숫자를 통해서 이를 더 구체화 해보면 다음과 같습니다. 이렇게 계산되는 과정을 살펴보다 보면, 비효율적인 부분들이 발견됩니다. 어디일까요? 문제점을 발견하고 개선시켜나가 보겠습니다. Improvement1. Row Indexing맨 처음 인풋이 들어간 부분과 $W_{input}$을 통해서 타겟에 대한 벡터값을 얻게 됩니다. 만약 들어가는 인풋 단어가 엄청나게 많다면 어떻게 될까요? 수 많은 계산을 일일이 해주어야 합니다. 하지만, 생각을 한번 바꿔보면 어떨까요? 인풋 단어들은 one-hot index로 되어 있습니다. 따라서 $W_{inpit}$의 row index를 그냥 가져오기만 하면 되지 않을까요? 이렇게 row index를 가져오면 계산을 효과적으로 줄일 수 있습니다. 결과적으로는 같지만 처리해야할 연산이 획기적으로 줄게 되는 것입니다. 2. Negative Sampling이번엔 다음부분으로 넘어가 봅시다. 어떤 부분이 비효율적일까요?히든 레이어인 $h$와 $W_{output}$의 곱하는 부분과 softmax 계층의 병목이 보입니다. 생각해봅시다. latent vector $h$와 $W_{output}$도 역시 단어들이 많아지면 연산을 많이 처리해야 합니다. softmax 계층도 마찬가지입니다. 그러나 하나의 target word와 관련된 context word들은 window size내의 작은 word 정도밖에 안됩니다. 전체 단어를 굳이 다 처리해야 할 필요가 없는 것입니다. 즉, $h$와 $W_{output}$의 행렬 곱 연산은 인풋과 관련되어 있기 때문에 업데이트 되어야 할 단어는 몇 개 안되는데도 불구하고, vocabulary에 있는 모든 단어들과의 관계를 비교해야합니다. 여기서 비효율성이 발생하게 됩니다. 이를 해결하기 위해 Negative Sampling이 제안됩니다. Negative Sampling의 핵심은 지금의 multi-clas classification 문제를 간단한 binary classification 문제로 바꾸는 것입니다. 어떻게 바꿀까요? How to Negative Sampling먼저 알아야할 개념이 있습니다. positive example: target word의 context word negative example: target word의 context word가 아닌 word우리는 context word를 1로 처리할 것이고, 아닌 단어들은 0으로 처리할 것입니다. 연산을 줄이기 위함입니다. 예를 들어보겠습니다. 위의 그림에서 dot product된 결과가 나오고 있습니다. (숫자는 임의로 입력한 값입니다) positive example들은 초록색으로 표시되어 label을 1로 처리하고, negative example들은 주황색으로 표시되어 label을 0으로 처리합니다. 이렇게 연산을 줄여서 처리한 뒤에 로스를 계산합니다. 단어가 많으면 많아질수록 줄어드는 연산의 양이 커질 것입니다. 그렇다면 의문점이 또 생깁니다. negative sampling을 하는 단어를 어떻게 정할까요? Negative Sampling은 Corpus 내에서 자주 등장하는 단어를 더 많이 추출하고 드물게 등장하는 단어는 적게 추출하고자 합니다. 이 목적을 달성하기 위해서 Probability distribution을 이용하는데 수식은 다음과 같습니다. $f(w_i)$는 모든 corpus의 단어 중에서 특정 단어 $w_i$가 얼마나 들어있는지를 의미합니다. 따라서 위의 식에서 $P(w_i$은 전체 코퍼스에서 특정단어 $w_i$가 얼마나 있는지를 나타내는 식입니다. 빈도 수와 관계가 있다고 할 수 있겠습니다. 그러면 왜 $3\\over4$ 라는 숫자를 사용할까요? 증명된 것은 없습니다. 논문에서 이 숫자를 사용했을 때 잘 되었던 것입니다. 이 파라미터는 수정이 가능하니 데이터에 맞게 조정하면 됩니다. 우리가 지금까지 했었던 것을 간단하게 정리해 보겠습니다.우리는 이 가정에서 출발했습니다. 가정을 이용해서 skip-gram 모델을 만들었습니다. 효율적으로 처리하기 위해 skip그램에 Row indexing을 사용했고 Negative sampling을 사용했습니다. 이를 통해서 우리가 얻고자 한 것은 다음의 수식입니다. 원래는 왼쪽의 수식의 값을 최대화 하는 것이었지만, 단어의 수가 많아지면 오른쪽의 수식으로 변경해 log-likielihood를 최대화 합니다. context word가 올 확률에 대해서는 다양한 선택이 있겠지만 연산 후의 softmax 결과 값을 사용합니다. 하지만 우리는 negative sampling을 통해 Multiclass Classification 문제를 Binary Classification로 변경했습니다. 결국 위의 softmax는 context 단어들의 존재 여부를 독립적으로 얘측에 대한 확률이 됩니다. negative sampling에 의해서 골라진 contex word에 대한 포지션은 $c$로 표현됩니다. Binary Logistic loss에 의해서, 우리는 다음과 같은 negative log likelihood 값을 얻을 수 있습니다. 밑 부분의 score값인 $s(w_t,w_c)$를 주의해서 봐주시기 바랍니다. 이제 모든 타겟 단어에 대해서 다음과 같은 수식으로 정리가 되면, skip gram model에 대한 설명은 끝납니다. Fasttext, SISGFasttext는 skip gram모델을 개선한 모델입니다. SISG(Subword Information Skip Gram)를 이용해 Skip gram모델의 성능을 개선합니다. SISG란 무엇일까요? SISG(Subword Information Skip Gram)SISG는 간단한 개념입니다. 각 단어 $w$는 n-gram의 bag을 담고 있다고 해봅시다. 이제 여기에 &lt;와 &gt;를 이용해서 단어의 시작과 끝을 알려줄 것입니다. 한 가지 더 단어 그 자체를 넣어 줄 것입니다. 각 단어 자체의 표현을 학습시키기 위함입니다. where이라는 단어의 처리에 관한 예를 들어봅시다. n=3일 때의 예시입니다. 위의 예시처럼 &lt;,&gt;를 한 글자씩 처리해서 나누면 re&gt;로 끝나게 됩니다. 그런데 여기에 &lt;where&gt;도 같이 넣어주는 것이 핵심입니다. 이유를 생각해봅시다. 중간에 her이란 단어가 눈에 띄었을 겁니다. her는 단어로 처리되어야 할까요? 아닙니다. her은 그냥 단어의 조각일 뿐이며 subword에 해당됩니다. 진짜 단어였다면 &lt;her&gt;로 표현이 되어야 할 것입니다. 이렇게 만든 후에 n-gram의 벡터표현을 모두 더한 score 값을 구하면, 그것이 바로 SISG의 목적 함수가 됩니다. 아까 봤던 score함수가 바뀌었습니다. score함수만 바꿔주면 fasttext의 목적합수 입니다. 이를 pseudo code로 나타내면.(뒷부분에 neg가 아니라 neg_sample인데 오타가 있습니다.) 참고바랍니다. 마지막으로 평가에 대해서 입니다. 임베딩의 각 버전에 대해, 임베딩만을 기반으로 주어진 account 셋의 topic을 예측하도록 분류기를 훈련합니다. hold out set의 account에 대해 예측 된 주제를 사람이 라벨링 한 topic과 비교하여 임베딩이 topic의 유사성을 얼마나 잘 파악하는지 평가할 수 있습니다. Fasttext는 Skip Gram 모델을 subword를 활용하자는 간단한 아이디어로 만들어진 모델입니다. 모델에서 활용되는 이런 센스들을 잘 익혀두면 다른 모델에 적용하거나, 다른 논문들을 읽을 때 큰 도움이 될 것 같습니다.","link":"/2020/08/14/Fasttext/"},{"title":"Kubernetes Beginning","text":"자 이제 쿠버네티스에 대해 공부해보자. Docker in kubernetes, k8s architecture, cluster, and pods. 지난 글들로부터 가상화에 대해서 공부했고, 도커에 대해서 살펴봤습니다. 쿠버네티스는 가상화와 도커가 기본이 되는 시스템입니다. 가상화와 도커에 대해서 기억이 잘 나지 않는다면 잠시 보고 와도 좋습니다. 인프라의 기초, Docker에 대해서 알아보자.클라우드 서비스의 기초. Virtualization, 가상화에 대해서 Kubernetes?쿠버네티스를 최초로 만들고 사용한 곳은 구글입니다. 시스템에 배포 가능한 어플리케이션 구성 요소의 수가 많아지면서 모든 구성 요소의 관리가 어려워질 수 밖에 없기 때문입니다. 구글은 전 세계에서 소프트웨어 구성 요소와 인프라를 잘 배치하고 관리하는 방법이 필요하다는 것을 최초로 깨달은 회사일 것입니다. 수십만 대의 서버를 운영하고 엄청난 규모의 배포관리를 처리하는 기업입니다. 이로 인해 구글은 수천 개의 소프트웨어 구성 요소를 관리하고 비용 효율적으로 개발, 배포할 수 있는 솔루션을 개발해야만 했습니다. 그래서 등장한 것이 쿠버네티스의 전신인 보그-오메가 이며, 2014년에 구글은 쿠버네티스를 출시하게 됩니다. 구글이 만든 시스템답게 정말 잘 됩니다! 만약 구글이 모놀리스 어플리케이션으로 개발을 했다면 어땠을까요? 아마 이 정도의 확장은 불가능 했을 것 입니다. 모놀리스 어플리케이션이란 한 덩어리처럼 개발을 하는 것입니다. 큰 돌로 이루어진 건축물에 비유를 할 수 있습니다. 이와 반대되는 개념은 마이크로스서비스 기반 어플리케이션입니다. 이는 단일 역할을 책임지는 여러 조각으로 구성된 소프웨어로 구성되어 있습니다. 구글은 각 부분이 알아서 잘 돌아가는 MSA구조로 이루어져 있기 때문에 이렇게 확장할 수 있었습니다. 그리고 그것을 가능하게 해준 것이 바로 쿠버네티스입니다. 쿠버네티스는 컨테이너화된 어플리케이션을 쉽게 배포하고 관리할 수 있게 해주는 소프트웨어 시스템입니다. 어플리케이션은 컨테이너에서 실행되어 동일한 서버에서 실행되는 다른 어플리케이션에 영향을 주지 않아 동일한 하드웨어의 다른 조직의 어플리케이션이 실행될 때 영향을 주지 않습니다. 이를 통해 쿠버네티스를 사용하게 되면, 제공된 하드웨어를 최대한으로 사용할 수 있게 됩니다. 쿠버네티스를 활용하면 모든 노드가 하나의 거대한 컴퓨터 처럼 수천대의 노드에서 어플리케이션을 실행할 수 있습니다. Kubernetes을 사용하면 좋은 점먼저 개발자가 어플리케이션 핵심 기능에 집중할 수 있게 됩니다.쿠버네티스를 클러스터의 운영체제로도 생각할 수 있습니다. 이렇게 되면 어플리케이션 개발자가 특정 인프라 관련 서비스를 어플리케이션에 구현하지 않아도 됩니다. 쿠버네티스를 활용하면 되니까요. 쿠버네티스에는 서비스 디스커버리, 스케일링, 로드밸런싱, 자가치유, 리더선출 같은 것들이 들어갑니다. 따라서 어플리케이션 개발자는 기능을 구현하는 데 집중을 하면 되고, 인프라에 관련된 고민을 하지 않아도 됩니다. 운영 팀이 효과적으로 리소스를 활용할 수 있습니다.쿠버네티스는 클러스터에 컨테이너화된 어플리케이션을 실행하고 구성 요소들 간에 연결되는 방법에 대한 정보를 제공하며, 이를 이용해 모든 어플리케이션이 계속 실행되도록 합니다. 어플리케이션은 어떤 노드에서 실행되든 상관이 없기 때문에 쿠버네티스는 언제든지 어플리케이션을 재배치하고 조합함으로써 주어진 리소스를 잘 활용할 수 있게 됩니다. Kubernetes Architecture쿠버네티스 클러스터가 어떻게 구성되어 있는지 알아봅시다. 쿠버네티스는 두 가지 유형으로 나눌 수 있습니다. 마스터 노드와 워커 노드입니다. 마스터 노드는 쿠버네티스 컨트롤 플레인을 실행합니다. 컨트롤 플레인은 전체 쿠버네티스 시스템을 제어하고 관리합니다. 워커 노드는 실제 배포되는 컨테이너 어플리케이션을 실행합니다. Control Plain컨트롤 플레인은 클러스터를 제어하고 작동시키는 역할입니다. 하나의 마스터 노드에서 실행하거나 여러 노드로 분할되고 복제되어 고가용성을 보장할 수 있는 여러 구성 요소로 구성 되어 있습니다. 그 구성요소는 다음과 같습니다. Kubernetes API 서버 : 사용자 , 컨트롤 플레인 구성 요소와 통신한다. Scheduler : 어플리케이션의 배포를 담당한다.(스케쥴링이라는 용어는 파드가 특정 노드에 할당됨을 의미합니다.) 노드가 배정되지 않은, 새로 생성된 파드를 감지하고 실행할 노드를 선택하는 컨트롤 플레인 컴포넌트입니다. 스케쥴링 결정을 위해서 고려되는 요소들은 다음과 같습니다. 리소스에 대한 개별 및 총체적 요구 사항 하드웨어/소프트웨어/정책적 제약 어피니티(affinity) 및 안티-어피니티(anti-affinity) 명세 데이터 지역성 워크로드-간 간섭 데드라인 Controller Manager : 컨트롤러 매니저는 구성요소 복제본, 워커 노드 추적, 노드 장애 처리 등과 같은 클러스터 단의 기능을 수행한다. etcd : 클러스터 구성을 지속적으로 저장하는 신뢰할 수 있는 분산 데이터 저장소이다. Node워커 노드는 컨테이너화된 어플리케이션을 실행하는 시스템입니다. 어플리케이션을 실행하고 관리하며 서비스를 제공하는 작업은 다음 구성 요소에 의해 수행됩니다. Contianer Runtime : 컨테이너를 실행하는 도커나 rkt 등등 kubelet : API 서버와 통신하고 노드의 컨테이너를 관리한다. kubelet은 파드를 관리한다고 생각하면 됩니다. 클러스터의 각 노드에서 실행되는 에이전트로 kubelet은 파드에서 컨테이너가 확실하게 동작하도록 관리합니다. kubelet은 다양한 메커니즘을 통해 제공된 파드 스펙의 집합을 받아서 컨테이너가 해당 파드 스펙에 따라 원할하게(Health) 동작하게 합니다. kubelet은 쿠버네티스를 통해 생성되지 않은 컨테이너는 관리하지 않습니다. kube-proxy : 어플리케이션 구성 요소 간에 네트워크 트래픽을 로드밸런싱한다. kube-proxy는 클러스터의 통신을 담당합니다. 클러스터의 각 노드에서 실행되는 네트워크 프록시로, 쿠버네티스의 서비스 개념의 구현부입니다. kube-proxy는 노드의 네트워크 규칙을 유지 관리합니다. 이 네트워크 규칙이 내부 네트워크 세션이나 클러스터 밖에서 파드로 네트워크 통신을 할 수 있도록 해줍니다.kube-proxy는 운영 체제에 가용한 패킷 필터링 계층이 있는 경우, 이를 사용합니다. 그렇지 않으면 kube-proxy는 트래픽 자체를 forward합니다. Kubernetes cluster가 상호작용하는 방법클러스터는 마스터와 노드들로 구성이 되어있고 각 노드들은 도커, kubelet, kube-proxy 데몬을 실행합니다. 우리가 설치한 kubectl 명령어를 실행하면 마스터 노드에 쿠버네티스 API 서버로 REST요청을 보내서 클러스터와 상호작용하게 됩니다. 1$ kubectl get nodes 이제 명령어로 node들을 확인할 수 있습니다. 이렇게 간단하게 쿠버네티스 클러스터 구조에 대해 살펴봤습니다. Docker in Kubernetes지난 번 도커 글을 연장해서 쿠버네티스로 이어지는 흐름을 만들어보겠습니다. 지난 도커 글에서는 간략한 소개만 했기 때문에 이미지와 이미지 레이어에 대한 이야기를 깊게 못해서 조금 더 설명을 해보겠습니다. Docker Image우리가 어떤 어플리케이션을 만들었다고 가정합시다. 이 어플리케이션을 이미지로 구성해서 내 환경과 같이 만들어, 어떤 환경에서 누구나 작업하게 만들고 싶습니다. 이렇게 이미지로 패키징하기 위해서는 Dockerfile을 만들어야 합니다. 이 파일에는 도커가 이미지를 생성하기 위해 수행해야 할 지시 사항들이 담겨져 있습니다. 이 Dockerfile은 .py와 같은 파일과 같은 위치에 있어야 합니다. Dockerfile을 살펴보겠습니다. 책 Kubernetes in Action의 예제를 사용하겠습니다. 123FROM node:7ADD app.js /app.jsENTRYPOINT [\"node\", \"app.js\"] FROM은 기본 이미지로 사용할 컨테이너 이미지를 정의합니다. ADD는 로컬 디렉토리의 app.js 파일을 이미지의 루트 디렉토리에 동일한 이름으로 추가한다는 뜻입니다. ENTRYPOINT는 이미지를 실행했을 때 수행돼야 할 명령어를 나타냅니다. 1234$ docker build -t &lt;name&gt; .#실제 사용$ docker build -t kubia . 이 명령어를 이용하면 이름대로 이미지가 빌드됩니다. 도커에게 현재 디렉토리의 컨텐츠를 기반으로 이름으로 적힌 이미지를 빌드하라고 요청합니다. 도커는 Dockerfile을 보고 명령에 기반해 이미지를 빌드합니다. 이미지가 컴퓨터에 저장돼 있지 않으면, 도커는 Docker Hub에서 이미지를 다운 받습니다. 도커의 이미지는 하나의 큰 덩어리 파일이 아니라 여러개의 레이어로 구성됩니다. 그래서 빌드를 할 때 필요한 이미지가 있다면 필요한 이미지 레이어만 다운로드 하는 것 입니다. 도커 이미지를 배포하는 것은 저번 글에서 설명을 했습니다. 이제 컨테이너 이미지에 어플리케이션을 패키징하고 도커 허브를 통해 사용할 수 있게 됐습니다. 123456$ docker tag kubia luksa/kubia$ docker push luksa/kubia # 버전 명을 적지 않으면 최신 버전으로 알아서 작성해 주지만 시간이 더 걸립니다.# 다른 머신에서 이미지 실행하기$ docker run -p 8080:8080 -d luksa/kubia 이제 쿠버네티스 클러스터에 어플리케이션을 배포할 수 있게 됐습니다. 배포를 위해서는 쿠버네티스 클러스터가 필요합니다. 쿠버네티스 클러스터를 구성하는 법은 다양합니다. mini kube를 설치하는 방법도 있고, AWS에 EKS나 GCP를 이용해서 node를 만들고 쿠버네티스 환경을 만들 수 있습니다. 각자의 환경이 구성되었다고 가정하고 진행하겠습니다. Kubernetes client 설치kubectl CLI 클라이언트가 있어야 쿠버네티스를 다룰 수 있습니다. 12345678910* 리눅스 기준#파일 다운로드curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s \\ https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl#파일 권한 변경chmod +x ./kubectl#파일을 PATH가 설정된 폴더로 옮긴다(alias 명령어로 실행하기 위해)sudo mv ./kubectl /usr/local/bin/kubectl#설치 확인kubectl version --client kubectl 정보가 나오면 성공입니다.이제 아까 전에 만든 node.js 어플리케이션을 구동해 보겠습니다. 12$ kubectl run kubia --image=luksa/kubia --port=8080 --generator=run/v1&gt; replicationcontroller '\"kubia\" created –image=luksa/kubia 부분은 실행하는 컨테이너 이미지를 명시하는 것이며, -port=8080은 쿠버네티스에 어플리케이션이 8080포트를 수신 대기해야 한다는 것을 알려줍니다. –generator부분은 디플로이먼트 대신 레플리케이션컨트롤러를 생성하기 때문에 사용했습니다. 레플리케이션 컨트롤러는 조금 이따 설명하겠습니다. 백그라운드에서 무슨 일이 일어나고 있는 것일까?일단 지금 무슨 일이 일어난 것인지 설명해보겠습니다. 현재 도커 이미지를 푸시해서 도커 허브에 공유해놓았고 공유된 이미지를 가져와서 kubectl을 이용해 구동했습니다. kubectl 명령은 쿠버네티스의 API서버로 REST http요청을 전달하고 클러스터에 새 레플리케이션 컨트롤러 오브젝트를 생성합니다. 레플리케이션 컨트롤러는 새 파드를 만들고 스케쥴러에 의해 워커 노드 중 하나에 스케줄링을 완료합니다. kubelet은 파드가 할당된 것을 보고 이미지가 로컬에 없는 것을 확인합니다. 이미지를 받기 위해 도커에게 특정 이미지를 풀하도록 지시하고 luksa/kubia 이미지를 받은 후 도커는 컨테이너를 이미지를 이용해 생성하고 실행합니다. 과정을 살펴봤듯이, 우리가 직접 컨테이너를 직접 생성하거나 동작하지 않습니다. 파드만을 이용했습니다. 하지만 파드도 직접 만들지 않았고 쿠버네티스에서 알아서 잘 만들었습니다. kubectl run을 하면 레플리케이션 컨트롤러를 생성하고 이것이 실제 파드를 만들어 냅니다. 클러스터 외부에서 파드에 접근하기 위해 쿠버네티스에게 레플리케이션 컨트롤러에 의해 관리되는 모든 파드를 단일 서비스로 노출하도록 명령합니다. 결국 쿠버네티스의 시스템은 레플리케이션 컨트롤러, 파드, 서비스로 구성됩니다. 가장 중요한 구성요소는 파드입니다.여기서 파드는 하나의 컨테이너를 갖고 있지만, 보통 파드는 원하는 갯수의 컨테이너를 가질 수 있습니다. 컨테이너 내부에는 현재 Node.js 프로세스가 있고 포트 8080에 바인딩 되어 http요청을 기다리고 있습니다. 파드는 자체의 고유한 private ip와 host name을 갖습니다. 레플리케이션 컨트롤러를 살펴보겠습니다.레플리케이션 컨트롤러는 항상 정확히 하나의 파드 인스턴스를 실행하도록 지정합니다. 보통 파드를 복제하고 항상 실행 상태로 만듭니다. 여기서는 파드의 replicas를 지정하지 않았습니다. 따라서 파드를 하나만 생성했습니다. 어떤 장애 등의 이유로 파드가 죽어버리면 레플리케이션 컨트롤러는 지정된 파드 수를 채우기 위해 새로운 파드를 생성할 것입니다. 이제 서비스를 살펴보겠습니다.kubia-http 서비스가 필요한 이유를 알기위해 파드의 주요 특성을 더 이해해 볼 필요가 있습니다. 파드는 영원하지 않습니다. ephemeral하기 때문에 사라질 수 있습니다. 파드가 죽어버릴 수 있고 누군가 파드를 kill할 수도 있으며, 비정상 노드에서 파드가 제거될 수 있습니다. 물론 사라지면 레플리케이션 컨트롤러에 의해서 다시 새 파드로 대체됩니다. 중요한 것은 새 파드는 다른 IP주소를 할당받는 다는 것입니다. 그렇기 때문에 서비스가 필요합니다. 서비스를 이용하면, 파드의 IP주소가 바뀌는 문제와 여러 개의 단일 파드를 단일 IP와 포트의 쌍으로 노출하는 문제를 해결할 수 있습니다. 서비스가 생성되면 정적 IP를 할당받습니다. 그리고 서비스가 있는동안 변경되지 않습니다. 파드에 직접 연결해야 하지만 클라이언트는 서비스의 IP 주소를 통해 연결해야 합니다. 서비스는 어떤 파드가 어떤 IP를 받는지 상관없이 파드 중 하나로 연결해 요청을 처리합니다.서비스는 동일한 서비스를 제공하는 하나 이상의 파드 그룹의 정적인 위치를 나타냅니다. 서비스의 IP와 포트로 요청이 들어오면 그 순간 서비스에 속해 있는 파드 중 하나로 요청을 전달하게 됩니다. Pod쿠버네티스에 대해서 조금이라도 본 사람은 파드에 대해서도 들어본 적이 있을 것입니다. 파드는 노드 안에 구성된 여러개의 컨테이너를 가질 수 있는 컨테이너의 그룹입니다. 쿠버네티스는 개별 컨테이너를 직접 다루지 않고 파드를 다룹니다. 이 파드들은 kubectl로 조회할 수 있습니다. 1$ kubectl get pods 혹시 파드의 상태가 Pending상태라면 파드의 단일 컨테이너가 준비가 되지 않은 것이다. 할당된 워커 노드가 컨테이너를 실행하기 전에 컨테이너 이미지를 다운로드하는 중이기 때문일 수도 있고, 리소스 부족일 수도 있다. 조금 기다려본 후 계속 pending으로 나온다면 리소스를 늘려주면 된다. 다운로드가 완료되고 파드의 컨테이너가 생성되면 Running으로 나온다. 파드에 대해서는 간단히 설명하고 나중에 따로 깊게 다뤄 보도록 하겠습니다. Summary.쿠버네티스에 대해서 공부한 내용을 간단하게 요약해 보겠습니다. 모놀리스 어플리케이션은 처음에 구축하기 쉽지만 가면 갈수록 유지 관리하기가 힘들고 확장이 불가능해 집니다. 마이크로서비스 기반 어플리케이션 아키텍쳐는 각 구성 요소의 개발을 요이하게 하지만 하나의 시스템으로 작동하도록 배포하고 구성하기 어렵습니다. 쿠버네티스는 전체 데이터 센터를 어플리케이션 실행을 위한 컴퓨팅 리소스로 제공하게 합니다. 쿠버네티스를 통해서 개발자는 시스템 관리자의 도움 없이도 어플리케이션을 배포할 수 있게 됩니다. 쿠버네티스를 통해서 시스템 관리자는 쿠버네티스가 고장 난 노드가 자동으로 처리되게 함으로써 더 편하게 잠을 잘 수 있게 됩니다. 쿠버네티스는 마스터 노드와 워커 노드로 이루어져있습니다. 어플리케이션을 컨테이너 이미지로 패키징하고 원격에 푸시해 누구나 사용하게 할 수 있습니다. 쿠버네티스는 파드를 이용해 어플리케이션을 배포합니다. 여러 내용을 전달하려고 하다보니 글이 장황해진 것 같습니다. 다음 글에서는 파드에 대해서 더 깊게 알아보고 네임스페이스로 파드를 겹치지 않는 그룹으로 나누는 방법을 살펴보겠습니다.","link":"/2020/07/30/k8s-genesis/"},{"title":"글또  5기 다짐하기","text":"글또 5기를 시작하며, 다짐하기. 글또 5기를 시작하며막상 이 글을 적으려니 어떻게 시작해야 감이 잡히지 않았다. 그래서 지난 기수에 썼던 내 다짐글을 확인했다. 글또 4기 다짐글지난 번 글을 보니 꽤 들뜬 마음에 글을 작성한 느낌이 든다. 생각해보니, 9개월 전에 비해 요즘은 들뜰 일이 많이 없는 것 같다. 시간이 지나면서 무뎌지고 회사생활에 견디는 것으로 세월을 보내는 느낌이다. 글또 4기가 끝난 후에 한동안 글도 쓰지 않고 뒹굴 뒹굴 거리며 열심히 쳇바퀴 도는 삶을 지속했었다. 최근들어 이렇게 의미없이 살고 싶지 않다고 다짐했는데, 마침 글또 5기가 시작됐고 이전에 비해 좀 더 나은 글을 쓰면서 하루 하루를 다시 기록해봐야겠다고 마음먹었다. 지나고보니, 그렇게 지나간 시간이 너무나 아까웠기 때문이다. 어떤 글을 쓸까, 혹은 어떻게 성장할까개발 관련 글은 다른 글과는 다르게 내가 경험하거나 공부한 내용들이 주이기 때문에, 글또 활동을 하며 어떤 글을 쓸지 결정하는 것은 어떻게 성장할지, 방향을 결정하는 것과 같다고 생각한다. 4기에서 썼었던 글을 쭉 보면, 논문 리뷰, pyspark, airflow 등 데이터 사이언스와 엔지니어링 그 사이인 것 같다. 사실 업무적으로는 추천시스템이나 모델링 관련 일을 더 많이 했는데, 새로 배운 개념들이 주로 엔지니어링 파트라 새로 얻은 내용을 주로 글로 작성했던 것 같다. (근데 지나고 보니 생각보다 글을 안쓴 것 같아서 민망하다.) 이번 5기에서도 엔지니어링 글들과 인프라 관련 글들을 작성해 나갈 것 같다. 회사에서의 담당 업무가 데이터 엔지니어링 쪽으로 옮겨졌기 때문이다. 옮겨졌다고 하니 강제적으로 배치된 느낌이긴 하지만, 사실 그렇지는 않고 엔지니어링 업무에 흥미가 생겼다. 그래서 거의 자원하다시피 된 것이고 관련 업무를 맡게 되었다. 엔지니어링 업무를 통해 실제로 서비스가 어떻게 돌아가고 데이터를 어떻게 흘려보내고 관리해야 하는 지에 알게 되어 재미를 느끼고 있다. 목표하는 바는 데이터 엔지니어링에 더 나아가 머신러닝 엔지니어링이지만, 일단 바닥을 잘 닦아볼 생각이다. 글쓰기 외에, 내년에는글또 5기는 약 6개월 정도 진행될 예정이다. 그러니까 내년 5월까지인데, 연말도 되었고 앞으로 어떤 걸 해볼까에 대해서 처음으로 생각을 해보게 되었다. 사실 하고 싶은 건 많다. 예전부터 피아노를 배워보고 싶었고, 영상을 직접 찍고 편집해보고 싶기도 했다. 하지만 공부하고 운동하고 나면, 쉬느라 정신이 없었던 것 같다. 실제로 정신을 쓸 여유가 없었기도 했고, 연말 쯤 되니 아홉수 기운이 몰려와 안좋은 변화들을 마주하게 됐다. 많이 괴롭고 힘든 시기였지만, 어떻게 하겠는가? 이미 맞닥뜨린 상황에 좌절만 하고 있을 수는 없다. 최근에 MMA에 조금 빠지게 되면서 알게된 말이 있는데, 맥스 할러웨이의 “It is what it is”란 말이다. 맥스는 챔피언전에서 아주 훌륭하게 싸우고 판정에서 패배한 일이 있다. SNS에서도 많은 팬들과 선수들이 말도 안되는 일이라는 게시물을 남겼다. 하지만 맥스는 “It is what it was” 라며 패배를 받아들였고 파이터로서 바뀐 것은 없다고 말했다. 그래서 나도 좌절하지 않고 나아가기로 했다. 앞서 말한 것처럼 좌절하면서 보내버린 시간이 너무나 아깝기 때문이었다. 패배감에 빠져 내가 할 일을 못하게 되는 게 더 비참해 보였다. 그래서 올해의 마지막과 내년은 더 튼튼하게 살 예정이고 주변 사람들과 직장 동료들을 잘 챙기면서 앞으로 쭉 가볼 생각이다. 혼자만 잘 살기 보다는 동료들과 같이 성장하는 것을 더 선호하기 때문에, 같이 도와주고 끌어주면서 발전해 나가고 싶다. 글이 조금 다른 쪽으로 많이 갔는데, 다시 돌아와서 내년에 내가 하고 싶은 것은 다음과 같다. 운동 꾸준히 하기, 주 3회 이상 쇠질 목표는 3대 350 달리기 목표는 5km 25분 안에 들어오기 학습 데이터 파이프라인 흐름 완벽하게 파악하기 사용하고 있는 스택의 기능 제대로 파악하기 Kubernetes kubeflow나 airflow에 적용하기 실시간 처리 모델 서비스 하기 데이터 엔지니어링 흐름이 다 보인다면 논문 보기 수학 공부 다시하기 삶 기술 분야 외에 다른 책, 두 달에 한 권은 읽기 핸드폰 사용시간 줄이기 자투리 시간이나 이동시간 활용하기 소중한 사람들을 잘 살피기 대략적인, 글로 공개할 수 있는 목표는 이 정도인 것 같다. 언제 다 이룰 수 있을까 했지만, 꾸준히 하다보면 이루어지겠지. 이번 6개월도 열심히 살아보자!","link":"/2020/11/15/geultto5/"},{"title":"데이터 사이언스를 위한 네트워크 Part 1","text":"막상 공부하기는 귀찮은 네트워크에 대해 공부해보자 데이터 사이언스를 위한 네트워크 Part 1왜 네트워크를 공부하게 되었나?데이터 분야의 많은 분들이 자주 접하시지만 잘 느끼지 못하는 부분 중에 하나가 네트워크라고 생각합니다. 저만 하더라도 데이터 엔지니어링 업무를 맡기 전까지는 데이터가 어떻게 전달되고 받는 것인지 몰랐고, 알 필요도 없다고 생각했습니다. 하지만 업무를 진행하면 할 수록, 특히 AWS를 자주 접하게 될 수록 벽에 부딪히는 느낌이 들었습니다. 두 가지 벽이었는데 하나는 컴퓨터가 어떻게 작동하는지 모른다는 점이었고(CS, OS영역) 다른 하나는 네트워크에 대한 것이었습니다. 이 한계를 극복하고자 공부를 시작했고 이를 통해 조금이나마 넓어진 시야를 얻게되어 제가 갖게된 지식의 일부분을 잘 정리해보려고 합니다. Part 1에서는 전반적인 개요와 이론적인 설명 -OSI 7 Layer에 대해서 설명할 예정이며, Part2에서는 AWS를 사용할 때 자주 등장하는 용어들에 대한 정리와 설명을 할 예정입니다. 네트워크네트워크는 소셜이라는 말과 같이 있을 때 훨씬 더 친숙하게 느껴지는 것 같습니다. ‘소셜 네트워크 서비스’란 사용자 간의 자유로운 의사소통과 정보 공유, 그리고 인맥 확대 등을 통해 사회적 관계를 생성하고 강화해주는 온라인 플랫폼을 의미합니다. 이 SNS에서 가장 중요한 부분은 이 서비스를 통해 사회적 관계망을 형성하는 것입니다.(출처: 위키백과) 결국 네트워크는 어떤 망이라는 것을 뜻합니다. 소셜 네트워크는 사람 간의 사회적인 관계망을 의미하고, 우리가 파고들 네트워크는 인터넷 망, 그리고 컴퓨터 연결 망을 뜻하는 것입니다. 위키백과에서는 네트워크를 다음과 같이 설명합니다. “컴퓨터 네트워크 또는 컴퓨터망은 노드들이 자원을 공유할 수 있게 하는 디지털 전기통신망의 하나. 즉, 분산되어 있는 컴퓨터를 통신망으로 연결한 것을 말한다. 컴퓨터 네트워크에서 컴퓨팅 장치들은 노드 간 연결을 사용하여 서로에게 데이터를 교환한다.” 네트워크의 구조는 집단 크기와 사용자의 대역폭에 따라 달라집니다. 사람이 얼마나 많이 쓰느냐에 따라 구조를 바꿔야 되는 것입니다. 업종에 따라서도 달라질 수 있습니다. 서비스의 중요도에 따라서 달라지는 것인데, 예를 들어 공공기관과 게임 업종의 네트워크의 구조는 그 서비스의 중요도에 따라서 차이가 나게 됩니다. 네트워크의 통신은 서버-클라이언트 방식, 그리고 익숙한 P2P(Peer to Peer)가 있습니다. 네트워크를 구성하는 형태는 다양하며 대표적으로 star, ring, mesh, bus, tree, redundancy가 있습니다. 소개만 하고 넘어가겠습니다. 네트워크는 집단의 크기나 서비스의 중요도나 대역폭에 따라 달라진다고 앞서 말씀 드렸습니다. 이에 따라 크게 세 종류로 나눠볼 수 있습니다. 홈 네트워크, 기업용 네트워크, 그리고 클라우드 네트워크 입니다. 네트워크의 종류에 따라 인터넷이 거치게 되는 경로가 달라지게 되는데 간단하게 요약하자면 다음과 같습니다. 홈 네트워크 인터넷→ISP(회선 업체)→모뎀(모뎀에는 한 대의 컴퓨터만 연결)→공유기(여러대 사용가능)→컴퓨터 기업용 네트워크 ISP → 전용선 → 라우터 → 방화벽 → L3 백본 → L2 스위치 → 서버,컴퓨터 → L4 로드밸런서 → DMZ(외부 공개) 클라우드 네트워크(AWS) 인터넷 → Route53(DNS 서비스) → IGW → VPC → ELB → Auto Scaling → Security Group → EC2 홈 네트워크의 경로만 눈에 익숙하고 다른 경로들에서는 낯선 용어들이 많이 보일 것으로 생각됩니다. 그나마 클라우드 네트워크에서 조금 다뤄본 용어들이 등장하는 것 같습니다. 이제 본격적으로 네트워크를 다뤄보면서 용어들에 익숙해져 보도록 하겠습니다. OSI 7 Layer(Open Systems Interconnection)네트워크에 대해서 조금 맛 만이라도 본 분들은 OSI 7 Layer를 들어보셨을 것입니다. 네트워크 하면 바로 나오는 게 사실 OSI 7 Layer입니다. 하지만 누군가가 갑자기 와서 Layer 4가 뭐지 라고 물어보거나 OSI 7 Layer 자체가 무엇이냐고 물어보면 당황하는 표정을 숨기며 애써 뭐였지 뭐였지 배웠는데, 기억하는 척을 하는 자신을 만나게 될 것입니다. 이제 OSI 7 Layer에 대해 알아보면서 데이터의 흐름이 어떻게 되는 건지 파악하면서 자연스럽게 머리에 남겨 보도록 합시다. OSI 7 Layer는 개방형 시스템으로, 네트워크 프로토콜과 통신을 7계층으로 표현한 것입니다. 프로토콜을 기능 별로 나누고 계층 별로 구분을 하기 위해서 만들어 졌습니다. 이렇게 나눈 이유는 벤더 간에 호환성을 위한 표준이 필요했기 때문입니다. 이런 체계가 잡힌 이후에 표준에 의한 용이성과 쉬운 접근성, 그리고 유지관리의 수월성을 통해 기술의 발전이 일어나게 되었고 지금처럼 네트워크의 트래픽 흐름을 살펴볼 수 있게 되었습니다. 자잘한 역사는 넘어가도록 하겠습니다. OSI 7 Layers의 구조 아래로 갈수록 기계가 받아들이기 쉬운 언어이며 위로 갈수록 사람이 보기 편한 언어입니다. 즉, 전기적 신호가 인간이 이해하는 언어로 바뀌어 가는 과정이라고 보면 이해가 되실 겁니다. 이 레이어를 거치면서 어떻게 전기 신호가 인간에게 도달하는 지 살펴 보겠습니다. 간단하게 살펴보고 깊게 들어가보겠습니다. 가장 밑단의 Layer는 1단계부터 시작합니다. 1단계에서부터 시작하고 7단계로 올라가서 우리가 볼수있는, 인간에게 친숙한 변환된 정보를 마주하게 됩니다. 이제, 인터넷 선을 컴퓨터에 꽂겠습니다. Layer 1Physical. 물리 계층으로서 네트워크 하드웨어 전송 기술들로 구성되어 있습니다. 장치(컴퓨터)와 통신 매체 사이의 비정형 데이터의 전송을 담당하며 하드웨어 부분에 해당됩니다. 디지털 bit인 0,1을 전기 무선 또는 광 신호로 변환하고, 전송되는 방법과 제어 신호, 기계적 속성 등을 정의합니다. 데이터 전달의 역할만을 합니다. 대표적인 장비로는 케이블, 인터페이스(110V 220V), 허브, 리피터 등이 있습니다. Layer 2Data Link. 이더넷, 랜카드, MAC 통신을 담당하며 에러를 검출하고 재전송하는 역할을 합니다. 동일 네트워크 내에서 데이터를 전송하고 링크를 통해서 연결을 설정하고 관리하고 물리계층에서 발생할 수 있는 오류를 감지하고 수정합니다. 두 장치 간의 신뢰성 있는 정보 전송을 담당합니다. 전송 단위는 Frame이며 데이터 링크 계층은 MAC과 LLC로 이루어져 있습니다. 캡슐화하는 프레이밍과 흐름제어, 에러제어를 담당합니다. MAC(Media Access Control) 장비들이 통신하기 위한 각각의 일련번호 LLC(Logical Link Control) 모뎀, 스위치, 브릿지가 대표적인 장비입니다. Layer 3Network. IP통신과 라우팅을 담당합니다. 다른 네트워크로 데이터를 전송하고 IP 주소로 통신 출발지 IP에서 목적지 IP로 데이터 통신 시 중간에서 어떻게 중계할 것 인가를 정합니다. 전송단위는 패킷이고 목적지까지 경로 설정을 합니다. 처리 데이터가 큰 경우에는 분할(패킷화)하고 전송 후에 목적지에서 재 조립하여 붙여진 sequence 넘버대로 메시지를 구현합니다. 대표적인 장비로는 L3 스위치, 라우터가 있습니다. Layer 4Transport. TCP / UDP 통신 후 어떤 서비스를 할지 결정합니다. 호스트 간의 데이터(서비스) 전송을 포트로 정의해주는 레이어로 오류 복구 및 흐름 제어, 완벽한 데이터 전송을 보장합니다. TCP/UDP 방식이 존재하고 전송단위는 세그먼트이며 이 레이어 부터 소프트웨어 레벨에 해당합니다. 포트를 제어한다는 의미로 L4 로드밸런서가 있습니다. L4스위치는 3계층에서 온 트래픽을 분석하여 서비스 종류를 구분해 줍니다. 로드밸런서의 역할?EC2에서 로드 밸런서를 보셨을 겁니다. L4장비인 로드밸런서의 역할을 거의 그대로 하고 있다고 생각하면 됩니다. 로드밸런서는 날라오는 요청들을 받아서 잘 나누어 주는 역할을 합니다. 로드밸런서가 없다면, 요청이 한 곳으로 몰려서 비효율적으로 작업을 처리하게 되거나 심할 경우, 서버에 장애가 생겨 안정적인 서비스를 하기 힘들어집니다. Layer 5Session. TCP/IP 통신 연결을 수립 유지 중단하는 역할을 합니다. 로컬 및 원격 애플리케이션 간의 IP/ Port 연결을 관리합니다. Session Table에 구성되어 있으며 데이터는 테이블에 적혀있는 IP/Port로 연결됩니다. 1netstat -an #명령어로 Session Table 확인가능 Layer 6Presentation. 인코딩, 암호화 압축을 담당하고 데이터를 사람이 이해하도록 인코딩합니다. 사용자 프로그램과 네트워크 형식간에 데이터를 변환해 표현과 독립성을 제공합니다. 인코딩, 디코딩, 암호화, 압축이 이 레이어에 속하게 됩니다. ASCII, JPG, MPEG 등의 번역을 합니다. Layer 7Application. 다양한 응용 서비스 계층입니다. HTTP, SMTP, 모바일 서비스 등이 여기에 해당됩니다. 응용 프로세스 간의 정보 교환을 담당하며, 사용자와 가장 밀접한 소프트웨어 단입니다. 이메일 서비스(SMTP), 파일전송(FTP) 근데 데이터를 어떻게 전송하고 받는거지…OSI 7 Layer를 공부해도 데이터가 어떻게 왔다갔다 하는지 잘 모르겠습니다. 이제 이론을 배웠으니 흐름이 어떻게 되는 건지 알아보도록 합시다. 데이터는 인캡슐레이션과 디캡슐레이션을 통해서 Layer 위로 올라가기도 하고 내려가기도 합니다. 인캡슐레이션은 쉽게 말해서 헤더를 붙이는 것을 말하고, 디캡슐레이션이란 헤더를 떼는 것을 말합니다. 인캡슐레이션과 디캡슐레이션이 왜 필요한데요? 라고 물어보실 수 있습니다. 왜냐면 데이터 자체로는 상대방에게 전달이 되지 않기 때문입니다. 우리가 어떤 물건을 상대방에게 보내려고 할때 택배회사를 이용하는 것과 비슷하다고 보면 이해가 되실 겁니다. 물론 상대방에게 물건을 갖다가 주거나 앞에서 매몰차게 던지는 경우도 있긴 하지만, 품격있는 사회인인 우리는 택배회사를 이용한다고 가정해보겠습니다. 우리가 물건을 받으면 물건은 포장에 아름답게 쌓여져 있습니다. 우리는 소중한 존재이기에 예쁜 포장지로 쌓여져 있다고 생각해보겠습니다. 겉 리본을 풀고 예쁜 포장지를 풀고, 뾱뾱이를 제거하고 종이포장을 제거하고 … 해서 우리가 원하는 물건을 받게 됩니다. 데이터 역시 이와 같습니다. 데이터 또한 헤더로 쌓여져 있는 포장지를 디캡슐레이션 해나가서 최종단계인 Application 레이어에 도달해 우리가 이해할 수 있게 되어 있는 데이터를 마주하게 되는 것입니다. 다른 상대방에게 보낼 때는? 똑같이 역으로 예쁘게 포장을 해서 상대방에게 보내면 됩니다. 우리가 상대방에게 데이터를 전송한다고 해봅시다. 흐름은 다음 같습니다. 데이터(인코딩) → 세그먼트(Layer4) → 패킷(Layer3) → 프레임(Layer2) → 비트(Layer1) 인간이 이해하기 쉬운 형태로 되어있는 데이터가 기계가 이해할 수 있게 인코딩되고 헤더를 붙여나가면서, 최종적으로는 비트로 표현되어 다른 곳으로 보내어질 준비를 마치게 됩니다. OSI 7 Layer 요약 Layer PUD Protocol Device Function 7 (Application) Data HTTP, FTP, DNS, DHCP, SMTP, NFS, RTSP 사용자가 네트워크에 접근할 수 있도록 해주는 계층이다.사용자 인터페이스, 전자우편, 데이터베이스 관리 등 서비스를 제공한다. 6 (Presentation) Data JPEG, MPEG, SMB, AFP 데이터를 하나의 표현 형태로 변환한다.필요한 번역을 수행하여 두 장치가 일과되게 전송 데이터를 이해할 수 있도록 한다. 5 (Session) Data(Payload) SSH, TLS, ISO8327, Apple talk, NetBIOS 통신 세션을 구성하는 계층으로 포트(port) 연결이라고도 할 수 있다. 통신 장치 간의 상호 작용을 설정하고 유지하며 동기화한다. 사용자 간의 포트 연결(세션) 이 유효한지 확인하고 설정한다. 4 (Transport) Segments TCP, UDP, RTP, SCTP, SPX Gateway End to End Commnication전체메시지를 발신지 대 목적지 간 제어와 에러를 관리한다. 패킷들의 전송이 유효한지 확인하고 실패한 패킷은 다시 보내는 등 신뢰성 있는 통신을 보장하며 머리 말에는 Segment 가 포함된다. 3 (Network) Packets IP, ARP, ICMP, IGMP, RIP, IPX, DDP,OSPF, IS-IS, BGP Router 다중 네트워크 링크에서 패킷을 발신지로부터 목적지로 최단 거리로 전달할 책임을 갖는다. 각 패킷이 시작지점에서 최종목적지까지 성공적으로 전달 되도록 한다. 2 (Data Link) Frames Ethernet, PPP, Wireless Len Bridge, Switch 오류없이 한 장치에서 다른 장치로 프레임(비트의 모음)을 전달하는 역할로 스위치 같은 장비의 경우 MAC 주소를 이용하여 정확한 장치로 정보를 전달한다. 1 (Physical) Bits Ethernet, RS-232C,Modem Hub, Reafiter 물리적 매체를 통해 비트 흐름을 전송하기 위해 요구되는 기능들을 조정한다.케이블, 연결 장치 등과 같은 기본적인 물리적 연결기의 전기적 명세를 정하고네트워크의 두 노드를 물리적으로 연결시켜준다. 출처 : https://kin3303.tistory.com/247 Referencehttps://ko.wikipedia.org/wiki/OSI_모형 http://blog.naver.com/PostView.nhn?blogId=pst8627&amp;logNo=221670903384 https://jhnyang.tistory.com/194 https://kin3303.tistory.com/247","link":"/2020/11/29/network-part1/"},{"title":"데이터 사이언스를 위한 네트워크 Part 2","text":"Part 1에서 다룬 이론을 바탕으로 AWS 화면과 친숙해지자 데이터 사이언스를 위한 네트워크 Part 2이번 글에서는 AWS를 다루면서 보는 익숙하지만 생소한 용어들에서 다루려고 합니다. 자주봐서 익숙하기는 하지만 어떤 역할을 하는지, 어떤 개념인지는 잘 모르고 넘어가는 경우가 많습니다. 경험 상 이러한 개념적인 부분이 제대로 다져지지 않으면 업무를 할때마다 찾아보게 되서 생산성이 낮아지곤 했습니다. 이번 글을 통해 개념을 제대로 다지는 계기가 되었으면 좋겠습니다. AWS? GCP는 뭐에요?AWS란? 이라고 검색했을때 나오는 결과들은 대체로 ‘클라우드 컴퓨팅 플랫폼’이라고 설명하고 있습니다. 클라우드 컴퓨팅 플랫폼 서비스라고 하면 Amazon같은 거대한 회사에 있는 거대한 서버를 비용을 지불하고 내가 쓸 만큼만 사용하는 서비스라고 생각하실 겁니다. 맞습니다. 하지만 한 발자국 더 들어가서 생각해보면, ‘내가 쓸 만큼? 어떻게 내가 쓰는 부분을 따로 나눌 수 있는거지?’라는 의문이 생길 수 있습니다. 실제로 사용하면서도 어떻게 내 것만 잘 분리가 되는지 의문이 드실 수 있습니다. VPC(Virtual Private Cloud)AWS는 VPC를 이용해서 다른 사용자와 나를 분리시킵니다. 설명에 들어가기 전에 먼저 VPN에서 다뤄보겠습니다. VPN은 Virtual Private Network으로 실제로는 같은 네트워크 망에 있지만, 논리적으로 다른 네트워크인 것처럼 동작하는 것을 말합니다. VPC역시 같은 클라우드지만, 논리적으로 구역을 나눠놓은 것입니다. VPC를 사용하게 되면 위 그림에서 아래 그림으로 구조가 변경됩니다. 복잡하던 시스템이 간단하게 바뀌었고 각각의 VPC는 완전히 독립된 네트워크처럼 작동할 수 있게 됩니다. VPC를 구축하기 위해서는 사설 아이피, Private IP 대역에 맞춰야 합니다. 일반적인 IP는 Public IP로, 외부에서 사용하는, 한 곳에서만 사용할 수 있는 아이피입니다. 어떤 사람이 IP를 점유했다면, 그 IP는 다른 사람이 사용할 수 없습니다. Private IP는 내부 사용자들끼리 사용하는 IP입니다. 따라서 192.168.0.53이라는 IP는 누군가의 집 컴퓨터에서 사용하고 있는 Private IP일 수 있습니다. 결론적으로 클라우드 서비스 시스템은 어떻게 내가 사용할 클라우드를 나누냐면, 이 Private IP 범위를 이용해서 구역을 나누어 관리하는 것입니다. Public IP와 Private IP가 쉽게 다가오지 않을 수 있을 것 같습니다. 비유를 해보자면, 서울시 강남구 XX아파트 1302동 704호는 Public IP라고 할 수 있고 그 집 내부의 안방은 Private IP라고 할 수 있습니다. 아파트의 주소는 세상에 딱 하나이지만, 안방은 어떤 집에서도 갖고 있는 곳이기 때문입니다. 이렇게 한번 설정된 아이피 대역은 수정이 불가하며 각 VPC는 하나의 리전에 종속됩니다. 각각의 VPC는 완전히 독립적이며, 만약 VPC간 통신을 원한다면 VPC 피어링 서비스를 고려해볼 수 있습니다. 서브넷 이제 서브넷을 구성할 수 있습니다. 처음 AWS EC2를 만들 때 어렵고 난감했던 개념이 서브넷이였습니다. 서브넷은 그림에서 보듯이 VPC를 더 잘게 쪼갠 것입니다. 목적에 따라서 퍼블릭 또는 프라이빗으로 설정할 수 있습니다. 퍼블릭은 누구나 들어올 수 있도록 설정한 것이고, 프라이빗은 특정 대상만 들어오는 것을 허용한다는 것입니다. 누구나 들어올 수 있다는 것은 인터넷 통신이 자유롭다는 말입니다. 보통 회사에서는 보안 때문에 프라이빗 서브넷을 구성해놓고 특정 대역을 열어 서비스하는 경우가 많습니다. 그런데 만약 프라이빗 서브넷의 EC2 인스턴스에서 텐서플로를 사용할 일이 있어 pip install tensorflow 를 한다면 어떻게 될까요? 프라이빗 서브넷은 인터넷 통신이 자유롭지 않기 때문에 별도의 설정이 없다면 설치가 되지 않을 것입니다. NAT그래서 NAT 게이트웨이가 필요합니다. NAT 게이트 웨이는 프라이빗 서브넷이 인터넷과 통신하기 위한 아웃바운드 인스턴스입니다. 프라이빗 네트워크가 외부에서 요청되는 인바운드는 필요 없더라도 인스턴스의 펌웨어나 혹은 주기적인 업데이트가 필요하여 아웃바운드 트래픽만 허용되야 할 경우가 있습니다. 이때 퍼블릭 서브넷 상에서 동작하는 NAT 게이트웨이는 프라이빗 서브넷에서 외부로 요청하는 아웃바운드 트래픽을 받아 인터넷 게이트웨이와 연결합니다. 이렇게 되면 프라이빗 서브넷 안에서도 자유롭게 텐서플로를 설치할 수 있습니다. 보안그룹위에 개념을 숙지하고 호기롭게 AWS 클라우드를 이용하려고 하면 두 번째로 마주치는 벽이 바로 보안그룹입니다. 일종의 방화벽으로 쉽게 말하자면, 허용할 IP대역과 Port를 지정하는 것입니다. 관리하는 부분은 인바운드 트래픽과 아웃바운드 트래픽입니다. 보통 아웃바운드 트래픽은 제한을 걸어두지 않습니다. 네트워크가 나가는 것이기 때문에 크게 신경을 쓰지 않는 경우가 대부분입니다. 중요한 것은 들어오는, 인바운드 트래픽입니다. 회사나 집에 아무나 들어올 수 없는 것처럼, 클라우드에서도 보안그룹을 이용해서 들어올 인터넷 통신을 관리합니다. 방화벽 역할을 하는 것에는 사실 네트워크 ACL도 있지만 생략하도록 하겠습니다. 만약 보안정책인 네트워크 ACL과 보안그룹이 충돌한다면 보안그룹이 더 높은 우선순위를 갖습니다. Port &amp; Protocol보안그룹 설정을 실제로 하려고 들어가면 설정할 것이 Port와 Protocol입니다. 프로토콜을 먼저 선택하게 되는데, 실제로 클릭하게 되면 각종 알수없는 영어들의 리스트가 주르륵 흘러나옵니다. 대표적인 몇 가지만 알아보도록 하겠습니다. 프로토콜 유형에서 모든 트래픽을 기준으로 설명을 할 수 있을 것 같습니다. 모든 트래픽의 윗 부분은 TCP/UDP에 관한 내용이고 그 아랫부분은 well-known 포트로 많이 사용하는 포트에 대해서 미리 지정해 놓은 것입니다. 보통 well-known 포트는 0~1023의 범위입니다. TCP / UDPTCP와 UDP는 OCI 7 Layer에서 4번째 Layer인 Transport Layer에 해당됩니다. 이 레이어의 역할은 저번 글에서도 다뤘듯이, End to End 서비스 , 커넥션(연결)을 관리하고 서비스와 서비스의 연결을 관리하며 TCP UDP 소켓을 통한 프로세스 별 통신을 하게 됩니다. 이 레이어에서는 port to port로 통신, 데이터는 segment라고 불리우고 있습니다. TCP와 UDP는 성격이 조금 다릅니다. TCP는 통신에서 정확성(신뢰성 있는 전송기능)을 중시한다면 UDP는 속도를 더 중시합니다. TCP 프로토콜은 언제 사용할까요? 은행 서비스를 생각해 봅시다. 빠른 속도를 위해 신뢰성은 신경쓰지않고 은행 서비스를 구축했고 가족에게 1억원을 송금했습니다. 그런데 처리되는 도중에 에러가 발생해 1만원을 보낸 것이라고 처리가 되었습니다. 재앙이 발생했습니다. TCP는 이렇듯 신뢰성있는 데이터 전송(RDT, Reliable Data Transfer)이 필요할 때 사용하게 됩니다. 그 외에 연결 제어, 흐름 제어, 혼잡 제어 기능을 수행할 수 있습니다. UDP는 TCP에 비해 간단합니다. TCP에서 제공하는 기능을 제공하지 않으면 UDP기 때문입니다. 기능이 적어졌으므로 굉장히 가벼워 Overhead가 매우 적습니다. 따라서 많은 요청에 대해서 처리해야 하고, 신뢰성이 필요하지 않은 서비스에서 유용하게 사용됩니다. 혹시 UDP가 익숙하지 않으신가요? 학교 컴퓨터실에서 스타크래프트를 할 때 방을 만들때면 꼭, UDP를 사용했었던 것이 어렴풋이 기억이 나실 수도 있겠습니다. 이렇듯, 온라인 게임이나 DNS, 음성 인터넷 프로토콜, 동영상 서비스 등에서 UDP 프로토콜을 사용하고 있습니다. Referencehttps://medium.com/harrythegreat/aws-가장쉽게-vpc-개념잡기-71eef95a7098 https://brunch.co.kr/@toughrogrammer/15","link":"/2020/11/29/network-part2/"},{"title":"Airflow EC2에 구축하기","text":"Airflow를 AWS EC2에서 실행시켜 보자! AWS EC2 위에 Airflow 구축하기Airflow를 로컬에 구축할 수도 있지만 안정성을 위해서라면 꺼지지 않는 컴퓨터에 Airflow를 구축하는 것이 좋을 것입니다. 이번 글에서는 실제 서비스를 위해서 Airflow를 EC2 위에 구축하는 내용을 담아보겠습니다. Airflow에 대한 기본 개념이 궁금하신 분들은 Airflow Basic 을 확인하시면 좋습니다. 준비물Airflow를 구축하기 위해서는 준비물이 필요합니다. 첫번째로는 EC2 서버입니다. 인스턴스 사이즈가 작아도 크게 상관은 없겠지만 여유롭게 t3.large를 선택하도록 하겠습니다. 인스턴스 설정이 끝나면 인스턴스를 생성하고 바로 인스턴스 안으로 들어가 보도록 합니다. ssh 명령어를 이용해서 인스턴스 주소를 입력해 접속합니다. 접속이 안된다면 보안그룹에서 22번 포트를 열어줍시다. 이제 본격적으로 Airflow를 구축해 볼 것입니다. 구축 시에 확인해야 할 체크리스트를 만들어 둡니다. Airflow 기본 세팅 Redis MySQL 기타 라이브러리 설치 AMI 이미지 생성하기 마지막에 이미지를 생성하는 이유는 이 작업을 반복하려면 너무 귀찮고 힘들기 때문입니다. 완벽히 구축이 되고 잘 돌아가는 Airflow 이미지를 생성해 두면 나중에 활용할 일이 많습니다. Airflow를 클러스터화 한다던가 그런 곳에 이미지를 사용하면 쉽게 구축할 수 있습니다. 레크리스트에는 Redis와 MySQL이 포함되어있습니다. 기본 세팅으로 실행해도 상관없지만 기본 세팅의 Executor는 Sequential Executor로 병렬로 Task를 수행할 수밖에 없습니다. 실제 서비스에는 많은 Task가 동시에 실행해야 할 경우가 자주 발생하므로 병렬 처리가 가능한 Celery Executor를 사용할 것이고 이 Executor는 메시지 브로커를 사용합니다. 메시지 브로커에는 RabbitMQ나 Redis 등이 사용되는데 이번 글에서는 Redis를 사용해 보겠습니다. 기본 세팅에 대한 의문점Redis vs RabbitMQ?Redis는 NoSQL DB로 잘 알려져 있습니다. In Memeory 방식이며 key-value데이터 구조 스토어 이기 때문에 빠른 Read, Write 성능을 보장합니다. RabbitMQ는 DB보다는 메세징 브로커로 잘 알려져 있습니다. 메시지의 우선순위를 지원하며 크고 복잡한 메시지를 다룰때 적합합니다. Airflow의 브로커로 어떤걸 사용할지는 현재 서비스할 비즈니스 프로세스에 따라 판단하면 됩니다. 제가 구축할 서비스의 비즈니스 프로세스에는 복잡한 메시지를 다루지는 않습니다. 제가 판단하기에 이 서비스에는 속도가 더 중요하다고 생각했습니다. 그래서 In Memory 방식의 Redis은 성능이 보장되기 때문에 Redis로 선택을 했습니다. Airflow를 구축하시는 분들도 무작정 따라하기보다는 비즈니스 프로세스를 생각해보시고 알맞는 로직에 따라 어떤걸 사용할지 선택하시면 좋을 것 같습니다. 참고! Celery란?Celery 는 Python 으로 작성된 분산 메시지 전달을 기반으로 한 비동기 작업 큐로, Worker 의 한 종류입니다.별도로 실행 중인 Worker Process가 Broker로부터 Message를 전달 받아 작업을 대신 수행해 주는 라이브러리입니다.출처 : https://velog.io/@jisoo1170/Redis-RabbitMQ-%EC%B0%A8%EC%9D%B4%EC%A0%90%EC%9D%84-%EC%95%8C%EC%95%84%EB%B3%B4%EC%9E%90 Sqlite vs MySQL?또한 Airflow의 meta store는 sqlite인데 sqlite로는 Hello World 정도만 테스트할 정도의 수준이기 때문에 MySQL을 DB로 사용해 볼 것입니다. 보통 sqlite는 로컬에서 혼자 사용하는 용도이며 많은 요청을 처리하기에는 버겁습니다. 반면 MySQL은 여러 개의 작업과 사용자의 SQL을 처리할 수 있도록 구현되어 있기 때문에 실제 서비스에 sqlite보다 적합할 것입니다. 의문점이 해결되었다면 본격적으로 Airflow를 구축해보도록 하겠습니다. 1. Airflow 기본 세팅Airflow 구축 전에 반드시 해야할 것 중에 하나는 Airflow Home 경로를 설정하는 것입니다. 저의 경우에는 항상 경로 세팅을 나중에 하다가 잊어버려서 에러가 나는 경우가 많아서 꼭 먼저 설정해두곤 합니다. 출처 : https://airflow.apache.org/docs/apache-airflow/stable/start.html Airflow 공식 문서에 따르면 기본 경로는 위 사진에 나와있는 것과 같습니다. 그냥 놔둬도 되지만 나중에 경로를 옮길 때를 대비해서 한번 세팅해봅시다. 12# EC2export AIRFLOW_HOME=/home/ec2-user/airflow EC2 인스턴스라면 기본 경로를 다음과 같이 설정해 줍니다. (여기서 airflow를 바로 설치해도 되긴 하지만 pip3 install apache-airflow로 설치를 하게되면 자잘한 에러들을 만날 수 있습니다. 아래 과정을 마친 뒤 명령을 실행하면 깔끔하게 설치되니 잘 따라가보도록 합니다. ) 2. Redis 세팅이제 브로커로 사용할 Redis를 설치해 줍시다. 먼저 AWS Linux의 패키지 설치 도구인 yum을 업데이트하고 필요한 라이브러리를 설치합니다. 12$ sudo yum -y update$ sudo yum -y install gcc make 이제 Redis를 다운 받습니다. 12345$ cd /tmp$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz$ tar xzf redis-4.0.0.tar.gz$ cd redis-4.0.0$ make Redis 디렉토리를 만들고 파일을 복사합니다. 1234$ sudo mkdir /etc/redis $ sudo mkdir /var/lib/redis$ sudo cp src/redis-server src/redis-cli /usr/local/bin/$ sudo cp redis.conf /etc/redis/ Redis의 configure 파일을 수정합니다. 1$ sudo vim /etc/redis/redis.conf 1234567891011121314#/etc/redis/redis.conf[..]daemonize yes[..] [..]bind 0.0.0.0[..] [..]dir /var/lib/redis[..] logfile /var/log/redis_6379.log Redis-Server initializize 스크립트를 세팅합니다. 12$ cd /tmp$ wget https://raw.github.com/saxenap/install-redis-amazon-linux-centos/master/redis-server 12345$ sudo mv redis-server /etc/init.d$ sudo chmod 755 /etc/init.d/redis-server$ sudo vim /etc/init.d/redis-server -&gt; redis=\"/usr/local/bin/redis-server\" 확인 12$ sudo chkconfig --add redis-server$ sudo chkconfig --level 345 redis-server on 서버 실행! 12345$ sudo service redis-server start$ redis-cli ping -&gt; PONG# 강제 종료시$sudo service redis-server stop sudo service redis-server start를 해도 별 반응이 없다면 ctrl+c로 종료하고 실제로 돌아가는 프로세스가 있는지 확인합니다. 1ps -ef|grep redis 제대로 세팅이 되었다면 다음과 같이 나올 것입니다. 브로커는 구축이 되었습니다. 3. MySQL 세팅Airflow 구축을 하면서 가장 삽질도 많이 하고 시간을 많이 낭비한 부분입니다. 익숙하지 않아서인지 이상하게 MySQL을 다룰 때마다 에러핸들링을 오래 하게 되는 것 같습니다. MySQL 설치 1$ sudo yum install mysql56-server 웹에 검색하면 위와 같은 명령어가 많이 등장합니다. 예전 버전의 AWS CLI였으면 명령어가 적용되었을 것 같은데 mysql이 설치가 되지 않습니다. 아래의 명령어를 사용해서 설치를 해야합니다. 123$ sudo wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm$ sudo yum localinstall mysql57-community-release-el7-11.noarch.rpm $ sudo yum install mysql-community-server 설치가 완료되면 mysql 데몬을 실행합니다. 1systemctl start mysqld.service 이렇게 실행하라는 소리가 많은데 역시나 실행이 되지 않습니다. 왜 꼭 한번에 되게 정리해 놓은 문서는 없을까요? 답답하면 제가 정리해서 글을 올리면 됩니다. 묵-직하게! 에러가 난다면 코드를 다음과 바꿔서 쳐봅니다. 1sudo service mysqld start mysqld가 실행되었고 이제 mysql 명령어를 이용하는 게 가능해졌습니다. Airflow에서 MySQL을 DB로 사용하기 때문에 해주어야 할 작업이 남았습니다. Airflow 유저를 만들어 줘야하고 airflow database를 만들어줘야 합니다. 유저가 없다면 database에 접근이 불가능하며, airflow database가 없다면 Airflow 실행에 필요한 테이블들을 만들지 못합니다. 따라서 root유저로 접속해서 유저를 만들고 데이터베이스를 만들어줘야 합니다. MySQL 세부 세팅123$ mysql -u root -p# 패스워드를 입력Enter Password : 루트 유저로 접속하려면 비밀번호를 요구합니다. 하지만 방금 MySQL을 설치했기 때문에 비밀번호를 설정한 적이 없습니다. 그래서 그냥 엔터를 쳐봅니다. 될리가 없습니다. MySQL은 설치가 될때 root 유저에 대한 임시 비밀번호를 만들어 놓기 때문에 접속이 안되는 것입니다. 임시 비밀번호를 찾아 와야 합니다. 임시 비밀번호는 휴대폰 인증을 통해서 이메일로 받는게 정석이지만 MySQL은 조금 다릅니다. 1sudo vim /var/log/mysqld.log /var/log/mysqld.log 여기에 임시비밀번호가 있습니다. 비밀번호를 복사해 놓은 후 다시 입력해 봅니다. mysql 프롬포트가 나왔다면 성공입니다. 이제 root 유저의 비밀번호를 다시 설정해주고 airflow 유저를 생성하고 database를 만들어 주면 세팅은 끝납니다. 12UPDATE mysql.user SET Password=PASSWORD('패스워드') WHERE User='root'; 검색해서 나온 위 명령어로 비밀번호를 세팅해줍니다. 여기서 에러가 발생하는데 두 가지 에러가 발생합니다. syntax 에러 Your password does not satisfy the current policy requirements 1번 에러의 경우에는 ' 를 잘 살펴보고 모양이 맞는지 한번 잘 확인해 봅니다. 혹은 mysql 버전에 맞지 않는 명령어일 가능성이 있습니다. mysql 5.7이상의 명령어인지 확인을 다시 해봅니다. 2번 에러의 경우에는 비밀번호가 너무 쉽다는 것입니다. mysql에는 비밀번호 정책이 상 중 하로 나누어져 있는데 기본 설정은 MEDIUM입니다. 복잡한 비밀번호를 사용하기는 싫었기 때문에 저는 이것을 LOW로 변경할 것입니다. 12set global validate_password_policy='LOW';# 쿼리 성공! LOW로 설정되었다면 비밀번호는 8자 이상으로만 세팅하면 됩니다. 123alter user 'root'@'localhost' identified by '8자 이상 패스워드';use mysql;flush privileges; 이제 airflow 사용자를 만들어주고 database도 만들어줍니다. 1234567891011# 사용자 생성create user 'airflow'@'localhost' identified by '비밀번호';# DB 권한 부여$ grant all privileges on *.* to 'airflow'@'localhost';$ grant all privileges on DB이름.* to 'airflow'@'localhost';# database 생성create database airflow;flush privileges; 1sudo pip3 install 'apache-airflow[mysql]' 험난 했던 MySQL 세팅은 완료되었습니다. 4. Airflow 설치 및 세팅이제 Airflow를 설치해봅시다. 123456789$ sudo yum update -y$ sudo yum install group \"Development tools\" -y$ sudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel python3-devel.x86_64 cyrus-sasl-devel.x86_64 -y$ sudo yum install libevent-devel -y$ sudo pip3 install apache-airflow 설치가 완료되었다면 airflow 폴더애 있는 airflow.cfg를 수정해서 앞서 설치한 redis와 mysql을 airflow와 이어줘야 합니다. 하지만 아무리 찾아봐도 airflow 폴터가 보이지 않습니다. 분명히 설치를 했는데! airflow를 입력해서 airflow 명령어가 동작하는지 먼저 확인합니다. 만약 명령어가 작동한다면 아까 설정해둔 AIRFLOW_HOME경로에 airflow 폴더가 생성될 것입니다. 그 안에 configuration 파일이 있습니다. 만약 airflow: command not found에러가 발생한다면 1export PATH=$PATH:~/.local/bin 환경변수를 조정해서 airflow가 bin에서 실행되도록 합니다. airflow.cfg 설정123456789101112131415161718192021# 사용할 dag 폴더 지정# subfolder in a code repository. This path must be absolute. 꼭 절대경로!dags_folder = /home/ec2-user/airflow/dags# executor = SequentialExecutorexecutor = CeleryExecutor# sql_alchemy_conn = sqlite:////home/airflow/airflow/airflow.dbsql_alchemy_conn = mysql+pymysql://airflow:비밀번호@127.0.0.1:3306/airflow# catchup_by_default = Truecatchup_by_default = False# broker_url = sqla+mysql://airflow:airflow@127.0.0.1:3306/airflowbroker_url = redis://airflow@127.0.0.1:6379/0# result_backend = db+mysql://airflow:airflow@localhost:3306/airflowresult_backend = db+mysql://airflow:비밀번호@127.0.0.1:3306/airflow# load_examples = Trueload_examples = False 이 정도만 세팅 해줍니다. 123sudo pip3 install boto3sudo pip3 install celerysudo pip3 install redis 실행에 필요한 라이브러를 설치해주고 airflow db를 초기화 해봅시다. 1airflow initdb Done!이 나왔다면 성공입니다. Airflow 실행Airflow는 스케쥴러, 웹서버, 워커로 구성되어있습니다. 하나하나 백그라운드로 실행시켜줍니다. 12345nohup airflow webserver &gt; /dev/null 2&gt;&amp;1 &amp;nohup airflow scheduler &gt; /dev/null 2&gt;&amp;1 &amp;# queue를 설정했다면 -q를 통해 추가한다nohup airflow worker &gt; /dev/null 2&gt;&amp;1 &amp; 웹 UI가 예쁘게 뜬다면 성공입니다! example dag를 실행해보고 안된다면 백그라운드로 실행된 프로세스를 종료한 뒤 nohup명령어를 빼고 실행합니다. 로그가 나오므로 해당 에러를 모두 해결한 뒤에 백그라운드로 실행시켜 주면 완료가 됩니다. 자잘한 에러 핸들링 Exception: Global variable explicit_defaults_for_timestamp needs to be on (1) for mysql 위 에러가 등장한다면 mysql에서 timestamp 설정을 변경해 주어야 한다. root 계정으로 들어가서 아래 명령어를 실행해준다. 1SET GLOBAL explicit_defaults_for_timestamp = 1; ModuleNotFoundError: No module named 'MySQLdb' 또는 OSError: mysql_config not found 이런 에러 메세지로 등장할 수 있다. 아래 명령어로 해결한다. 123sudo yum install mysql-develpip3 install 'apache-airflow[mysql]' Referencehttps://airflow.apache.org/docs/ https://boomkim.github.io/2020/01/08/airflow-install-amazon-linux-2/ https://openmind8735.com/aws/redis/2017/07/21/aws-ec2-인스턴스에-redis-설치하기/ https://serverfault.com/questions/894457/amazon-linux-2-ami-aws-how-to-install-mysql-in-amazon-linux-2 https://lemontia.tistory.com/943 https://m.blog.naver.com/aim4u/221766568746","link":"/2020/12/13/airflow-on-ec2/"},{"title":"20201227. 2020년 회고하기","text":"약속들이 눈에 보이지 않는 캘린더를 보며, 일 년이 왜 이렇게 빠른지 생각하다가, 한 해를 잘 보냈다는 뿌듯함 보다는 허무함이 몰려왔다. 2020년 돌아보기 먼저 캘린더를 열어봤다. 올 한해 많은 일이 일어났다고 생각했다. 머리 속이 복잡해서 캘린더를 보면서 하나씩 정리하고 싶어졌다. 캘린더를 열어서 1월부터 몇 번 아래로 내려가니 벌써 2020년 12월이었다. 업무 내용이나 약속이 간단하게 적힌, 그나마도 한 달 한 달이 스크롤을 내릴수록 개인적인 일들과 약속들이 눈에 보이지 않는 캘린더를 보며, 일 년이 왜 이렇게 빠른지 생각하다가, 한 해를 잘 보냈다는 뿌듯함 보다는 허무함이 몰려왔다. 올 해에 내가 한 일들을 정리하고 새로운 계획을 세워봐야겠다고 머릿속으로 다짐했는데 마음이 먹어지지가 않는다. 새 계획은 내년에 하기로 하고, 그보다는 나에게 마음정리가 더 필요한 것 같다. 1월부터 6월은 2020년이 시작되고 나서는 굉장히 바빴었다. 작년에 연구소 조직이 개편되면서 TFT가 조직되었고 인수인계 사항으로 나온 Spark를 업무에 적용해야 했었다. 기본적인 Spark에 대한 이해부터 시작해서 Pyspark, conf 세팅 등, 하루하루 해야할 게 많았고 배우고 성장하기 바빴다. 회사에서 워크샵도 갔었고 연봉협상도 처음해보고 새로 접하는 일이 많았다. 데이터 엔지니어링에도 관심이 생겨서 세션이 있으면 주말에도 참가해서 정리를 했고, 내용이 괜찮으면 이를 정리해서 블로그에 업로드를 하던가 팀원들에게 공유를 했다. 배우고 공부하는게 재밌었던 것 같았다. 팀 내부적으로도 스터디가 계속 지속되었었고 구매확률 예측과 같은 업무 내용도 흥미로웠다. 3월 4월에는 새로운 팀원들이 들어와서 팀 규모가 커졌다. 새 팀원 중 한 분은 기존에 서비스되고 있던 모델에 관심이 많았고, 해당 분야를 다뤄본 경험도 있어 모델을 고도화해보기도 했다. 모델 고도화 측면 뿐 아니라, 새 사람들이 들어오니 새로운 시각에서 서비스를 돌아볼 수도 있었고 업무 내용에 대해 다양한 논의를 할 수 있었다. 코로나가 이 때 갑자기 심해지기 시작하면서 행동에 제약이 생기기 시작했지만, 기존 TFT 팀원들의 업무능력들이 조화를 이루면서 1년도 안되는 시간만에, 나름 데이터 팀 답게 일을 할 수 있을 것 같다고 생각했다. 6월에는 제주도 여행을 떠났다. 7월 부터 12월은 어떤 일에 제대로 마음먹기 시작할 때 항상 제동이 걸렸다. 개인적인 일이든 업무적인 일이든 제대로 하려고 하면 잘 안됐다. 뭐든 간에 사람이 가장 중요하다는 걸 제대로 느끼게 되었다. 개인적인 일은 제쳐두고 일에 대해서만 생각해보자. 하반기가 흘러가면서 퇴사자분들이 많이 나왔다. 개발팀에서 특히 많이 나왔는데, 오래 계셨던 분들이 주로 나가게 되었고, 이로 인해 회사 분위기가 어수선했었다. 팀 내부적으로라도 결속시키고 싶었고, 뭐든 새로 하는 일에 열심히 하려고 했고 새 마음가짐을 가지려고 했는데, 옆에 있는 동료가 힘들어하니 잘 안되더라. 애매한 상태로 시간이 지났고, 결국 몇 명이 팀을 떠나게 되었다. 그 중에는 오래 일을 같이 하고 믿었던 동료도 있었다. 덕분에 데이터 엔지니어링 파트에 대해서 인수인계를 받게되고 업무를 시작하게 되었다. 이와 더불어 여러가지 일이 한꺼번에 덮치면서 멘탈 챙기기가 힘들어졌다. 거기에 코로나까지 난리를 쳐대는 바람에 헬스장이 문을 닫게 되었고, 거의 유일한 스트레스 해소 창구이자, 나에게 집중할 수 있는 시간이었던 운동시간이 사라지게 되었다. 운동을 계속하다가 안하게 되니까 스트레스에 대응하는게 많이 힘들어졌다. 업무적으로 힘들거나 다른 일로 힘들어도 헬스장에 가서 운동 한번 하고 오면 말끔하게 고민이 사라졌는데, 그럴 수 없었다. 동시에 목과 어깨가 아프기 시작해서 일에 집중하기도 힘들어졌다. 평소대로라면 도수치료라도 받으러 가겠지만, 굉장히 밀접하게 접촉해야하는 환경이라 가기가 꺼려졌다. 잠깐 힘들어했었는데, ‘그래 근데 뭐 어쩌라고’ 하는 생각으로 다시 뛰기 시작했다. 미세먼지가 엄청 심하지 않으면 나가서 3키로 이상 뛰었던 것 같다. 운동하니까 확실히 나아졌다. 몸의 병이든 마음의 병이든 일단 움직여야 낫는 것 같다. 다시 멘탈이 좀 나아지니 팀을 좀 추스려야겠다는 생각을 했다. 갑자기 인원조정을 하게 되었고, 업무 담당자도 바뀌게 되어 정리를 하지 않으면 혼란이 있을 것 같았다. 어떻게 해야할까 고민하다가, 어디서부터 잘못되었을까, 뭘 안하기 시작했을까 생각했다. 팀이 힘들어지면서는 10시에 했었던 스크럼 미팅이 생략되었고 코드 리뷰, 문서 정리, 팀 내부 미팅과 스터디 등이 점차적으로 사라지게 되었다. 개인이 맡은 업무만 하게 되고 얻은 지식은 고사하고 진행사항도 파악하기 힘들어졌다. 여기서부터 시작해야겠다고 생각했다. 아마 남은 팀원들 모두 이렇게 생각했을 것 같다. 당장 스크럼을 다시 재개했고 하루에 어떤 일을 할 건지 지난 업무 진행상황과 어려운 점을 간략하게 얘기했다. 스크럼에서 다 같이 논의해서 해야할 문제가 있으면 미팅을 잡아서 따로 얘기했다. 이런 상황에서 데이터 팀을 위한 칸반보드가 필요하다는 의견이 나왔고, 해외 기업들의 데이터 조직의 스크럼 방식과 칸반보드 등을 조합해서 우리만의 칸반보드를 만들어냈다(물론 블로그의 다음 글은 이 내용이 될 것이다). 업무 내용을 정리하고 내년에 단기적으로 그리고 장기적으로 해야할 일들을 작성했다. 이렇게 정리하고 나니 하루하루 뭘 해야할지 명확해졌고 어떤 내용을 공부할지도 정해졌다. 스터디가 자연스레 필요해졌다. 팀원들끼리 모여서 어떤 내용을 같이 공부할지 의논했고 그 내용에 대한 도서를 구매하게 되었다. 물론 아직 여러 걱정거리들이 남긴했지만, 적은 팀원들로도 할 수 있구나란 생각이 들었고, 어떤 업무에 대해서 인원을 충원해야할 지도 명확해진 것 같다. 그리고 팀이 예전과 비슷한 정도의 활기를 되찾게 되었다. 마음이 잘 맞는 사람들이라 정말 다행이고 감사하다. 좋은 사람들이다. 남은 2020년에는 연차를 썼다 연차를 계산해보니 생각보다 많이 남아서 24일부터 남은 연차를 모두 썼다. 그러니까 글을 쓰고난 내일에도 나는 출근을 하지 않는다. 당장 내일은 할게 좀 많아 보인다. 회사에서 작성하라는 Self Review를 늦장부리다가 못 썼기 때문이다. 내일은 SR을 마저 쓰고 사놓은 책들을 찬찬히 볼 예정이다. 다행히도 오늘까지 보고싶었던 넷플릭스의 스위트홈을 몰아서 다 봤고, 웹툰까지 3만원을 결제해서 결말까지 봐버렸다. 방해할게 아무것도 없어서 맘 편히 책을 보고 공부할 수 있을 것 같다. 다만 하나 걸리는 점이 있다면 목뼈가 너무 아프다는 건데, 얼른 고쳐버려서 뭘 하든 제약에 걸리지 않고 바로 할 수 있도록 만들어야겠다. 2020년이 다 가기 전에 남은 힘든 점과 기분좋아지게 하는 점들을 써놓아야겠다. 힘든 것들 목과 어깨가 스트레칭을 해도 아프다. 적어도 한 5년치 묵혀놓은 거라 잘 안풀리겠지만 치료해보자. 하도 근력운동을 안하다 보니 근육이 많이 빠지고 살이 쪘다. 맨몸운동 열심히 하자. 연말이 되니 인간관계에 대해 생각이 많아진다. 적게 생각하고 연락이나 한 번 해보자. 뭐가 힘든지 사실 대충 생각해놨었는데, 그 중 하나는 1년을 회고하려는 생각에 마음이 많이 무거웠다는 점이다. 결코 올 해는 안 좋았던 날이 좋았던 날보다 많았던 날 같고 뭔가 뒤숭숭했다. 그런데 글을 쓰다보니 괜찮아졌다. 다 쓰고보니 작년보다 고민이 좀 없어진 것 같기도 하고… 기분 좋아지게 하는 것들 코로나가 심해지면서 재택근무가 시작되었다. 3일에 한번 재택을 하는 거긴 하지만, 경기도에서 출퇴근하다가 1초만에 출퇴근하게 되는게 너무 행복하다. 물론 일은 계속 잡고있게 되서 고민이긴 하지만. 골수 나얼 팬인데 최근에 나얼 님이 유튜브를 시작했다. 나얼의 음악세계 한 10년 전에 KBS 심야 라디오에서 진행했던 프로그램 명과 동일하다. 일단 잠을 자야해서 라디오를 녹음하고 다음 날에 챙겨 들었었던 기억이 있다. 근데 이제 토요일마다 유튜브에서 LP판으로 나얼 형이 직접 틀어준다. 옛날 생각나고 너무 행복하다. 직접 만든 사과잼이 너무 맛있다. 재택할때 아침마다 빵이랑 먹었는데 살 많이 쪘다. 오늘 글을 쓰다가 ‘Justice Der’라는 기타 아티스트를 알게 되었다. 유튜브에 또 혜자스럽게 2시간 40분짜리 플레이리스트를 만들어뒀다. 적어도 1주일은 들을 것 같다.","link":"/2020/12/27/20201227/"},{"title":"AWS 비용관리의 서막, EMR","text":"FinOps? EMR 비용관리 비용관리의 서막, FinOps?비용관리를 본격적으로 시작하게되는 때는 언제일까요? 비용관리에 대한 책에 감명을 받아서? 물론 FinOps에 관한 책을 추천 받았고 이런 책이 있다는 것에 대해서 놀라기도 했습니다. 본격적으로 비용관리를 체계를 갖추어 해야겠다는 생각도 같이 들었습니다. 하지만 일반적으로는 AWS 요금이 과하게 부과된 날이지 않을까 합니다. 12월의 평화로운 어느 날에 개발 팀장님께서 조용히 저를 부르셨고, 충격적인 12월의 요금을 보면서 비용관리의 필요성을 깨닫게 되었습니다. 앞서 FinOps에 관한 책을 추천 받았다고 했는데, 이 책을 기반으로 비용 관리를 해야겠다고 마음을 먹었습니다. 이 책은 마이클 풀러의 Cloud FinOps: Collaborative, Real-Time Cloud Financial Management 입니다.책에 대한 소개글을 작성하려고 한게 아니기 때문에 간단하게 책에 대해서 읽어본 소감을 말하자면, 엄청나게 세밀하게 관리포인트를 집어주지는 않습니다. 제가 아는 내용이 많지 않아서 그럴 수도 있겠습니다. 비용관리라는 말을 듣고 어떻게 관리할 수 있을까? 에 대해서 생각해보면 나오는 개념들이 정확한 용어로 설명이 되어있기는 합니다. 나쁜 책은 아니라고 생각합니다. 실무자보다는 윗 분들이 먼저 읽었으면 합니다. FinOpsFinOps는 Finance + DevOps가 결합된 말입니다. 클라우드 환경에서 재무와 개발/운영을 긴밀히 결합함으로써 비용을 최적화하여 관리하고 통제할 수 있도록 하는 것입니다. 클라우드 비용 최적화를 위한 베스트 프렉티스와 공통된 표준 정책으로 기술과 비즈니스, 재무 전문가를 결합함으로써 비즈니스 가치 극대화하려는 것입니다.(출처 : Bespin Global)회사에서 비용관리를 제대로 시작하게 되면서 FinOps라는 말을 붙이지는 않았습니다. 재무에 관련된 인원을 포함하기 어려웠기 때문입니다. 대신에 비용관리에 관련된 사람들이 모였습니다. AWS의 비용 급상승에 관련된 부서의 실무자와 개발 팀장님, TA, 기획 팀장님입니다. 사실상 운영에 관련된 모든 사람이 모이게 되었습니다. 이렇게 모인 사람들과 비용에 대해서 논의했고 현재는 1차적인 최적화 작업을 완료하였습니다. 비용의 원인, EMR다시 문제의 12월의 끝자락으로 돌아와서, 원인 파악을 해보기 시작했습니다. 원인 중 하나는 EMR 비용의 갑작스러운 증가였습니다. EMR이 무엇일까요? 구글에 검색하면 기다렸다는 듯이 AWS는 문서를 만들어 놓았습니다. 쉽게 말하자면, EMR은 AWS 위에서 Hadoop이나 그 기반 프레임워크들을 위한 빅 데이터 프레임워크 실행을 간소화하는 관리형 클러스터 플랫폼입니다. 저희의 경우 Spark를 사용하기 위해 EMR을 쓰고 있습니다. 예상되는 비용 포인트 많은 고객사에 대한 대응 오버 사이징된 클러스터 Airflow TerminateJobFlow EMR Cluster 관리 첫 번째는 많은 고객사에 대한 대응이였습니다. 새롭게 오픈한 서비스로 인해 고객사가 갑자기 많이 들어오게 되었습니다. FinOps 책에 나온대로, 비용은 $사용량\\times요율$ 이므로 사용량이 늘어났기 때문에 비용이 갑자기 증가될 수 밖에 없었습니다. 갑자기 늘어난 고객사를 줄일 수도 없는 노릇이므로, 다른 관리 포인트를 찾아 봤습니다. 두 번째는 오버 사이징된 클러스터였습니다. 현재 제공하고 있는 서비스들은 대부분 Airflow를 사용해서 Spark로 작업을 처리한 뒤 종료되는 구조를 가지고 있습니다. 작업을 할 때 만들어지는 클러스터들의 스펙은 대부분 동일한데, 소규모 고객사에 대해서 서비스를 할 때도 같은 스펙을 사용하고 있었습니다. 고객사의 일 평균 방문 수와 상품의 sku 수를 따져서 고객사 규모를 측정하는데, 이 기준에서 작은 고객사로 분류되는 고객사들의 클러스터 스펙을 조정해 주었습니다. 세 번째는 Airflow의 TerminateJobFlow입니다. 앞서 설명한대로, EMR 클러스터는 작업을 마치면 Airflow의 TerminateJobFlow 오퍼레이터를 이용해 작업을 종료해 클러스터를 삭제하게 됩니다. EMR은 오래 돌 수록 요금이 어마어마하게 부과되기 때문에 이런 파이프라인을 만들어 두었습니다. 그런데 가끔 이 오퍼레이터가 말을 안듣는 경우가 발생하곤 합니다. 현재의 Airflow에 있는 DAG들은 새벽에도 많이 실행되고 있습니다. 그래서 출근했을 때 새벽 내내 돌고 있는 EMR 클러스터들을 마주하게 되는 경우가 종종 있었습니다. 이런 와중에 EMR 콘솔 페이지에 나오지는 않지만 EC2 페이지에 등장하는 EMR 클러스터들이 발견되었습니다. 12월 초 부터 쌩쌩 도는 인스턴스들이었는데, 네임 태그도 달려있지 않은, 알 수 없는 것들이었습니다. 12월 초 부터 계속 돌고 있었기 때문에, 여기서 큰 비용이 발생하고 있었습니다. 그래서 이 인스턴스들이 왜 남아있는지 알기 위해서 AWS측에 문의를 넣었습니다. 몇 번의 핑퐁이 오갔고“As you can observe above, that either the cluster didn’t finish the step it was running or the step itself didn’t get submitted to it or the TerminateJobFlows API call was not made by the Airflow workflow.”이러한 답변과 기타 등등을 얻을 수 있었습니다. 결국 미팅을 잡고 이야기 하기로 했습니다.(참고로 보통 이런 미팅을 하게 되면 인도 출신 엔지니어와 이야기를 하게 됩니다. 저는 인도 악센트에 친숙하지 않아서 애를 많이 먹었습니다. 전화 미팅이었지만 이역만리 타국과의 통화에 감사함보다는 실시간 채팅에 더 감사하게 되었다는 후기) AWS 측과 얘기를 한 후 추가로 알게 된 사실이 있었습니다. 그것은 VisibleToAllUsers라는 옵션이었습니다. AWS EMR에는 독특하게 모든 유저에게 클러스터를 보이는, VisibleToAllUsers이 존재합니다. 이 옵션이 체크되어 있지 않으면, 클러스터 페이지에서 해당 클러스터를 볼 수 없게 됩니다. 12월에 종료되지 않은 이 클러스터들은 이 옵션에 체크되어 있지 않은 상태로 생성이 되었고, 그래서 클러스터 콘솔 페이지에서 관리를 할 수 없었던 것이었습니다. Airflow에서 EMR 클러스터를 생성할 때도 마찬가지입니다. VisibleToAllUsers옵션을 지정해서 클러스터를 생성할 수 있습니다. 이 옵션이 제대로 지정되지 않았고, 그래서 삭제되지 않고 관리되지 못한 인스턴스들이 발생한 것이었습니다. 12월 초에 새로운 서비스를 위한 Airflow 인스턴스를 새로 만들었는데, 이 파라미터값을 관리하는 Connection 부분이 초기화 되었던 것입니다. 당시에 인수인계다 뭐다 정신이 없었고 그런 상황에서 체크를 제대로 하지 않고 넘어갔던 것이 화근이었습니다. 아무튼 이렇게 해서 Connection에서 emr_default 부분에 VisibleToAllUsers을 true로 변경해 주었고, 클러스터 페이지에 나타나지 않는 인스턴스들은 사라지게 되었습니다. 하지만 추가 대응 방안이 필요했습니다. (AWS 측에서는 Airflow 매니지드 서비스를 이용하라고 했지만, 진심 너무 비쌉니다. 이렇게 된거 GCP로 옮기면 어떨까? Composer 참 싼데, 보고 있나 AWS?? You See Me??) 네 번째는 EMR Cluster 관리에 대한 것입니다. EMR Cluster 페이지에 직접 들어가서 삭제되지 않은 클러스터들을 일일이 보는 프로세스가 맘에 들지 않았습니다. 자동화에 집착하는 집착맨이기 때문에, 좀 더 세련된 방법을 찾고 싶었습니다. TA 분과 이야기 해 본 결과 boto3를 이용하면 클러스터에 페이지에 쉽게 접근할 수 있다는 것을 알 게 되었습니다. boto3로 클러스터 관리하기Boto3는 AWS에서 제공하는 파이썬 용 SDK(software development kit)입니다. boto3를 사용하면, S3나 EMR등 AWS의 다양한 서비스에 접근이 가능합니다. boto3가 없다면 pip3 install boto3로 설치해 주시고, AWS에 로그인 해줍니다. 12345678import boto3from pprint import pprintclient = boto3.client('emr')response = client.describe_cluster(ClusterId='j-xxxxxxxxxx')pprint(response) 이렇게 입력을 해주면 해당 클러스터 아이디에 대한 값들이 쫙 나오게 됩니다. 너무 쉽죠? AWS를 쓰세요. boto3는 참 좋습니다. 대신 더럽게 비쌉니다. 이 정보들은 dictionary로 되어 있으니, 키 값을 잘 조회 해서 값에 접근하면 됩니다. 대표적인 키 값은 Cluster, Ec2InstanceAttributes, Id, Name, NormalizedInstanceHours, Status, Timeline 등입니다. 저는 TimeLine을 이용해서 생성시간과 현재시간의 차를 구한 다음, 항상 띄어놓는 분석용 EMR 시간보다 작고 5시간 이상 RUNNING하거나 WAITING하고 있는 클러스터들을 종료시킬 생각입니다. 활용방안은 다양하니 각자의 관리포인트대로 작업하시면 될 것 같습니다. 이렇게 해서비용관리가 발생한 상황과, FinOps, EMR, 관리 방법에 대해서 간단하게 작성하게 되었습니다. 문제 상황부터 지금까지 우여곡절이 참 많았는데, 정리하니까 내용은 그렇게 많지 않네요. 삽질을 많이 했다는 뜻입니다. 여러분들은 부디 삽질을 최소화하시고 많은 이득을 취하시길 바랍니다. 짧막한 삽질 글을 읽어주셔서 감사합니다.","link":"/2021/01/24/cost-mgt-emr/"},{"title":"Airflow Basic. 두 번째","text":"Airflow 구조 파악하기, 실수 줄이기 글에 들어가기 전에…Airflow의 구조에 대해서 들어가기 전에, Airflow의 컨셉에 대해서 간략하게 설명한 글이 있습니다. 기본적인 개념에 대해서 먼저 알고 싶으신 분은 Airflow Basic을 읽어주시면 감사하겠습니다. 간단하게 컨셉을 이해한 뒤에 이 글을 읽으시면 더욱 좋습니다. Airflow 구성요소Airflow의 구성요소는 크게 3개입니다. Airflow Webserver, Scheduler 그리고 Worker 입니다. 실제로 Airflow를 설치하셨다면, airflow webserver, airflow scheduler, airflow worker 명령어를 입력하면 됩니다. 세 명령어를 다 입력했다면, airflow를 다 띄웠다고 할 수 있습니다. 운영하고 있는 Airflow 서버에서도 이상이 있으면 체크하는 것이 명령어를 입력한 세 개의 구성요소입니다. 먼저 Airflow의 구성요소에 대한 큰 그림을 보고 각 구성요소들에 대해서 설명해보도록 하겠습니다. Airflow SchedulerAirflow의 스케쥴러는 말 그대로 Airflow의 작업들을 스케쥴링 해줍니다. DAG들을 파싱해서 스케쥴된 작업들의 작업 간격을 확인하고 실제로 작업 명령을 워커에 전달합니다. 또한 DAG에 걸려있는 의존성을 확인하고 걸려있다면, 실행 큐에 더해줍니다. 이와같이 스케쥴러는 Airflow의 심장과 같은 역할을 수행하고 DAG와 직접적으로 붙어있습니다. 붙어야 하는 DAG의 위치는 airflow.cfg 파일에서 지정할 수 있습니다. airflow.cfg는 airflow에 대한 설정들이 모여져 있는 파일입니다. 하나의 서버에서 돌아가는 싱글구조의 Airflow라면 이 설정에서 딱히 조정할 것은 없습니다. 다만 mysql에 접속할때 사용하는 user명과 비밀번호를 넣어줘야 합니다. 싱글구조가 아니라면(워커와 스케쥴러가 분리되어 있다면) 스케쥴러의 ip주소도 넣어줍니다. configuration에서 result_backend와 sql_alchemy_conn을 찾아서 수정해줍니다. Airflow의 mysql 설정이 궁금하신 분들은 Airflow EC2에 구축하기를 참고하시면 좋습니다. Airflow WorkerAirflow 워커는 스케쥴러에서 할당해놓은 작업들을 DB에서 갖고와서 실제로 실행합니다. 작업들은 DAG를 구성할때 queue를 이용해서 어떤 워커에서 수행할지 결정되고 워커는 큐를 설정해서 기동합니다. 워커는 airflow worker -q main 과 같이 큐를 설정할 수 있습니다. DAG에서 queue를 아래와 같이 지정해놓으면 스케쥴러는 해당 큐에 맞게 작업을 나눠놓고 워커는 자신의 큐에 맞는 작업을 찾아서 갖고옵니다. 찾은 큐를 워커에서 실행하고, 실행후 나온 로그들을 회수합니다. 이 로그들은 Metastore(Airflow DB)에 저장되고 저장된 로그들은 웹 서버를 통해 확인할 수 있습니다. 1234567891011121314151617t1 = BashOperator( task_id='t1', bash_command=\"sleep 1\", queue=\"main\", dag=dag)t2 = BashOperator( task_id='t2', bash_command=\"sleep 1\", queue=\"main2\", dag=dag)t3 = BashOperator( task_id='t3', bash_command=\"sleep 1\", queue=\"main3\", dag=dag) 워커가 큐를 찾아 가져오는 구조가 좀 특이합니다. 일반적으로 생각하기에는 스케쥴러가 워커에 작업을 보내줄 것 같은데, 그렇지 않습니다. 이 큐를 가져오는 구조 라는 특성 때문에 Airflow를 싱글구조에서 워커와 DB, 스케쥴러로 나누는 구조로 변경할때, 보안그룹 설정에서 스케쥴러의 인바운드 포트만 열어주게 됩니다. 스케쥴러의 인바운드 포트만 열어주고, 워커의 airflow.cfg로 들어가서 어떤 metastore에서 작업을 할당받을지 주소를 적어주면 끝입니다. 사실상 스케쥴러 인스턴스의 airflow.cfg만 가져와서 그대로 붙여주면 아주 쉽게 설정이 끝납니다. Airflow Webserver웹 서버는 Airflow의 Metastore 저장된 로그를 보여주거나 스케쥴러에 의해 파싱된 DAG들을 시각화해서 제공합니다. 이 UI를 통해 DAG들이 돌아가는 상황과 결과들을 확인할 수 있습니다. Dag를 만든 후에 스케쥴러가 Dag폴더의 위치를 찾으면 여기에 있는 Dag를 파싱해서 웹으로 보여줍니다. 가장 왼쪽에 On이라고 되어 있는 부분이 있는데, 맨 처음에는 Off로 되어 있습니다. 이것을 On으로 바꿔주고, 현재 시간이 Dag에 설정된 start_date 보다 나중 시간이라면 정상적으로 작업을 실행하고, 그렇지 않으면 작업을 수행하지 않습니다. 주의해야 할 점 Timezone, - UTC, KST dag_name 병렬작업 설정 Airflow Basic에 있는 내용들을 종합하면 간단한 Dag는 작성해서 파이프라인을 만들 수 있을 것입니다. 여기서부터는 작업하면서 실수가 잦았던 부분에 대해서 다뤄보려고 합니다. 첫 번째는 시간대 설정입니다. 이 글을 보고 계시는 대부분의 분들은 KST시간대를 사용하시는 분들일 것입니다. 하지만 Airflow는 한국인이 만든 것이기 아니기 때문에, 모든 사람들이 다 사용할 수 있게 시간대를 UTC로 설정해두었습니다. 그래서, 작업하는 시간대와 Webserver에서 보여지는 시간대는 UTC를 사용합니다. 위 UI에서도 Last Run이 보이는데, 이것은 UTC시간대를 적용해서 나오는 시간입니다. 하지만, 만약에 Airflow를 AWS위에 올려서 사용하고 cron 스케쥴을 잡아서 실행한다면, 이 cron스케쥴은 KST를 적용받습니다. 위의 사진에서 보든이 schedule_interval에는 cron 스케쥴이 들어갑니다. 작성된 스케쥴을 그대로 읽으면 AM 01:30분에 돌아가게 됩니다. 지금은 글로 설명되어 있어서 ‘이게 왜 헷갈리지’ 라고 생각하실 수도 있겠지만, Last Run이나 Airflow의 Tree View를 보면서 작업 수행 시간을 확인하다 보면 스케쥴 인터벌에 UCT로 작성하는 큰 실수를 할 수 있습니다. 물론 이것은 개인의 경험에 의한 경고입니다. 시간을 잘 확인하는 습관을 들입시다. Airflow 1.10.10 버전 부터는 UI에서 Timezone을 선택할 수 있다고 합니다. https://github.com/apache/airflow/pull/8046 두 번째는 dag_name입니다. Dag를 만들고 잘 돌아가면 보통 Dag를 새로 만들어서 구성하기 보다는, 복사해서 붙여넣고 자잘한 부분만을 수정하는 경우가 많습니다. 저도 그렇습니다. 이럴 경우에 다른 자잘한, 중요한 로직과 관련된 부분들은 수정을 잘 하는데, dag_name을 바꿔놓지 않는 경우가 많습니다. 이렇게 되면, Webserver에서 Graph view를 볼 때, Dag가 새로고침 할때마다 변경되는 기이한 현상을 목격할 수 있습니다. 분명히 변경이 된 것을 Code에서 확인을 했는데, 실제로 Graph View에 나오는 것은 이전 코드입니다. 새로 고침을 하면 또 수정된 코드로 보입니다. 이것은 만들어진 dag파일명은 다르지만 dag_name이 같아서 웹 서버가 같은 dag_name을 호출하기 때문입니다. 흔히 dag파일명으로 Airflow 웹 서버가 구분할 것이라고 생각하지만, 웹 서버는 dag_name을 읽어옵니다. 이 dag_name은 위 사진에서처럼, with DAG()안에 들어갑니다. 세 번째는 병렬작업 설정입니다. 어떤 작업을 사진처럼 병렬로 예쁘게 실행시키고 싶을 때가 있습니다. 물론 이것은 그림으로만 병렬이고, 실제로는 작업이 하나씩 돌아가는 구조입니다. 그 이유는 코드를 확인해보면 됩니다. 1234567891011121314151617181920for file in files: step_adder = EmrAddStepsOperator( task_id=\"emr_connection_{}\".format(file), job_flow_id=created_job_flow_id, aws_conn_id=\"aws_default\", steps=get_pyspark_execute_step(file), queue=\"main\", dag=dag ) step_checker = EmrStepSensor( task_id=\"job_execution_{}\".format(file), job_flow_id=created_job_flow_id, step_id=\"{{{{ task_instance.xcom_pull('emr_connection_{}', key='return_value')[0] }}}}\".format(file), aws_conn_id=\"aws_default\", queue=\"main\", dag=dag ) dataloader_checker &gt;&gt; step_adder &gt;&gt; step_checker &gt;&gt; preprocessing_end 병렬 구조로 돌리는 것은 사실 for문으로 묶여있습니다. 병렬처럼 묶는 방법은 다양하지만 여기서는 for문을 사용했습니다. 이렇게 for문으로 구성하고 작업 순서를 부등호를 이용해서 정해주면 끝이지만, 여기서 실수가 자주 발생합니다. indentation 을 집중해서 보셔야 합니다. 여기서 작업 순서가 적힌 dataloader~~ 부분을 부면 for문 안쪽으로 indentation이 잡혀있습니다. 보통 for문을 다 작성하면 ‘다 됐다!’라는 생각에 tab을 치지 않고 작업순서를 넣는데, 이렇게 되면 병렬로 잡힐거라고 생각했던 작업이 다 깨져버리게 됩니다. Tab을 꼭 한번 눌러주고 작업 순서를 작성해주시면 아주 좋습니다. 덧붙여,Airflow를 이용하면 위 코드에서처럼 EMR을 연계해서 작업을 수행할 수 있습니다. Airflow의 장점중에 하나인데, 다양한 Hook과 Operator가 많다는 것입니다. Hook과 Operator를 조합하면 다양한 서비스 파이프라인을 개발할 수 있습니다. Airflow와 EMR을 연계해서 Spark job을 실행하고 값을 저장 부분은 다음 글로 작성해보도록 하겠습니다. 부족한 글 읽어주셔서 감사합니다.","link":"/2021/02/21/airflow-basic2/"},{"title":"Apriori와 FP-Growth. 추천 시스템 시리즈","text":"추천 시스템에 대해서, Apriori와 FP-Growth 추천시스템은?추천시스템은 사용자에게 상품을 제안하는 소프트웨어 도구 이자 기술입니다. 추천 시스템은 사용자 입장에서 수십 만개의 상품 풀에서 원하는 것을 빠르게 찾을 수 있다는 점에서 큰 의의가 있습니다. 제가 가장 많이 접하는 Youtube 추천 영상을 생각해보면 이해가 빠릅니다. 영상은 하루에도 수억개가 올라올테지만, 제가 원하는 것이 아주 잘 올라오는 것을 확인할 수 있습니다. 추천 시스템이 없다면 영상을 찾는데 시간이 너무 오래 걸릴 것입니다. 기업 입장에서도 추천은 매우 중요합니다. 사용자가 영상을 찾는데 너무 많은 시간을 소비한다면, 짜증을 느낄 것이고 이 플랫폼에 대한 이용율이 떨어질 것입니다. 또한 커머스라면 적절한 상품을 추천을 함으로써 매출을 극대화 시킬 수 있겠습니다. 그렇다면 추천 시스템에서는 사용자가 누구인지 타겟팅 하는 것이 중요해집니다. 하지만, 사용자를 타겟팅하는 것은 사업부나 마케팅 입장에서 아주 상이한 점이 있습니다. 그리고 추천 목적도 도메인 마다 상이하므로 이 부분은 생략하고 넘어가도록 하겠습니다. 추천 시스템의 흐름 추천 시스템이 어떻게 발전해 왔는지 살펴보겠습니다. 05년에 먼저 시작된 것은 연관 상품 추천, 장바구니 분석으로도 잘 알려진 Association Rule과 Apriori 알고리즘입니다. 이 알고리즘은 같이 구매하는 상품의 패턴을 잘 파악할 수 있는 알고리즘이였습니다. 이것이 발전하는 와중에 협업 필터링이 새롭게 등장했고 넷플릭스 추천대회를 통해 ALS등의 고급 테크닉이 나오게 되었습니다. 그러던 와중 Apriori의 한계를 극복하려는 시도로 만들어진 FP-Growth가 등장했습니다. 2010년대가 넘어가면서 많은 데이터를 쌓아두기 시작하는 빅데이터 시대가 열리게 되었고, 이 역사적 흐름에 맞춰 Spark와 Spark를 이용한 다양한 추천 알고리즘이 등장하기 시작했습니다. 이후에는 딥러닝을 이용한 다양한 추천시스템이 나왔고, 최근에는 개인화 추천시스템으로 넘어가고 있습니다. 이 흐름에서 오늘은 추천의 흐름 맨 앞단인, 연관상품 추천 부분을 다뤄보겠습니다. Association AnalysisAssociation Analysis는 룰 기반의 분석입니다. 상품과 상품 사이에 어떤 연관이 있는지 알아보기 위해 시작한 분석입니다. 우리말로 해석하면 연관 분석이 되겠습니다. 여기서 연관에 대해서 정의해볼 필요가 있습니다. 연관은 무엇일까요? 연관은 얼마나 잦은 빈도로 구매를 하는지, A에서 B를 구매하는 패턴을 의미합니다. 이 연관 분석은 흔히 장바구니 분석이라고도 불리며, 가장 유명한 예시는 역시 맥주과 기저귀 분석일 것입니다. 하지만 이 맥주와 기저귀 예시에서 보더라도, 이 분석으로 나온 결과가 상관 관계가 그리 크지 않았습니다. Association Analysis, Metric이 규칙의 평가 지표에 대해서 알아보겠습니다. 어떤 분석에는 항상 지표가 있어야 평가를 할 수 있고, 이 평가를 통해서 분석 방법을 개선할수도 있겠습니다. 이 지표는 세 가지로 나뉩니다. support, 지지도 A→B에 대해서 조건 A에 대한 확률을 보는 것. P(A) 또는 P(A,B) $P(A)$ confidence, 신뢰도 A상품을 구매했을 때, B상품의 구매까지 이어질 확률 $confidence(A\\rightarrow B)=\\dfrac {P(A,B)}{P(A)}$ lift, 향상도 사건이 동시에 얼마나 발생하는지 비율, 독립성을 측정 $lift(A\\rightarrow B)=\\dfrac {P(A,B)}{P(A)\\cdot P(B)}$ 연관 분석 관련 알고리즘은 흔히 지지도를 베이스로 잡고 연관도를 측정하는데, 신뢰도와 향상도를 항상 고려하면서 추천 분석을 진행해야 좋은 결과가 나올 수 있습니다. Association Analysis 적용하기이제 본격적으로 연관 분석을 진행해 보겠습니다. 먼저 상품이 4개 있다고 가정하겠습니다. 이 연관 분석은 가능한 모든 경우의 수를 찾아내고 지지도와 신뢰도, 향상도가 높은 규칙들을 찾아내야 합니다. 그렇다면 모든 경우의 수는 몇 가지가 될까요? 고등학교 수학시간에 배운 개념을 한 번 떠올려 봅시다. 4개의 상품일 때 경우의 수 ${}_4 \\mathrm{C}_1+ {}_4 \\mathrm{C}_2 +{}_4 \\mathrm{C}_3 + {}_4 \\mathrm{C}_4 = 15$ 15개의 경우의 수가 나왔습니다. 이렇게 연관 분석은 모든 경우의 수를 계산해서 진행해야 합니다. 아이템 수가 많아진다면 어떻게 될까요? 결과가 예상되기 시작합니다. 아이템이 100개라면 $1.26\\times10^{30}$ 으로 엄청난 코스트가 발생합니다. 이를 극복하기 위해서, Aprirori가 등장했습니다. AprioriApriori는 아이템 셋의 증가를 효과적으로 줄이기 위해 다음과 같은 가정을 사용합니다. 빈번한 아이템셋은 하위 아이템 셋 또한 빈번할 것 빈번하지 않은 아이템셋은 하위 아이템 셋도 빈번하지 않다. 고로 빼버리자! 글로는 이해가 잘 안되니 큰 그림을 갖고 이해를 한 번 해보도록 하겠습니다. 맨 처음에 4개의 아이템에 대해서 단일 항목집단을 만들어 냅니다. 2,3 아이템 셋이 빈번하지 않다고 한다면(서포트가 가장 낮다면), 가정에 따라 그 하위인 023, 123, 0123도 빈번하지 않을 것이다! $Support(2,3) &gt; Support(0,2,3), (1,2,3)$ 이런 식으로 진행이 됩니다. 이것을 정리하자면 다음과 같습니다. k개의 아이템을 가지고 단일항목집단 생성 단일항목집단에서 지지도 계산 후 최소 지지도 이상의 항목만 선택 2에서 선택된 항목 만을 대상으로 2개항목집단 생성 2개항목집단에서 최소 지지도 혹은 신뢰도 이상의 항목만 선택 위의 과정을 k개의 k-item frequent set을 생성할 때까지 반복 이제 본격적으로 예를 들어 진행해보도록 하겠습니다. Apriori 적용하기 다음과 같이 위에 Transaction 데이터가 주어졌습니다. 이 데이터를 잘 분리해서 0과 1로 이루어지는 Sparse Matrix로 만들어줍니다. 재료가 준비되었다면, 아까와 같이 단일 항목집단을 생성해줍니다. 단일 항목집단 생성 및 최소 지지도 계산 위의 매트릭스를 갖고 각 항목에 대해 서포트를 계산해봅시다. 우유는 전체 4건의 거래번호에서 2개만 있으므로 0.5가 되고, 양상추를 4건에서 3개가 있으므로 0.75가 됩니다. 이렇게 보듯이 서포트는 확률에 대한 값을 의미합니다. 여기서 최소의 서포트를 가지는 쥬스를 과감하게 삭제해줍니다. 2개 항목집단 생성 위에서 선택한 항목을 기반으로 2개 항목 집단을 만듭니다. 4개의 항목을 가지고 2개씩 붙여주는 집합을 구성하면 됩니다. 조합을 이용하면 ${}_4 \\mathrm{C}_2=6$ 6개의 집단이 만들어집니다. 이 역시 서포트를 계산해주고 최소 서포트를 가진 집단을 지워줍니다. 이것을 정리해주면 위와 같이 진행됩니다. 생각보다 많은 연산이 줄어든 것을 확인할 수 있습니다. 여기까지 Support를 바탕으로 알고리즘을 진행했습니다. confidence와 lift도 활용해서 진행할 수 있고, 다른 지표들도 잘 확인해야 한다는 점을 꼭 알아 두시면 좋습니다. Apriori의 장 단점이 알고리즘의 장 단점은 명확합니다. 장점은 일단 너무 쉽습니다. 원리가 간단해서 쉽게 이해할 수 있고 이 의미를 쉽게 파악할 수 있습니다. 또한 유의한 연관성을 갖는 구매패턴을 다양하게 찾아주는 장점을 가지고 있습니다. 다만, 단점은 데이터가 커질 때 발생합니다. 데이터가 크다면 속도가 느리고 연산량이 그래도 또 많아집니다. 그리고 실제 알고리즘을 사용하게 되면 너무 많은 연관상품들이 나타나는 문제가 발생합니다. 그리고 장점에서도 보이듯이 상품들의 유의한 연관성, 즉 연관 상품들이 상관관계는 의미하지만 그것이 인과를 의미하진 않습니다. 결과만을 본다면 어떤 것이 원인인지 파악하기 힘듭니다. 치킨과 치킨무를 예를 들어보면, 이 둘은 장바구니에 같이 담기는 패턴이 굉장히 많을 것입니다. 서포트가 높기 때문에 추천 셋으로 많이 등장할 것입니다. 하지만 결과만을 보고 치킨무를 샀기 때문에 치킨을 구매한 것인지, 치킨 때문에 치킨무를 산 것인지 알기는 힘듭니다. 우리의 경험에 의한 지식으로 당연히 치킨이 원인이라는 것을 단번에 알 수는 있지만, 복잡한 패턴이 나오면 판단하기 쉽지 않습니다. FP-Growth(Frequent Pattern)FP-Growth는 데이터가 점점 많이 쌓이기 시작하면서 Apriori의 연산 속도를 개선하기 위해 등장했습니다. 이 연관 규칙을 어떻게 효과적으로 빠르게 만들 수 있었을까요? 코딩 알고리즘을 배우면 많이 등장하는 개념이 있습니다. Tree 입니다. 핵심 아이디어 : 연관 규칙을 트리로 만들어 단점을 개선해보자!FP-Growth도 기본적인 성질은 Apriori를 따라갑니다. 따라서 기본적인 설정은 Apriori와 거의 같습니다. Tree 구조를 활용했다는 점만 다르다고 볼 수 있습니다. 알고리즘을 적용하는 순서는 다음과 같습니다. 모든 거래를 확인해 각 아이템마다의 서포트를 계산하고 최소 지지도 이상의 아이템만 선택 모든 거래에서 빈도가 높은 아이템 순서대로 순서를 정렬(여기서 부터 달라짐) 부모 노드를 중심으로 거래를 자식노드로 추가해주면서 tree를 생성 새로운 아이템이 나올 경우에는 부모노드부터 시작하고, 그렇지 않으면 기존의 노드에서 확장 위의 과정을 모든 거래에 대해 반복하여 FP Tree를 만들고 최소 지지도 이상의 패턴만을 추출 자, 이제 아까의 데이터를 갖고 FP-Growth를 적용해봅시다. FP-Growth 적용하기 데이터는 똑같이 준비해 놓았습니다. 주스는 똑같이 삭제 주스를 삭제하고 정리를 한번 해줍니다. 트리 형성 트리를 만드는 과정은 생각보다 간단합니다. 먼저 Root 노드를 만들어주고, 가장 빈도가 높은 순으로 노드 옆에 붙여줍니다. 여기에서는 기저귀와 양상추로 노드를 구성해보겠습니다. 거래번호 0의 아이템대로 기저귀 옆에 우유 노드를 붙여주고, 양상추에도 기저귀와 맥주, 우유를 붙여줍니다. 거래번호를 따라가면서 같은 아이템이 나온다면, 그림과 같이 아이템 옆에 숫자를 +1 시켜줍니다. 연관 규칙 찾기, 지지도 낮은 순서 부터 조건부 패턴을 생성 (우유부터 시작) 이제 서포트가 낮은 순서부터 조건부 패턴을 만들어줍니다. 우유의 서포트가 가장 낮으므로 우유가 붙어있는 노드를 따라가서 조건부 패턴을 파악해줍니다. 이런식으로 조건부 패턴을 다 파악했다면, 어떤 아이템이 들어왔을 때 트리를 통해서 추천 아이템을 전달할 수 있게됩니다. 위의 조건부 패턴 표를 보고, 맥주를 구매한 고객이 등장한 경우를 생각해보겠습니다. 맥주가 들어온 고객에게는 기저귀를 구매한 경우와 양상추를 구매한 경우로 나뉘게 됩니다. 양상추를 구매한 경우는 거기서 더 진행이 되지 않으므로 끝이나고, 기저귀를 구매했다면 더 깊게 들어가 양상추까지 구매한 패턴을 알 수 있게 됩니다. Tree를 타고 진행이 된다는 것이 느껴지시나요? Tree구조라는 점이 FP-Growth의 가장 큰 특징입니다. FP-Growth 장 단점장점은 역시 Tree구조이기 때문에 Apriori보다 훨씬 빠르며, DB에서 스캔하는 횟수도 줄어들게 됩니다. Apriori와 비교하면, Apriori는 최소 한 번의 DB스캔에서부터 가장 긴 트랜잭션의 아이템 셋을 뒤져야 할 가능성이 있는 반면, FP-Growth는 첫번째 스캔으로 단일 항목집단을 만들고, 두 번째 스캔으로 Tree구조를 완성합니다. 완성한 FP-Tree를 이용해서 분석하면 되니까, 딱 2번만 스캔하면 됩니다. 후보 Itemset을 생성할 필요없이, Tree만 구성하면 끝인 것입니다. 단점은 아직도 대용량 데이터 셋에서 메모리를 효율적으로 사용하지 않는다는 점입니다. 초기 알고리즘이라 그런 점이 있지만 이 한계 때문에 다양한 시도들이 제안되었습니다. 또한 Apriori에 비해 설계하기 어렵고, 서포트의 계산은 무조건 FP-Tree가 만들어져야 가능하다는 단점이 존재합니다. 연관 분석 알고리즘의 한계마지막으로 이 알고리즘들의 한계에 대해서 지적해보면서 글을 마무리 해보려고 합니다. 글을 쭉 읽다보면, 최소 서포트에 대한 개념이 자주 등장합니다. 최소 서포트는 아이템이 많아지게 되면서 설계자가 직접 지정해야 합니다. 아이템 셋을 얼마나 뽑아낼지, 얼마나 상관관계가 괜찮은지에 따라 서포트 값을 설정해주어야 하는 것입니다. 하지만, 최소 지지도를 선정하는 정답은 존재하지 않습니다. 계산 공식은 없고 여러 지표들을 조합해서 분석하면서 최적의 값을 지정하곤 합니다. 가장 큰 한계점으로, 실제 서비스에서는 최적 서포트 값이 매번 바뀌므로 계산하는 공식이 필요한데, 사실상 만들기는 불가해서 서비스에 잘 사용하지 않고 있습니다. 다만 많은 패턴들을 보여주는 장점이 있기 때문에 베이스라인으로 사용해서 EDA하는 식으로 많이 활용하고 있습니다. 또한 인과를 알기 어렵다는 한계가 발견됩니다. 이렇게 원인과 결과를 확실하게 분석하고 싶다는 아이디어가 등장하면서 Casual Inference라는 연구분야가 자연스럽게 만들어졌습니다. 인과 추론에 관심있는 분들은 Causual Inference 키워드로 찾아보시면 좋습니다. 그리고 메모리와 시간을 너무 많이 소비하는 알고리즘이기 때문에, 즉, 가성비가 잘 나오지 않아 사용하기 힘듭니다. 최근이야 Spark를 통해서 대용량 처리도 가능해졌긴 하지만, 굳이 큰 비용을 들이면서까지 연관분석을 하려는 시도는 많이 보이지 않는 것 같습니다. 위에서 지적한대로 지표를 직접 지정해줘야하는 문제와 더불어 서비스에 잘 활용되지 않는 큰 이유 중에 하나입니다. 이런 한계들 때문에 Collaborative Filtering 계열 알고리즘이 부상하기 시작했고, 목적에 따라 user based추천, item based 추천을 생성하기 시작했습니다. CF의 한계와 더 좋은 성능을 위해서 딥러닝을 활용해 패턴을 인식시키고자 하였고, 최근에는 개인화 추천까지 발전해오고 있는 흐름입니다. Reference T-Academy : https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=194 ratsgo’s blog 연관규칙 : https://ratsgo.github.io/machine learning/2017/04/08/apriori/ 시간여행자 블로그 : http://openuiz.blogspot.com/2018/11/fp-growth-algorithm-frequent-pattern.html 떡춘님 블로그 : https://blog.naver.com/sindong14/220661064114","link":"/2021/03/07/apriori-FP/"},{"title":"Airflow Clusterization","text":"Airflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인 Airflow ClusterizationAirflow를 나눠봤습니다. 그런데 이제 Autoscaling을 곁들인Airflow의 구성요소들을 다 쪼개볼 겁니다. 어떻게 나눌 것이냐면, 크게 Airflow Main, Airflow DB, Airflow Worker입니다. Airflow Main에는 웹 서버와 스케쥴러를 돌아가게 만들 것이고, DB에는 MySQL을 띄워 놓을 것입니다. Worker는 오토스케일링을 걸어둘 것이구요. 귀찮게 왜 이렇게 하냐구요? 기존에 Airflow는 하나의 서버에서 잘 돌아가고 있었습니다. 그래서 Airflow Basic 글을 통해서 정리해 놓기도 했습니다. 문제는 처리할 DAG와 Task가 너무 많아진 것이었습니다. Airflow의 구성요소 각각이 어떤 역할을 하는지 궁금하시거나 잘 기억이 안나시는 분은 이 글을 읽어보시면 좋습니다. 처리할 작업이 많아지면 Worker 하나로는 처리하기 부담스러워집니다. 스케쥴러가 내려준 많은 작업을 처리하다가 다른 작업을 못하게 현상이 자주 발생하게 됩니다. 그래서! 워커를 늘려줘야겠다는 생각을 한 것입니다. 워커에 오토스케일링을 걸어서, 작업 부하가 걸리면 워커를 늘려서 처리하고, 부하가 줄어들면 워커를 줄이려는 것입니다. 작업 계획은 다음과 같습니다. Airflow Main 세팅 Airflow DB 세팅 Airflow Worker 기본 이미지 세팅 Airflow Worker 세팅 Assemble! Airflow MainAirflow Main에는 Webserver와 Scheduler를 구성해 놓을 것입니다. Airflow Main의 기본적인 설정은 이 글에서의 설정과 같습니다. 여기서 MySQL을 따로 가져갈 것이니 MySQL설정만 빼놓고 따라하시면 좋을 것 같습니다. 다만 저는 Airflow 1.10.14 버전을 사용하도록 하겠습니다. 최신 2.0이상 버전은 DAG가 조금 달라져서 수정을 해줘야 하거든요. Airflow Home먼저 저는 AWS환경을 이용해서 구축을 할 것이기 때문에 EC2를 하나 만들어주고, 새로 생긴 인스턴스에 Airflow home을 잡아주도록 하겠습니다. 1export AIRFLOW_HOME=/home/ec2-user/airflow #기본주소 이 HOME주소는 꽤나 중요합니다. Airflow가 시작되는 곳이기도 하고, 설정 파일을 불러오는 곳이기도 하기 때문입니다. 원하는 주소로 작성하시되, Worker에도 동일한 주소를 입력해주셔야 합니다. 그렇지 않으면 execute_command encountered a CalledProcessError ,Celery command Failed 를 만나시게 될 것입니다. 1 Airflow의 Dag를 실행하려면 airflow를 구성하고 있는 모든 서버에 동일한 경로와 이름으로 DAG파일이 존재해야 한다는 것을 기억해주세요. Redis 설정레디스 설정은 다행히 저번 글에 나와 있는 것과 똑같습니다. 그대로 따라하시면 됩니다. Airflow 설치1234567891011sudo yum update -ysudo yum install group \"Development tools\" -ysudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel python3-devel.x86_64 cyrus-sasl-devel.x86_64 -ysudo yum install libevent-devel -ysudo pip3 install apache-airflow==1.10.14airflow version #버전이 뜬다면 성공! 설치가 되었으면 airflow initdb , airflow webserver를 사용해서 airflow가 뜨는지 확인해 봅니다. 에러가 발생한다구요? 그렇다면 아래 Trouble Shooting 부분을 참고해보세요. (sqlalchemy version 조정) 2 MySQL 설정Airflow Main 세팅이 끝났다면, DB로 사용할 새 인스턴스를 만들고 MySQL을 설치해줍니다. 이전 글과 달라지는 부분이 DB 세팅에서 발생합니다. mysql 5.7버전으로 설치하는 것 까지는 동일합니다. 123456sudo wget https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpmsudo yum localinstall mysql57-community-release-el7-11.noarch.rpm sudo yum install mysql-community-server# 설치된 이후sudo service mysqld start airflow 사용자를 추가해봅시다. 먼저 root로 접속을 합니다. 123$ mysql -u root -p# 패스워드를 입력Enter Password : #/var/log/mysqld.log에 있는 임시비밀번호 사용 달라지는 부분이 발생하는 곳이 여기입니다. airflow 사용자를 생성할 때 모든 접속 % 에 대해서 허용해주어야 합니다. 기존에는 내부 주소, localhost에 대해서 허용했습니다. 1234567# 사용자 생성create user 'airflow'@'%' identified by '비밀번호';# DB 권한 부여grant all privileges on *.* to 'airflow'@'%';flush privileges; DB가 설정되었으면 Airflow Main과 연결을 해주어야 합니다. 이 DB 서버가 사용하는 Private IP가 있습니다. 이 주소를 AWS에서 확인하고 Airflow Main의 airflow.cfg에 넣어줘서 어떤 DB를 바라볼지 Airflow Main에게 알려줘야 합니다. 기본적인 airflow main의 airflow.cfg 설정은 이렇게 해줍니다. 123456789101112131415161718192021# 사용할 dag 폴더 지정# subfolder in a code repository. This path must be absolute. 꼭 절대경로!dags_folder = /home/ec2-user/airflow/dags #원하는 위치와 디렉토리로 지정# executor = SequentialExecutorexecutor = CeleryExecutor #원하는 Executor 선택# sql_alchemy_conn = sqlite:////home/airflow/airflow/airflow.dbsql_alchemy_conn = mysql+pymysql://airflow:@[mysql서버PrivateIP]:3306/airflow# catchup_by_default = Truecatchup_by_default = False# broker_url = sqla+mysql://airflow:airflow@127.0.0.1:3306/airflowbroker_url = redis://airflow@[main서버PrivateIP]:6379/0# result_backend = db+mysql://airflow:airflow@localhost:3306/airflowresult_backend = db+mysql://airflow:비밀번호@[mysql서버PrivateIP]:3306/airflow# load_examples = Trueload_examples = False #예시를 보고 싶다면 True broker_url , cluster_address에는 Main의 Private IP주소를 넣고, sql_alchemy_conn, result_backend 에는 DB의 Private IP주소를 넣습니다. 이렇게 설정해주고 다시 한번 airflow initdb 를 해주면 새로 작성해준 주소로 DB경로가 입력이 됩니다. 추가로, MySQL서버에도 Redis를 설치해주어야 합니다. broker인 Redis를 통해 scheduler와 executor를 연결해주어야 하기 때문입니다. 위 그림에서 보듯이 main은 webserver와 scheduler를 담당하고 MySQL 서버는 DB와 executor쪽을 담당하게 됩니다. executor에 보낼 매개가 필요하기 때문에 브로커인 Redis를 설정해줍니다. RBACwebserver를 띄우고 잘 되는지 확인해봅시다. 잘 나오면 다행입니다. 그런데 1.10.10 이상 버전으로 오면서 UI에 변화된 부분이 있습니다. 아주 좋은 기능이라고 생각하는 것 중 하나인데, 그건 시간대를 드디어 설정해서 볼 수 있다는 것입니다. 기존에는 UTC가 고정이어서 +9해서 계산하는게 영 별로였거든요. 사진에서 보다시피 KST로 설정할 수 있습니다. 그리고 유저도 설정해서 볼 수 있죠. 이걸 적용하기 위해서는 config에서 하나를 더 수정해주어야 합니다. 다시 airflow.cfg로 들어가서 12# Use FAB-based webserver with RBAC featurerbac=True 이렇게 설정해줍니다. RBAC는 Role-Based Access Control의 약자로써, 아까 보셨듯이 유저별로 접근을 통제하는 시스템입니다. 그렇다면 유저도 만들어주어야겠죠. Airflow Main의 쉘로 들어가서 1airflow create_user -r Admin -u admin -e email주소 -f admin -l user -p 비밀번호 이렇게 입력을 해줍시다. 각 플래그의 의미는 다음과 같습니다. -r : Role, 역할. Admin, Op, User, Viewer, Public 이 정해져 있고, 커스텀 롤 생성 가능. (자세한 내용은 여기를 참고하세요) -u : User명 -e : Email 주소 -f : First Name -l : Last Name -p : Password, 비밀번호 이제 webserver를 열어주고 들어가면 아름다운 원 운동을 바라보면서 멘탈이 흔들릴 수 있습니다. 하지만 이렇게 나오게 된 것은 한 가지를 빼먹었기 때문입니다. 앞서서 우리가 중요한 config를 바꿨을 때는 DB에 알려준 것을 기억하실 것입니다. RBAC는 보안에 관련된 것이니까 중요하다고 볼 수 있겠습니다. 그렇다면 DB에도 뭐가 바뀌었는지 알려주어야 합니다. 1airflow upgradedb 이 명령어는 DB를 초기화 시키지 않고 설정값을 DB에 업데이트 시켜주는 역할을 합니다. 이 명령어를 입력해주면? 짠! 1.10.14의 UI가 등장했습니다. 오른쪽 상단의 시간 설정이 가능하다면 성공하신 것입니다. Fernet KeyRBAC를 통해 보안이 강화되었습니다. 이를 통해 활성화 된 것이 하나 더 있다면, fernet_key 를 통한 encryption입니다. Variable이나 Connection을 이용하는 분이라면, DAG를 실행시켰을 때 JSONDecodeError: Expecting value: line 1 column 1 이런 에러를 마주할 가능성이 높습니다. 실제 에러가 난 부분을 보면 Variable의 Value를 가져오는 부분에서 문제가 발생하고 있습니다. 실제로 어떤 Value를 가져오는지 확인해보면, “g8dgasv90s8fd09x9adxfcx” 같이 알 수 없는 암호문으로 되어 있는 것을 목격할 수 있습니다. ‘아 그렇다면 encrypt 옵션을 False로 바꿔야지!’라는 생각으로 mysql로 들어가서 variables를 찾은 뒤에 is_encrypt 를 다 0으로 변경해봤습니다. 하지만 이렇게 해도 이미 Variable에 등록할 때에 암호화된 코드로 DB에 들어가기 때문에 key값으로 value를 조회해도 나오는 값은 위에 있는 알 수 없는 암호문입니다. 결국에는 이것을 해독해주어야 합니다. 이 암호를 해독해주는 열쇠가 바로 fernet_key 입니다. fernet_key는 한번 생성해주어야 합니다. 123from cryptography.fernet import Fernetfernet_key= Fernet.generate_key()print(fernet_key.decode()) 여기서 나온 키 값을 복사해서 airflow.cfg의 fernet_key 부분에 넣어줍니다. 그리고 이 키 값은 저장해놨다가 airflow worker에 있는 airflow.cfg에도 동일하게 적용을 해줍니다. Airflow Worker 기본 이미지 세팅Airflow Worker를 세팅해보겠습니다. 그 전에 기본 이미지를 설정해 줄 것입니다. 물론 Worker를 쫙 설치하고 세팅해준다음에 AMI를 만들어서 오토스케일링을 진행해도 되지만, 만약 설정값을 바꿀일이 생긴다면? 그때마다 AMI를 새로 만들어야 할 것입니다. 당연히 AWS관리자나 TA분과의 관계가 좋지 않아질 것입니다. 제가 선택한 방법은 Docker입니다. Docker를 이용해서 위에서 진행한 세팅을 한번에 잡아줄 것이고, 이 이미지를 시작 템플릿으로 설정할 것입니다. 설정 값을 바꿀 일이 있다면 이미지가 저장된 곳에 들어가서 변경한 후 다시 Push해주면 됩니다. 기본 이미지는 Docker와 기타 사용할 명령어에 대한 라이브러리가 설치된 정도면 됩니다. 저는 Docker정도만 설치했고 이것을 AMI로 만들어 줬습니다. 이 AMI를 시작템플릿에 넣을 것이고 시작템플릿에 있는 고급 설정을 통해 서버가 시작되면서 Docker Image를 Run 해 줄 수 있는 명령어를 넣어줄 것입니다. Docker Airflow Worker 이미지 구성본격적으로 Worker 이미지를 만들어보겠습니다. 구성요소는 다음과 같습니다. Dockerfile files airflow.cfg config log_config.py sources airflow.sh cron hostname_resolver.py requirements.txt 디렉토리명은 임의로 정해놓은 것이기 때문에 다르게 설정하셔도 됩니다. files부터 설명드리면, airflow.cfg는 위에서 보셨던 그 설정파일입니다. airflow main에서 설정했던 파일을 복사해서 넣어주시면 좋습니다. config 폴더에는 airflow의 log설정과 관련된 파일이 있습니다. sources에는 airflow.sh로 쉘 스크립트를 통해 실행하는 명령이 담겨 있고, cron작업을 위한 cron, 도커 내부에서 동작하기 때문에 도커의 호스트를 알려주는 hostname_resolver.py, 그리고 필요한 python 라이브러리 설치를 위한 requirements.txt가 있습니다. Dockerfile12345678910111213141516171819202122232425262728293031323334FROM ubuntu:18.04RUN apt-get update &amp;&amp; apt-get install -y \\ git \\ cron \\ vim \\ python3-pip \\ python3-dev \\ build-essential \\ libmysqlclient-dev \\ libssl-dev \\ libkrb5-dev \\ libsasl2-dev \\&amp;&amp; rm -rf /var/lib/apt/lists/*ADD ./source/* /app/ADD ./files/airflow.cfg /app/ADD ./files/config/ /data/airflow/config/ADD ./source/hostname_resolver.py /usr/local/lib/python3.6/dist-packages/airflow/RUN chmod 0744 /app/airflow.shRUN export AIRFLOW__CORE__HOSTNAME_CALLABLE=airflow.hostname_resolver:resolveRUN export AIRFLOW__CORE__FERNET_KEY=***ADD ./source/cron /etc/cron.d/RUN chmod 0744 /etc/cron.d/cronRUN crontab /etc/cron.d/cronRUN touch /var/log/cron.logWORKDIR /appRUN mkdir -p /data/airflow/DAGS #제가 만든 DAG 폴더RUN git clone DAGS #제 github에 있는 DAGsRUN pip3 install -r requirements.txtRUN python3 -m pip install sqlalchemy==1.3.15 #에러 방지#CMD [\"tail\",\"-f\",\"/var/log/cron.log\"]CMD ./airflow.sh $NAME 기본 세팅으로 들어가는 라이브러리가 좀 있습니다. git이나 cron, vim등은 자주 사용하기 때문입니다. 그 외에 ADD 부분을 보시면 /source라든가 /files가 있는데 이것은 제가 빌드하기 전에 만들어 놓은 디렉토리 입니다. 이 디렉토리 안에 필요한 파일들을 넣어놨고, 빌드 후에 원하는 위치로 옮기고 실행해줄 것입니다. export로 환경 설정 해주는 부분이 있는데 여기에 원래는 AIRFLOW_HOME 설정도 해줬습니다. 하지만 이것은 docker run할 때 넣어주기 때문에 빼놨습니다. 그리고 fernet key부분은 아래에서 설명하겠습니다. airflow.sh1234567#!/bin/shexport C_FORCE_ROOT=\"true\"crontab /etc/cron.d/cron/usr/local/bin/airflow initdbmv /app/airflow.cfg /data/airflow//usr/local/bin/airflow worker -q $1 &amp;cron &amp;&amp; tail -f /var/log/cron.log 쉘 스크립트에는 Celery 작업을 위한 C_FORCE_ROOT가 있습니다. 이것은 root 권한으로 셀러리를 돌리겠다는 표시이구요. 그 외에는 cron 등록, db initialize 등이 있는데 worker -q에 $1이 있는 이유는 docker를 run할 때 어떤 큐 값으로 실행시킬 건지 정하기 위해서 입니다. 워커를 여러 대 둘 수 있는데 큐를 따로 관리할 수 있으면 좋으니까요. cron12* * * * * cd /data/airflow/DAGs &amp;&amp; git fetch --all &amp;&amp; git reset --hard origin/master* * * * * find /data/airflow/logs/* -ctime +7 -exec rm -f {} \\; cron 작업에는 사용할 DAG들이 있는 git repo를 받는 부분과, log관리하는 코드가 같이 들어있습니다. hostname_resolver.py1234import osimport requestsdef resolve(): return requests.get('http://169.254.169.254/latest/meta-data/local-ipv4').text hostname_resolver.py에는 aws의 메타 데이터를 이용해서 docker의 host명을 가져오는 코드가 들어있습니다. 혹시 웹 서버에서 DAG 작업로그를 보려고 하는데 나오지 않는다면, 이 부분을 의심해 보세요. requirements.txt1234567boto3==1.12.0pymongo==3.10.1celery==4.4.0mysqlclient==1.4.6pymysql==0.9.3redis==3.4.1apache-airflow==1.10.14 requirements에는 필요한 라이브러리들이 담겨있습니다. 이렇게 워커 이미지를 구성했으면, 빌드를 시작해봅시다. Docker Build and Run만든 이미지를 AWS ECR에 올려볼 것입니다. 이를 위해 다음과 같은 순서의 작업을 수행하여야 합니다. 인증 토큰을 검색하고 레지스트리에 대해 Docker 클라이언트를 인증합니다. AWS CLI 사용: aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin ******* 참고: AWS CLI를 사용하는 중 오류가 발생하면 최신 버전의 AWS CLI 및 Docker가 설치되어 있는지 확인하십시오. 다음 명령을 사용하여 도커 이미지를 빌드합니다. 도커 파일을 처음부터 새로 빌드하는 방법에 대한 자세한 내용은 여기 지침을 참조하십시오. 이미지를 이미 빌드한 경우에는 이 단계를 건너뛸 수 있습니다. docker build -t 이미지명 . 빌드가 완료되면 이미지에 태그를 지정하여 이 리포지토리에 푸시할 수 있습니다. docker tag 이미지명:tag 리포지토리주소/이미지명:tag 다음 명령을 실행하여 이 이미지를 새로 생성한 AWS 리포지토리로 푸시합니다.docker push 리포지토리주소/이미지명:tag 위에 있는 것들은 ECR의 푸시 명령에 있는 것들이기 때문에 해당 페이지에 있는 코드를 복사해서 넣으시면 됩니다. 빌드가 되었다면, 이미지 구성은 끝입니다. 이제 필요한 것은 시작 템플릿을 방금까지 만든 이미지를 이용해서 구성하고, 이것을 오토스케일링 그룹에 넣어서 워커를 쫙 만들어 주면 됩니다. Start Template시작 템플릿 구성 시작 템플릿 이름은 잘 넣어주시면 되고, AMI에는 아까 구성한 Docker AMI를 넣어주도록 합니다. 이 도커 AMI를 기본으로 설치하고 그 위에 도커 이미지를 Run 해 줄 것입니다. 이제 맨 밑에 고급 세부 정보로 들어와서 사용자 데이터를 넣어줍니다. 사용자 데이터는 이 시작 템플릿이 시작될 때 실행할 수 있는 명령어입니다. 여기에 docker run 명령어를 넣을 것입니다. 12#!/bin/bashsu - ubuntu -c \"aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin *******;docker run -p 8793:8793 -e AIRFLOW_HOME=/data/airflow -e NAME=main -d ***********/작성한이미지명:태그\" shebang을 꼭 넣어주셔야 합니다. shebang을 넣지 않으면 이 명령어를 인식할 수 없어서 에러가 발생합니다. 한 번 더 강조합니다. shebang 꼭 넣어야 한다고! 빼먹어서 저 처럼 삽질 많이 하지 마십시오. 그 뒷 부분에는 su ubuntu가 있습니다. 이후에 실행할 명령어는 ubuntu유저로 실행해야하기 때문입니다. 그래서 switch user를 해주시고 다음의 명령어를 입력해줍니다. 하나씩 자세히 볼까요? 1aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin ******* 이것은 아까 사용했던 명령어로, 인증 토큰을 검색하고 레지스트리에 대해 Docker 클라이언트를 인증하는 것입니다. 1docker run -p 8793:8793 -e AIRFLOW_HOME=/data/airflow -e NAME=main -d ***********/작성한이미지명:태그 도커 이미지를 run하는 명령어입니다. 8793포트를 열어줘야 워커에 대해서 통신이 가능합니다. -p를 이용해 포트를 적어주세요. -e 명령어는 도커에서 환경설정 관련된 부분입니다. airflow home을 airflow main과 같은 위치로 잡아주시면 됩니다. NAME은 worker -q 다음에 들어갈 큐의 topic 명입니다. 제가 작성한 DAG들은 main 토픽을 사용하므로 main으로 했습니다. 다른 topic을 사용하시는 분은 다른 걸로 넣어주세요. -d는 백그라운드로 실행시킨다는 의미이며, 그 다음에는 ECR 이미지의 주소를 넣어줍니다. 시작템플릿 작성이 끝났습니다. Assemble!이제 구성이 모두 되었습니다. airflow main에 들어가서 airflow webserver와 airflow scheduler를 올려줍니다. 12nohup airflow webserver &amp;nohup airflow scheduler &amp; 오토스케일링 그룹에도 워커를 늘려주고 도커 이미지가 올라올 때까지 조금 기다려줍니다. 다 올라오고 나면 dag를 잘 돌리는지 test 해 봅니다. 12airflow list_tasks [TASK명]airflow [TASK명] [DAG id] [Task id] [date] 잘 돌았다면 성공입니다! Trouble Shooting1. DAG가 무한 생성? 갓 만든 따끈따끈한 DAG를 On했습니다. 그런데 DAG하나가 끝나기도 전에 새로운 DAG들이 계속해서 실행되는 현상을 목격할 수 있습니다. 이런 경우는 catch_up 옵션이 활성화 되어있기 때문입니다. catupup은 start_date부터 현재 시간까지 실행하지 못한 DAG들을 실행하겠다는 의미입니다. 임시로 실행하는 DAG야 신경 안써도 큰 문제는 되지 않겠지만, API가 엮어있거나 대용량 데이터를 끌어서 사용하는 DAG라면 문제가 커질 수 있습니다. 이는 airflow.cfg에서 해결할 수 있습니다. 1catchup_by_default = False airflow.cfg에서 catchup_by_default 옵션을 찾아서 False로 변경해주시면 됩니다. 기본값은 True입니다. 혹시 이렇게 했는데도 문제가 계속 발생한다면, 웹 서버와 스케쥴러를 모두 종료하신 후에 다시 실행시켜 수정한 옵션 값을 적용해주면 됩니다. 2. pip3 permission denied열심히 글을 따라 치는데 뜬금없이 pip3 permission denied 이런 에러가 발생할 수 있습니다. 이런 경우에는 1234python3 -m pip install [library]# 위 명령어가 안된다면python3 -m pip install --user [library] 위와 같은 명령어로 해결할 수 있습니다. 3. Command python setup.py egg_info failed with error code 1역시나 ec2-user에서 열심히 설치를 하는 중에 이 에러를 마주칠 수 있습니다. 이 에러는 pip 업데이트가 되지 않아 발생한 에러로 볼 수 있습니다. 1234sudo -H pip install --upgrade --ignore-installed pip setuptools# 또는python3 -m pip install -U pip 4. ModuleNotFoundError: No module named ‘sqlalchemy.ext.declarative.clsregistry’ 2위의 에러는 sqlalchemy 버전이 맞지 않아서 발생하는 문제입니다. 그렇다면 버전을 낮춰주면 됩니다. 12#sqlalchemy 버전 낮추면서 해결python3 -m pip install sqlalchemy==1.3.15 Reference 1 출처 : https://louisdev.tistory.com/26 https://yahwang.github.io/posts/87 https://potensj.tistory.com/73 https://oboki.net/workspace/data-engineering/airflow/rbac/ https://github.com/puckel/docker-airflow/issues/387 https://airflow.apache.org/docs/apache-airflow/stable/security/secrets/fernet.html#rotating-encryption-keys https://devlog.jwgo.kr/2019/07/05/celery-daemonization/ https://aws.amazon.com/ko/blogs/korea/build-end-to-end-machine-learning-workflows-with-amazon-sagemaker-and-apache-airflow/","link":"/2021/04/04/airflow-clusterization/"},{"title":"AWS_Immersion_DAY, 추천 파이프라인","text":"AWS Immersion Day를 참석했다… 추천 파이프라인 다시 생각해보기 AWS Immersion Day(AWS 광고아님 주의, 회사 지원 세션) 평화로운 오후를 보내던 어느날 Analytics for E-Commerce Immersion Day 행사 안내 메일이 왔습니다. 분석 시스템에 대해 AWS에 문의한 적이 있었는데, 관련된 웨비나 행사 메일을 척척 보내주시니 아주 감사했습니다. 더군다나 Kinesis나 Kafka에 관심이 있었던지라 학습을 어떻게 해볼까 고민했었는데, 이런 행사가 잡히니 고민이 해결될 수 있겠다는 기대감이 있었습니다. 행사는 다음과 같이 진행된다고 했습니다. 저는 다 필요없고 Hands On Lab이 아주 기대가 되었습니다. 행사의 시작!15일 13시 부터 시작이 되었습니다. 물론 점심시간을 최대로 활용하는 바람에 늦어버려서 여유롭게 커피한잔과 함께 13시 20분에 접속을 했습니다. AWS의 분석 파트에 있는 제품들을 소개시켜 주셨는데 흥미로운 내용이었지만 제품의 소개같아서 13시부터 16시까지 진행했던 내용을 요약해서 작성을 할 것 입니다. 오늘의 주제는 Hands On Lab이고 이 실습을 하면서 얻은 교훈(?)을 정리할 것이거든요. AWS Analytics데이터 분석에 대한 전반적인 내용이 주로 이뤘습니다. 데이터 분석의 목적에 대해서 간략하게 설명해주시면서, 분석을 통해 의사결정을 도와주거나, 어떤 서비스로 이루어져야 한다는 내용이었습니다. 그 중에 눈에 들어왔던 것은 데이터 분석의 속도에 대한 것과 추천 시스템이었습니다. 데이터 분석이 빠르게 이루어질 수록 그 가치가 커진다는 것인데, 이는 회사생활을 하면서 어느정도 느끼고 있던 부분이었습니다. 빠른 데이터 분석을 위해서는 당연하게도 파이프라인이 잘 구축되어 있어야 하겠습니다. 내용이 이어지면서 Kinesis제품을 소개해주셨습니다. Kinesis는 세 종류의 제품이 있는데 Kinesis Data Firehose, Kinesis Data Streams, Kinesis Data Analytics 입니다. Firehose는 데이터를 DW나 S3등의 데이터 스토어로 쉽게 넣을 수 있는 시스템이고, Streams는 실시간 데이터 스트리밍 서비스로 이를 통해 이상탐지에 적용하거나 실시간 대시보드로 활용할 수 있습니다. 마지막으로 Data Analytics는 말 그대로 스트리밍 데이터를 변환 및 분석해주는 시스템입니다. 이커머스 영역에서는 속도가 매우 중요하기 때문에 스트리밍 데이터가 필수적이라고 할 수 있습니다. 하지만 스트리밍 데이터 처리를 위한 시스템 구축은 생각보다 힘이 많이 듭니다. 그래서 이 세션에서는 극복해야할 과제로 설명해주셨습니다. Kafka를 예로 들어 설명을 많이 해주셨는데, Kafka는 쉽다고는 하지만 설치가 어렵고 유지보수도 어렵고 신경쓸게 참 많습니다. 그래서! 이 포인트에서 Kinesis를 적극 권장하고 있었습니다. 사용하면 물론 좋겠지만 Kinesis는 비싼 편이라 고려를 좀 해봐야겠습니다. Hands On Lab드디어 기다렸던 Hands On Lab시간입니다. 아까 말씀드렸다시피, 데이터 분석을 이용해서 어떤 서비스로 이루어질수 있고 그 중 대표적인 것은 추천시스템입니다. 핸즈온 시간에는 Analytics에서 설명한 제품들을 갖고 추천시스템 파이프라인을 구성하고 캠페인까지 만들어 보는 시간을 가졌습니다. 가상 시나리오를 주고 실습하는 부분이 아주 맘에 쏙 들었습니다. 여러분들은 반려동물 용품을 판매하는 가상의 E-commerce 회사인 ‘몽스토어’ 회사를 운영하고 있습니다. 지금까지 비즈니스는 꾸준히 성장해왔지만 반년전부터 매출이 크게 성장하지 못하고 멈춰 있는 상태입니다. 이에 따라 데이터 분석에 대한 니즈가 발생하였으며, 데이터 분석을 통해 개인별 추천서비스를 도입하여 매출의 성장을 도모할 때라는 결론에 이르렀습니다. 투잡 뛰는 느낌이랄까? 아주 설레는 마음으로 실습을 진행했습니다. 실습내용의 전체 아키텍쳐는 다음과 같습니다. AWS 시스템 내에서 AWS 제품을 갖고 AWS를 이용한 추천 제품인 AWS Personalize를 사용해서, 추천 캠페인을 진행해보자는 것입니다. 주제가 뭐라구요? AWS냐구요? 맞긴한데 거기에 ‘추천 파이프라인을 만들어봅시다!’ 까지가 주제입니다. 본격 실습실습 내용은 최대한 간결하게 요약해서 정리할 것입니다. 이걸 일일이 쓰는 것보다 들어가서 확인하면 되거든요. 워크샵 링크 정말 궁금하신 분들은 링크로 들어가서 쭉 따라가보시면 됩니다. 물론 실습 엔진은 제공되지 않고 Cloud Formation도 제공되지 않습니다. 실습 순서는 다음과 같았습니다. Cloud Formation으로 환경(스택) 구성하기 S3 생성하기 Glue를 이용한 RDS DataBase 크롤링 크롤링한 데이터 S3로 ETL 전송하기 Web 로그 데이터 Kinesis Firehose활용하여 수집 수집 데이터 S3로 ETL S3 데이터 Glue활용하여 ETL Athena를 사용해서 데이터 살펴보기 Personalize를 사용해 추천 데이터 생성하기 Cloud Formation을 활용해서 위 그림과 같은 환경을 만들어 주게 됩니다. 일일이 세팅해서 환경 구성을 하려면 시간이 너무 들게 되니까 빠르게 환경을 구축해줍니다. Cloud Formation을 활용해 구성한 항목은 위 그림에서 S3기준 왼쪽 부분이라고 할 수 있겠습니다. 망 설정과 webserver, 그리고 RDS입니다. S3를 만들어 줄 것인데 S3는 여기서 Data Lake로 활용됩니다. Data Lake는 대규모 데이터를 기본 형식으로 저장하고 있는 Storage로 그럴싸해보이지만 제가 보기에는 데이터를 다 때려넣는 곳이라는 생각이 듭니다. 다른 의견이 있으신 분은 댓글 남겨주세요. 아무튼 S3를 만들어 주고 Glue를 사용해보겠습니다. GlueGlue는 데이터 처리에 사용됩니다. 데이터는 여기서 두 종류로 나뉘는데, RDS에 갖고 있는 구매이력 데이터와, 수집되고 있는 행동 데이터, 즉, 로그데이터 입니다. 구매이력 데이터 처리 Glue RDS에서 크롤링, 데이터 베이스의 데이터 구조, 스키마를 바로 알 수 없음 스키마, 파티션 구조 추론 뒤 데이터 카탈로그 생성 크롤링 → RDS에 있는 데이터 테이블 확인 가능 2차 크롤링 데이터 카탈로그를 생성한 뒤 데이터를 S3로 보내기(ETL) Transform할때는 스크립트를 입력, pyspark 코드를 활용함 Transform한 뒤 S3에 적재[Load] 행동 데이터, 로그 데이터 처리 Webserver에서 생성되는 데이터를 Kinesis Data Firehose를 사용해 수집 수집한 데이터는 S3에 적재 자 이제 Glue를 통해서 S3에 사용할 데이터를 모두 적재해 놓았습니다. 이제 추천데이터를 생성해보겠습니다. AWS Personalize를 사용해 볼 것인데, Personalize는 사용할 데이터 형식이 따로 존재합니다. 그렇기 때문에 원하는 형식에 맞게 전처리를 해주어야 합니다. 또 다시 Glue를 사용해서 전처리하고 Personalize에 전달해보도록 하겠습니다. 추천 데이터 생성 전처리, Glue Glue를 통해 ETL처리를 해줌, 과정은 위와 유사함 [구매이력] 123456789101112131415161718192021222324252627import sysfrom awsglue.transforms import *from awsglue.utils import getResolvedOptionsfrom pyspark.context import SparkContextfrom awsglue.context import GlueContextfrom awsglue.job import Job## @params: [JOB_NAME]args = getResolvedOptions(sys.argv, ['JOB_NAME'])sc = SparkContext()glueContext = GlueContext(sc)spark = glueContext.spark_sessionjob = Job(glueContext)job.init(args['JOB_NAME'], args)##create dynamic framedigital_df = glueContext.create_dynamic_frame.from_catalog(database='demogo-mongstore-database', table_name='product').toDF()digital_df.createGlobalTempView(\"productview\")##sql queryclient_df = spark.sql(\"SELECT productcode as ITEM_ID, productname as PRODUCTNAME, category1||'|'||category2||'|'||category3 as CATEGORY FROM global_temp.productview\")##write output to S3client_df.repartition(1).write.format('csv').option('header', 'true').save('s3://demogo-mongstore-[사용자이름]/personalize-items')job.commit() spark sql을 사용해서 Transformation 뒤 S3에 적재하는 구조 AS-IS, 이 데이터를 TO-BE, 이렇게 바꿀 것 로그데이터 처리 123456789101112131415161718192021222324252627282930import sysfrom awsglue.transforms import *from awsglue.utils import getResolvedOptionsfrom pyspark.context import SparkContextfrom awsglue.context import GlueContextfrom awsglue.job import Jobimport datetime## @params: [JOB_NAME]args = getResolvedOptions(sys.argv, ['JOB_NAME'])sc = SparkContext()glueContext = GlueContext(sc)spark = glueContext.spark_sessionjob = Job(glueContext)job.init(args['JOB_NAME'], args)##create dynamic framedigital_df = glueContext.create_dynamic_frame.from_catalog(database='demogo-mongstore-database', table_name='purchase').toDF()digital_df.createGlobalTempView(\"purchaseview\")digital_df = glueContext.create_dynamic_frame.from_catalog(database='demogo-mongstore-database', table_name='accesslog2021').toDF()digital_df.createGlobalTempView(\"accesslog2021view\")##sql queryclient_df = spark.sql(\"SELECT userid as USER_ID, REGEXP_REPLACE(pageurl, '[^0-9]+','') as ITEM_ID, to_unix_timestamp(CAST(time AS timestamp)) as TIMESTAMP, 'view' as EVENT_TYPE FROM global_temp.accesslog2021view UNION ALL SELECT userid as USER_ID, productcode as ITEM_ID, to_unix_timestamp(ordertime) as TIMESTAMP, 'order' as EVENT_TYPE FROM global_temp.purchaseview\")##write output to S3client_df.repartition(1).write.format('csv').option('header', 'true').option('header', 'true').save('s3://demogo-mongstore-[사용자이름]/personalize-interactions')job.commit() AS-IS, 이 데이터를 TO-BE, 이렇게 처리 추천 데이터 생성은 데이터 스키마 매핑 후 데이터 임포트를 해주면 됩니다. 이제 솔루션 생성을 해주면 되는데 여기에는 1시간 이상이 소요되네요. 캠페인을 생성해서 데이터를 보면. 유사도 별 랭킹이 매겨진 추천 데이터를 확인할 수 있습니다. 실습 종료, 무엇을 얻었나이 실습을 통해 현재 서비스하고 있는 추천 시스템의 구조에 대해서 다시 생각해보게 되었습니다. 현재 서비스하고 있는 추천 시스템은 자세하게 밝히지는 못하지만, 간략하게 구조를 말씀드리면, 로그 데이터를 NoSQL DB로 받아 넣고 이것을 추천용 DB에 일부 전처리 하여 넣어주는 형태입니다. 이 추천용 DB에 데이터를 끌어와서 한 고객사의 추천 데이터를 AWS EMR을 이용해서 배치 스케쥴마다 생성하고 EMR을 종료하고 있습니다. 실습에서 제시된 구조와는 많은 차이가 존재합니다. 물론 실습은 B2C 서비스로 제시되었고 현재 회사는 B2B 서비스기 때문에 차이는 있습니다만, 차이점을 정리하자면 다음과 같습니다. Data Lake ETL 파이프라인의 부재 Transform한 데이터의 적재 먼저 Data Lake가 없습니다.Data Lake가 꼭 필요하다고는 할 수 없고, 목적에 따라 구성항목에 넣기도 합니다. 물론 회사에서도 전체 데이터가 다 들어있는 DB는 있습니다만, S3제품만큼 고가용성이 보장된다고는 할 수 없을 것 같습니다. S3에도 connection pool size가 있습니다, 그렇지만 현재 사용하고 있는 DB보다야 훨씬 size가 크고 관리하기도 편하다고 생각합니다. 그리고 이렇게 고가용성이 보장되는 Data Lake가 있으면 새로운 서비스를 구상하더라도 connection pool이나 메모리 때문에 장애나는 상황이 거의 없기 때문에 안정적으로 새 서비스를 생각해보고 토이 프로젝트를 해볼 수 있습니다. 현재 운영과 개발DB가 있긴 하지만 개발DB에 문제가 좀… ETL 파이프라인이 제대로 구성되지 않은 것 같습니다.이 실습에서는 E, T, L이 명확하게 나뉘어져 있다는 것이 느껴지는데, 이 실습을 하고 회사 서비스를 돌아보니 어디부터 어디까지가 ETL인지 구분이 잘 되지 않았습니다. Transform을 하긴 하는데 Load를 안하는 것 같고… Transform이 제대로 되고 있는 건지… 뭐 이런 생각을 하게 되었습니다. 하지만 Glue를 사용한다고 생각하고 구성을 생각해봤을 때, 크게 어려울 것 같지 않았습니다. Glue를 간단히 살펴보긴 했지만, ETL프로세스이고 Transform은 거의 Pyspark로 돌아가고 있었습니다. Pyspark로 이미 추천 데이터를 생성하고 있기에 파이프라인을 정리해주면 금방 적용할 수 있지 않을까 생각해봤습니다. 어디서 끌어오고, 적재할지를 잘 정하는 게 중요할 것입니다. 그렇다면 DB를 잘 알아야 하는데, 이렇게 공부 포인트가 늘어났습니다! 하하 Transform한 데이터는 어디?큰 문제점 중에 하나라고 생각하는데, 현재 비용 효율적인 아키텍쳐를 지향하고 있기 때문에 EMR을 상시 구동하고 있지 않습니다. 생성한 추천 데이터는 DB에 넣고 있지만, 전처리한 데이터는 여지없이 삭제되고 맙니다. 실습을 진행하고 생각해보니 이 데이터가 너무 아깝다는 생각을 하게 되었습니다. 만약 전처리한 데이터가 남아있다면 다른 서비스에 적용을 해볼 수 있지 않을까 생각이 들었습니다. 추천에 사용되는 데이터가 어떻게 보면 구매나, 클릭, View에 대한 패턴 분석된 데이터인데, 이것을 고객사 레포팅이나 기타 분석 시스템에 활용할 여지가 많을 것 같았습니다. Kaizen!그러면 어떻게 개선할 수 있을까요? TA님이나 팀장님과 의논을 같이 하면서 구체화해야겠지만 우선 생각나는 개선점은 다음과 같습니다. Extract하는 데이터 포인트를 변경 Data Warehouse 상시 구동 EMR서버 구성 이게 명확한 답이 될지 모르겠습니다. 하지만 우선 이렇게 글로 만들어놓고 다른 사람들의 의견을 받아서 두들겨 맞으며 고쳐나가는 게 맞다고 생각합니다. 일단 저질러야 변화가 생기니까요. 데이터 포인트를 변경한다는 것은 Extract하는 데이터 베이스가 혹사당하고 있기 때문입니다. 너무 자주 데이터를 끌어오고 나가고 있는 상황이기 때문에, 사용하는 데이터를 따로 저장하는 DB를 만들면 어떨까 생각합니다. 그리고 이는 2번과 이어지는데, 이것을 Data Warehouse로 사용하는 것입니다. 이 DW는 Big Query를 검토하고 있습니다. Big Query에 일단 적재를 하고 전처리 스케쥴을 걸어서 추천에 사용할 데이터를 아주 예쁘게 구성해 놓을 예정입니다. 마치 Personalize에 Glue를 사용해서 데이터를 전처리하고 넣는 것 처럼요. 동시에 EMR 서버를 상시 구성해놔서 추천 생성할 때 발생하는 일부 데이터를 DW에 저장해놓으려고 합니다. 물론 중간에 데이터를 확인해보고 이게 사용할만한 가치가 있을지 분석가분들과 고민해봐야겠지만, 구상은 이렇게 해놓고 있습니다. 어떻게 보면 DW에 스케쥴을 걸어서 작업을 돌려놓으면 중간에 EMR로 작업하면서 나오는 데이터는 굳이 필요가 없을 수도 있겠습니다. 헛된 구상일 수 있겠지만, 발전할 수 있는 포인트를 어느정도 찾은 것 같아서 얻은게 있는 세션이었다고 생각합니다. 이제 해야할 것은 TA님과 팀장님과의 미팅, 그리고 부족한 부분을 채워넣는 학습 시간이겠습니다.","link":"/2021/04/18/AWS-Immersion-DAY/"},{"title":"글또 5기를 끝내고, 회고하기","text":"글또 5기를 마치면서, 나는 어떻게 일하고 있는가…라고 하지만 넋두리 및 내 작은 목표에 대해서 글또 5기가 끝났다11월 부터 시작되었던 글또 5기가 끝이 났다. 어떻게 하다보니 해가 바뀌었고 벌써 5월이 되었고, 글또 5기도 끝이 나버렸다. 시간이 정말 빠르게 흘러가고 있다. 이렇게 속절없이 흘러가는 시간 속에서 글또 활동을 어떻게 해왔는지, 그리고 글또 외에 내가 어떻게 살아왔는지 회고해보는 시간을 가져보려고 한다. 무엇을 썼을까이번 기수에서는 Airflow와 관련된 글을 정말 많이 썼다. 글또에서 같은 채널의 온라인 회고시간을 가졌을 때도, Airflow 글 쓰는 사람으로 다 알고 계시는 것 같았다. 다른 글들도 많이 작성하고 싶었는데 업무 상… 다른 걸 공부할 시간이 없었다고 말하고 싶지만, 핑계처럼 보이기도 했다. 아카이브를 보니 10개의 글을 작성했다. (글또5기 다짐하기 글이 짤렸음) 생각보다 많이 썼다. 글 쓴 것만 봐도 Airflow쳐돌이라는 것을 잘 알 수 있다. Airflow 글들과 그 배경지식을 위한 네트워크 글, 추천시스템 스터디를 하면서 쓴 FP-Growth, 추천 파이프라인 구성 글로 채워져 있다. 채널에 있는 분들에게는 매우 생소할 수도 있고, 그래서 오히려 재밌게 느껴질 수 있겠다는 것은 내 생각을 뿐이겠고, 실제로 피드백 들어오는 걸 보니까 무슨 말인지 몰라서 피드백 하기 어려워 하시는게 많이 느껴졌다. 그 분들에게는 핵노잼 글(ㅠㅠㅜ) 이었을 것 같아 가슴이 아프다. 엔지니어링 관련 글이 피드백 당첨되고 피드백을 해야 할 때면 참 막막했으리라… 물론 나름 쉽게 적는다고 쉽게 적고, 여러 짤을 사용해서 접근하기 쉽게 만들긴 했는데, 효과는 미미했던 것 같다. 하지만 알아주시는 분이 있어서 매우 감사하게 생각하고 그로 인해 힘이 많이 되었다. 이전 기수에서는 이전 기수에서도 물론 엔지니어링 관련된 기본적인 내용을 많이 정리하고 작성했다. 하지만 확실히 5기에 비해서 논문 정리 내용이 몇 개 더 있는 게 확인된다(사진 길이 때문에 CRAFT 논문 요약이 짤림…). 4기 때는 딥러닝이나 모델링 관련해서도 서치를 많이 했고 엔지니어링 쪽에도 신경을 쓴 것 같다. 사진 찍으려고 아카이브를 다시 보다가 5기에 들어서면서 엔지니어링에 완전 집중한 게 확실히 느껴졌다. 달라진 업무일이렇게 글이 달라진 이유는 뭐니뭐니해도 일 때문이다. 아무래도 일하면서 얻은 정보나 지식을 정리하게 되기 때문인 것 같다. 21년 2월 부터 직무가 데이터 사이언티스트에서 데이터 엔지니어로 옮겨졌다. 직무상 분류해 놓긴한 건데, 이로 인해서 데이터 엔지니어링 업무에 더 집중하게 된 것 같다. 책임감이 더 생겨서 그런 것일까? 아무튼 데이터 엔지니어로 변경된 후 부터 기존에 서비스 되고 있던 구조를 더 효율적인 구조로 변경하고, 확장하는 등의 프로젝트를 많이 진행하게 되었다. 이를 통해서 네트워크의 중요성을 느끼게 되었고 구조를 업그레이드를 하려면, 그 툴을 제대로 알아야 하기 때문에 사용하는 툴에 대해서 더 파고들어서 공부하게 되었다. 이렇게 탄생한게 바로 Airflow 시리즈 글…! 🌟Airflow 시리즈 글🌟 일하는 방식데이터 엔지니어링에 집중하게 된 것은 조직의 변화가 있었기 때문이다. 5기때에는 성장통이라고 해야 할까, 빠른 속도로 변화하는 조직과 그 방향을 정하는 데에서 여러 시행착오를 겪고 있었다. 2020년 회고하기 글에도 나와있긴 하지만 갑자기 팀원이 몇 명 나가게 되면서 분위기가 굉장히 어수선 했었다. 이럴때일수록 프로세스를 만들고 지키는 게 중요하다고 생각해서 팀원들과 앞으로의 방향, 그리고 일하는 방식, 어떻게 체크하고 공유할 것인지 이야기를 많이 나눴다. 그래서 탄생한 것이 데이터 사이언스팀의 칸반 보드였고, 자체 오전 스크럼 시간이 부활하게 되었다(칸반 만드는 것도 참 우여곡절이 많았지만 생략…). 금요일마다 칸반 보드에 작성한 업무들의 진행상황을 확인했고 늦춰지고 있으면 왜 늦춰지고 있는지 이야기를 나눴고, 완료된 일은 코드리뷰를 통해서 확인한 후 Task Complete에 넣어놨다. 그리고 이런 프로세스를 통해서 각자의 업무가 더 명확해졌다. 내 직무를 변경했기 때문일지도 모르겠지만, 각자의 일이 구체화 되었고 이를 통해서 어떤 업무의 담당자를 정할 수 있었다. 이런 프로세스가 만들어진 데에는 좋은 동료들과 끊임없이 소통을 했기 때문이기도 했지만, 개인적으로는 팀장님의 도움이 컸다. 기획 팀장님께서 데이터 사이언스 팀을 겸직해서 맡게 되셨는데, 기획팀 팀장으로서 기획팀과 개발팀의 프로세스를 잘 구축했었던 경험이 있었던 분이었다. 팀장님께서 주도적으로 프로세스를 만들어주신 것은 아니지만, 기획-개발의 프로세스를 만드는 모습과 설명을 들으면서 많은 걸 배울 수 있었고, 이를 잘 적용할 수 있었다. ‘어른이란 이런 거구나’, ‘직장 선배란 이런 것이구나’를 느끼게 해주신 분이기도 하다. Data Enginnering회고를 하다보니 앞으로의 쓸 글들도 데이터 엔지니어링 글이 될 것 같아서 이에 대한 생각을 잠시 작성해보려고 한다. 각종 커뮤니티를 보면, 직무 명 데이터 엔지니어는 하는 일이 어느정도 정해진 것 같은데 내 생각은 조금 다르다. 아직도 많은 회사에서 데이터 엔지니어를 구하고, 보유하고 있지만 각 데이터 엔지니어가 하는 일이 회사마다 다 다른것 같다는 생각이다. 회사가 작은지 큰지, 직종이 IT인지 제조업인지, B2C인지 B2B인지에 따라 하는 일이 참 많이 달라지는 것 같다는 걸 느끼고 있다. 지금 있는 이 회사에는 데이터 엔지니어가 없었다. 물론 데이터 사이언티스트도 없었다. 내가 들어오면서 데이터 팀이 구성되기 시작했고, 데이터의 중요성에 대해서 자각하기 시작했다(수 없이 여기에 대해서 자료를 공유하고 설득했다). 여담인데 생각해보니 이 회사의 1호 데이터 사이언티스트이지 데이터 엔지니어가 되었다. 허허 참… 다시 본론으로 돌아와서, 회사에서 데이터에 대한 중요성을 느끼기 시작해서 데이터 인프라를 제대로 구성하기 시작했다. 그래서 내가 요즘 하는 일은 데이터 인프라를 설계하고 구성하는 일과 여러 API를 만들고 FastAPI라는 새로운 프레임 워크로 갈아끼는 일, 그리고 의사 결정권자 및 실무자들을 설득하는 일이다. 아무 걱정없이 코드만 만지고 인프라를 구성하고 싶은데 그게 참 어렵다. 물론 예전에는 아무 걱정없이 모델링하고 데이터를 분석하고 싶었었다. 데이터 사이언티스트였으니깐! 하지만 계속 안된다고 하는 벽에 막혔었다. 필요한 데이터가 없거나 할 수 없는 구조이거나, 기타 등등의 이유로. 그래서 답답해서 직접하게 되다 보니 이렇게 되었다. 벽이 하나씩 뚫리고 있는 느낌이라 재밌긴 하지만 동시에 새로운 벽이 나타나는 것 같다는 힘든 점이 있다. 그래서 요즘 하는 일을 좀 더 자세히 적어보자면, 내 궁극적 목표는 데이터가 흐르는 조직 만들기 이다. 첫 단추로 회사에 GCP의 BigQuery를 도입하고자 한다. 여러 데이터 서비스를 하기 위해서는, 그리고 고객들에게 가치가 높은 서비스를 제공하려면 속도가 생명이라고 생각한다. 그런데 데이터 전처리하는데, 그리고 데이터를 끌어오는데 시간이 너무 오래 걸리는 문제가 지속적으로 발생하고 있다. 그래서 큰 데이터를 빠르게 쿼리할 수 있고 OLAP성으로 활용할 수 있는 BigQuery를 사용해보려고 한다. 이를 위해서 기존 DB에서 BigQuery에 데이터를 적재할 때 Embulk라는 새로운 오픈소스를 사용해야할 것 같다. 또한 BigQuery에 넣고 꺼내 쓸 때도 불필요한 작업을 하고 싶지 않기 때문에 AWS의 Glue처럼 ETL처리를 해서 전처리 된 데이터를 바로바로 사용할 수 있게 만들 것이다. 이렇게 BigQuery에 데이터를 담아놓게 되면, 여러 다른 제품들과도 결합해 사용할 수 있기 때문에 확장성 또한 확보할 수 있을 것이라고 생각한다. BigQuery에서 사용하는 주제 별로 주기적으로 전처리해 테이블을 만들어 데이터 마트를 구성할 수도 있겠고, 이를 통해 다른 부서에서 자유롭게 데이터 분석이 가능할 것이다. 또한 Data Studio나 Redash를 이용해서 자유롭게 데이터를 시각화 할 수 있게 만들 것이다. 무엇보다도 BigQuery에 데이터가 쌓이기 시작하면, 조직 내 SQL교육을 실시할 것이다. 데이터를 다루는 기본적인 언어인 SQL을 교육함으로써 모두가 데이터에 접근해 원하는 데이터를 확인하고 이 결과를 통해 의사결정을 내릴 수 있게 된다면… 그렇게 된다면 회사가 많이 바뀌지 않을까. 개인적으로 그래서 기대를 많이 하고 있다. 글을 마치며…이렇게 글또 5기 작성한 글들을 보고 회고 및 넋두리 하는 시간이 끝났다. 누구에게 보여지려고 하는 회고가 어디있으랴, 회고는 본질적으로 넋두리가 기본이 아닐까 하는 생각으로 애써 정리를 하려고 했지만 정리가 잘 안된 글을 쓴 나를 위로하면서, 그리고 넋두리를 통해 글또 6기에 작성할 글에 대해서 잠시 고민해보면서 글을 마무리한다.","link":"/2021/05/02/geultto5-end/"}],"tags":[{"name":"Ensemble","slug":"Ensemble","link":"/tags/Ensemble/"},{"name":"RandomForest","slug":"RandomForest","link":"/tags/RandomForest/"},{"name":"A/B test","slug":"A-B-test","link":"/tags/A-B-test/"},{"name":"MAB","slug":"MAB","link":"/tags/MAB/"},{"name":"글또","slug":"글또","link":"/tags/%EA%B8%80%EB%98%90/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"elice","slug":"elice","link":"/tags/elice/"},{"name":"Boosting","slug":"Boosting","link":"/tags/Boosting/"},{"name":"Gradient Boosting","slug":"Gradient-Boosting","link":"/tags/Gradient-Boosting/"},{"name":"Ada Boosting","slug":"Ada-Boosting","link":"/tags/Ada-Boosting/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"Information","slug":"Information","link":"/tags/Information/"},{"name":"Entropy","slug":"Entropy","link":"/tags/Entropy/"},{"name":"debug","slug":"debug","link":"/tags/debug/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"sql","slug":"sql","link":"/tags/sql/"},{"name":"programmers","slug":"programmers","link":"/tags/programmers/"},{"name":"GDG","slug":"GDG","link":"/tags/GDG/"},{"name":"Information Value","slug":"Information-Value","link":"/tags/Information-Value/"},{"name":"feature selection","slug":"feature-selection","link":"/tags/feature-selection/"},{"name":"PCA","slug":"PCA","link":"/tags/PCA/"},{"name":"Dimensional Reduction","slug":"Dimensional-Reduction","link":"/tags/Dimensional-Reduction/"},{"name":"Timeseries","slug":"Timeseries","link":"/tags/Timeseries/"},{"name":"Metrics","slug":"Metrics","link":"/tags/Metrics/"},{"name":"Accuracy","slug":"Accuracy","link":"/tags/Accuracy/"},{"name":"airflow","slug":"airflow","link":"/tags/airflow/"},{"name":"class","slug":"class","link":"/tags/class/"},{"name":"Cross Entropy","slug":"Cross-Entropy","link":"/tags/Cross-Entropy/"},{"name":"KL Divergence","slug":"KL-Divergence","link":"/tags/KL-Divergence/"},{"name":"float","slug":"float","link":"/tags/float/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"trouble shooting","slug":"trouble-shooting","link":"/tags/trouble-shooting/"},{"name":"oop","slug":"oop","link":"/tags/oop/"},{"name":"Supervised Learning","slug":"Supervised-Learning","link":"/tags/Supervised-Learning/"},{"name":"Linear Regression","slug":"Linear-Regression","link":"/tags/Linear-Regression/"},{"name":"Naive Bayes","slug":"Naive-Bayes","link":"/tags/Naive-Bayes/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"server","slug":"server","link":"/tags/server/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"vision","slug":"vision","link":"/tags/vision/"},{"name":"paper","slug":"paper","link":"/tags/paper/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"mongoDB","slug":"mongoDB","link":"/tags/mongoDB/"},{"name":"Task","slug":"Task","link":"/tags/Task/"},{"name":"GCP","slug":"GCP","link":"/tags/GCP/"},{"name":"infra","slug":"infra","link":"/tags/infra/"},{"name":"Virtualization","slug":"Virtualization","link":"/tags/Virtualization/"},{"name":"backend","slug":"backend","link":"/tags/backend/"},{"name":"engineering","slug":"engineering","link":"/tags/engineering/"},{"name":"다짐","slug":"다짐","link":"/tags/%EB%8B%A4%EC%A7%90/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"기록","slug":"기록","link":"/tags/%EA%B8%B0%EB%A1%9D/"},{"name":"EMR","slug":"EMR","link":"/tags/EMR/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"recommender","slug":"recommender","link":"/tags/recommender/"}],"categories":[{"name":"ML","slug":"ML","link":"/categories/ML/"},{"name":"Algorithm","slug":"Algorithm","link":"/categories/Algorithm/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Statistics","slug":"ML/Statistics","link":"/categories/ML/Statistics/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"Information Theory","slug":"ML/Information-Theory","link":"/categories/ML/Information-Theory/"},{"name":"Automation","slug":"Algorithm/Automation","link":"/categories/Algorithm/Automation/"},{"name":"sql","slug":"sql","link":"/categories/sql/"},{"name":"coding test","slug":"coding-test","link":"/categories/coding-test/"},{"name":"coding test","slug":"python/coding-test","link":"/categories/python/coding-test/"},{"name":"Statistics","slug":"Statistics","link":"/categories/Statistics/"},{"name":"engineering","slug":"engineering","link":"/categories/engineering/"},{"name":"cs","slug":"cs","link":"/categories/cs/"},{"name":"Statistics","slug":"ML/Information-Theory/Statistics","link":"/categories/ML/Information-Theory/Statistics/"},{"name":"debug","slug":"sql/debug","link":"/categories/sql/debug/"},{"name":"Algorithm","slug":"coding-test/Algorithm","link":"/categories/coding-test/Algorithm/"},{"name":"trouble shooting","slug":"trouble-shooting","link":"/categories/trouble-shooting/"},{"name":"Algorithm","slug":"python/coding-test/Algorithm","link":"/categories/python/coding-test/Algorithm/"},{"name":"cs","slug":"python/cs","link":"/categories/python/cs/"},{"name":"book","slug":"book","link":"/categories/book/"},{"name":"ubuntu","slug":"ubuntu","link":"/categories/ubuntu/"},{"name":"event","slug":"blog/event","link":"/categories/blog/event/"},{"name":"spark","slug":"spark","link":"/categories/spark/"},{"name":"code","slug":"sql/code","link":"/categories/sql/code/"},{"name":"python","slug":"cs/python","link":"/categories/cs/python/"},{"name":"삽질","slug":"sql/debug/삽질","link":"/categories/sql/debug/%EC%82%BD%EC%A7%88/"},{"name":"blog","slug":"coding-test/Algorithm/blog","link":"/categories/coding-test/Algorithm/blog/"},{"name":"study","slug":"trouble-shooting/study","link":"/categories/trouble-shooting/study/"},{"name":"ML","slug":"book/ML","link":"/categories/book/ML/"},{"name":"troubleshooting","slug":"ubuntu/troubleshooting","link":"/categories/ubuntu/troubleshooting/"},{"name":"engineering","slug":"spark/engineering","link":"/categories/spark/engineering/"},{"name":"papers","slug":"papers","link":"/categories/papers/"},{"name":"GCP","slug":"engineering/GCP","link":"/categories/engineering/GCP/"},{"name":"infra","slug":"infra","link":"/categories/infra/"},{"name":"backend","slug":"infra/backend","link":"/categories/infra/backend/"},{"name":"engineering","slug":"infra/backend/engineering","link":"/categories/infra/backend/engineering/"},{"name":"글또","slug":"글또","link":"/categories/%EA%B8%80%EB%98%90/"},{"name":"Engieering","slug":"Engieering","link":"/categories/Engieering/"},{"name":"recommender","slug":"recommender","link":"/categories/recommender/"},{"name":"event","slug":"event","link":"/categories/event/"}]}